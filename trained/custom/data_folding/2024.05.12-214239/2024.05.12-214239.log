2024-05-12 21:42:39,990 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.12-214239/2024.05.12-214239.log
2024-05-12 21:42:57,135 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2024-05-12 21:42:57,138 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2024-05-12 21:42:57,269 - Dataset sizes:
	training=6941
	validation=1736
	test=1736
2024-05-12 21:42:57,272 - Reading compression schedule from: policies/schedule-cifar100.yaml
2024-05-12 21:42:57,287 - 

2024-05-12 21:42:57,289 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:43:10,452 - Epoch: [0][  100/  217]    Overall Loss 4.115597    Objective Loss 4.115597                                        LR 0.001000    Time 0.131383    
2024-05-12 21:43:19,095 - Epoch: [0][  200/  217]    Overall Loss 3.827268    Objective Loss 3.827268                                        LR 0.001000    Time 0.108756    
2024-05-12 21:43:20,355 - Epoch: [0][  217/  217]    Overall Loss 3.799732    Objective Loss 3.799732    Top1 24.590164    Top5 37.704918    LR 0.001000    Time 0.106022    
2024-05-12 21:43:20,866 - --- validate (epoch=0)-----------
2024-05-12 21:43:20,868 - 1736 samples (32 per mini-batch)
2024-05-12 21:43:27,659 - Epoch: [0][   55/   55]    Loss 3.329706    Top1 30.587558    Top5 39.976959    
2024-05-12 21:43:28,255 - ==> Top1: 30.588    Top5: 39.977    Loss: 3.330

2024-05-12 21:43:28,279 - ==> Best [Top1: 30.588   Top5: 39.977   Sparsity:0.00   Params: 382208 on epoch: 0]
2024-05-12 21:43:28,282 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 21:43:28,426 - 

2024-05-12 21:43:28,427 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:43:39,655 - Epoch: [1][  100/  217]    Overall Loss 3.298125    Objective Loss 3.298125                                        LR 0.001000    Time 0.112004    
2024-05-12 21:43:48,768 - Epoch: [1][  200/  217]    Overall Loss 3.265697    Objective Loss 3.265697                                        LR 0.001000    Time 0.101429    
2024-05-12 21:43:50,069 - Epoch: [1][  217/  217]    Overall Loss 3.258350    Objective Loss 3.258350    Top1 34.426230    Top5 52.459016    LR 0.001000    Time 0.099454    
2024-05-12 21:43:50,574 - 

2024-05-12 21:43:50,576 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:44:04,791 - Epoch: [2][  100/  217]    Overall Loss 3.003555    Objective Loss 3.003555                                        LR 0.001000    Time 0.141823    
2024-05-12 21:44:12,212 - Epoch: [2][  200/  217]    Overall Loss 2.970352    Objective Loss 2.970352                                        LR 0.001000    Time 0.107896    
2024-05-12 21:44:13,143 - Epoch: [2][  217/  217]    Overall Loss 2.964079    Objective Loss 2.964079    Top1 31.147541    Top5 54.098361    LR 0.001000    Time 0.103713    
2024-05-12 21:44:13,568 - 

2024-05-12 21:44:13,569 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:44:26,683 - Epoch: [3][  100/  217]    Overall Loss 2.701318    Objective Loss 2.701318                                        LR 0.001000    Time 0.130826    
2024-05-12 21:44:36,515 - Epoch: [3][  200/  217]    Overall Loss 2.694894    Objective Loss 2.694894                                        LR 0.001000    Time 0.114410    
2024-05-12 21:44:38,113 - Epoch: [3][  217/  217]    Overall Loss 2.692567    Objective Loss 2.692567    Top1 39.344262    Top5 57.377049    LR 0.001000    Time 0.112783    
2024-05-12 21:44:38,685 - 

2024-05-12 21:44:38,687 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:44:49,457 - Epoch: [4][  100/  217]    Overall Loss 2.494378    Objective Loss 2.494378                                        LR 0.001000    Time 0.107447    
2024-05-12 21:44:57,531 - Epoch: [4][  200/  217]    Overall Loss 2.436878    Objective Loss 2.436878                                        LR 0.001000    Time 0.093951    
2024-05-12 21:44:58,950 - Epoch: [4][  217/  217]    Overall Loss 2.418211    Objective Loss 2.418211    Top1 54.098361    Top5 70.491803    LR 0.001000    Time 0.093101    
2024-05-12 21:44:59,608 - 

2024-05-12 21:44:59,610 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:45:14,716 - Epoch: [5][  100/  217]    Overall Loss 2.181956    Objective Loss 2.181956                                        LR 0.001000    Time 0.150712    
2024-05-12 21:45:22,447 - Epoch: [5][  200/  217]    Overall Loss 2.171846    Objective Loss 2.171846                                        LR 0.001000    Time 0.113886    
2024-05-12 21:45:23,775 - Epoch: [5][  217/  217]    Overall Loss 2.174338    Objective Loss 2.174338    Top1 52.459016    Top5 67.213115    LR 0.001000    Time 0.111058    
2024-05-12 21:45:24,345 - 

2024-05-12 21:45:24,348 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:45:38,290 - Epoch: [6][  100/  217]    Overall Loss 1.898468    Objective Loss 1.898468                                        LR 0.001000    Time 0.139107    
2024-05-12 21:45:46,069 - Epoch: [6][  200/  217]    Overall Loss 1.943779    Objective Loss 1.943779                                        LR 0.001000    Time 0.108281    
2024-05-12 21:45:46,937 - Epoch: [6][  217/  217]    Overall Loss 1.934625    Objective Loss 1.934625    Top1 52.459016    Top5 75.409836    LR 0.001000    Time 0.103780    
2024-05-12 21:45:47,310 - 

2024-05-12 21:45:47,311 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:45:56,983 - Epoch: [7][  100/  217]    Overall Loss 1.703961    Objective Loss 1.703961                                        LR 0.001000    Time 0.096458    
2024-05-12 21:46:06,303 - Epoch: [7][  200/  217]    Overall Loss 1.698141    Objective Loss 1.698141                                        LR 0.001000    Time 0.094690    
2024-05-12 21:46:07,616 - Epoch: [7][  217/  217]    Overall Loss 1.706661    Objective Loss 1.706661    Top1 54.098361    Top5 73.770492    LR 0.001000    Time 0.093294    
2024-05-12 21:46:08,193 - 

2024-05-12 21:46:08,195 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:46:21,126 - Epoch: [8][  100/  217]    Overall Loss 1.507526    Objective Loss 1.507526                                        LR 0.001000    Time 0.129011    
2024-05-12 21:46:27,350 - Epoch: [8][  200/  217]    Overall Loss 1.497934    Objective Loss 1.497934                                        LR 0.001000    Time 0.095538    
2024-05-12 21:46:28,946 - Epoch: [8][  217/  217]    Overall Loss 1.502684    Objective Loss 1.502684    Top1 60.655738    Top5 80.327869    LR 0.001000    Time 0.095377    
2024-05-12 21:46:29,570 - 

2024-05-12 21:46:29,572 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:46:43,229 - Epoch: [9][  100/  217]    Overall Loss 1.269843    Objective Loss 1.269843                                        LR 0.001000    Time 0.136266    
2024-05-12 21:46:52,361 - Epoch: [9][  200/  217]    Overall Loss 1.310242    Objective Loss 1.310242                                        LR 0.001000    Time 0.113661    
2024-05-12 21:46:53,425 - Epoch: [9][  217/  217]    Overall Loss 1.312066    Objective Loss 1.312066    Top1 73.770492    Top5 90.163934    LR 0.001000    Time 0.109640    
2024-05-12 21:46:53,828 - 

2024-05-12 21:46:53,829 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:47:04,550 - Epoch: [10][  100/  217]    Overall Loss 1.074940    Objective Loss 1.074940                                        LR 0.001000    Time 0.106940    
2024-05-12 21:47:13,235 - Epoch: [10][  200/  217]    Overall Loss 1.089280    Objective Loss 1.089280                                        LR 0.001000    Time 0.096741    
2024-05-12 21:47:14,488 - Epoch: [10][  217/  217]    Overall Loss 1.094072    Objective Loss 1.094072    Top1 73.770492    Top5 88.524590    LR 0.001000    Time 0.094910    
2024-05-12 21:47:15,088 - --- validate (epoch=10)-----------
2024-05-12 21:47:15,090 - 1736 samples (32 per mini-batch)
2024-05-12 21:47:22,161 - Epoch: [10][   55/   55]    Loss 2.040942    Top1 51.843318    Top5 70.506912    
2024-05-12 21:47:22,779 - ==> Top1: 51.843    Top5: 70.507    Loss: 2.041

2024-05-12 21:47:22,789 - ==> Best [Top1: 51.843   Top5: 70.507   Sparsity:0.00   Params: 382208 on epoch: 10]
2024-05-12 21:47:22,790 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 21:47:22,892 - 

2024-05-12 21:47:22,894 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:47:34,130 - Epoch: [11][  100/  217]    Overall Loss 0.889032    Objective Loss 0.889032                                        LR 0.001000    Time 0.112068    
2024-05-12 21:47:42,551 - Epoch: [11][  200/  217]    Overall Loss 0.917938    Objective Loss 0.917938                                        LR 0.001000    Time 0.098001    
2024-05-12 21:47:43,442 - Epoch: [11][  217/  217]    Overall Loss 0.928610    Objective Loss 0.928610    Top1 67.213115    Top5 88.524590    LR 0.001000    Time 0.094410    
2024-05-12 21:47:43,910 - 

2024-05-12 21:47:43,911 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:47:56,170 - Epoch: [12][  100/  217]    Overall Loss 0.721752    Objective Loss 0.721752                                        LR 0.001000    Time 0.122299    
2024-05-12 21:48:05,097 - Epoch: [12][  200/  217]    Overall Loss 0.750352    Objective Loss 0.750352                                        LR 0.001000    Time 0.105647    
2024-05-12 21:48:06,409 - Epoch: [12][  217/  217]    Overall Loss 0.756235    Objective Loss 0.756235    Top1 81.967213    Top5 93.442623    LR 0.001000    Time 0.103389    
2024-05-12 21:48:07,001 - 

2024-05-12 21:48:07,003 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:48:17,924 - Epoch: [13][  100/  217]    Overall Loss 0.593208    Objective Loss 0.593208                                        LR 0.001000    Time 0.108982    
2024-05-12 21:48:26,390 - Epoch: [13][  200/  217]    Overall Loss 0.610443    Objective Loss 0.610443                                        LR 0.001000    Time 0.096683    
2024-05-12 21:48:27,719 - Epoch: [13][  217/  217]    Overall Loss 0.613937    Objective Loss 0.613937    Top1 78.688525    Top5 91.803279    LR 0.001000    Time 0.095211    
2024-05-12 21:48:28,309 - 

2024-05-12 21:48:28,312 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:48:40,819 - Epoch: [14][  100/  217]    Overall Loss 0.452530    Objective Loss 0.452530                                        LR 0.001000    Time 0.124768    
2024-05-12 21:48:48,652 - Epoch: [14][  200/  217]    Overall Loss 0.467103    Objective Loss 0.467103                                        LR 0.001000    Time 0.101416    
2024-05-12 21:48:49,755 - Epoch: [14][  217/  217]    Overall Loss 0.468854    Objective Loss 0.468854    Top1 93.442623    Top5 98.360656    LR 0.001000    Time 0.098532    
2024-05-12 21:48:50,225 - 

2024-05-12 21:48:50,226 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:49:02,223 - Epoch: [15][  100/  217]    Overall Loss 0.331398    Objective Loss 0.331398                                        LR 0.001000    Time 0.119679    
2024-05-12 21:49:12,262 - Epoch: [15][  200/  217]    Overall Loss 0.354234    Objective Loss 0.354234                                        LR 0.001000    Time 0.109877    
2024-05-12 21:49:13,774 - Epoch: [15][  217/  217]    Overall Loss 0.357275    Objective Loss 0.357275    Top1 86.885246    Top5 96.721311    LR 0.001000    Time 0.108204    
2024-05-12 21:49:14,370 - 

2024-05-12 21:49:14,371 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:49:25,717 - Epoch: [16][  100/  217]    Overall Loss 0.261359    Objective Loss 0.261359                                        LR 0.001000    Time 0.113186    
2024-05-12 21:49:34,999 - Epoch: [16][  200/  217]    Overall Loss 0.277011    Objective Loss 0.277011                                        LR 0.001000    Time 0.102843    
2024-05-12 21:49:36,525 - Epoch: [16][  217/  217]    Overall Loss 0.275744    Objective Loss 0.275744    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.101791    
2024-05-12 21:49:37,153 - 

2024-05-12 21:49:37,155 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:49:49,847 - Epoch: [17][  100/  217]    Overall Loss 0.176180    Objective Loss 0.176180                                        LR 0.001000    Time 0.126625    
2024-05-12 21:49:56,996 - Epoch: [17][  200/  217]    Overall Loss 0.185534    Objective Loss 0.185534                                        LR 0.001000    Time 0.098934    
2024-05-12 21:49:58,182 - Epoch: [17][  217/  217]    Overall Loss 0.190096    Objective Loss 0.190096    Top1 95.081967    Top5 100.000000    LR 0.001000    Time 0.096576    
2024-05-12 21:49:58,801 - 

2024-05-12 21:49:58,803 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:50:09,596 - Epoch: [18][  100/  217]    Overall Loss 0.160089    Objective Loss 0.160089                                        LR 0.001000    Time 0.107703    
2024-05-12 21:50:17,756 - Epoch: [18][  200/  217]    Overall Loss 0.161451    Objective Loss 0.161451                                        LR 0.001000    Time 0.094525    
2024-05-12 21:50:18,991 - Epoch: [18][  217/  217]    Overall Loss 0.161298    Objective Loss 0.161298    Top1 96.721311    Top5 98.360656    LR 0.001000    Time 0.092778    
2024-05-12 21:50:19,407 - 

2024-05-12 21:50:19,409 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:50:31,109 - Epoch: [19][  100/  217]    Overall Loss 0.119212    Objective Loss 0.119212                                        LR 0.001000    Time 0.116704    
2024-05-12 21:50:39,862 - Epoch: [19][  200/  217]    Overall Loss 0.117668    Objective Loss 0.117668                                        LR 0.001000    Time 0.101976    
2024-05-12 21:50:41,136 - Epoch: [19][  217/  217]    Overall Loss 0.117551    Objective Loss 0.117551    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.099835    
2024-05-12 21:50:41,711 - 

2024-05-12 21:50:41,713 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:50:53,791 - Epoch: [20][  100/  217]    Overall Loss 0.076195    Objective Loss 0.076195                                        LR 0.001000    Time 0.120506    
2024-05-12 21:51:01,256 - Epoch: [20][  200/  217]    Overall Loss 0.081380    Objective Loss 0.081380                                        LR 0.001000    Time 0.097452    
2024-05-12 21:51:02,805 - Epoch: [20][  217/  217]    Overall Loss 0.083413    Objective Loss 0.083413    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.096929    
2024-05-12 21:51:03,481 - --- validate (epoch=20)-----------
2024-05-12 21:51:03,486 - 1736 samples (32 per mini-batch)
2024-05-12 21:51:11,299 - Epoch: [20][   55/   55]    Loss 2.517649    Top1 51.555300    Top5 69.470046    
2024-05-12 21:51:11,988 - ==> Top1: 51.555    Top5: 69.470    Loss: 2.518

2024-05-12 21:51:12,011 - ==> Best [Top1: 51.843   Top5: 70.507   Sparsity:0.00   Params: 382208 on epoch: 10]
2024-05-12 21:51:12,013 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 21:51:12,159 - 

2024-05-12 21:51:12,161 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:51:24,088 - Epoch: [21][  100/  217]    Overall Loss 0.065507    Objective Loss 0.065507                                        LR 0.001000    Time 0.119002    
2024-05-12 21:51:30,829 - Epoch: [21][  200/  217]    Overall Loss 0.071126    Objective Loss 0.071126                                        LR 0.001000    Time 0.093093    
2024-05-12 21:51:32,211 - Epoch: [21][  217/  217]    Overall Loss 0.072798    Objective Loss 0.072798    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.092141    
2024-05-12 21:51:32,851 - 

2024-05-12 21:51:32,853 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:51:46,724 - Epoch: [22][  100/  217]    Overall Loss 0.061432    Objective Loss 0.061432                                        LR 0.001000    Time 0.138372    
2024-05-12 21:51:55,341 - Epoch: [22][  200/  217]    Overall Loss 0.063882    Objective Loss 0.063882                                        LR 0.001000    Time 0.112129    
2024-05-12 21:51:56,499 - Epoch: [22][  217/  217]    Overall Loss 0.065774    Objective Loss 0.065774    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.108663    
2024-05-12 21:51:56,892 - 

2024-05-12 21:51:56,893 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:52:08,348 - Epoch: [23][  100/  217]    Overall Loss 0.068793    Objective Loss 0.068793                                        LR 0.001000    Time 0.114243    
2024-05-12 21:52:17,758 - Epoch: [23][  200/  217]    Overall Loss 0.079490    Objective Loss 0.079490                                        LR 0.001000    Time 0.104020    
2024-05-12 21:52:19,245 - Epoch: [23][  217/  217]    Overall Loss 0.082930    Objective Loss 0.082930    Top1 93.442623    Top5 100.000000    LR 0.001000    Time 0.102698    
2024-05-12 21:52:19,790 - 

2024-05-12 21:52:19,791 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:52:31,893 - Epoch: [24][  100/  217]    Overall Loss 0.193328    Objective Loss 0.193328                                        LR 0.001000    Time 0.120748    
2024-05-12 21:52:39,246 - Epoch: [24][  200/  217]    Overall Loss 0.395440    Objective Loss 0.395440                                        LR 0.001000    Time 0.097013    
2024-05-12 21:52:40,818 - Epoch: [24][  217/  217]    Overall Loss 0.435164    Objective Loss 0.435164    Top1 73.770492    Top5 90.163934    LR 0.001000    Time 0.096626    
2024-05-12 21:52:41,617 - 

2024-05-12 21:52:41,621 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:52:54,520 - Epoch: [25][  100/  217]    Overall Loss 0.582599    Objective Loss 0.582599                                        LR 0.001000    Time 0.128657    
2024-05-12 21:53:02,402 - Epoch: [25][  200/  217]    Overall Loss 0.500304    Objective Loss 0.500304                                        LR 0.001000    Time 0.103611    
2024-05-12 21:53:03,437 - Epoch: [25][  217/  217]    Overall Loss 0.489145    Objective Loss 0.489145    Top1 90.163934    Top5 98.360656    LR 0.001000    Time 0.100243    
2024-05-12 21:53:03,812 - 

2024-05-12 21:53:03,814 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:53:15,398 - Epoch: [26][  100/  217]    Overall Loss 0.163888    Objective Loss 0.163888                                        LR 0.001000    Time 0.115547    
2024-05-12 21:53:24,074 - Epoch: [26][  200/  217]    Overall Loss 0.148459    Objective Loss 0.148459                                        LR 0.001000    Time 0.100992    
2024-05-12 21:53:25,220 - Epoch: [26][  217/  217]    Overall Loss 0.146005    Objective Loss 0.146005    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.098333    
2024-05-12 21:53:25,726 - 

2024-05-12 21:53:25,727 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:53:39,170 - Epoch: [27][  100/  217]    Overall Loss 0.065361    Objective Loss 0.065361                                        LR 0.001000    Time 0.134141    
2024-05-12 21:53:48,356 - Epoch: [27][  200/  217]    Overall Loss 0.059357    Objective Loss 0.059357                                        LR 0.001000    Time 0.112870    
2024-05-12 21:53:49,717 - Epoch: [27][  217/  217]    Overall Loss 0.058663    Objective Loss 0.058663    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.110270    
2024-05-12 21:53:50,260 - 

2024-05-12 21:53:50,261 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:53:57,825 - Epoch: [28][  100/  217]    Overall Loss 0.029323    Objective Loss 0.029323                                        LR 0.001000    Time 0.075461    
2024-05-12 21:54:07,474 - Epoch: [28][  200/  217]    Overall Loss 0.028410    Objective Loss 0.028410                                        LR 0.001000    Time 0.085826    
2024-05-12 21:54:08,855 - Epoch: [28][  217/  217]    Overall Loss 0.029942    Objective Loss 0.029942    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.085441    
2024-05-12 21:54:09,471 - 

2024-05-12 21:54:09,472 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:54:23,189 - Epoch: [29][  100/  217]    Overall Loss 0.019196    Objective Loss 0.019196                                        LR 0.001000    Time 0.136852    
2024-05-12 21:54:30,664 - Epoch: [29][  200/  217]    Overall Loss 0.020353    Objective Loss 0.020353                                        LR 0.001000    Time 0.105678    
2024-05-12 21:54:31,600 - Epoch: [29][  217/  217]    Overall Loss 0.021156    Objective Loss 0.021156    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.101696    
2024-05-12 21:54:32,099 - 

2024-05-12 21:54:32,101 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:54:45,533 - Epoch: [30][  100/  217]    Overall Loss 0.017609    Objective Loss 0.017609                                        LR 0.001000    Time 0.133994    
2024-05-12 21:54:55,276 - Epoch: [30][  200/  217]    Overall Loss 0.016261    Objective Loss 0.016261                                        LR 0.001000    Time 0.115573    
2024-05-12 21:54:56,700 - Epoch: [30][  217/  217]    Overall Loss 0.015914    Objective Loss 0.015914    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.113053    
2024-05-12 21:54:57,315 - --- validate (epoch=30)-----------
2024-05-12 21:54:57,318 - 1736 samples (32 per mini-batch)
2024-05-12 21:55:03,122 - Epoch: [30][   55/   55]    Loss 2.457472    Top1 55.933180    Top5 71.428571    
2024-05-12 21:55:03,600 - ==> Top1: 55.933    Top5: 71.429    Loss: 2.457

2024-05-12 21:55:03,614 - ==> Best [Top1: 55.933   Top5: 71.429   Sparsity:0.00   Params: 382208 on epoch: 30]
2024-05-12 21:55:03,616 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 21:55:03,729 - 

2024-05-12 21:55:03,730 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:55:15,524 - Epoch: [31][  100/  217]    Overall Loss 0.013538    Objective Loss 0.013538                                        LR 0.001000    Time 0.117666    
2024-05-12 21:55:24,374 - Epoch: [31][  200/  217]    Overall Loss 0.013793    Objective Loss 0.013793                                        LR 0.001000    Time 0.102867    
2024-05-12 21:55:25,743 - Epoch: [31][  217/  217]    Overall Loss 0.013910    Objective Loss 0.013910    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.101092    
2024-05-12 21:55:26,319 - 

2024-05-12 21:55:26,321 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:55:37,829 - Epoch: [32][  100/  217]    Overall Loss 0.011516    Objective Loss 0.011516                                        LR 0.001000    Time 0.114807    
2024-05-12 21:55:47,462 - Epoch: [32][  200/  217]    Overall Loss 0.012096    Objective Loss 0.012096                                        LR 0.001000    Time 0.105429    
2024-05-12 21:55:48,910 - Epoch: [32][  217/  217]    Overall Loss 0.012277    Objective Loss 0.012277    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.103812    
2024-05-12 21:55:49,408 - 

2024-05-12 21:55:49,409 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:56:00,828 - Epoch: [33][  100/  217]    Overall Loss 0.010659    Objective Loss 0.010659                                        LR 0.001000    Time 0.113913    
2024-05-12 21:56:08,985 - Epoch: [33][  200/  217]    Overall Loss 0.012742    Objective Loss 0.012742                                        LR 0.001000    Time 0.097518    
2024-05-12 21:56:10,374 - Epoch: [33][  217/  217]    Overall Loss 0.013101    Objective Loss 0.013101    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.096250    
2024-05-12 21:56:11,029 - 

2024-05-12 21:56:11,031 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:56:24,329 - Epoch: [34][  100/  217]    Overall Loss 0.010860    Objective Loss 0.010860                                        LR 0.001000    Time 0.132684    
2024-05-12 21:56:31,797 - Epoch: [34][  200/  217]    Overall Loss 0.010752    Objective Loss 0.010752                                        LR 0.001000    Time 0.103561    
2024-05-12 21:56:32,731 - Epoch: [34][  217/  217]    Overall Loss 0.010859    Objective Loss 0.010859    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.099731    
2024-05-12 21:56:33,097 - 

2024-05-12 21:56:33,098 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:56:45,335 - Epoch: [35][  100/  217]    Overall Loss 0.009614    Objective Loss 0.009614                                        LR 0.001000    Time 0.122083    
2024-05-12 21:56:54,252 - Epoch: [35][  200/  217]    Overall Loss 0.009611    Objective Loss 0.009611                                        LR 0.001000    Time 0.105482    
2024-05-12 21:56:55,659 - Epoch: [35][  217/  217]    Overall Loss 0.009658    Objective Loss 0.009658    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.103676    
2024-05-12 21:56:56,255 - 

2024-05-12 21:56:56,257 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:57:07,391 - Epoch: [36][  100/  217]    Overall Loss 0.005846    Objective Loss 0.005846                                        LR 0.001000    Time 0.111090    
2024-05-12 21:57:15,807 - Epoch: [36][  200/  217]    Overall Loss 0.008320    Objective Loss 0.008320                                        LR 0.001000    Time 0.097488    
2024-05-12 21:57:17,378 - Epoch: [36][  217/  217]    Overall Loss 0.008202    Objective Loss 0.008202    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.097050    
2024-05-12 21:57:17,979 - 

2024-05-12 21:57:17,983 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:57:32,327 - Epoch: [37][  100/  217]    Overall Loss 0.005847    Objective Loss 0.005847                                        LR 0.001000    Time 0.143120    
2024-05-12 21:57:37,923 - Epoch: [37][  200/  217]    Overall Loss 0.007787    Objective Loss 0.007787                                        LR 0.001000    Time 0.099403    
2024-05-12 21:57:38,753 - Epoch: [37][  217/  217]    Overall Loss 0.007621    Objective Loss 0.007621    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.095423    
2024-05-12 21:57:39,161 - 

2024-05-12 21:57:39,163 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:57:50,238 - Epoch: [38][  100/  217]    Overall Loss 0.006388    Objective Loss 0.006388                                        LR 0.001000    Time 0.110514    
2024-05-12 21:57:59,574 - Epoch: [38][  200/  217]    Overall Loss 0.007665    Objective Loss 0.007665                                        LR 0.001000    Time 0.101787    
2024-05-12 21:58:00,908 - Epoch: [38][  217/  217]    Overall Loss 0.007777    Objective Loss 0.007777    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.099932    
2024-05-12 21:58:01,414 - 

2024-05-12 21:58:01,416 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:58:11,975 - Epoch: [39][  100/  217]    Overall Loss 0.337088    Objective Loss 0.337088                                        LR 0.001000    Time 0.105324    
2024-05-12 21:58:21,422 - Epoch: [39][  200/  217]    Overall Loss 0.978255    Objective Loss 0.978255                                        LR 0.001000    Time 0.099760    
2024-05-12 21:58:22,934 - Epoch: [39][  217/  217]    Overall Loss 1.009842    Objective Loss 1.009842    Top1 52.459016    Top5 86.885246    LR 0.001000    Time 0.098886    
2024-05-12 21:58:23,502 - 

2024-05-12 21:58:23,504 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:58:35,776 - Epoch: [40][  100/  217]    Overall Loss 0.656723    Objective Loss 0.656723                                        LR 0.001000    Time 0.122417    
2024-05-12 21:58:44,119 - Epoch: [40][  200/  217]    Overall Loss 0.617138    Objective Loss 0.617138                                        LR 0.001000    Time 0.102777    
2024-05-12 21:58:45,288 - Epoch: [40][  217/  217]    Overall Loss 0.613978    Objective Loss 0.613978    Top1 86.885246    Top5 98.360656    LR 0.001000    Time 0.100086    
2024-05-12 21:58:45,868 - --- validate (epoch=40)-----------
2024-05-12 21:58:45,870 - 1736 samples (32 per mini-batch)
2024-05-12 21:58:50,700 - Epoch: [40][   55/   55]    Loss 2.461935    Top1 52.937788    Top5 70.161290    
2024-05-12 21:58:51,224 - ==> Top1: 52.938    Top5: 70.161    Loss: 2.462

2024-05-12 21:58:51,236 - ==> Best [Top1: 55.933   Top5: 71.429   Sparsity:0.00   Params: 382208 on epoch: 30]
2024-05-12 21:58:51,237 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 21:58:51,304 - 

2024-05-12 21:58:51,305 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:59:04,812 - Epoch: [41][  100/  217]    Overall Loss 0.211875    Objective Loss 0.211875                                        LR 0.001000    Time 0.134757    
2024-05-12 21:59:13,836 - Epoch: [41][  200/  217]    Overall Loss 0.188725    Objective Loss 0.188725                                        LR 0.001000    Time 0.112370    
2024-05-12 21:59:15,083 - Epoch: [41][  217/  217]    Overall Loss 0.186026    Objective Loss 0.186026    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.109287    
2024-05-12 21:59:15,643 - 

2024-05-12 21:59:15,648 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:59:26,978 - Epoch: [42][  100/  217]    Overall Loss 0.074626    Objective Loss 0.074626                                        LR 0.001000    Time 0.113005    
2024-05-12 21:59:34,919 - Epoch: [42][  200/  217]    Overall Loss 0.069364    Objective Loss 0.069364                                        LR 0.001000    Time 0.096085    
2024-05-12 21:59:36,279 - Epoch: [42][  217/  217]    Overall Loss 0.069676    Objective Loss 0.069676    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.094796    
2024-05-12 21:59:36,860 - 

2024-05-12 21:59:36,862 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 21:59:49,083 - Epoch: [43][  100/  217]    Overall Loss 0.036665    Objective Loss 0.036665                                        LR 0.001000    Time 0.121921    
2024-05-12 21:59:57,785 - Epoch: [43][  200/  217]    Overall Loss 0.034364    Objective Loss 0.034364                                        LR 0.001000    Time 0.104338    
2024-05-12 21:59:59,229 - Epoch: [43][  217/  217]    Overall Loss 0.034112    Objective Loss 0.034112    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.102790    
2024-05-12 21:59:59,691 - 

2024-05-12 21:59:59,692 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:00:09,683 - Epoch: [44][  100/  217]    Overall Loss 0.022546    Objective Loss 0.022546                                        LR 0.001000    Time 0.099660    
2024-05-12 22:00:17,577 - Epoch: [44][  200/  217]    Overall Loss 0.021700    Objective Loss 0.021700                                        LR 0.001000    Time 0.089163    
2024-05-12 22:00:18,768 - Epoch: [44][  217/  217]    Overall Loss 0.021504    Objective Loss 0.021504    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.087641    
2024-05-12 22:00:19,367 - 

2024-05-12 22:00:19,370 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:00:32,765 - Epoch: [45][  100/  217]    Overall Loss 0.016235    Objective Loss 0.016235                                        LR 0.001000    Time 0.133646    
2024-05-12 22:00:40,452 - Epoch: [45][  200/  217]    Overall Loss 0.015938    Objective Loss 0.015938                                        LR 0.001000    Time 0.105142    
2024-05-12 22:00:41,214 - Epoch: [45][  217/  217]    Overall Loss 0.016006    Objective Loss 0.016006    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.100395    
2024-05-12 22:00:41,609 - 

2024-05-12 22:00:41,611 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:00:52,777 - Epoch: [46][  100/  217]    Overall Loss 0.012378    Objective Loss 0.012378                                        LR 0.001000    Time 0.111378    
2024-05-12 22:01:01,636 - Epoch: [46][  200/  217]    Overall Loss 0.013221    Objective Loss 0.013221                                        LR 0.001000    Time 0.099834    
2024-05-12 22:01:02,968 - Epoch: [46][  217/  217]    Overall Loss 0.013465    Objective Loss 0.013465    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.098121    
2024-05-12 22:01:03,659 - 

2024-05-12 22:01:03,660 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:01:14,473 - Epoch: [47][  100/  217]    Overall Loss 0.010276    Objective Loss 0.010276                                        LR 0.001000    Time 0.107865    
2024-05-12 22:01:23,482 - Epoch: [47][  200/  217]    Overall Loss 0.010988    Objective Loss 0.010988                                        LR 0.001000    Time 0.098824    
2024-05-12 22:01:24,544 - Epoch: [47][  217/  217]    Overall Loss 0.011329    Objective Loss 0.011329    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.095952    
2024-05-12 22:01:25,078 - 

2024-05-12 22:01:25,081 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:01:36,740 - Epoch: [48][  100/  217]    Overall Loss 0.008180    Objective Loss 0.008180                                        LR 0.001000    Time 0.116260    
2024-05-12 22:01:45,189 - Epoch: [48][  200/  217]    Overall Loss 0.009063    Objective Loss 0.009063                                        LR 0.001000    Time 0.100241    
2024-05-12 22:01:46,701 - Epoch: [48][  217/  217]    Overall Loss 0.009298    Objective Loss 0.009298    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.099320    
2024-05-12 22:01:47,386 - 

2024-05-12 22:01:47,391 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:02:01,333 - Epoch: [49][  100/  217]    Overall Loss 0.006264    Objective Loss 0.006264                                        LR 0.001000    Time 0.139050    
2024-05-12 22:02:09,764 - Epoch: [49][  200/  217]    Overall Loss 0.008424    Objective Loss 0.008424                                        LR 0.001000    Time 0.111538    
2024-05-12 22:02:11,513 - Epoch: [49][  217/  217]    Overall Loss 0.008277    Objective Loss 0.008277    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.110828    
2024-05-12 22:02:12,142 - 

2024-05-12 22:02:12,147 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:02:24,576 - Epoch: [50][  100/  217]    Overall Loss 0.008054    Objective Loss 0.008054                                        LR 0.001000    Time 0.123963    
2024-05-12 22:02:34,338 - Epoch: [50][  200/  217]    Overall Loss 0.008924    Objective Loss 0.008924                                        LR 0.001000    Time 0.110637    
2024-05-12 22:02:35,768 - Epoch: [50][  217/  217]    Overall Loss 0.009659    Objective Loss 0.009659    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.108530    
2024-05-12 22:02:36,431 - --- validate (epoch=50)-----------
2024-05-12 22:02:36,432 - 1736 samples (32 per mini-batch)
2024-05-12 22:02:42,778 - Epoch: [50][   55/   55]    Loss 2.636122    Top1 55.414747    Top5 72.235023    
2024-05-12 22:02:43,327 - ==> Top1: 55.415    Top5: 72.235    Loss: 2.636

2024-05-12 22:02:43,335 - ==> Best [Top1: 55.933   Top5: 71.429   Sparsity:0.00   Params: 382208 on epoch: 30]
2024-05-12 22:02:43,336 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:02:43,424 - 

2024-05-12 22:02:43,426 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:02:55,453 - Epoch: [51][  100/  217]    Overall Loss 0.010138    Objective Loss 0.010138                                        LR 0.001000    Time 0.119979    
2024-05-12 22:03:05,257 - Epoch: [51][  200/  217]    Overall Loss 0.010718    Objective Loss 0.010718                                        LR 0.001000    Time 0.108853    
2024-05-12 22:03:06,733 - Epoch: [51][  217/  217]    Overall Loss 0.010557    Objective Loss 0.010557    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.107106    
2024-05-12 22:03:07,376 - 

2024-05-12 22:03:07,378 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:03:20,174 - Epoch: [52][  100/  217]    Overall Loss 0.008771    Objective Loss 0.008771                                        LR 0.001000    Time 0.127674    
2024-05-12 22:03:26,582 - Epoch: [52][  200/  217]    Overall Loss 0.009008    Objective Loss 0.009008                                        LR 0.001000    Time 0.095768    
2024-05-12 22:03:28,065 - Epoch: [52][  217/  217]    Overall Loss 0.008795    Objective Loss 0.008795    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.095071    
2024-05-12 22:03:28,888 - 

2024-05-12 22:03:28,892 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:03:42,613 - Epoch: [53][  100/  217]    Overall Loss 0.010389    Objective Loss 0.010389                                        LR 0.001000    Time 0.136872    
2024-05-12 22:03:52,117 - Epoch: [53][  200/  217]    Overall Loss 0.014807    Objective Loss 0.014807                                        LR 0.001000    Time 0.115807    
2024-05-12 22:03:53,168 - Epoch: [53][  217/  217]    Overall Loss 0.020860    Objective Loss 0.020860    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.111553    
2024-05-12 22:03:53,623 - 

2024-05-12 22:03:53,624 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:04:04,950 - Epoch: [54][  100/  217]    Overall Loss 0.819156    Objective Loss 0.819156                                        LR 0.001000    Time 0.112946    
2024-05-12 22:04:14,022 - Epoch: [54][  200/  217]    Overall Loss 0.874195    Objective Loss 0.874195                                        LR 0.001000    Time 0.101690    
2024-05-12 22:04:15,503 - Epoch: [54][  217/  217]    Overall Loss 0.856242    Objective Loss 0.856242    Top1 85.245902    Top5 98.360656    LR 0.001000    Time 0.100522    
2024-05-12 22:04:16,109 - 

2024-05-12 22:04:16,111 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:04:26,984 - Epoch: [55][  100/  217]    Overall Loss 0.291784    Objective Loss 0.291784                                        LR 0.001000    Time 0.108464    
2024-05-12 22:04:34,714 - Epoch: [55][  200/  217]    Overall Loss 0.261498    Objective Loss 0.261498                                        LR 0.001000    Time 0.092743    
2024-05-12 22:04:36,073 - Epoch: [55][  217/  217]    Overall Loss 0.255818    Objective Loss 0.255818    Top1 95.081967    Top5 100.000000    LR 0.001000    Time 0.091717    
2024-05-12 22:04:36,630 - 

2024-05-12 22:04:36,631 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:04:46,897 - Epoch: [56][  100/  217]    Overall Loss 0.074940    Objective Loss 0.074940                                        LR 0.001000    Time 0.102419    
2024-05-12 22:04:56,683 - Epoch: [56][  200/  217]    Overall Loss 0.073834    Objective Loss 0.073834                                        LR 0.001000    Time 0.099993    
2024-05-12 22:04:58,108 - Epoch: [56][  217/  217]    Overall Loss 0.073447    Objective Loss 0.073447    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.098697    
2024-05-12 22:04:58,735 - 

2024-05-12 22:04:58,736 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:05:10,858 - Epoch: [57][  100/  217]    Overall Loss 0.033487    Objective Loss 0.033487                                        LR 0.001000    Time 0.120938    
2024-05-12 22:05:18,032 - Epoch: [57][  200/  217]    Overall Loss 0.031797    Objective Loss 0.031797                                        LR 0.001000    Time 0.096214    
2024-05-12 22:05:19,145 - Epoch: [57][  217/  217]    Overall Loss 0.031506    Objective Loss 0.031506    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.093780    
2024-05-12 22:05:19,646 - 

2024-05-12 22:05:19,648 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:05:31,957 - Epoch: [58][  100/  217]    Overall Loss 0.018812    Objective Loss 0.018812                                        LR 0.001000    Time 0.122803    
2024-05-12 22:05:41,825 - Epoch: [58][  200/  217]    Overall Loss 0.018158    Objective Loss 0.018158                                        LR 0.001000    Time 0.110595    
2024-05-12 22:05:42,974 - Epoch: [58][  217/  217]    Overall Loss 0.017726    Objective Loss 0.017726    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.107200    
2024-05-12 22:05:43,383 - 

2024-05-12 22:05:43,384 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:05:54,403 - Epoch: [59][  100/  217]    Overall Loss 0.013073    Objective Loss 0.013073                                        LR 0.001000    Time 0.109892    
2024-05-12 22:06:03,917 - Epoch: [59][  200/  217]    Overall Loss 0.012357    Objective Loss 0.012357                                        LR 0.001000    Time 0.102365    
2024-05-12 22:06:05,321 - Epoch: [59][  217/  217]    Overall Loss 0.012630    Objective Loss 0.012630    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.100784    
2024-05-12 22:06:06,018 - 

2024-05-12 22:06:06,021 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:06:20,409 - Epoch: [60][  100/  217]    Overall Loss 0.009034    Objective Loss 0.009034                                        LR 0.001000    Time 0.143596    
2024-05-12 22:06:28,187 - Epoch: [60][  200/  217]    Overall Loss 0.009735    Objective Loss 0.009735                                        LR 0.001000    Time 0.110562    
2024-05-12 22:06:29,691 - Epoch: [60][  217/  217]    Overall Loss 0.009865    Objective Loss 0.009865    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.108801    
2024-05-12 22:06:30,433 - --- validate (epoch=60)-----------
2024-05-12 22:06:30,436 - 1736 samples (32 per mini-batch)
2024-05-12 22:06:37,013 - Epoch: [60][   55/   55]    Loss 2.653153    Top1 55.587558    Top5 73.041475    
2024-05-12 22:06:37,655 - ==> Top1: 55.588    Top5: 73.041    Loss: 2.653

2024-05-12 22:06:37,669 - ==> Best [Top1: 55.933   Top5: 71.429   Sparsity:0.00   Params: 382208 on epoch: 30]
2024-05-12 22:06:37,671 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:06:37,752 - 

2024-05-12 22:06:37,753 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:06:48,471 - Epoch: [61][  100/  217]    Overall Loss 0.009539    Objective Loss 0.009539                                        LR 0.001000    Time 0.106956    
2024-05-12 22:06:55,780 - Epoch: [61][  200/  217]    Overall Loss 0.009089    Objective Loss 0.009089                                        LR 0.001000    Time 0.089907    
2024-05-12 22:06:57,436 - Epoch: [61][  217/  217]    Overall Loss 0.009077    Objective Loss 0.009077    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.090451    
2024-05-12 22:06:58,034 - 

2024-05-12 22:06:58,036 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:07:10,731 - Epoch: [62][  100/  217]    Overall Loss 0.007381    Objective Loss 0.007381                                        LR 0.001000    Time 0.126638    
2024-05-12 22:07:19,989 - Epoch: [62][  200/  217]    Overall Loss 0.007805    Objective Loss 0.007805                                        LR 0.001000    Time 0.109462    
2024-05-12 22:07:20,962 - Epoch: [62][  217/  217]    Overall Loss 0.007649    Objective Loss 0.007649    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.105351    
2024-05-12 22:07:21,412 - 

2024-05-12 22:07:21,414 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:07:33,748 - Epoch: [63][  100/  217]    Overall Loss 0.006650    Objective Loss 0.006650                                        LR 0.001000    Time 0.123036    
2024-05-12 22:07:42,866 - Epoch: [63][  200/  217]    Overall Loss 0.006438    Objective Loss 0.006438                                        LR 0.001000    Time 0.106969    
2024-05-12 22:07:44,081 - Epoch: [63][  217/  217]    Overall Loss 0.006755    Objective Loss 0.006755    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.104164    
2024-05-12 22:07:44,580 - 

2024-05-12 22:07:44,582 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:07:55,749 - Epoch: [64][  100/  217]    Overall Loss 0.005675    Objective Loss 0.005675                                        LR 0.001000    Time 0.111406    
2024-05-12 22:08:05,471 - Epoch: [64][  200/  217]    Overall Loss 0.005779    Objective Loss 0.005779                                        LR 0.001000    Time 0.104170    
2024-05-12 22:08:06,705 - Epoch: [64][  217/  217]    Overall Loss 0.006067    Objective Loss 0.006067    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.101670    
2024-05-12 22:08:07,308 - 

2024-05-12 22:08:07,310 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:08:20,570 - Epoch: [65][  100/  217]    Overall Loss 0.005821    Objective Loss 0.005821                                        LR 0.001000    Time 0.132279    
2024-05-12 22:08:26,773 - Epoch: [65][  200/  217]    Overall Loss 0.005678    Objective Loss 0.005678                                        LR 0.001000    Time 0.097044    
2024-05-12 22:08:27,755 - Epoch: [65][  217/  217]    Overall Loss 0.006291    Objective Loss 0.006291    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.093938    
2024-05-12 22:08:28,122 - 

2024-05-12 22:08:28,123 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:08:40,868 - Epoch: [66][  100/  217]    Overall Loss 0.003627    Objective Loss 0.003627                                        LR 0.001000    Time 0.127139    
2024-05-12 22:08:50,294 - Epoch: [66][  200/  217]    Overall Loss 0.005225    Objective Loss 0.005225                                        LR 0.001000    Time 0.110556    
2024-05-12 22:08:51,562 - Epoch: [66][  217/  217]    Overall Loss 0.005399    Objective Loss 0.005399    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.107714    
2024-05-12 22:08:52,116 - 

2024-05-12 22:08:52,117 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:09:03,289 - Epoch: [67][  100/  217]    Overall Loss 0.006599    Objective Loss 0.006599                                        LR 0.001000    Time 0.111444    
2024-05-12 22:09:10,106 - Epoch: [67][  200/  217]    Overall Loss 0.005686    Objective Loss 0.005686                                        LR 0.001000    Time 0.089688    
2024-05-12 22:09:11,655 - Epoch: [67][  217/  217]    Overall Loss 0.005453    Objective Loss 0.005453    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.089770    
2024-05-12 22:09:12,231 - 

2024-05-12 22:09:12,233 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:09:24,094 - Epoch: [68][  100/  217]    Overall Loss 0.003755    Objective Loss 0.003755                                        LR 0.001000    Time 0.118333    
2024-05-12 22:09:31,392 - Epoch: [68][  200/  217]    Overall Loss 0.004052    Objective Loss 0.004052                                        LR 0.001000    Time 0.095525    
2024-05-12 22:09:32,624 - Epoch: [68][  217/  217]    Overall Loss 0.004758    Objective Loss 0.004758    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.093696    
2024-05-12 22:09:33,183 - 

2024-05-12 22:09:33,185 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:09:46,709 - Epoch: [69][  100/  217]    Overall Loss 0.004490    Objective Loss 0.004490                                        LR 0.001000    Time 0.134898    
2024-05-12 22:09:55,447 - Epoch: [69][  200/  217]    Overall Loss 0.003916    Objective Loss 0.003916                                        LR 0.001000    Time 0.110997    
2024-05-12 22:09:56,621 - Epoch: [69][  217/  217]    Overall Loss 0.004316    Objective Loss 0.004316    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.107692    
2024-05-12 22:09:57,152 - 

2024-05-12 22:09:57,155 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:10:10,780 - Epoch: [70][  100/  217]    Overall Loss 0.003787    Objective Loss 0.003787                                        LR 0.001000    Time 0.135898    
2024-05-12 22:10:18,389 - Epoch: [70][  200/  217]    Overall Loss 0.005107    Objective Loss 0.005107                                        LR 0.001000    Time 0.105869    
2024-05-12 22:10:19,734 - Epoch: [70][  217/  217]    Overall Loss 0.006905    Objective Loss 0.006905    Top1 98.360656    Top5 98.360656    LR 0.001000    Time 0.103741    
2024-05-12 22:10:20,353 - --- validate (epoch=70)-----------
2024-05-12 22:10:20,356 - 1736 samples (32 per mini-batch)
2024-05-12 22:10:26,253 - Epoch: [70][   55/   55]    Loss 3.061192    Top1 52.419355    Top5 69.815668    
2024-05-12 22:10:26,747 - ==> Top1: 52.419    Top5: 69.816    Loss: 3.061

2024-05-12 22:10:26,764 - ==> Best [Top1: 55.933   Top5: 71.429   Sparsity:0.00   Params: 382208 on epoch: 30]
2024-05-12 22:10:26,766 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:10:26,896 - 

2024-05-12 22:10:26,897 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:10:40,199 - Epoch: [71][  100/  217]    Overall Loss 0.619695    Objective Loss 0.619695                                        LR 0.001000    Time 0.132695    
2024-05-12 22:10:46,127 - Epoch: [71][  200/  217]    Overall Loss 0.816150    Objective Loss 0.816150                                        LR 0.001000    Time 0.095896    
2024-05-12 22:10:47,250 - Epoch: [71][  217/  217]    Overall Loss 0.822164    Objective Loss 0.822164    Top1 81.967213    Top5 96.721311    LR 0.001000    Time 0.093536    
2024-05-12 22:10:47,656 - 

2024-05-12 22:10:47,657 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:11:00,427 - Epoch: [72][  100/  217]    Overall Loss 0.328482    Objective Loss 0.328482                                        LR 0.001000    Time 0.127406    
2024-05-12 22:11:08,621 - Epoch: [72][  200/  217]    Overall Loss 0.279402    Objective Loss 0.279402                                        LR 0.001000    Time 0.104545    
2024-05-12 22:11:09,739 - Epoch: [72][  217/  217]    Overall Loss 0.276938    Objective Loss 0.276938    Top1 95.081967    Top5 98.360656    LR 0.001000    Time 0.101487    
2024-05-12 22:11:10,383 - 

2024-05-12 22:11:10,385 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:11:19,440 - Epoch: [73][  100/  217]    Overall Loss 0.076364    Objective Loss 0.076364                                        LR 0.001000    Time 0.090312    
2024-05-12 22:11:27,181 - Epoch: [73][  200/  217]    Overall Loss 0.070822    Objective Loss 0.070822                                        LR 0.001000    Time 0.083732    
2024-05-12 22:11:28,651 - Epoch: [73][  217/  217]    Overall Loss 0.070859    Objective Loss 0.070859    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.083925    
2024-05-12 22:11:29,211 - 

2024-05-12 22:11:29,213 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:11:42,368 - Epoch: [74][  100/  217]    Overall Loss 0.030236    Objective Loss 0.030236                                        LR 0.001000    Time 0.131245    
2024-05-12 22:11:49,171 - Epoch: [74][  200/  217]    Overall Loss 0.027370    Objective Loss 0.027370                                        LR 0.001000    Time 0.099518    
2024-05-12 22:11:50,080 - Epoch: [74][  217/  217]    Overall Loss 0.027319    Objective Loss 0.027319    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.095885    
2024-05-12 22:11:50,486 - 

2024-05-12 22:11:50,487 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:12:04,241 - Epoch: [75][  100/  217]    Overall Loss 0.014902    Objective Loss 0.014902                                        LR 0.001000    Time 0.137219    
2024-05-12 22:12:12,434 - Epoch: [75][  200/  217]    Overall Loss 0.015181    Objective Loss 0.015181                                        LR 0.001000    Time 0.109428    
2024-05-12 22:12:13,774 - Epoch: [75][  217/  217]    Overall Loss 0.015113    Objective Loss 0.015113    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.107000    
2024-05-12 22:12:14,359 - 

2024-05-12 22:12:14,361 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:12:25,951 - Epoch: [76][  100/  217]    Overall Loss 0.011472    Objective Loss 0.011472                                        LR 0.001000    Time 0.115639    
2024-05-12 22:12:35,141 - Epoch: [76][  200/  217]    Overall Loss 0.012281    Objective Loss 0.012281                                        LR 0.001000    Time 0.103625    
2024-05-12 22:12:36,532 - Epoch: [76][  217/  217]    Overall Loss 0.012163    Objective Loss 0.012163    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.101881    
2024-05-12 22:12:37,121 - 

2024-05-12 22:12:37,122 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:12:49,601 - Epoch: [77][  100/  217]    Overall Loss 0.010359    Objective Loss 0.010359                                        LR 0.001000    Time 0.124509    
2024-05-12 22:12:58,128 - Epoch: [77][  200/  217]    Overall Loss 0.010423    Objective Loss 0.010423                                        LR 0.001000    Time 0.104752    
2024-05-12 22:12:59,343 - Epoch: [77][  217/  217]    Overall Loss 0.010392    Objective Loss 0.010392    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.102119    
2024-05-12 22:12:59,899 - 

2024-05-12 22:12:59,901 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:13:13,761 - Epoch: [78][  100/  217]    Overall Loss 0.007781    Objective Loss 0.007781                                        LR 0.001000    Time 0.138288    
2024-05-12 22:13:22,947 - Epoch: [78][  200/  217]    Overall Loss 0.009163    Objective Loss 0.009163                                        LR 0.001000    Time 0.114934    
2024-05-12 22:13:24,200 - Epoch: [78][  217/  217]    Overall Loss 0.008977    Objective Loss 0.008977    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.111678    
2024-05-12 22:13:24,659 - 

2024-05-12 22:13:24,660 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:13:34,077 - Epoch: [79][  100/  217]    Overall Loss 0.006531    Objective Loss 0.006531                                        LR 0.001000    Time 0.093886    
2024-05-12 22:13:43,559 - Epoch: [79][  200/  217]    Overall Loss 0.007606    Objective Loss 0.007606                                        LR 0.001000    Time 0.094215    
2024-05-12 22:13:44,876 - Epoch: [79][  217/  217]    Overall Loss 0.007400    Objective Loss 0.007400    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.092877    
2024-05-12 22:13:45,431 - 

2024-05-12 22:13:45,433 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:13:57,878 - Epoch: [80][  100/  217]    Overall Loss 0.004940    Objective Loss 0.004940                                        LR 0.001000    Time 0.124132    
2024-05-12 22:14:05,893 - Epoch: [80][  200/  217]    Overall Loss 0.006112    Objective Loss 0.006112                                        LR 0.001000    Time 0.101999    
2024-05-12 22:14:07,155 - Epoch: [80][  217/  217]    Overall Loss 0.006259    Objective Loss 0.006259    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.099800    
2024-05-12 22:14:07,623 - --- validate (epoch=80)-----------
2024-05-12 22:14:07,625 - 1736 samples (32 per mini-batch)
2024-05-12 22:14:13,408 - Epoch: [80][   55/   55]    Loss 2.792271    Top1 55.529954    Top5 72.580645    
2024-05-12 22:14:14,032 - ==> Top1: 55.530    Top5: 72.581    Loss: 2.792

2024-05-12 22:14:14,041 - ==> Best [Top1: 55.933   Top5: 71.429   Sparsity:0.00   Params: 382208 on epoch: 30]
2024-05-12 22:14:14,042 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:14:14,110 - 

2024-05-12 22:14:14,111 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:14:23,372 - Epoch: [81][  100/  217]    Overall Loss 0.004996    Objective Loss 0.004996                                        LR 0.001000    Time 0.092348    
2024-05-12 22:14:30,345 - Epoch: [81][  200/  217]    Overall Loss 0.005496    Objective Loss 0.005496                                        LR 0.001000    Time 0.080919    
2024-05-12 22:14:31,754 - Epoch: [81][  217/  217]    Overall Loss 0.005725    Objective Loss 0.005725    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.081045    
2024-05-12 22:14:32,364 - 

2024-05-12 22:14:32,369 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:14:45,945 - Epoch: [82][  100/  217]    Overall Loss 0.005639    Objective Loss 0.005639                                        LR 0.001000    Time 0.135422    
2024-05-12 22:14:53,604 - Epoch: [82][  200/  217]    Overall Loss 0.006972    Objective Loss 0.006972                                        LR 0.001000    Time 0.105885    
2024-05-12 22:14:54,495 - Epoch: [82][  217/  217]    Overall Loss 0.006876    Objective Loss 0.006876    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.101678    
2024-05-12 22:14:54,986 - 

2024-05-12 22:14:54,987 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:15:02,333 - Epoch: [83][  100/  217]    Overall Loss 0.005567    Objective Loss 0.005567                                        LR 0.001000    Time 0.073216    
2024-05-12 22:15:11,184 - Epoch: [83][  200/  217]    Overall Loss 0.005876    Objective Loss 0.005876                                        LR 0.001000    Time 0.080713    
2024-05-12 22:15:12,440 - Epoch: [83][  217/  217]    Overall Loss 0.005928    Objective Loss 0.005928    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.080148    
2024-05-12 22:15:12,982 - 

2024-05-12 22:15:12,986 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:15:25,601 - Epoch: [84][  100/  217]    Overall Loss 0.004808    Objective Loss 0.004808                                        LR 0.001000    Time 0.125835    
2024-05-12 22:15:31,854 - Epoch: [84][  200/  217]    Overall Loss 0.005414    Objective Loss 0.005414                                        LR 0.001000    Time 0.094076    
2024-05-12 22:15:32,973 - Epoch: [84][  217/  217]    Overall Loss 0.005233    Objective Loss 0.005233    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.091843    
2024-05-12 22:15:33,447 - 

2024-05-12 22:15:33,449 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:15:46,069 - Epoch: [85][  100/  217]    Overall Loss 0.005329    Objective Loss 0.005329                                        LR 0.001000    Time 0.125906    
2024-05-12 22:15:53,966 - Epoch: [85][  200/  217]    Overall Loss 0.005038    Objective Loss 0.005038                                        LR 0.001000    Time 0.102302    
2024-05-12 22:15:55,163 - Epoch: [85][  217/  217]    Overall Loss 0.005165    Objective Loss 0.005165    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.099778    
2024-05-12 22:15:55,827 - 

2024-05-12 22:15:55,828 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:16:09,342 - Epoch: [86][  100/  217]    Overall Loss 0.006111    Objective Loss 0.006111                                        LR 0.001000    Time 0.134821    
2024-05-12 22:16:17,322 - Epoch: [86][  200/  217]    Overall Loss 0.008304    Objective Loss 0.008304                                        LR 0.001000    Time 0.107180    
2024-05-12 22:16:18,316 - Epoch: [86][  217/  217]    Overall Loss 0.009312    Objective Loss 0.009312    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.103346    
2024-05-12 22:16:18,788 - 

2024-05-12 22:16:18,789 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:16:33,664 - Epoch: [87][  100/  217]    Overall Loss 0.370040    Objective Loss 0.370040                                        LR 0.001000    Time 0.148432    
2024-05-12 22:16:43,170 - Epoch: [87][  200/  217]    Overall Loss 0.549569    Objective Loss 0.549569                                        LR 0.001000    Time 0.121600    
2024-05-12 22:16:44,578 - Epoch: [87][  217/  217]    Overall Loss 0.549315    Objective Loss 0.549315    Top1 88.524590    Top5 95.081967    LR 0.001000    Time 0.118538    
2024-05-12 22:16:45,212 - 

2024-05-12 22:16:45,214 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:16:56,491 - Epoch: [88][  100/  217]    Overall Loss 0.208555    Objective Loss 0.208555                                        LR 0.001000    Time 0.112503    
2024-05-12 22:17:05,391 - Epoch: [88][  200/  217]    Overall Loss 0.187291    Objective Loss 0.187291                                        LR 0.001000    Time 0.100609    
2024-05-12 22:17:07,070 - Epoch: [88][  217/  217]    Overall Loss 0.184879    Objective Loss 0.184879    Top1 91.803279    Top5 100.000000    LR 0.001000    Time 0.100434    
2024-05-12 22:17:07,719 - 

2024-05-12 22:17:07,724 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:17:22,411 - Epoch: [89][  100/  217]    Overall Loss 0.058393    Objective Loss 0.058393                                        LR 0.001000    Time 0.146500    
2024-05-12 22:17:30,746 - Epoch: [89][  200/  217]    Overall Loss 0.053496    Objective Loss 0.053496                                        LR 0.001000    Time 0.114808    
2024-05-12 22:17:31,977 - Epoch: [89][  217/  217]    Overall Loss 0.053224    Objective Loss 0.053224    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.111461    
2024-05-12 22:17:32,463 - 

2024-05-12 22:17:32,465 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:17:45,144 - Epoch: [90][  100/  217]    Overall Loss 0.024430    Objective Loss 0.024430                                        LR 0.001000    Time 0.126493    
2024-05-12 22:17:55,357 - Epoch: [90][  200/  217]    Overall Loss 0.021875    Objective Loss 0.021875                                        LR 0.001000    Time 0.114153    
2024-05-12 22:17:56,842 - Epoch: [90][  217/  217]    Overall Loss 0.022389    Objective Loss 0.022389    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.112028    
2024-05-12 22:17:57,457 - --- validate (epoch=90)-----------
2024-05-12 22:17:57,459 - 1736 samples (32 per mini-batch)
2024-05-12 22:18:01,404 - Epoch: [90][   55/   55]    Loss 2.783199    Top1 55.875576    Top5 72.753456    
2024-05-12 22:18:01,718 - ==> Top1: 55.876    Top5: 72.753    Loss: 2.783

2024-05-12 22:18:01,721 - ==> Best [Top1: 55.933   Top5: 71.429   Sparsity:0.00   Params: 382208 on epoch: 30]
2024-05-12 22:18:01,721 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:18:01,750 - 

2024-05-12 22:18:01,751 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:18:12,454 - Epoch: [91][  100/  217]    Overall Loss 0.014060    Objective Loss 0.014060                                        LR 0.001000    Time 0.106756    
2024-05-12 22:18:21,546 - Epoch: [91][  200/  217]    Overall Loss 0.013404    Objective Loss 0.013404                                        LR 0.001000    Time 0.098698    
2024-05-12 22:18:22,788 - Epoch: [91][  217/  217]    Overall Loss 0.013399    Objective Loss 0.013399    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.096663    
2024-05-12 22:18:23,336 - 

2024-05-12 22:18:23,338 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:18:36,626 - Epoch: [92][  100/  217]    Overall Loss 0.008637    Objective Loss 0.008637                                        LR 0.001000    Time 0.132588    
2024-05-12 22:18:46,133 - Epoch: [92][  200/  217]    Overall Loss 0.008279    Objective Loss 0.008279                                        LR 0.001000    Time 0.113679    
2024-05-12 22:18:47,731 - Epoch: [92][  217/  217]    Overall Loss 0.008041    Objective Loss 0.008041    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.112101    
2024-05-12 22:18:48,351 - 

2024-05-12 22:18:48,355 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:19:03,446 - Epoch: [93][  100/  217]    Overall Loss 0.005523    Objective Loss 0.005523                                        LR 0.001000    Time 0.150546    
2024-05-12 22:19:13,454 - Epoch: [93][  200/  217]    Overall Loss 0.006175    Objective Loss 0.006175                                        LR 0.001000    Time 0.125163    
2024-05-12 22:19:15,241 - Epoch: [93][  217/  217]    Overall Loss 0.005980    Objective Loss 0.005980    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.123557    
2024-05-12 22:19:16,026 - 

2024-05-12 22:19:16,028 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:19:30,715 - Epoch: [94][  100/  217]    Overall Loss 0.004883    Objective Loss 0.004883                                        LR 0.001000    Time 0.146412    
2024-05-12 22:19:40,366 - Epoch: [94][  200/  217]    Overall Loss 0.005675    Objective Loss 0.005675                                        LR 0.001000    Time 0.121316    
2024-05-12 22:19:41,780 - Epoch: [94][  217/  217]    Overall Loss 0.005930    Objective Loss 0.005930    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.118299    
2024-05-12 22:19:42,421 - 

2024-05-12 22:19:42,423 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:19:55,982 - Epoch: [95][  100/  217]    Overall Loss 0.004210    Objective Loss 0.004210                                        LR 0.001000    Time 0.135299    
2024-05-12 22:20:05,337 - Epoch: [95][  200/  217]    Overall Loss 0.005047    Objective Loss 0.005047                                        LR 0.001000    Time 0.114288    
2024-05-12 22:20:06,937 - Epoch: [95][  217/  217]    Overall Loss 0.005134    Objective Loss 0.005134    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.112678    
2024-05-12 22:20:07,579 - 

2024-05-12 22:20:07,582 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:20:20,305 - Epoch: [96][  100/  217]    Overall Loss 0.004317    Objective Loss 0.004317                                        LR 0.001000    Time 0.126904    
2024-05-12 22:20:30,277 - Epoch: [96][  200/  217]    Overall Loss 0.004515    Objective Loss 0.004515                                        LR 0.001000    Time 0.113154    
2024-05-12 22:20:31,801 - Epoch: [96][  217/  217]    Overall Loss 0.004766    Objective Loss 0.004766    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.111279    
2024-05-12 22:20:32,404 - 

2024-05-12 22:20:32,408 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:20:44,831 - Epoch: [97][  100/  217]    Overall Loss 0.004530    Objective Loss 0.004530                                        LR 0.001000    Time 0.123966    
2024-05-12 22:20:52,634 - Epoch: [97][  200/  217]    Overall Loss 0.004251    Objective Loss 0.004251                                        LR 0.001000    Time 0.100869    
2024-05-12 22:20:54,091 - Epoch: [97][  217/  217]    Overall Loss 0.004669    Objective Loss 0.004669    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.099652    
2024-05-12 22:20:54,681 - 

2024-05-12 22:20:54,682 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:21:08,699 - Epoch: [98][  100/  217]    Overall Loss 0.003468    Objective Loss 0.003468                                        LR 0.001000    Time 0.139846    
2024-05-12 22:21:18,789 - Epoch: [98][  200/  217]    Overall Loss 0.003834    Objective Loss 0.003834                                        LR 0.001000    Time 0.120218    
2024-05-12 22:21:19,736 - Epoch: [98][  217/  217]    Overall Loss 0.004680    Objective Loss 0.004680    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.115141    
2024-05-12 22:21:20,090 - 

2024-05-12 22:21:20,091 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:21:31,684 - Epoch: [99][  100/  217]    Overall Loss 0.002971    Objective Loss 0.002971                                        LR 0.001000    Time 0.115603    
2024-05-12 22:21:39,654 - Epoch: [99][  200/  217]    Overall Loss 0.004445    Objective Loss 0.004445                                        LR 0.001000    Time 0.097529    
2024-05-12 22:21:40,941 - Epoch: [99][  217/  217]    Overall Loss 0.004338    Objective Loss 0.004338    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.095791    
2024-05-12 22:21:41,507 - 

2024-05-12 22:21:41,508 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:21:53,069 - Epoch: [100][  100/  217]    Overall Loss 0.004359    Objective Loss 0.004359                                        LR 0.000250    Time 0.115338    
2024-05-12 22:22:02,246 - Epoch: [100][  200/  217]    Overall Loss 0.003524    Objective Loss 0.003524                                        LR 0.000250    Time 0.103424    
2024-05-12 22:22:03,338 - Epoch: [100][  217/  217]    Overall Loss 0.003413    Objective Loss 0.003413    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.100335    
2024-05-12 22:22:03,774 - --- validate (epoch=100)-----------
2024-05-12 22:22:03,776 - 1736 samples (32 per mini-batch)
2024-05-12 22:22:09,121 - Epoch: [100][   55/   55]    Loss 2.791857    Top1 56.682028    Top5 73.559908    
2024-05-12 22:22:09,738 - ==> Top1: 56.682    Top5: 73.560    Loss: 2.792

2024-05-12 22:22:09,751 - ==> Best [Top1: 56.682   Top5: 73.560   Sparsity:0.00   Params: 382208 on epoch: 100]
2024-05-12 22:22:09,752 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:22:09,864 - 

2024-05-12 22:22:09,865 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:22:22,868 - Epoch: [101][  100/  217]    Overall Loss 0.003402    Objective Loss 0.003402                                        LR 0.000250    Time 0.129720    
2024-05-12 22:22:30,853 - Epoch: [101][  200/  217]    Overall Loss 0.003383    Objective Loss 0.003383                                        LR 0.000250    Time 0.104646    
2024-05-12 22:22:32,003 - Epoch: [101][  217/  217]    Overall Loss 0.003265    Objective Loss 0.003265    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.101720    
2024-05-12 22:22:32,532 - 

2024-05-12 22:22:32,535 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:22:42,914 - Epoch: [102][  100/  217]    Overall Loss 0.002822    Objective Loss 0.002822                                        LR 0.000250    Time 0.103520    
2024-05-12 22:22:52,402 - Epoch: [102][  200/  217]    Overall Loss 0.003289    Objective Loss 0.003289                                        LR 0.000250    Time 0.099051    
2024-05-12 22:22:53,927 - Epoch: [102][  217/  217]    Overall Loss 0.003185    Objective Loss 0.003185    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.098286    
2024-05-12 22:22:54,540 - 

2024-05-12 22:22:54,544 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:23:06,019 - Epoch: [103][  100/  217]    Overall Loss 0.003492    Objective Loss 0.003492                                        LR 0.000250    Time 0.114448    
2024-05-12 22:23:13,367 - Epoch: [103][  200/  217]    Overall Loss 0.003075    Objective Loss 0.003075                                        LR 0.000250    Time 0.093841    
2024-05-12 22:23:14,804 - Epoch: [103][  217/  217]    Overall Loss 0.002962    Objective Loss 0.002962    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.093085    
2024-05-12 22:23:15,394 - 

2024-05-12 22:23:15,396 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:23:26,292 - Epoch: [104][  100/  217]    Overall Loss 0.003137    Objective Loss 0.003137                                        LR 0.000250    Time 0.108684    
2024-05-12 22:23:34,970 - Epoch: [104][  200/  217]    Overall Loss 0.002842    Objective Loss 0.002842                                        LR 0.000250    Time 0.097581    
2024-05-12 22:23:36,070 - Epoch: [104][  217/  217]    Overall Loss 0.002887    Objective Loss 0.002887    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.094982    
2024-05-12 22:23:36,551 - 

2024-05-12 22:23:36,553 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:23:48,816 - Epoch: [105][  100/  217]    Overall Loss 0.002950    Objective Loss 0.002950                                        LR 0.000250    Time 0.122316    
2024-05-12 22:23:58,365 - Epoch: [105][  200/  217]    Overall Loss 0.002884    Objective Loss 0.002884                                        LR 0.000250    Time 0.108733    
2024-05-12 22:23:59,786 - Epoch: [105][  217/  217]    Overall Loss 0.002775    Objective Loss 0.002775    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.106734    
2024-05-12 22:24:00,305 - 

2024-05-12 22:24:00,306 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:24:11,057 - Epoch: [106][  100/  217]    Overall Loss 0.002113    Objective Loss 0.002113                                        LR 0.000250    Time 0.107237    
2024-05-12 22:24:19,769 - Epoch: [106][  200/  217]    Overall Loss 0.002669    Objective Loss 0.002669                                        LR 0.000250    Time 0.097036    
2024-05-12 22:24:21,218 - Epoch: [106][  217/  217]    Overall Loss 0.002953    Objective Loss 0.002953    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.096091    
2024-05-12 22:24:21,844 - 

2024-05-12 22:24:21,846 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:24:35,169 - Epoch: [107][  100/  217]    Overall Loss 0.002696    Objective Loss 0.002696                                        LR 0.000250    Time 0.132935    
2024-05-12 22:24:42,602 - Epoch: [107][  200/  217]    Overall Loss 0.002757    Objective Loss 0.002757                                        LR 0.000250    Time 0.103513    
2024-05-12 22:24:43,567 - Epoch: [107][  217/  217]    Overall Loss 0.002634    Objective Loss 0.002634    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.099829    
2024-05-12 22:24:43,918 - 

2024-05-12 22:24:43,919 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:24:56,853 - Epoch: [108][  100/  217]    Overall Loss 0.002229    Objective Loss 0.002229                                        LR 0.000250    Time 0.129020    
2024-05-12 22:25:06,008 - Epoch: [108][  200/  217]    Overall Loss 0.002665    Objective Loss 0.002665                                        LR 0.000250    Time 0.110141    
2024-05-12 22:25:07,485 - Epoch: [108][  217/  217]    Overall Loss 0.002539    Objective Loss 0.002539    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.108290    
2024-05-12 22:25:08,021 - 

2024-05-12 22:25:08,024 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:25:19,233 - Epoch: [109][  100/  217]    Overall Loss 0.003576    Objective Loss 0.003576                                        LR 0.000250    Time 0.111790    
2024-05-12 22:25:27,653 - Epoch: [109][  200/  217]    Overall Loss 0.003424    Objective Loss 0.003424                                        LR 0.000250    Time 0.097862    
2024-05-12 22:25:29,089 - Epoch: [109][  217/  217]    Overall Loss 0.003366    Objective Loss 0.003366    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.096741    
2024-05-12 22:25:29,702 - 

2024-05-12 22:25:29,704 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:25:39,812 - Epoch: [110][  100/  217]    Overall Loss 0.003137    Objective Loss 0.003137                                        LR 0.000250    Time 0.100864    
2024-05-12 22:25:46,755 - Epoch: [110][  200/  217]    Overall Loss 0.002511    Objective Loss 0.002511                                        LR 0.000250    Time 0.085033    
2024-05-12 22:25:48,142 - Epoch: [110][  217/  217]    Overall Loss 0.002556    Objective Loss 0.002556    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.084736    
2024-05-12 22:25:48,697 - --- validate (epoch=110)-----------
2024-05-12 22:25:48,699 - 1736 samples (32 per mini-batch)
2024-05-12 22:25:55,069 - Epoch: [110][   55/   55]    Loss 2.861530    Top1 56.278802    Top5 73.847926    
2024-05-12 22:25:55,690 - ==> Top1: 56.279    Top5: 73.848    Loss: 2.862

2024-05-12 22:25:55,703 - ==> Best [Top1: 56.682   Top5: 73.560   Sparsity:0.00   Params: 382208 on epoch: 100]
2024-05-12 22:25:55,704 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:25:55,775 - 

2024-05-12 22:25:55,777 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:26:08,706 - Epoch: [111][  100/  217]    Overall Loss 0.002456    Objective Loss 0.002456                                        LR 0.000250    Time 0.128968    
2024-05-12 22:26:15,620 - Epoch: [111][  200/  217]    Overall Loss 0.002808    Objective Loss 0.002808                                        LR 0.000250    Time 0.098940    
2024-05-12 22:26:16,838 - Epoch: [111][  217/  217]    Overall Loss 0.002688    Objective Loss 0.002688    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.096776    
2024-05-12 22:26:17,436 - 

2024-05-12 22:26:17,440 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:26:27,524 - Epoch: [112][  100/  217]    Overall Loss 0.003068    Objective Loss 0.003068                                        LR 0.000250    Time 0.100583    
2024-05-12 22:26:32,845 - Epoch: [112][  200/  217]    Overall Loss 0.002631    Objective Loss 0.002631                                        LR 0.000250    Time 0.076780    
2024-05-12 22:26:34,053 - Epoch: [112][  217/  217]    Overall Loss 0.002672    Objective Loss 0.002672    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.076305    
2024-05-12 22:26:34,530 - 

2024-05-12 22:26:34,532 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:26:45,661 - Epoch: [113][  100/  217]    Overall Loss 0.002644    Objective Loss 0.002644                                        LR 0.000250    Time 0.111022    
2024-05-12 22:26:53,484 - Epoch: [113][  200/  217]    Overall Loss 0.002242    Objective Loss 0.002242                                        LR 0.000250    Time 0.094496    
2024-05-12 22:26:54,631 - Epoch: [113][  217/  217]    Overall Loss 0.002518    Objective Loss 0.002518    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.092354    
2024-05-12 22:26:55,088 - 

2024-05-12 22:26:55,090 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:27:06,080 - Epoch: [114][  100/  217]    Overall Loss 0.002445    Objective Loss 0.002445                                        LR 0.000250    Time 0.109641    
2024-05-12 22:27:14,796 - Epoch: [114][  200/  217]    Overall Loss 0.002362    Objective Loss 0.002362                                        LR 0.000250    Time 0.098267    
2024-05-12 22:27:16,086 - Epoch: [114][  217/  217]    Overall Loss 0.002385    Objective Loss 0.002385    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.096488    
2024-05-12 22:27:16,594 - 

2024-05-12 22:27:16,596 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:27:30,188 - Epoch: [115][  100/  217]    Overall Loss 0.002765    Objective Loss 0.002765                                        LR 0.000250    Time 0.135630    
2024-05-12 22:27:39,641 - Epoch: [115][  200/  217]    Overall Loss 0.002817    Objective Loss 0.002817                                        LR 0.000250    Time 0.114931    
2024-05-12 22:27:41,241 - Epoch: [115][  217/  217]    Overall Loss 0.002733    Objective Loss 0.002733    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.113270    
2024-05-12 22:27:41,912 - 

2024-05-12 22:27:41,913 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:27:53,430 - Epoch: [116][  100/  217]    Overall Loss 0.001664    Objective Loss 0.001664                                        LR 0.000250    Time 0.114900    
2024-05-12 22:28:02,466 - Epoch: [116][  200/  217]    Overall Loss 0.002361    Objective Loss 0.002361                                        LR 0.000250    Time 0.102485    
2024-05-12 22:28:03,695 - Epoch: [116][  217/  217]    Overall Loss 0.002418    Objective Loss 0.002418    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.100096    
2024-05-12 22:28:04,207 - 

2024-05-12 22:28:04,211 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:28:19,036 - Epoch: [117][  100/  217]    Overall Loss 0.002054    Objective Loss 0.002054                                        LR 0.000250    Time 0.147867    
2024-05-12 22:28:27,013 - Epoch: [117][  200/  217]    Overall Loss 0.002266    Objective Loss 0.002266                                        LR 0.000250    Time 0.113693    
2024-05-12 22:28:28,294 - Epoch: [117][  217/  217]    Overall Loss 0.002272    Objective Loss 0.002272    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.110667    
2024-05-12 22:28:28,863 - 

2024-05-12 22:28:28,865 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:28:41,321 - Epoch: [118][  100/  217]    Overall Loss 0.001930    Objective Loss 0.001930                                        LR 0.000250    Time 0.124298    
2024-05-12 22:28:49,687 - Epoch: [118][  200/  217]    Overall Loss 0.002450    Objective Loss 0.002450                                        LR 0.000250    Time 0.103855    
2024-05-12 22:28:50,828 - Epoch: [118][  217/  217]    Overall Loss 0.002482    Objective Loss 0.002482    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.100954    
2024-05-12 22:28:51,272 - 

2024-05-12 22:28:51,273 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:29:02,959 - Epoch: [119][  100/  217]    Overall Loss 0.002929    Objective Loss 0.002929                                        LR 0.000250    Time 0.116565    
2024-05-12 22:29:10,956 - Epoch: [119][  200/  217]    Overall Loss 0.002178    Objective Loss 0.002178                                        LR 0.000250    Time 0.098138    
2024-05-12 22:29:12,471 - Epoch: [119][  217/  217]    Overall Loss 0.002416    Objective Loss 0.002416    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.097399    
2024-05-12 22:29:13,012 - 

2024-05-12 22:29:13,014 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:29:20,698 - Epoch: [120][  100/  217]    Overall Loss 0.001918    Objective Loss 0.001918                                        LR 0.000250    Time 0.076658    
2024-05-12 22:29:29,746 - Epoch: [120][  200/  217]    Overall Loss 0.002304    Objective Loss 0.002304                                        LR 0.000250    Time 0.083433    
2024-05-12 22:29:31,071 - Epoch: [120][  217/  217]    Overall Loss 0.002386    Objective Loss 0.002386    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.082978    
2024-05-12 22:29:31,521 - --- validate (epoch=120)-----------
2024-05-12 22:29:31,523 - 1736 samples (32 per mini-batch)
2024-05-12 22:29:35,448 - Epoch: [120][   55/   55]    Loss 2.861112    Top1 55.990783    Top5 73.329493    
2024-05-12 22:29:35,873 - ==> Top1: 55.991    Top5: 73.329    Loss: 2.861

2024-05-12 22:29:35,879 - ==> Best [Top1: 56.682   Top5: 73.560   Sparsity:0.00   Params: 382208 on epoch: 100]
2024-05-12 22:29:35,879 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:29:35,932 - 

2024-05-12 22:29:35,934 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:29:48,639 - Epoch: [121][  100/  217]    Overall Loss 0.002570    Objective Loss 0.002570                                        LR 0.000250    Time 0.126782    
2024-05-12 22:29:56,102 - Epoch: [121][  200/  217]    Overall Loss 0.002736    Objective Loss 0.002736                                        LR 0.000250    Time 0.100583    
2024-05-12 22:29:57,618 - Epoch: [121][  217/  217]    Overall Loss 0.003338    Objective Loss 0.003338    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.099659    
2024-05-12 22:29:58,248 - 

2024-05-12 22:29:58,251 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:30:10,036 - Epoch: [122][  100/  217]    Overall Loss 0.005657    Objective Loss 0.005657                                        LR 0.000250    Time 0.117594    
2024-05-12 22:30:18,400 - Epoch: [122][  200/  217]    Overall Loss 0.005380    Objective Loss 0.005380                                        LR 0.000250    Time 0.100489    
2024-05-12 22:30:19,512 - Epoch: [122][  217/  217]    Overall Loss 0.005304    Objective Loss 0.005304    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.097720    
2024-05-12 22:30:20,127 - 

2024-05-12 22:30:20,131 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:30:32,606 - Epoch: [123][  100/  217]    Overall Loss 0.002775    Objective Loss 0.002775                                        LR 0.000250    Time 0.124447    
2024-05-12 22:30:39,900 - Epoch: [123][  200/  217]    Overall Loss 0.002968    Objective Loss 0.002968                                        LR 0.000250    Time 0.098575    
2024-05-12 22:30:41,225 - Epoch: [123][  217/  217]    Overall Loss 0.003305    Objective Loss 0.003305    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.096926    
2024-05-12 22:30:41,770 - 

2024-05-12 22:30:41,774 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:30:52,455 - Epoch: [124][  100/  217]    Overall Loss 0.002135    Objective Loss 0.002135                                        LR 0.000250    Time 0.106537    
2024-05-12 22:31:01,834 - Epoch: [124][  200/  217]    Overall Loss 0.002560    Objective Loss 0.002560                                        LR 0.000250    Time 0.100030    
2024-05-12 22:31:03,010 - Epoch: [124][  217/  217]    Overall Loss 0.002431    Objective Loss 0.002431    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.097590    
2024-05-12 22:31:03,423 - 

2024-05-12 22:31:03,425 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:31:14,038 - Epoch: [125][  100/  217]    Overall Loss 0.002017    Objective Loss 0.002017                                        LR 0.000250    Time 0.105849    
2024-05-12 22:31:22,156 - Epoch: [125][  200/  217]    Overall Loss 0.002102    Objective Loss 0.002102                                        LR 0.000250    Time 0.093390    
2024-05-12 22:31:23,024 - Epoch: [125][  217/  217]    Overall Loss 0.002265    Objective Loss 0.002265    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.090056    
2024-05-12 22:31:23,486 - 

2024-05-12 22:31:23,488 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:31:35,503 - Epoch: [126][  100/  217]    Overall Loss 0.001752    Objective Loss 0.001752                                        LR 0.000250    Time 0.119867    
2024-05-12 22:31:43,043 - Epoch: [126][  200/  217]    Overall Loss 0.002255    Objective Loss 0.002255                                        LR 0.000250    Time 0.097495    
2024-05-12 22:31:44,281 - Epoch: [126][  217/  217]    Overall Loss 0.002242    Objective Loss 0.002242    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.095540    
2024-05-12 22:31:44,883 - 

2024-05-12 22:31:44,885 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:31:56,833 - Epoch: [127][  100/  217]    Overall Loss 0.001789    Objective Loss 0.001789                                        LR 0.000250    Time 0.119178    
2024-05-12 22:32:04,948 - Epoch: [127][  200/  217]    Overall Loss 0.001806    Objective Loss 0.001806                                        LR 0.000250    Time 0.100019    
2024-05-12 22:32:06,376 - Epoch: [127][  217/  217]    Overall Loss 0.002014    Objective Loss 0.002014    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.098735    
2024-05-12 22:32:06,864 - 

2024-05-12 22:32:06,866 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:32:18,486 - Epoch: [128][  100/  217]    Overall Loss 0.001695    Objective Loss 0.001695                                        LR 0.000250    Time 0.115910    
2024-05-12 22:32:26,618 - Epoch: [128][  200/  217]    Overall Loss 0.002028    Objective Loss 0.002028                                        LR 0.000250    Time 0.098480    
2024-05-12 22:32:27,978 - Epoch: [128][  217/  217]    Overall Loss 0.001921    Objective Loss 0.001921    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.097011    
2024-05-12 22:32:28,476 - 

2024-05-12 22:32:28,478 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:32:36,934 - Epoch: [129][  100/  217]    Overall Loss 0.002197    Objective Loss 0.002197                                        LR 0.000250    Time 0.084363    
2024-05-12 22:32:42,534 - Epoch: [129][  200/  217]    Overall Loss 0.002129    Objective Loss 0.002129                                        LR 0.000250    Time 0.070092    
2024-05-12 22:32:43,837 - Epoch: [129][  217/  217]    Overall Loss 0.002148    Objective Loss 0.002148    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.070581    
2024-05-12 22:32:44,447 - 

2024-05-12 22:32:44,448 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:32:55,425 - Epoch: [130][  100/  217]    Overall Loss 0.001777    Objective Loss 0.001777                                        LR 0.000250    Time 0.109491    
2024-05-12 22:33:02,994 - Epoch: [130][  200/  217]    Overall Loss 0.002046    Objective Loss 0.002046                                        LR 0.000250    Time 0.092468    
2024-05-12 22:33:04,223 - Epoch: [130][  217/  217]    Overall Loss 0.002103    Objective Loss 0.002103    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.090863    
2024-05-12 22:33:04,718 - --- validate (epoch=130)-----------
2024-05-12 22:33:04,720 - 1736 samples (32 per mini-batch)
2024-05-12 22:33:11,179 - Epoch: [130][   55/   55]    Loss 3.002011    Top1 56.105991    Top5 72.753456    
2024-05-12 22:33:11,670 - ==> Top1: 56.106    Top5: 72.753    Loss: 3.002

2024-05-12 22:33:11,681 - ==> Best [Top1: 56.682   Top5: 73.560   Sparsity:0.00   Params: 382208 on epoch: 100]
2024-05-12 22:33:11,682 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:33:11,755 - 

2024-05-12 22:33:11,756 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:33:23,269 - Epoch: [131][  100/  217]    Overall Loss 0.002024    Objective Loss 0.002024                                        LR 0.000250    Time 0.114839    
2024-05-12 22:33:30,763 - Epoch: [131][  200/  217]    Overall Loss 0.001963    Objective Loss 0.001963                                        LR 0.000250    Time 0.094775    
2024-05-12 22:33:31,875 - Epoch: [131][  217/  217]    Overall Loss 0.002138    Objective Loss 0.002138    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.092450    
2024-05-12 22:33:32,420 - 

2024-05-12 22:33:32,422 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:33:43,610 - Epoch: [132][  100/  217]    Overall Loss 0.001605    Objective Loss 0.001605                                        LR 0.000250    Time 0.111612    
2024-05-12 22:33:52,824 - Epoch: [132][  200/  217]    Overall Loss 0.002210    Objective Loss 0.002210                                        LR 0.000250    Time 0.101735    
2024-05-12 22:33:54,249 - Epoch: [132][  217/  217]    Overall Loss 0.002137    Objective Loss 0.002137    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.100310    
2024-05-12 22:33:54,880 - 

2024-05-12 22:33:54,881 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:34:06,976 - Epoch: [133][  100/  217]    Overall Loss 0.003450    Objective Loss 0.003450                                        LR 0.000250    Time 0.120635    
2024-05-12 22:34:14,932 - Epoch: [133][  200/  217]    Overall Loss 0.002417    Objective Loss 0.002417                                        LR 0.000250    Time 0.099951    
2024-05-12 22:34:15,995 - Epoch: [133][  217/  217]    Overall Loss 0.002281    Objective Loss 0.002281    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.096998    
2024-05-12 22:34:16,473 - 

2024-05-12 22:34:16,474 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:34:28,317 - Epoch: [134][  100/  217]    Overall Loss 0.001631    Objective Loss 0.001631                                        LR 0.000250    Time 0.118161    
2024-05-12 22:34:36,279 - Epoch: [134][  200/  217]    Overall Loss 0.002313    Objective Loss 0.002313                                        LR 0.000250    Time 0.098767    
2024-05-12 22:34:37,654 - Epoch: [134][  217/  217]    Overall Loss 0.002180    Objective Loss 0.002180    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.097340    
2024-05-12 22:34:38,167 - 

2024-05-12 22:34:38,169 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:34:49,309 - Epoch: [135][  100/  217]    Overall Loss 0.000811    Objective Loss 0.000811                                        LR 0.000250    Time 0.111154    
2024-05-12 22:34:57,365 - Epoch: [135][  200/  217]    Overall Loss 0.001766    Objective Loss 0.001766                                        LR 0.000250    Time 0.095731    
2024-05-12 22:34:58,577 - Epoch: [135][  217/  217]    Overall Loss 0.001824    Objective Loss 0.001824    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.093795    
2024-05-12 22:34:59,030 - 

2024-05-12 22:34:59,032 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:35:10,680 - Epoch: [136][  100/  217]    Overall Loss 0.001872    Objective Loss 0.001872                                        LR 0.000250    Time 0.116200    
2024-05-12 22:35:17,686 - Epoch: [136][  200/  217]    Overall Loss 0.001831    Objective Loss 0.001831                                        LR 0.000250    Time 0.093005    
2024-05-12 22:35:19,190 - Epoch: [136][  217/  217]    Overall Loss 0.001848    Objective Loss 0.001848    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.092625    
2024-05-12 22:35:19,762 - 

2024-05-12 22:35:19,764 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:35:30,181 - Epoch: [137][  100/  217]    Overall Loss 0.002309    Objective Loss 0.002309                                        LR 0.000250    Time 0.103918    
2024-05-12 22:35:38,802 - Epoch: [137][  200/  217]    Overall Loss 0.001681    Objective Loss 0.001681                                        LR 0.000250    Time 0.094935    
2024-05-12 22:35:40,044 - Epoch: [137][  217/  217]    Overall Loss 0.001764    Objective Loss 0.001764    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.093195    
2024-05-12 22:35:40,501 - 

2024-05-12 22:35:40,502 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:35:51,484 - Epoch: [138][  100/  217]    Overall Loss 0.000590    Objective Loss 0.000590                                        LR 0.000250    Time 0.109556    
2024-05-12 22:35:58,859 - Epoch: [138][  200/  217]    Overall Loss 0.001798    Objective Loss 0.001798                                        LR 0.000250    Time 0.091530    
2024-05-12 22:36:00,236 - Epoch: [138][  217/  217]    Overall Loss 0.001690    Objective Loss 0.001690    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.090682    
2024-05-12 22:36:00,762 - 

2024-05-12 22:36:00,765 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:36:08,874 - Epoch: [139][  100/  217]    Overall Loss 0.001284    Objective Loss 0.001284                                        LR 0.000250    Time 0.080866    
2024-05-12 22:36:17,258 - Epoch: [139][  200/  217]    Overall Loss 0.003477    Objective Loss 0.003477                                        LR 0.000250    Time 0.082226    
2024-05-12 22:36:18,459 - Epoch: [139][  217/  217]    Overall Loss 0.004082    Objective Loss 0.004082    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.081298    
2024-05-12 22:36:18,838 - 

2024-05-12 22:36:18,839 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:36:31,024 - Epoch: [140][  100/  217]    Overall Loss 0.006237    Objective Loss 0.006237                                        LR 0.000250    Time 0.121560    
2024-05-12 22:36:37,919 - Epoch: [140][  200/  217]    Overall Loss 0.005706    Objective Loss 0.005706                                        LR 0.000250    Time 0.095145    
2024-05-12 22:36:39,289 - Epoch: [140][  217/  217]    Overall Loss 0.006391    Objective Loss 0.006391    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.093978    
2024-05-12 22:36:39,832 - --- validate (epoch=140)-----------
2024-05-12 22:36:39,834 - 1736 samples (32 per mini-batch)
2024-05-12 22:36:47,230 - Epoch: [140][   55/   55]    Loss 3.205448    Top1 54.608295    Top5 71.774194    
2024-05-12 22:36:47,583 - ==> Top1: 54.608    Top5: 71.774    Loss: 3.205

2024-05-12 22:36:47,592 - ==> Best [Top1: 56.682   Top5: 73.560   Sparsity:0.00   Params: 382208 on epoch: 100]
2024-05-12 22:36:47,593 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:36:47,644 - 

2024-05-12 22:36:47,645 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:36:59,357 - Epoch: [141][  100/  217]    Overall Loss 0.004806    Objective Loss 0.004806                                        LR 0.000250    Time 0.116841    
2024-05-12 22:37:06,306 - Epoch: [141][  200/  217]    Overall Loss 0.004037    Objective Loss 0.004037                                        LR 0.000250    Time 0.093054    
2024-05-12 22:37:07,336 - Epoch: [141][  217/  217]    Overall Loss 0.004120    Objective Loss 0.004120    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.090489    
2024-05-12 22:37:07,790 - 

2024-05-12 22:37:07,791 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:37:18,929 - Epoch: [142][  100/  217]    Overall Loss 0.002008    Objective Loss 0.002008                                        LR 0.000250    Time 0.111132    
2024-05-12 22:37:27,505 - Epoch: [142][  200/  217]    Overall Loss 0.002200    Objective Loss 0.002200                                        LR 0.000250    Time 0.098316    
2024-05-12 22:37:28,612 - Epoch: [142][  217/  217]    Overall Loss 0.002205    Objective Loss 0.002205    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.095693    
2024-05-12 22:37:29,036 - 

2024-05-12 22:37:29,036 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:37:39,679 - Epoch: [143][  100/  217]    Overall Loss 0.002847    Objective Loss 0.002847                                        LR 0.000250    Time 0.106132    
2024-05-12 22:37:49,193 - Epoch: [143][  200/  217]    Overall Loss 0.002353    Objective Loss 0.002353                                        LR 0.000250    Time 0.100499    
2024-05-12 22:37:50,093 - Epoch: [143][  217/  217]    Overall Loss 0.002230    Objective Loss 0.002230    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.096755    
2024-05-12 22:37:50,474 - 

2024-05-12 22:37:50,475 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:38:02,801 - Epoch: [144][  100/  217]    Overall Loss 0.001868    Objective Loss 0.001868                                        LR 0.000250    Time 0.122979    
2024-05-12 22:38:09,730 - Epoch: [144][  200/  217]    Overall Loss 0.001724    Objective Loss 0.001724                                        LR 0.000250    Time 0.096017    
2024-05-12 22:38:11,194 - Epoch: [144][  217/  217]    Overall Loss 0.001877    Objective Loss 0.001877    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.095218    
2024-05-12 22:38:11,712 - 

2024-05-12 22:38:11,714 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:38:22,088 - Epoch: [145][  100/  217]    Overall Loss 0.002589    Objective Loss 0.002589                                        LR 0.000250    Time 0.103480    
2024-05-12 22:38:30,247 - Epoch: [145][  200/  217]    Overall Loss 0.001992    Objective Loss 0.001992                                        LR 0.000250    Time 0.092418    
2024-05-12 22:38:31,167 - Epoch: [145][  217/  217]    Overall Loss 0.001868    Objective Loss 0.001868    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.089397    
2024-05-12 22:38:31,711 - 

2024-05-12 22:38:31,713 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:38:43,266 - Epoch: [146][  100/  217]    Overall Loss 0.001193    Objective Loss 0.001193                                        LR 0.000250    Time 0.115270    
2024-05-12 22:38:50,505 - Epoch: [146][  200/  217]    Overall Loss 0.001464    Objective Loss 0.001464                                        LR 0.000250    Time 0.093716    
2024-05-12 22:38:51,791 - Epoch: [146][  217/  217]    Overall Loss 0.001639    Objective Loss 0.001639    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.092276    
2024-05-12 22:38:52,381 - 

2024-05-12 22:38:52,383 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:39:03,298 - Epoch: [147][  100/  217]    Overall Loss 0.001602    Objective Loss 0.001602                                        LR 0.000250    Time 0.108898    
2024-05-12 22:39:11,533 - Epoch: [147][  200/  217]    Overall Loss 0.001457    Objective Loss 0.001457                                        LR 0.000250    Time 0.095497    
2024-05-12 22:39:12,534 - Epoch: [147][  217/  217]    Overall Loss 0.001666    Objective Loss 0.001666    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.092612    
2024-05-12 22:39:12,740 - 

2024-05-12 22:39:12,740 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:39:20,574 - Epoch: [148][  100/  217]    Overall Loss 0.001418    Objective Loss 0.001418                                        LR 0.000250    Time 0.078111    
2024-05-12 22:39:29,138 - Epoch: [148][  200/  217]    Overall Loss 0.001691    Objective Loss 0.001691                                        LR 0.000250    Time 0.081729    
2024-05-12 22:39:30,448 - Epoch: [148][  217/  217]    Overall Loss 0.001709    Objective Loss 0.001709    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.081334    
2024-05-12 22:39:31,205 - 

2024-05-12 22:39:31,208 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:39:43,990 - Epoch: [149][  100/  217]    Overall Loss 0.002229    Objective Loss 0.002229                                        LR 0.000250    Time 0.127524    
2024-05-12 22:39:53,126 - Epoch: [149][  200/  217]    Overall Loss 0.001718    Objective Loss 0.001718                                        LR 0.000250    Time 0.109304    
2024-05-12 22:39:54,546 - Epoch: [149][  217/  217]    Overall Loss 0.001625    Objective Loss 0.001625    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.107256    
2024-05-12 22:39:55,143 - 

2024-05-12 22:39:55,147 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:40:07,281 - Epoch: [150][  100/  217]    Overall Loss 0.001482    Objective Loss 0.001482                                        LR 0.000063    Time 0.121034    
2024-05-12 22:40:15,705 - Epoch: [150][  200/  217]    Overall Loss 0.001610    Objective Loss 0.001610                                        LR 0.000063    Time 0.102506    
2024-05-12 22:40:16,786 - Epoch: [150][  217/  217]    Overall Loss 0.001504    Objective Loss 0.001504    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.099432    
2024-05-12 22:40:17,352 - --- validate (epoch=150)-----------
2024-05-12 22:40:17,355 - 1736 samples (32 per mini-batch)
2024-05-12 22:40:23,566 - Epoch: [150][   55/   55]    Loss 3.162135    Top1 57.085253    Top5 72.926267    
2024-05-12 22:40:24,146 - ==> Top1: 57.085    Top5: 72.926    Loss: 3.162

2024-05-12 22:40:24,158 - ==> Best [Top1: 57.085   Top5: 72.926   Sparsity:0.00   Params: 382208 on epoch: 150]
2024-05-12 22:40:24,159 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:40:24,278 - 

2024-05-12 22:40:24,280 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:40:35,907 - Epoch: [151][  100/  217]    Overall Loss 0.001498    Objective Loss 0.001498                                        LR 0.000063    Time 0.115972    
2024-05-12 22:40:43,132 - Epoch: [151][  200/  217]    Overall Loss 0.001475    Objective Loss 0.001475                                        LR 0.000063    Time 0.093992    
2024-05-12 22:40:44,279 - Epoch: [151][  217/  217]    Overall Loss 0.001476    Objective Loss 0.001476    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.091888    
2024-05-12 22:40:44,775 - 

2024-05-12 22:40:44,777 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:40:56,775 - Epoch: [152][  100/  217]    Overall Loss 0.001121    Objective Loss 0.001121                                        LR 0.000063    Time 0.119736    
2024-05-12 22:41:05,935 - Epoch: [152][  200/  217]    Overall Loss 0.001413    Objective Loss 0.001413                                        LR 0.000063    Time 0.105542    
2024-05-12 22:41:07,229 - Epoch: [152][  217/  217]    Overall Loss 0.001433    Objective Loss 0.001433    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.103215    
2024-05-12 22:41:07,780 - 

2024-05-12 22:41:07,782 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:41:19,650 - Epoch: [153][  100/  217]    Overall Loss 0.001774    Objective Loss 0.001774                                        LR 0.000063    Time 0.118399    
2024-05-12 22:41:27,009 - Epoch: [153][  200/  217]    Overall Loss 0.001367    Objective Loss 0.001367                                        LR 0.000063    Time 0.095877    
2024-05-12 22:41:28,255 - Epoch: [153][  217/  217]    Overall Loss 0.001397    Objective Loss 0.001397    Top1 98.360656    Top5 100.000000    LR 0.000063    Time 0.094083    
2024-05-12 22:41:28,735 - 

2024-05-12 22:41:28,736 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:41:40,133 - Epoch: [154][  100/  217]    Overall Loss 0.001540    Objective Loss 0.001540                                        LR 0.000063    Time 0.113712    
2024-05-12 22:41:49,354 - Epoch: [154][  200/  217]    Overall Loss 0.001423    Objective Loss 0.001423                                        LR 0.000063    Time 0.102834    
2024-05-12 22:41:50,541 - Epoch: [154][  217/  217]    Overall Loss 0.001445    Objective Loss 0.001445    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.100199    
2024-05-12 22:41:50,996 - 

2024-05-12 22:41:50,997 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:42:03,150 - Epoch: [155][  100/  217]    Overall Loss 0.001410    Objective Loss 0.001410                                        LR 0.000063    Time 0.121258    
2024-05-12 22:42:10,334 - Epoch: [155][  200/  217]    Overall Loss 0.001168    Objective Loss 0.001168                                        LR 0.000063    Time 0.096431    
2024-05-12 22:42:11,660 - Epoch: [155][  217/  217]    Overall Loss 0.001342    Objective Loss 0.001342    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.094957    
2024-05-12 22:42:12,193 - 

2024-05-12 22:42:12,196 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:42:26,204 - Epoch: [156][  100/  217]    Overall Loss 0.001106    Objective Loss 0.001106                                        LR 0.000063    Time 0.139715    
2024-05-12 22:42:35,100 - Epoch: [156][  200/  217]    Overall Loss 0.001464    Objective Loss 0.001464                                        LR 0.000063    Time 0.114202    
2024-05-12 22:42:36,408 - Epoch: [156][  217/  217]    Overall Loss 0.001370    Objective Loss 0.001370    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.111257    
2024-05-12 22:42:36,857 - 

2024-05-12 22:42:36,859 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:42:44,540 - Epoch: [157][  100/  217]    Overall Loss 0.000910    Objective Loss 0.000910                                        LR 0.000063    Time 0.076569    
2024-05-12 22:42:53,435 - Epoch: [157][  200/  217]    Overall Loss 0.001166    Objective Loss 0.001166                                        LR 0.000063    Time 0.082624    
2024-05-12 22:42:54,476 - Epoch: [157][  217/  217]    Overall Loss 0.001342    Objective Loss 0.001342    Top1 98.360656    Top5 100.000000    LR 0.000063    Time 0.080927    
2024-05-12 22:42:54,861 - 

2024-05-12 22:42:54,862 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:43:06,737 - Epoch: [158][  100/  217]    Overall Loss 0.001533    Objective Loss 0.001533                                        LR 0.000063    Time 0.118473    
2024-05-12 22:43:13,268 - Epoch: [158][  200/  217]    Overall Loss 0.001372    Objective Loss 0.001372                                        LR 0.000063    Time 0.091778    
2024-05-12 22:43:14,524 - Epoch: [158][  217/  217]    Overall Loss 0.001283    Objective Loss 0.001283    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.090348    
2024-05-12 22:43:15,020 - 

2024-05-12 22:43:15,023 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:43:24,812 - Epoch: [159][  100/  217]    Overall Loss 0.000703    Objective Loss 0.000703                                        LR 0.000063    Time 0.097623    
2024-05-12 22:43:33,054 - Epoch: [159][  200/  217]    Overall Loss 0.001320    Objective Loss 0.001320                                        LR 0.000063    Time 0.089895    
2024-05-12 22:43:33,956 - Epoch: [159][  217/  217]    Overall Loss 0.001375    Objective Loss 0.001375    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.086990    
2024-05-12 22:43:34,302 - 

2024-05-12 22:43:34,304 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:43:43,371 - Epoch: [160][  100/  217]    Overall Loss 0.001323    Objective Loss 0.001323                                        LR 0.000063    Time 0.090491    
2024-05-12 22:43:51,244 - Epoch: [160][  200/  217]    Overall Loss 0.001383    Objective Loss 0.001383                                        LR 0.000063    Time 0.084495    
2024-05-12 22:43:52,586 - Epoch: [160][  217/  217]    Overall Loss 0.001294    Objective Loss 0.001294    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.084036    
2024-05-12 22:43:53,135 - --- validate (epoch=160)-----------
2024-05-12 22:43:53,137 - 1736 samples (32 per mini-batch)
2024-05-12 22:43:57,501 - Epoch: [160][   55/   55]    Loss 3.098314    Top1 57.027650    Top5 72.753456    
2024-05-12 22:43:57,980 - ==> Top1: 57.028    Top5: 72.753    Loss: 3.098

2024-05-12 22:43:57,988 - ==> Best [Top1: 57.085   Top5: 72.926   Sparsity:0.00   Params: 382208 on epoch: 150]
2024-05-12 22:43:57,989 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:43:58,056 - 

2024-05-12 22:43:58,057 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:44:08,220 - Epoch: [161][  100/  217]    Overall Loss 0.001404    Objective Loss 0.001404                                        LR 0.000063    Time 0.101394    
2024-05-12 22:44:15,284 - Epoch: [161][  200/  217]    Overall Loss 0.001065    Objective Loss 0.001065                                        LR 0.000063    Time 0.085902    
2024-05-12 22:44:16,445 - Epoch: [161][  217/  217]    Overall Loss 0.001342    Objective Loss 0.001342    Top1 98.360656    Top5 100.000000    LR 0.000063    Time 0.084497    
2024-05-12 22:44:16,935 - 

2024-05-12 22:44:16,936 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:44:27,532 - Epoch: [162][  100/  217]    Overall Loss 0.001433    Objective Loss 0.001433                                        LR 0.000063    Time 0.105681    
2024-05-12 22:44:34,570 - Epoch: [162][  200/  217]    Overall Loss 0.001208    Objective Loss 0.001208                                        LR 0.000063    Time 0.087912    
2024-05-12 22:44:35,602 - Epoch: [162][  217/  217]    Overall Loss 0.001349    Objective Loss 0.001349    Top1 98.360656    Top5 100.000000    LR 0.000063    Time 0.085757    
2024-05-12 22:44:36,240 - 

2024-05-12 22:44:36,242 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:44:48,541 - Epoch: [163][  100/  217]    Overall Loss 0.001105    Objective Loss 0.001105                                        LR 0.000063    Time 0.122734    
2024-05-12 22:44:56,078 - Epoch: [163][  200/  217]    Overall Loss 0.001237    Objective Loss 0.001237                                        LR 0.000063    Time 0.098933    
2024-05-12 22:44:57,436 - Epoch: [163][  217/  217]    Overall Loss 0.001269    Objective Loss 0.001269    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.097407    
2024-05-12 22:44:57,971 - 

2024-05-12 22:44:57,973 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:45:08,867 - Epoch: [164][  100/  217]    Overall Loss 0.000920    Objective Loss 0.000920                                        LR 0.000063    Time 0.108658    
2024-05-12 22:45:17,994 - Epoch: [164][  200/  217]    Overall Loss 0.001265    Objective Loss 0.001265                                        LR 0.000063    Time 0.099828    
2024-05-12 22:45:19,179 - Epoch: [164][  217/  217]    Overall Loss 0.001320    Objective Loss 0.001320    Top1 98.360656    Top5 100.000000    LR 0.000063    Time 0.097444    
2024-05-12 22:45:19,645 - 

2024-05-12 22:45:19,646 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:45:32,580 - Epoch: [165][  100/  217]    Overall Loss 0.001069    Objective Loss 0.001069                                        LR 0.000063    Time 0.129046    
2024-05-12 22:45:42,064 - Epoch: [165][  200/  217]    Overall Loss 0.001122    Objective Loss 0.001122                                        LR 0.000063    Time 0.111813    
2024-05-12 22:45:43,373 - Epoch: [165][  217/  217]    Overall Loss 0.001393    Objective Loss 0.001393    Top1 98.360656    Top5 100.000000    LR 0.000063    Time 0.109057    
2024-05-12 22:45:43,920 - 

2024-05-12 22:45:43,921 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:45:55,139 - Epoch: [166][  100/  217]    Overall Loss 0.001949    Objective Loss 0.001949                                        LR 0.000063    Time 0.111897    
2024-05-12 22:46:03,989 - Epoch: [166][  200/  217]    Overall Loss 0.001456    Objective Loss 0.001456                                        LR 0.000063    Time 0.100070    
2024-05-12 22:46:05,147 - Epoch: [166][  217/  217]    Overall Loss 0.001358    Objective Loss 0.001358    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.097543    
2024-05-12 22:46:05,619 - 

2024-05-12 22:46:05,621 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:46:18,724 - Epoch: [167][  100/  217]    Overall Loss 0.000905    Objective Loss 0.000905                                        LR 0.000063    Time 0.130735    
2024-05-12 22:46:27,868 - Epoch: [167][  200/  217]    Overall Loss 0.001123    Objective Loss 0.001123                                        LR 0.000063    Time 0.110950    
2024-05-12 22:46:29,244 - Epoch: [167][  217/  217]    Overall Loss 0.001322    Objective Loss 0.001322    Top1 98.360656    Top5 100.000000    LR 0.000063    Time 0.108571    
2024-05-12 22:46:29,773 - 

2024-05-12 22:46:29,774 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:46:41,647 - Epoch: [168][  100/  217]    Overall Loss 0.001622    Objective Loss 0.001622                                        LR 0.000063    Time 0.118447    
2024-05-12 22:46:50,694 - Epoch: [168][  200/  217]    Overall Loss 0.001276    Objective Loss 0.001276                                        LR 0.000063    Time 0.104327    
2024-05-12 22:46:52,207 - Epoch: [168][  217/  217]    Overall Loss 0.001306    Objective Loss 0.001306    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.103101    
2024-05-12 22:46:53,067 - 

2024-05-12 22:46:53,070 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:47:05,494 - Epoch: [169][  100/  217]    Overall Loss 0.001124    Objective Loss 0.001124                                        LR 0.000063    Time 0.123960    
2024-05-12 22:47:12,498 - Epoch: [169][  200/  217]    Overall Loss 0.001244    Objective Loss 0.001244                                        LR 0.000063    Time 0.096905    
2024-05-12 22:47:13,998 - Epoch: [169][  217/  217]    Overall Loss 0.001288    Objective Loss 0.001288    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.096196    
2024-05-12 22:47:14,594 - 

2024-05-12 22:47:14,598 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:47:28,152 - Epoch: [170][  100/  217]    Overall Loss 0.001557    Objective Loss 0.001557                                        LR 0.000063    Time 0.135222    
2024-05-12 22:47:36,973 - Epoch: [170][  200/  217]    Overall Loss 0.001391    Objective Loss 0.001391                                        LR 0.000063    Time 0.111585    
2024-05-12 22:47:38,405 - Epoch: [170][  217/  217]    Overall Loss 0.001296    Objective Loss 0.001296    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.109412    
2024-05-12 22:47:39,053 - --- validate (epoch=170)-----------
2024-05-12 22:47:39,057 - 1736 samples (32 per mini-batch)
2024-05-12 22:47:45,496 - Epoch: [170][   55/   55]    Loss 3.175097    Top1 56.624424    Top5 72.177419    
2024-05-12 22:47:46,153 - ==> Top1: 56.624    Top5: 72.177    Loss: 3.175

2024-05-12 22:47:46,166 - ==> Best [Top1: 57.085   Top5: 72.926   Sparsity:0.00   Params: 382208 on epoch: 150]
2024-05-12 22:47:46,167 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:47:46,257 - 

2024-05-12 22:47:46,258 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:47:58,963 - Epoch: [171][  100/  217]    Overall Loss 0.000960    Objective Loss 0.000960                                        LR 0.000063    Time 0.126759    
2024-05-12 22:48:07,579 - Epoch: [171][  200/  217]    Overall Loss 0.001269    Objective Loss 0.001269                                        LR 0.000063    Time 0.106331    
2024-05-12 22:48:09,098 - Epoch: [171][  217/  217]    Overall Loss 0.001292    Objective Loss 0.001292    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.104973    
2024-05-12 22:48:09,786 - 

2024-05-12 22:48:09,788 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:48:25,513 - Epoch: [172][  100/  217]    Overall Loss 0.001250    Objective Loss 0.001250                                        LR 0.000063    Time 0.156885    
2024-05-12 22:48:34,734 - Epoch: [172][  200/  217]    Overall Loss 0.001310    Objective Loss 0.001310                                        LR 0.000063    Time 0.124413    
2024-05-12 22:48:35,948 - Epoch: [172][  217/  217]    Overall Loss 0.001222    Objective Loss 0.001222    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.120236    
2024-05-12 22:48:36,437 - 

2024-05-12 22:48:36,438 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:48:48,749 - Epoch: [173][  100/  217]    Overall Loss 0.000884    Objective Loss 0.000884                                        LR 0.000063    Time 0.122845    
2024-05-12 22:48:56,277 - Epoch: [173][  200/  217]    Overall Loss 0.001322    Objective Loss 0.001322                                        LR 0.000063    Time 0.098941    
2024-05-12 22:48:57,367 - Epoch: [173][  217/  217]    Overall Loss 0.001235    Objective Loss 0.001235    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.096189    
2024-05-12 22:48:57,902 - 

2024-05-12 22:48:57,904 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:49:08,836 - Epoch: [174][  100/  217]    Overall Loss 0.000610    Objective Loss 0.000610                                        LR 0.000063    Time 0.109033    
2024-05-12 22:49:16,596 - Epoch: [174][  200/  217]    Overall Loss 0.001243    Objective Loss 0.001243                                        LR 0.000063    Time 0.093194    
2024-05-12 22:49:17,683 - Epoch: [174][  217/  217]    Overall Loss 0.001281    Objective Loss 0.001281    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.090877    
2024-05-12 22:49:18,129 - 

2024-05-12 22:49:18,131 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:49:29,382 - Epoch: [175][  100/  217]    Overall Loss 0.000838    Objective Loss 0.000838                                        LR 0.000063    Time 0.112277    
2024-05-12 22:49:37,793 - Epoch: [175][  200/  217]    Overall Loss 0.001241    Objective Loss 0.001241                                        LR 0.000063    Time 0.098062    
2024-05-12 22:49:39,090 - Epoch: [175][  217/  217]    Overall Loss 0.001277    Objective Loss 0.001277    Top1 98.360656    Top5 100.000000    LR 0.000063    Time 0.096335    
2024-05-12 22:49:39,599 - 

2024-05-12 22:49:39,601 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:49:51,741 - Epoch: [176][  100/  217]    Overall Loss 0.001473    Objective Loss 0.001473                                        LR 0.000063    Time 0.121096    
2024-05-12 22:50:01,401 - Epoch: [176][  200/  217]    Overall Loss 0.001323    Objective Loss 0.001323                                        LR 0.000063    Time 0.108704    
2024-05-12 22:50:02,694 - Epoch: [176][  217/  217]    Overall Loss 0.001234    Objective Loss 0.001234    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.106122    
2024-05-12 22:50:03,283 - 

2024-05-12 22:50:03,285 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:50:15,234 - Epoch: [177][  100/  217]    Overall Loss 0.000866    Objective Loss 0.000866                                        LR 0.000063    Time 0.119209    
2024-05-12 22:50:23,173 - Epoch: [177][  200/  217]    Overall Loss 0.001294    Objective Loss 0.001294                                        LR 0.000063    Time 0.099174    
2024-05-12 22:50:24,387 - Epoch: [177][  217/  217]    Overall Loss 0.001312    Objective Loss 0.001312    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.096973    
2024-05-12 22:50:24,876 - 

2024-05-12 22:50:24,877 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:50:36,404 - Epoch: [178][  100/  217]    Overall Loss 0.000408    Objective Loss 0.000408                                        LR 0.000063    Time 0.115027    
2024-05-12 22:50:45,191 - Epoch: [178][  200/  217]    Overall Loss 0.001214    Objective Loss 0.001214                                        LR 0.000063    Time 0.101313    
2024-05-12 22:50:45,981 - Epoch: [178][  217/  217]    Overall Loss 0.001239    Objective Loss 0.001239    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.097002    
2024-05-12 22:50:46,411 - 

2024-05-12 22:50:46,413 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:50:55,028 - Epoch: [179][  100/  217]    Overall Loss 0.000929    Objective Loss 0.000929                                        LR 0.000063    Time 0.085892    
2024-05-12 22:51:03,250 - Epoch: [179][  200/  217]    Overall Loss 0.001252    Objective Loss 0.001252                                        LR 0.000063    Time 0.083931    
2024-05-12 22:51:04,407 - Epoch: [179][  217/  217]    Overall Loss 0.001166    Objective Loss 0.001166    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.082664    
2024-05-12 22:51:04,973 - 

2024-05-12 22:51:04,975 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:51:16,586 - Epoch: [180][  100/  217]    Overall Loss 0.000864    Objective Loss 0.000864                                        LR 0.000063    Time 0.115739    
2024-05-12 22:51:25,541 - Epoch: [180][  200/  217]    Overall Loss 0.001149    Objective Loss 0.001149                                        LR 0.000063    Time 0.102503    
2024-05-12 22:51:26,681 - Epoch: [180][  217/  217]    Overall Loss 0.001169    Objective Loss 0.001169    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.099688    
2024-05-12 22:51:27,518 - --- validate (epoch=180)-----------
2024-05-12 22:51:27,520 - 1736 samples (32 per mini-batch)
2024-05-12 22:51:35,320 - Epoch: [180][   55/   55]    Loss 3.215062    Top1 56.105991    Top5 72.926267    
2024-05-12 22:51:35,914 - ==> Top1: 56.106    Top5: 72.926    Loss: 3.215

2024-05-12 22:51:35,923 - ==> Best [Top1: 57.085   Top5: 72.926   Sparsity:0.00   Params: 382208 on epoch: 150]
2024-05-12 22:51:35,925 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:51:36,012 - 

2024-05-12 22:51:36,013 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:51:47,689 - Epoch: [181][  100/  217]    Overall Loss 0.000815    Objective Loss 0.000815                                        LR 0.000063    Time 0.116491    
2024-05-12 22:51:56,616 - Epoch: [181][  200/  217]    Overall Loss 0.001209    Objective Loss 0.001209                                        LR 0.000063    Time 0.102740    
2024-05-12 22:51:57,821 - Epoch: [181][  217/  217]    Overall Loss 0.001331    Objective Loss 0.001331    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.100226    
2024-05-12 22:51:58,353 - 

2024-05-12 22:51:58,355 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:52:11,311 - Epoch: [182][  100/  217]    Overall Loss 0.001236    Objective Loss 0.001236                                        LR 0.000063    Time 0.129273    
2024-05-12 22:52:20,578 - Epoch: [182][  200/  217]    Overall Loss 0.001184    Objective Loss 0.001184                                        LR 0.000063    Time 0.110822    
2024-05-12 22:52:21,825 - Epoch: [182][  217/  217]    Overall Loss 0.001300    Objective Loss 0.001300    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.107853    
2024-05-12 22:52:22,278 - 

2024-05-12 22:52:22,279 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:52:33,211 - Epoch: [183][  100/  217]    Overall Loss 0.000634    Objective Loss 0.000634                                        LR 0.000063    Time 0.109052    
2024-05-12 22:52:41,122 - Epoch: [183][  200/  217]    Overall Loss 0.001154    Objective Loss 0.001154                                        LR 0.000063    Time 0.093957    
2024-05-12 22:52:42,665 - Epoch: [183][  217/  217]    Overall Loss 0.001203    Objective Loss 0.001203    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.093678    
2024-05-12 22:52:43,278 - 

2024-05-12 22:52:43,280 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:52:54,825 - Epoch: [184][  100/  217]    Overall Loss 0.001164    Objective Loss 0.001164                                        LR 0.000063    Time 0.115151    
2024-05-12 22:53:04,391 - Epoch: [184][  200/  217]    Overall Loss 0.000969    Objective Loss 0.000969                                        LR 0.000063    Time 0.105268    
2024-05-12 22:53:05,542 - Epoch: [184][  217/  217]    Overall Loss 0.001137    Objective Loss 0.001137    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.102301    
2024-05-12 22:53:06,026 - 

2024-05-12 22:53:06,029 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:53:18,833 - Epoch: [185][  100/  217]    Overall Loss 0.001379    Objective Loss 0.001379                                        LR 0.000063    Time 0.127743    
2024-05-12 22:53:26,814 - Epoch: [185][  200/  217]    Overall Loss 0.001190    Objective Loss 0.001190                                        LR 0.000063    Time 0.103637    
2024-05-12 22:53:27,985 - Epoch: [185][  217/  217]    Overall Loss 0.001211    Objective Loss 0.001211    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.100889    
2024-05-12 22:53:28,546 - 

2024-05-12 22:53:28,548 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:53:39,119 - Epoch: [186][  100/  217]    Overall Loss 0.001076    Objective Loss 0.001076                                        LR 0.000063    Time 0.105472    
2024-05-12 22:53:48,522 - Epoch: [186][  200/  217]    Overall Loss 0.001122    Objective Loss 0.001122                                        LR 0.000063    Time 0.099608    
2024-05-12 22:53:49,806 - Epoch: [186][  217/  217]    Overall Loss 0.001180    Objective Loss 0.001180    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.097697    
2024-05-12 22:53:50,275 - 

2024-05-12 22:53:50,276 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:54:02,272 - Epoch: [187][  100/  217]    Overall Loss 0.001247    Objective Loss 0.001247                                        LR 0.000063    Time 0.119662    
2024-05-12 22:54:09,675 - Epoch: [187][  200/  217]    Overall Loss 0.001255    Objective Loss 0.001255                                        LR 0.000063    Time 0.096714    
2024-05-12 22:54:10,591 - Epoch: [187][  217/  217]    Overall Loss 0.001165    Objective Loss 0.001165    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.093341    
2024-05-12 22:54:11,022 - 

2024-05-12 22:54:11,026 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:54:23,927 - Epoch: [188][  100/  217]    Overall Loss 0.000120    Objective Loss 0.000120                                        LR 0.000063    Time 0.128670    
2024-05-12 22:54:31,561 - Epoch: [188][  200/  217]    Overall Loss 0.000852    Objective Loss 0.000852                                        LR 0.000063    Time 0.102379    
2024-05-12 22:54:32,933 - Epoch: [188][  217/  217]    Overall Loss 0.001127    Objective Loss 0.001127    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.100656    
2024-05-12 22:54:33,463 - 

2024-05-12 22:54:33,465 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:54:41,726 - Epoch: [189][  100/  217]    Overall Loss 0.000838    Objective Loss 0.000838                                        LR 0.000063    Time 0.082399    
2024-05-12 22:54:51,617 - Epoch: [189][  200/  217]    Overall Loss 0.001151    Objective Loss 0.001151                                        LR 0.000063    Time 0.090511    
2024-05-12 22:54:52,730 - Epoch: [189][  217/  217]    Overall Loss 0.001070    Objective Loss 0.001070    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.088528    
2024-05-12 22:54:53,113 - 

2024-05-12 22:54:53,115 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:55:06,458 - Epoch: [190][  100/  217]    Overall Loss 0.000855    Objective Loss 0.000855                                        LR 0.000063    Time 0.133155    
2024-05-12 22:55:14,363 - Epoch: [190][  200/  217]    Overall Loss 0.001477    Objective Loss 0.001477                                        LR 0.000063    Time 0.105980    
2024-05-12 22:55:15,544 - Epoch: [190][  217/  217]    Overall Loss 0.001375    Objective Loss 0.001375    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.103091    
2024-05-12 22:55:16,035 - --- validate (epoch=190)-----------
2024-05-12 22:55:16,036 - 1736 samples (32 per mini-batch)
2024-05-12 22:55:21,541 - Epoch: [190][   55/   55]    Loss 3.220368    Top1 55.933180    Top5 72.119816    
2024-05-12 22:55:22,034 - ==> Top1: 55.933    Top5: 72.120    Loss: 3.220

2024-05-12 22:55:22,052 - ==> Best [Top1: 57.085   Top5: 72.926   Sparsity:0.00   Params: 382208 on epoch: 150]
2024-05-12 22:55:22,054 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:55:22,181 - 

2024-05-12 22:55:22,183 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:55:32,518 - Epoch: [191][  100/  217]    Overall Loss 0.001267    Objective Loss 0.001267                                        LR 0.000063    Time 0.103047    
2024-05-12 22:55:39,751 - Epoch: [191][  200/  217]    Overall Loss 0.001100    Objective Loss 0.001100                                        LR 0.000063    Time 0.087575    
2024-05-12 22:55:40,705 - Epoch: [191][  217/  217]    Overall Loss 0.001133    Objective Loss 0.001133    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.085090    
2024-05-12 22:55:41,076 - 

2024-05-12 22:55:41,077 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:55:53,244 - Epoch: [192][  100/  217]    Overall Loss 0.001210    Objective Loss 0.001210                                        LR 0.000063    Time 0.121400    
2024-05-12 22:56:00,377 - Epoch: [192][  200/  217]    Overall Loss 0.001190    Objective Loss 0.001190                                        LR 0.000063    Time 0.096248    
2024-05-12 22:56:01,564 - Epoch: [192][  217/  217]    Overall Loss 0.001105    Objective Loss 0.001105    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.094154    
2024-05-12 22:56:02,102 - 

2024-05-12 22:56:02,104 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:56:12,948 - Epoch: [193][  100/  217]    Overall Loss 0.000817    Objective Loss 0.000817                                        LR 0.000063    Time 0.108178    
2024-05-12 22:56:21,673 - Epoch: [193][  200/  217]    Overall Loss 0.001057    Objective Loss 0.001057                                        LR 0.000063    Time 0.097585    
2024-05-12 22:56:22,717 - Epoch: [193][  217/  217]    Overall Loss 0.001098    Objective Loss 0.001098    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.094728    
2024-05-12 22:56:23,068 - 

2024-05-12 22:56:23,069 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:56:34,140 - Epoch: [194][  100/  217]    Overall Loss 0.001072    Objective Loss 0.001072                                        LR 0.000063    Time 0.110427    
2024-05-12 22:56:41,249 - Epoch: [194][  200/  217]    Overall Loss 0.001141    Objective Loss 0.001141                                        LR 0.000063    Time 0.090645    
2024-05-12 22:56:42,525 - Epoch: [194][  217/  217]    Overall Loss 0.001165    Objective Loss 0.001165    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.089399    
2024-05-12 22:56:43,090 - 

2024-05-12 22:56:43,094 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:56:54,021 - Epoch: [195][  100/  217]    Overall Loss 0.000965    Objective Loss 0.000965                                        LR 0.000063    Time 0.108982    
2024-05-12 22:57:01,873 - Epoch: [195][  200/  217]    Overall Loss 0.000898    Objective Loss 0.000898                                        LR 0.000063    Time 0.093635    
2024-05-12 22:57:03,119 - Epoch: [195][  217/  217]    Overall Loss 0.001056    Objective Loss 0.001056    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.092018    
2024-05-12 22:57:03,631 - 

2024-05-12 22:57:03,634 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:57:14,705 - Epoch: [196][  100/  217]    Overall Loss 0.001548    Objective Loss 0.001548                                        LR 0.000063    Time 0.110423    
2024-05-12 22:57:22,511 - Epoch: [196][  200/  217]    Overall Loss 0.001106    Objective Loss 0.001106                                        LR 0.000063    Time 0.094122    
2024-05-12 22:57:23,510 - Epoch: [196][  217/  217]    Overall Loss 0.001031    Objective Loss 0.001031    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.091333    
2024-05-12 22:57:24,007 - 

2024-05-12 22:57:24,009 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:57:36,132 - Epoch: [197][  100/  217]    Overall Loss 0.001233    Objective Loss 0.001233                                        LR 0.000063    Time 0.120967    
2024-05-12 22:57:45,104 - Epoch: [197][  200/  217]    Overall Loss 0.000820    Objective Loss 0.000820                                        LR 0.000063    Time 0.105213    
2024-05-12 22:57:46,556 - Epoch: [197][  217/  217]    Overall Loss 0.000987    Objective Loss 0.000987    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.103636    
2024-05-12 22:57:47,140 - 

2024-05-12 22:57:47,142 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:57:55,183 - Epoch: [198][  100/  217]    Overall Loss 0.001082    Objective Loss 0.001082                                        LR 0.000063    Time 0.080213    
2024-05-12 22:58:02,757 - Epoch: [198][  200/  217]    Overall Loss 0.001049    Objective Loss 0.001049                                        LR 0.000063    Time 0.077852    
2024-05-12 22:58:04,394 - Epoch: [198][  217/  217]    Overall Loss 0.000974    Objective Loss 0.000974    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.079270    
2024-05-12 22:58:05,097 - 

2024-05-12 22:58:05,099 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:58:17,171 - Epoch: [199][  100/  217]    Overall Loss 0.000513    Objective Loss 0.000513                                        LR 0.000063    Time 0.120428    
2024-05-12 22:58:27,210 - Epoch: [199][  200/  217]    Overall Loss 0.001039    Objective Loss 0.001039                                        LR 0.000063    Time 0.110261    
2024-05-12 22:58:28,506 - Epoch: [199][  217/  217]    Overall Loss 0.000967    Objective Loss 0.000967    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.107569    
2024-05-12 22:58:28,959 - 

2024-05-12 22:58:28,960 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:58:41,485 - Epoch: [200][  100/  217]    Overall Loss 0.000993    Objective Loss 0.000993                                        LR 0.000016    Time 0.124949    
2024-05-12 22:58:48,820 - Epoch: [200][  200/  217]    Overall Loss 0.000788    Objective Loss 0.000788                                        LR 0.000016    Time 0.099035    
2024-05-12 22:58:50,314 - Epoch: [200][  217/  217]    Overall Loss 0.000933    Objective Loss 0.000933    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.098134    
2024-05-12 22:58:50,944 - --- validate (epoch=200)-----------
2024-05-12 22:58:50,946 - 1736 samples (32 per mini-batch)
2024-05-12 22:58:57,765 - Epoch: [200][   55/   55]    Loss 3.321927    Top1 56.624424    Top5 72.868664    
2024-05-12 22:58:58,208 - ==> Top1: 56.624    Top5: 72.869    Loss: 3.322

2024-05-12 22:58:58,219 - ==> Best [Top1: 57.085   Top5: 72.926   Sparsity:0.00   Params: 382208 on epoch: 150]
2024-05-12 22:58:58,220 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 22:58:58,284 - 

2024-05-12 22:58:58,285 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:59:09,914 - Epoch: [201][  100/  217]    Overall Loss 0.001242    Objective Loss 0.001242                                        LR 0.000016    Time 0.116021    
2024-05-12 22:59:17,042 - Epoch: [201][  200/  217]    Overall Loss 0.000889    Objective Loss 0.000889                                        LR 0.000016    Time 0.093533    
2024-05-12 22:59:18,125 - Epoch: [201][  217/  217]    Overall Loss 0.000945    Objective Loss 0.000945    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.091171    
2024-05-12 22:59:18,695 - 

2024-05-12 22:59:18,696 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:59:30,310 - Epoch: [202][  100/  217]    Overall Loss 0.000833    Objective Loss 0.000833                                        LR 0.000016    Time 0.115869    
2024-05-12 22:59:39,160 - Epoch: [202][  200/  217]    Overall Loss 0.001046    Objective Loss 0.001046                                        LR 0.000016    Time 0.102049    
2024-05-12 22:59:40,385 - Epoch: [202][  217/  217]    Overall Loss 0.000974    Objective Loss 0.000974    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.099674    
2024-05-12 22:59:40,875 - 

2024-05-12 22:59:40,876 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 22:59:52,637 - Epoch: [203][  100/  217]    Overall Loss 0.001026    Objective Loss 0.001026                                        LR 0.000016    Time 0.117338    
2024-05-12 23:00:01,309 - Epoch: [203][  200/  217]    Overall Loss 0.000803    Objective Loss 0.000803                                        LR 0.000016    Time 0.101887    
2024-05-12 23:00:02,558 - Epoch: [203][  217/  217]    Overall Loss 0.000953    Objective Loss 0.000953    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.099636    
2024-05-12 23:00:03,026 - 

2024-05-12 23:00:03,027 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:00:14,389 - Epoch: [204][  100/  217]    Overall Loss 0.001220    Objective Loss 0.001220                                        LR 0.000016    Time 0.113379    
2024-05-12 23:00:22,558 - Epoch: [204][  200/  217]    Overall Loss 0.000771    Objective Loss 0.000771                                        LR 0.000016    Time 0.097412    
2024-05-12 23:00:23,680 - Epoch: [204][  217/  217]    Overall Loss 0.000929    Objective Loss 0.000929    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.094924    
2024-05-12 23:00:24,154 - 

2024-05-12 23:00:24,156 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:00:35,574 - Epoch: [205][  100/  217]    Overall Loss 0.000754    Objective Loss 0.000754                                        LR 0.000016    Time 0.113908    
2024-05-12 23:00:42,644 - Epoch: [205][  200/  217]    Overall Loss 0.000859    Objective Loss 0.000859                                        LR 0.000016    Time 0.092189    
2024-05-12 23:00:44,180 - Epoch: [205][  217/  217]    Overall Loss 0.000907    Objective Loss 0.000907    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.092022    
2024-05-12 23:00:44,799 - 

2024-05-12 23:00:44,803 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:00:56,730 - Epoch: [206][  100/  217]    Overall Loss 0.001010    Objective Loss 0.001010                                        LR 0.000016    Time 0.118961    
2024-05-12 23:01:06,213 - Epoch: [206][  200/  217]    Overall Loss 0.001011    Objective Loss 0.001011                                        LR 0.000016    Time 0.106755    
2024-05-12 23:01:07,169 - Epoch: [206][  217/  217]    Overall Loss 0.000939    Objective Loss 0.000939    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.102779    
2024-05-12 23:01:07,707 - 

2024-05-12 23:01:07,709 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:01:20,357 - Epoch: [207][  100/  217]    Overall Loss 0.000746    Objective Loss 0.000746                                        LR 0.000016    Time 0.126178    
2024-05-12 23:01:26,409 - Epoch: [207][  200/  217]    Overall Loss 0.000996    Objective Loss 0.000996                                        LR 0.000016    Time 0.093249    
2024-05-12 23:01:27,103 - Epoch: [207][  217/  217]    Overall Loss 0.000927    Objective Loss 0.000927    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.089132    
2024-05-12 23:01:27,251 - 

2024-05-12 23:01:27,252 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:01:38,551 - Epoch: [208][  100/  217]    Overall Loss 0.000781    Objective Loss 0.000781                                        LR 0.000016    Time 0.112733    
2024-05-12 23:01:47,907 - Epoch: [208][  200/  217]    Overall Loss 0.000784    Objective Loss 0.000784                                        LR 0.000016    Time 0.103012    
2024-05-12 23:01:49,173 - Epoch: [208][  217/  217]    Overall Loss 0.000942    Objective Loss 0.000942    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.100748    
2024-05-12 23:01:49,625 - 

2024-05-12 23:01:49,627 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:02:00,515 - Epoch: [209][  100/  217]    Overall Loss 0.001253    Objective Loss 0.001253                                        LR 0.000016    Time 0.108624    
2024-05-12 23:02:07,260 - Epoch: [209][  200/  217]    Overall Loss 0.000907    Objective Loss 0.000907                                        LR 0.000016    Time 0.087927    
2024-05-12 23:02:08,247 - Epoch: [209][  217/  217]    Overall Loss 0.000964    Objective Loss 0.000964    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.085563    
2024-05-12 23:02:08,649 - 

2024-05-12 23:02:08,650 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:02:21,670 - Epoch: [210][  100/  217]    Overall Loss 0.001471    Objective Loss 0.001471                                        LR 0.000016    Time 0.129912    
2024-05-12 23:02:31,587 - Epoch: [210][  200/  217]    Overall Loss 0.000786    Objective Loss 0.000786                                        LR 0.000016    Time 0.114395    
2024-05-12 23:02:33,147 - Epoch: [210][  217/  217]    Overall Loss 0.000925    Objective Loss 0.000925    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.112591    
2024-05-12 23:02:33,633 - --- validate (epoch=210)-----------
2024-05-12 23:02:33,634 - 1736 samples (32 per mini-batch)
2024-05-12 23:02:37,890 - Epoch: [210][   55/   55]    Loss 3.302549    Top1 56.451613    Top5 72.753456    
2024-05-12 23:02:38,331 - ==> Top1: 56.452    Top5: 72.753    Loss: 3.303

2024-05-12 23:02:38,340 - ==> Best [Top1: 57.085   Top5: 72.926   Sparsity:0.00   Params: 382208 on epoch: 150]
2024-05-12 23:02:38,341 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 23:02:38,410 - 

2024-05-12 23:02:38,412 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:02:50,992 - Epoch: [211][  100/  217]    Overall Loss 0.000617    Objective Loss 0.000617                                        LR 0.000016    Time 0.125541    
2024-05-12 23:03:00,693 - Epoch: [211][  200/  217]    Overall Loss 0.000784    Objective Loss 0.000784                                        LR 0.000016    Time 0.111142    
2024-05-12 23:03:01,999 - Epoch: [211][  217/  217]    Overall Loss 0.000942    Objective Loss 0.000942    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.108429    
2024-05-12 23:03:02,517 - 

2024-05-12 23:03:02,519 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:03:14,126 - Epoch: [212][  100/  217]    Overall Loss 0.000697    Objective Loss 0.000697                                        LR 0.000016    Time 0.115804    
2024-05-12 23:03:23,675 - Epoch: [212][  200/  217]    Overall Loss 0.000866    Objective Loss 0.000866                                        LR 0.000016    Time 0.105512    
2024-05-12 23:03:25,625 - Epoch: [212][  217/  217]    Overall Loss 0.000912    Objective Loss 0.000912    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.106208    
2024-05-12 23:03:26,094 - 

2024-05-12 23:03:26,098 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:03:37,504 - Epoch: [213][  100/  217]    Overall Loss 0.001210    Objective Loss 0.001210                                        LR 0.000016    Time 0.113775    
2024-05-12 23:03:46,800 - Epoch: [213][  200/  217]    Overall Loss 0.000988    Objective Loss 0.000988                                        LR 0.000016    Time 0.103227    
2024-05-12 23:03:48,287 - Epoch: [213][  217/  217]    Overall Loss 0.000919    Objective Loss 0.000919    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.101965    
2024-05-12 23:03:49,021 - 

2024-05-12 23:03:49,023 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:04:02,213 - Epoch: [214][  100/  217]    Overall Loss 0.000723    Objective Loss 0.000723                                        LR 0.000016    Time 0.131621    
2024-05-12 23:04:10,940 - Epoch: [214][  200/  217]    Overall Loss 0.000981    Objective Loss 0.000981                                        LR 0.000016    Time 0.109312    
2024-05-12 23:04:12,604 - Epoch: [214][  217/  217]    Overall Loss 0.000912    Objective Loss 0.000912    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.108383    
2024-05-12 23:04:13,313 - 

2024-05-12 23:04:13,316 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:04:26,219 - Epoch: [215][  100/  217]    Overall Loss 0.000751    Objective Loss 0.000751                                        LR 0.000016    Time 0.128718    
2024-05-12 23:04:35,659 - Epoch: [215][  200/  217]    Overall Loss 0.000993    Objective Loss 0.000993                                        LR 0.000016    Time 0.111421    
2024-05-12 23:04:37,299 - Epoch: [215][  217/  217]    Overall Loss 0.000922    Objective Loss 0.000922    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.110220    
2024-05-12 23:04:38,107 - 

2024-05-12 23:04:38,109 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:04:51,899 - Epoch: [216][  100/  217]    Overall Loss 0.000754    Objective Loss 0.000754                                        LR 0.000016    Time 0.137626    
2024-05-12 23:05:00,816 - Epoch: [216][  200/  217]    Overall Loss 0.000970    Objective Loss 0.000970                                        LR 0.000016    Time 0.113258    
2024-05-12 23:05:02,122 - Epoch: [216][  217/  217]    Overall Loss 0.000900    Objective Loss 0.000900    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.110385    
2024-05-12 23:05:02,770 - 

2024-05-12 23:05:02,771 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:05:15,609 - Epoch: [217][  100/  217]    Overall Loss 0.000525    Objective Loss 0.000525                                        LR 0.000016    Time 0.128111    
2024-05-12 23:05:25,279 - Epoch: [217][  200/  217]    Overall Loss 0.000984    Objective Loss 0.000984                                        LR 0.000016    Time 0.112279    
2024-05-12 23:05:27,017 - Epoch: [217][  217/  217]    Overall Loss 0.000913    Objective Loss 0.000913    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.111469    
2024-05-12 23:05:27,524 - 

2024-05-12 23:05:27,525 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:05:40,110 - Epoch: [218][  100/  217]    Overall Loss 0.001177    Objective Loss 0.001177                                        LR 0.000016    Time 0.125577    
2024-05-12 23:05:49,092 - Epoch: [218][  200/  217]    Overall Loss 0.000967    Objective Loss 0.000967                                        LR 0.000016    Time 0.107563    
2024-05-12 23:05:50,507 - Epoch: [218][  217/  217]    Overall Loss 0.000905    Objective Loss 0.000905    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.105589    
2024-05-12 23:05:51,079 - 

2024-05-12 23:05:51,083 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:06:04,701 - Epoch: [219][  100/  217]    Overall Loss 0.001180    Objective Loss 0.001180                                        LR 0.000016    Time 0.135860    
2024-05-12 23:06:13,958 - Epoch: [219][  200/  217]    Overall Loss 0.000982    Objective Loss 0.000982                                        LR 0.000016    Time 0.114080    
2024-05-12 23:06:15,551 - Epoch: [219][  217/  217]    Overall Loss 0.000911    Objective Loss 0.000911    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.112453    
2024-05-12 23:06:16,082 - 

2024-05-12 23:06:16,083 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:06:31,597 - Epoch: [220][  100/  217]    Overall Loss 0.000530    Objective Loss 0.000530                                        LR 0.000016    Time 0.154856    
2024-05-12 23:06:40,458 - Epoch: [220][  200/  217]    Overall Loss 0.000984    Objective Loss 0.000984                                        LR 0.000016    Time 0.121607    
2024-05-12 23:06:41,857 - Epoch: [220][  217/  217]    Overall Loss 0.000916    Objective Loss 0.000916    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.118499    
2024-05-12 23:06:42,453 - --- validate (epoch=220)-----------
2024-05-12 23:06:42,454 - 1736 samples (32 per mini-batch)
2024-05-12 23:06:48,904 - Epoch: [220][   55/   55]    Loss 3.312824    Top1 56.451613    Top5 73.387097    
2024-05-12 23:06:49,552 - ==> Top1: 56.452    Top5: 73.387    Loss: 3.313

2024-05-12 23:06:49,564 - ==> Best [Top1: 57.085   Top5: 72.926   Sparsity:0.00   Params: 382208 on epoch: 150]
2024-05-12 23:06:49,565 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 23:06:49,634 - 

2024-05-12 23:06:49,636 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:07:01,817 - Epoch: [221][  100/  217]    Overall Loss 0.000954    Objective Loss 0.000954                                        LR 0.000016    Time 0.121507    
2024-05-12 23:07:11,712 - Epoch: [221][  200/  217]    Overall Loss 0.000994    Objective Loss 0.000994                                        LR 0.000016    Time 0.110090    
2024-05-12 23:07:13,378 - Epoch: [221][  217/  217]    Overall Loss 0.000932    Objective Loss 0.000932    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.109112    
2024-05-12 23:07:14,067 - 

2024-05-12 23:07:14,071 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:07:27,109 - Epoch: [222][  100/  217]    Overall Loss 0.000716    Objective Loss 0.000716                                        LR 0.000016    Time 0.130078    
2024-05-12 23:07:36,347 - Epoch: [222][  200/  217]    Overall Loss 0.000984    Objective Loss 0.000984                                        LR 0.000016    Time 0.111095    
2024-05-12 23:07:37,880 - Epoch: [222][  217/  217]    Overall Loss 0.000913    Objective Loss 0.000913    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.109421    
2024-05-12 23:07:38,370 - 

2024-05-12 23:07:38,371 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:07:51,205 - Epoch: [223][  100/  217]    Overall Loss 0.000971    Objective Loss 0.000971                                        LR 0.000016    Time 0.128066    
2024-05-12 23:08:01,083 - Epoch: [223][  200/  217]    Overall Loss 0.000880    Objective Loss 0.000880                                        LR 0.000016    Time 0.113285    
2024-05-12 23:08:02,650 - Epoch: [223][  217/  217]    Overall Loss 0.000933    Objective Loss 0.000933    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.111596    
2024-05-12 23:08:03,251 - 

2024-05-12 23:08:03,254 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:08:17,309 - Epoch: [224][  100/  217]    Overall Loss 0.000309    Objective Loss 0.000309                                        LR 0.000016    Time 0.140249    
2024-05-12 23:08:25,697 - Epoch: [224][  200/  217]    Overall Loss 0.000910    Objective Loss 0.000910                                        LR 0.000016    Time 0.111937    
2024-05-12 23:08:27,109 - Epoch: [224][  217/  217]    Overall Loss 0.000944    Objective Loss 0.000944    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.109645    
2024-05-12 23:08:27,656 - 

2024-05-12 23:08:27,658 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:08:40,824 - Epoch: [225][  100/  217]    Overall Loss 0.001181    Objective Loss 0.001181                                        LR 0.000016    Time 0.131380    
2024-05-12 23:08:50,795 - Epoch: [225][  200/  217]    Overall Loss 0.000967    Objective Loss 0.000967                                        LR 0.000016    Time 0.115393    
2024-05-12 23:08:52,310 - Epoch: [225][  217/  217]    Overall Loss 0.000903    Objective Loss 0.000903    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.113302    
2024-05-12 23:08:52,955 - 

2024-05-12 23:08:52,956 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:09:06,363 - Epoch: [226][  100/  217]    Overall Loss 0.000757    Objective Loss 0.000757                                        LR 0.000016    Time 0.133788    
2024-05-12 23:09:15,169 - Epoch: [226][  200/  217]    Overall Loss 0.000759    Objective Loss 0.000759                                        LR 0.000016    Time 0.110797    
2024-05-12 23:09:16,766 - Epoch: [226][  217/  217]    Overall Loss 0.000914    Objective Loss 0.000914    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.109448    
2024-05-12 23:09:17,403 - 

2024-05-12 23:09:17,405 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:09:29,882 - Epoch: [227][  100/  217]    Overall Loss 0.001174    Objective Loss 0.001174                                        LR 0.000016    Time 0.124487    
2024-05-12 23:09:38,765 - Epoch: [227][  200/  217]    Overall Loss 0.000746    Objective Loss 0.000746                                        LR 0.000016    Time 0.106528    
2024-05-12 23:09:39,663 - Epoch: [227][  217/  217]    Overall Loss 0.000893    Objective Loss 0.000893    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.102299    
2024-05-12 23:09:40,020 - 

2024-05-12 23:09:40,022 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:09:52,027 - Epoch: [228][  100/  217]    Overall Loss 0.000952    Objective Loss 0.000952                                        LR 0.000016    Time 0.119804    
2024-05-12 23:09:59,875 - Epoch: [228][  200/  217]    Overall Loss 0.000980    Objective Loss 0.000980                                        LR 0.000016    Time 0.099024    
2024-05-12 23:10:01,334 - Epoch: [228][  217/  217]    Overall Loss 0.000909    Objective Loss 0.000909    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.097962    
2024-05-12 23:10:01,936 - 

2024-05-12 23:10:01,938 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:10:12,150 - Epoch: [229][  100/  217]    Overall Loss 0.000774    Objective Loss 0.000774                                        LR 0.000016    Time 0.101865    
2024-05-12 23:10:20,354 - Epoch: [229][  200/  217]    Overall Loss 0.000991    Objective Loss 0.000991                                        LR 0.000016    Time 0.091826    
2024-05-12 23:10:21,393 - Epoch: [229][  217/  217]    Overall Loss 0.000921    Objective Loss 0.000921    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.089404    
2024-05-12 23:10:21,701 - 

2024-05-12 23:10:21,702 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:10:34,006 - Epoch: [230][  100/  217]    Overall Loss 0.000516    Objective Loss 0.000516                                        LR 0.000016    Time 0.122760    
2024-05-12 23:10:41,929 - Epoch: [230][  200/  217]    Overall Loss 0.000646    Objective Loss 0.000646                                        LR 0.000016    Time 0.100869    
2024-05-12 23:10:43,372 - Epoch: [230][  217/  217]    Overall Loss 0.000920    Objective Loss 0.000920    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.099590    
2024-05-12 23:10:43,915 - --- validate (epoch=230)-----------
2024-05-12 23:10:43,917 - 1736 samples (32 per mini-batch)
2024-05-12 23:10:49,037 - Epoch: [230][   55/   55]    Loss 3.321055    Top1 56.221198    Top5 72.926267    
2024-05-12 23:10:49,525 - ==> Top1: 56.221    Top5: 72.926    Loss: 3.321

2024-05-12 23:10:49,537 - ==> Best [Top1: 57.085   Top5: 72.926   Sparsity:0.00   Params: 382208 on epoch: 150]
2024-05-12 23:10:49,538 - Saving checkpoint to: logs/2024.05.12-214239/checkpoint.pth.tar
2024-05-12 23:10:49,603 - 

2024-05-12 23:10:49,604 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:11:00,603 - Epoch: [231][  100/  217]    Overall Loss 0.000764    Objective Loss 0.000764                                        LR 0.000016    Time 0.109732    
2024-05-12 23:11:09,372 - Epoch: [231][  200/  217]    Overall Loss 0.000982    Objective Loss 0.000982                                        LR 0.000016    Time 0.098593    
2024-05-12 23:11:10,819 - Epoch: [231][  217/  217]    Overall Loss 0.000913    Objective Loss 0.000913    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.097513    
2024-05-12 23:11:11,352 - 

2024-05-12 23:11:11,354 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:11:22,611 - Epoch: [232][  100/  217]    Overall Loss 0.001166    Objective Loss 0.001166                                        LR 0.000016    Time 0.112286    
2024-05-12 23:11:30,454 - Epoch: [232][  200/  217]    Overall Loss 0.000841    Objective Loss 0.000841                                        LR 0.000016    Time 0.095232    
2024-05-12 23:11:31,475 - Epoch: [232][  217/  217]    Overall Loss 0.000885    Objective Loss 0.000885    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.092455    
2024-05-12 23:11:31,856 - 

2024-05-12 23:11:31,858 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:11:44,227 - Epoch: [233][  100/  217]    Overall Loss 0.001613    Objective Loss 0.001613                                        LR 0.000016    Time 0.123403    
2024-05-12 23:11:50,902 - Epoch: [233][  200/  217]    Overall Loss 0.000971    Objective Loss 0.000971                                        LR 0.000016    Time 0.094969    
2024-05-12 23:11:51,753 - Epoch: [233][  217/  217]    Overall Loss 0.000901    Objective Loss 0.000901    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.091438    
2024-05-12 23:11:52,092 - 

2024-05-12 23:11:52,093 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:11:59,785 - Epoch: [234][  100/  217]    Overall Loss 0.000750    Objective Loss 0.000750                                        LR 0.000016    Time 0.076621    
2024-05-12 23:12:09,257 - Epoch: [234][  200/  217]    Overall Loss 0.000743    Objective Loss 0.000743                                        LR 0.000016    Time 0.085534    
2024-05-12 23:12:10,396 - Epoch: [234][  217/  217]    Overall Loss 0.000931    Objective Loss 0.000931    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.084061    
2024-05-12 23:12:10,854 - 

2024-05-12 23:12:10,855 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:12:22,734 - Epoch: [235][  100/  217]    Overall Loss 0.000953    Objective Loss 0.000953                                        LR 0.000016    Time 0.118510    
2024-05-12 23:12:28,876 - Epoch: [235][  200/  217]    Overall Loss 0.000739    Objective Loss 0.000739                                        LR 0.000016    Time 0.089868    
2024-05-12 23:12:29,858 - Epoch: [235][  217/  217]    Overall Loss 0.000888    Objective Loss 0.000888    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.087337    
2024-05-12 23:12:30,282 - 

2024-05-12 23:12:30,283 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:12:41,416 - Epoch: [236][  100/  217]    Overall Loss 0.000950    Objective Loss 0.000950                                        LR 0.000016    Time 0.111071    
2024-05-12 23:12:49,754 - Epoch: [236][  200/  217]    Overall Loss 0.000947    Objective Loss 0.000947                                        LR 0.000016    Time 0.097108    
2024-05-12 23:12:51,015 - Epoch: [236][  217/  217]    Overall Loss 0.000879    Objective Loss 0.000879    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.095288    
2024-05-12 23:12:51,475 - 

2024-05-12 23:12:51,477 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:13:03,099 - Epoch: [237][  100/  217]    Overall Loss 0.001212    Objective Loss 0.001212                                        LR 0.000016    Time 0.115910    
2024-05-12 23:13:10,939 - Epoch: [237][  200/  217]    Overall Loss 0.001006    Objective Loss 0.001006                                        LR 0.000016    Time 0.097031    
2024-05-12 23:13:12,216 - Epoch: [237][  217/  217]    Overall Loss 0.000932    Objective Loss 0.000932    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.095290    
2024-05-12 23:13:12,800 - 

2024-05-12 23:13:12,801 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:13:24,440 - Epoch: [238][  100/  217]    Overall Loss 0.001221    Objective Loss 0.001221                                        LR 0.000016    Time 0.116112    
2024-05-12 23:13:31,909 - Epoch: [238][  200/  217]    Overall Loss 0.000974    Objective Loss 0.000974                                        LR 0.000016    Time 0.095281    
2024-05-12 23:13:32,970 - Epoch: [238][  217/  217]    Overall Loss 0.000905    Objective Loss 0.000905    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.092678    
2024-05-12 23:13:33,457 - 

2024-05-12 23:13:33,459 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:13:45,479 - Epoch: [239][  100/  217]    Overall Loss 0.000745    Objective Loss 0.000745                                        LR 0.000016    Time 0.119944    
2024-05-12 23:13:52,519 - Epoch: [239][  200/  217]    Overall Loss 0.000749    Objective Loss 0.000749                                        LR 0.000016    Time 0.095060    
2024-05-12 23:13:53,700 - Epoch: [239][  217/  217]    Overall Loss 0.000894    Objective Loss 0.000894    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.093032    
2024-05-12 23:13:54,414 - 

2024-05-12 23:13:54,415 - Initiating quantization aware training (QAT)...
2024-05-12 23:13:54,486 - 

2024-05-12 23:13:54,488 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:14:05,870 - Epoch: [240][  100/  217]    Overall Loss 2.023327    Objective Loss 2.023327                                        LR 0.000016    Time 0.113586    
2024-05-12 23:14:16,354 - Epoch: [240][  200/  217]    Overall Loss 1.608936    Objective Loss 1.608936                                        LR 0.000016    Time 0.109083    
2024-05-12 23:14:17,427 - Epoch: [240][  217/  217]    Overall Loss 1.562720    Objective Loss 1.562720    Top1 80.327869    Top5 93.442623    LR 0.000016    Time 0.105464    
2024-05-12 23:14:17,938 - --- validate (epoch=240)-----------
2024-05-12 23:14:17,939 - 1736 samples (32 per mini-batch)
2024-05-12 23:14:26,274 - Epoch: [240][   55/   55]    Loss 2.250665    Top1 49.884793    Top5 67.569124    
2024-05-12 23:14:26,912 - ==> Top1: 49.885    Top5: 67.569    Loss: 2.251

2024-05-12 23:14:26,927 - ==> Best [Top1: 49.885   Top5: 67.569   Sparsity:0.00   Params: 382208 on epoch: 240]
2024-05-12 23:14:26,928 - Saving checkpoint to: logs/2024.05.12-214239/qat_checkpoint.pth.tar
2024-05-12 23:14:27,017 - 

2024-05-12 23:14:27,018 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:14:37,984 - Epoch: [241][  100/  217]    Overall Loss 0.913104    Objective Loss 0.913104                                        LR 0.000016    Time 0.109412    
2024-05-12 23:14:46,014 - Epoch: [241][  200/  217]    Overall Loss 0.863809    Objective Loss 0.863809                                        LR 0.000016    Time 0.094739    
2024-05-12 23:14:46,984 - Epoch: [241][  217/  217]    Overall Loss 0.859782    Objective Loss 0.859782    Top1 81.967213    Top5 96.721311    LR 0.000016    Time 0.091769    
2024-05-12 23:14:47,317 - 

2024-05-12 23:14:47,318 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:15:00,235 - Epoch: [242][  100/  217]    Overall Loss 0.725749    Objective Loss 0.725749                                        LR 0.000016    Time 0.128922    
2024-05-12 23:15:08,834 - Epoch: [242][  200/  217]    Overall Loss 0.688765    Objective Loss 0.688765                                        LR 0.000016    Time 0.107343    
2024-05-12 23:15:09,952 - Epoch: [242][  217/  217]    Overall Loss 0.687692    Objective Loss 0.687692    Top1 88.524590    Top5 96.721311    LR 0.000016    Time 0.104066    
2024-05-12 23:15:10,111 - 

2024-05-12 23:15:10,112 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:15:18,554 - Epoch: [243][  100/  217]    Overall Loss 0.597083    Objective Loss 0.597083                                        LR 0.000016    Time 0.084229    
2024-05-12 23:15:27,752 - Epoch: [243][  200/  217]    Overall Loss 0.592373    Objective Loss 0.592373                                        LR 0.000016    Time 0.087978    
2024-05-12 23:15:28,997 - Epoch: [243][  217/  217]    Overall Loss 0.591121    Objective Loss 0.591121    Top1 86.885246    Top5 98.360656    LR 0.000016    Time 0.086803    
2024-05-12 23:15:29,470 - 

2024-05-12 23:15:29,472 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:15:42,533 - Epoch: [244][  100/  217]    Overall Loss 0.529054    Objective Loss 0.529054                                        LR 0.000016    Time 0.130348    
2024-05-12 23:15:52,067 - Epoch: [244][  200/  217]    Overall Loss 0.526243    Objective Loss 0.526243                                        LR 0.000016    Time 0.112716    
2024-05-12 23:15:53,640 - Epoch: [244][  217/  217]    Overall Loss 0.523094    Objective Loss 0.523094    Top1 86.885246    Top5 98.360656    LR 0.000016    Time 0.111113    
2024-05-12 23:15:54,183 - 

2024-05-12 23:15:54,185 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:16:05,561 - Epoch: [245][  100/  217]    Overall Loss 0.459820    Objective Loss 0.459820                                        LR 0.000016    Time 0.113503    
2024-05-12 23:16:16,060 - Epoch: [245][  200/  217]    Overall Loss 0.480352    Objective Loss 0.480352                                        LR 0.000016    Time 0.109092    
2024-05-12 23:16:17,359 - Epoch: [245][  217/  217]    Overall Loss 0.479975    Objective Loss 0.479975    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.106507    
2024-05-12 23:16:17,857 - 

2024-05-12 23:16:17,858 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:16:30,749 - Epoch: [246][  100/  217]    Overall Loss 0.436755    Objective Loss 0.436755                                        LR 0.000016    Time 0.128641    
2024-05-12 23:16:38,817 - Epoch: [246][  200/  217]    Overall Loss 0.454044    Objective Loss 0.454044                                        LR 0.000016    Time 0.104546    
2024-05-12 23:16:40,259 - Epoch: [246][  217/  217]    Overall Loss 0.448792    Objective Loss 0.448792    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.102978    
2024-05-12 23:16:40,800 - 

2024-05-12 23:16:40,803 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:16:53,802 - Epoch: [247][  100/  217]    Overall Loss 0.398257    Objective Loss 0.398257                                        LR 0.000016    Time 0.129715    
2024-05-12 23:17:04,045 - Epoch: [247][  200/  217]    Overall Loss 0.414328    Objective Loss 0.414328                                        LR 0.000016    Time 0.115951    
2024-05-12 23:17:05,077 - Epoch: [247][  217/  217]    Overall Loss 0.418654    Objective Loss 0.418654    Top1 80.327869    Top5 93.442623    LR 0.000016    Time 0.111606    
2024-05-12 23:17:05,525 - 

2024-05-12 23:17:05,527 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:17:17,887 - Epoch: [248][  100/  217]    Overall Loss 0.383372    Objective Loss 0.383372                                        LR 0.000016    Time 0.123337    
2024-05-12 23:17:26,919 - Epoch: [248][  200/  217]    Overall Loss 0.379057    Objective Loss 0.379057                                        LR 0.000016    Time 0.106717    
2024-05-12 23:17:28,511 - Epoch: [248][  217/  217]    Overall Loss 0.381640    Objective Loss 0.381640    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.105670    
2024-05-12 23:17:29,072 - 

2024-05-12 23:17:29,074 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:17:40,871 - Epoch: [249][  100/  217]    Overall Loss 0.355935    Objective Loss 0.355935                                        LR 0.000016    Time 0.117690    
2024-05-12 23:17:49,243 - Epoch: [249][  200/  217]    Overall Loss 0.370552    Objective Loss 0.370552                                        LR 0.000016    Time 0.100593    
2024-05-12 23:17:50,530 - Epoch: [249][  217/  217]    Overall Loss 0.368016    Objective Loss 0.368016    Top1 90.163934    Top5 100.000000    LR 0.000016    Time 0.098618    
2024-05-12 23:17:50,898 - 

2024-05-12 23:17:50,899 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:18:03,658 - Epoch: [250][  100/  217]    Overall Loss 0.327220    Objective Loss 0.327220                                        LR 0.000016    Time 0.127337    
2024-05-12 23:18:13,562 - Epoch: [250][  200/  217]    Overall Loss 0.342531    Objective Loss 0.342531                                        LR 0.000016    Time 0.113058    
2024-05-12 23:18:15,048 - Epoch: [250][  217/  217]    Overall Loss 0.340861    Objective Loss 0.340861    Top1 95.081967    Top5 98.360656    LR 0.000016    Time 0.111028    
2024-05-12 23:18:15,554 - --- validate (epoch=250)-----------
2024-05-12 23:18:15,556 - 1736 samples (32 per mini-batch)
2024-05-12 23:18:21,470 - Epoch: [250][   55/   55]    Loss 2.194855    Top1 53.686636    Top5 71.601382    
2024-05-12 23:18:22,073 - ==> Top1: 53.687    Top5: 71.601    Loss: 2.195

2024-05-12 23:18:22,083 - ==> Best [Top1: 53.687   Top5: 71.601   Sparsity:0.00   Params: 382208 on epoch: 250]
2024-05-12 23:18:22,085 - Saving checkpoint to: logs/2024.05.12-214239/qat_checkpoint.pth.tar
2024-05-12 23:18:22,182 - 

2024-05-12 23:18:22,184 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:18:32,314 - Epoch: [251][  100/  217]    Overall Loss 0.330932    Objective Loss 0.330932                                        LR 0.000016    Time 0.101137    
2024-05-12 23:18:41,680 - Epoch: [251][  200/  217]    Overall Loss 0.326462    Objective Loss 0.326462                                        LR 0.000016    Time 0.097272    
2024-05-12 23:18:43,161 - Epoch: [251][  217/  217]    Overall Loss 0.325286    Objective Loss 0.325286    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.096450    
2024-05-12 23:18:43,758 - 

2024-05-12 23:18:43,759 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:18:55,759 - Epoch: [252][  100/  217]    Overall Loss 0.301699    Objective Loss 0.301699                                        LR 0.000016    Time 0.119731    
2024-05-12 23:19:05,581 - Epoch: [252][  200/  217]    Overall Loss 0.314620    Objective Loss 0.314620                                        LR 0.000016    Time 0.108820    
2024-05-12 23:19:06,869 - Epoch: [252][  217/  217]    Overall Loss 0.313862    Objective Loss 0.313862    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.106209    
2024-05-12 23:19:07,180 - 

2024-05-12 23:19:07,181 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:19:19,453 - Epoch: [253][  100/  217]    Overall Loss 0.289860    Objective Loss 0.289860                                        LR 0.000016    Time 0.122460    
2024-05-12 23:19:27,418 - Epoch: [253][  200/  217]    Overall Loss 0.302742    Objective Loss 0.302742                                        LR 0.000016    Time 0.100939    
2024-05-12 23:19:28,681 - Epoch: [253][  217/  217]    Overall Loss 0.303278    Objective Loss 0.303278    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.098827    
2024-05-12 23:19:29,216 - 

2024-05-12 23:19:29,219 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:19:39,097 - Epoch: [254][  100/  217]    Overall Loss 0.293056    Objective Loss 0.293056                                        LR 0.000016    Time 0.098592    
2024-05-12 23:19:47,571 - Epoch: [254][  200/  217]    Overall Loss 0.289188    Objective Loss 0.289188                                        LR 0.000016    Time 0.091547    
2024-05-12 23:19:48,888 - Epoch: [254][  217/  217]    Overall Loss 0.289763    Objective Loss 0.289763    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.090421    
2024-05-12 23:19:49,381 - 

2024-05-12 23:19:49,382 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:20:01,151 - Epoch: [255][  100/  217]    Overall Loss 0.273450    Objective Loss 0.273450                                        LR 0.000016    Time 0.117424    
2024-05-12 23:20:09,624 - Epoch: [255][  200/  217]    Overall Loss 0.280712    Objective Loss 0.280712                                        LR 0.000016    Time 0.100955    
2024-05-12 23:20:11,022 - Epoch: [255][  217/  217]    Overall Loss 0.280359    Objective Loss 0.280359    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.099467    
2024-05-12 23:20:11,524 - 

2024-05-12 23:20:11,527 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:20:23,751 - Epoch: [256][  100/  217]    Overall Loss 0.265397    Objective Loss 0.265397                                        LR 0.000016    Time 0.121965    
2024-05-12 23:20:33,129 - Epoch: [256][  200/  217]    Overall Loss 0.265747    Objective Loss 0.265747                                        LR 0.000016    Time 0.107750    
2024-05-12 23:20:34,127 - Epoch: [256][  217/  217]    Overall Loss 0.264312    Objective Loss 0.264312    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.103889    
2024-05-12 23:20:34,469 - 

2024-05-12 23:20:34,469 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:20:47,320 - Epoch: [257][  100/  217]    Overall Loss 0.259639    Objective Loss 0.259639                                        LR 0.000016    Time 0.128241    
2024-05-12 23:20:55,670 - Epoch: [257][  200/  217]    Overall Loss 0.262280    Objective Loss 0.262280                                        LR 0.000016    Time 0.105740    
2024-05-12 23:20:57,208 - Epoch: [257][  217/  217]    Overall Loss 0.262472    Objective Loss 0.262472    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.104518    
2024-05-12 23:20:57,751 - 

2024-05-12 23:20:57,753 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:21:10,032 - Epoch: [258][  100/  217]    Overall Loss 0.221286    Objective Loss 0.221286                                        LR 0.000016    Time 0.122516    
2024-05-12 23:21:19,233 - Epoch: [258][  200/  217]    Overall Loss 0.241946    Objective Loss 0.241946                                        LR 0.000016    Time 0.107141    
2024-05-12 23:21:20,921 - Epoch: [258][  217/  217]    Overall Loss 0.242550    Objective Loss 0.242550    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.106504    
2024-05-12 23:21:21,469 - 

2024-05-12 23:21:21,470 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:21:33,461 - Epoch: [259][  100/  217]    Overall Loss 0.245153    Objective Loss 0.245153                                        LR 0.000016    Time 0.119646    
2024-05-12 23:21:42,406 - Epoch: [259][  200/  217]    Overall Loss 0.235952    Objective Loss 0.235952                                        LR 0.000016    Time 0.104423    
2024-05-12 23:21:43,698 - Epoch: [259][  217/  217]    Overall Loss 0.235488    Objective Loss 0.235488    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.102172    
2024-05-12 23:21:44,222 - 

2024-05-12 23:21:44,223 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:21:57,883 - Epoch: [260][  100/  217]    Overall Loss 0.213111    Objective Loss 0.213111                                        LR 0.000016    Time 0.136304    
2024-05-12 23:22:07,888 - Epoch: [260][  200/  217]    Overall Loss 0.224694    Objective Loss 0.224694                                        LR 0.000016    Time 0.118044    
2024-05-12 23:22:09,488 - Epoch: [260][  217/  217]    Overall Loss 0.228633    Objective Loss 0.228633    Top1 88.524590    Top5 100.000000    LR 0.000016    Time 0.116146    
2024-05-12 23:22:10,063 - --- validate (epoch=260)-----------
2024-05-12 23:22:10,065 - 1736 samples (32 per mini-batch)
2024-05-12 23:22:15,866 - Epoch: [260][   55/   55]    Loss 2.173871    Top1 54.377880    Top5 71.716590    
2024-05-12 23:22:16,364 - ==> Top1: 54.378    Top5: 71.717    Loss: 2.174

2024-05-12 23:22:16,374 - ==> Best [Top1: 54.378   Top5: 71.717   Sparsity:0.00   Params: 382208 on epoch: 260]
2024-05-12 23:22:16,376 - Saving checkpoint to: logs/2024.05.12-214239/qat_checkpoint.pth.tar
2024-05-12 23:22:16,460 - 

2024-05-12 23:22:16,461 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:22:29,514 - Epoch: [261][  100/  217]    Overall Loss 0.205582    Objective Loss 0.205582                                        LR 0.000016    Time 0.130275    
2024-05-12 23:22:39,470 - Epoch: [261][  200/  217]    Overall Loss 0.217196    Objective Loss 0.217196                                        LR 0.000016    Time 0.114779    
2024-05-12 23:22:41,010 - Epoch: [261][  217/  217]    Overall Loss 0.217080    Objective Loss 0.217080    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.112860    
2024-05-12 23:22:41,476 - 

2024-05-12 23:22:41,478 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:22:54,767 - Epoch: [262][  100/  217]    Overall Loss 0.205422    Objective Loss 0.205422                                        LR 0.000016    Time 0.132595    
2024-05-12 23:23:01,898 - Epoch: [262][  200/  217]    Overall Loss 0.215870    Objective Loss 0.215870                                        LR 0.000016    Time 0.101856    
2024-05-12 23:23:03,136 - Epoch: [262][  217/  217]    Overall Loss 0.218106    Objective Loss 0.218106    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.099560    
2024-05-12 23:23:03,577 - 

2024-05-12 23:23:03,578 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:23:18,134 - Epoch: [263][  100/  217]    Overall Loss 0.212046    Objective Loss 0.212046                                        LR 0.000016    Time 0.145267    
2024-05-12 23:23:28,708 - Epoch: [263][  200/  217]    Overall Loss 0.218148    Objective Loss 0.218148                                        LR 0.000016    Time 0.125365    
2024-05-12 23:23:30,274 - Epoch: [263][  217/  217]    Overall Loss 0.217524    Objective Loss 0.217524    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.122733    
2024-05-12 23:23:30,917 - 

2024-05-12 23:23:30,921 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:23:42,924 - Epoch: [264][  100/  217]    Overall Loss 0.201214    Objective Loss 0.201214                                        LR 0.000016    Time 0.119766    
2024-05-12 23:23:51,335 - Epoch: [264][  200/  217]    Overall Loss 0.208034    Objective Loss 0.208034                                        LR 0.000016    Time 0.101816    
2024-05-12 23:23:52,640 - Epoch: [264][  217/  217]    Overall Loss 0.211180    Objective Loss 0.211180    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.099828    
2024-05-12 23:23:53,138 - 

2024-05-12 23:23:53,141 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:24:05,039 - Epoch: [265][  100/  217]    Overall Loss 0.195531    Objective Loss 0.195531                                        LR 0.000016    Time 0.118732    
2024-05-12 23:24:14,863 - Epoch: [265][  200/  217]    Overall Loss 0.200559    Objective Loss 0.200559                                        LR 0.000016    Time 0.108359    
2024-05-12 23:24:16,604 - Epoch: [265][  217/  217]    Overall Loss 0.200379    Objective Loss 0.200379    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.107865    
2024-05-12 23:24:17,282 - 

2024-05-12 23:24:17,283 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:24:31,583 - Epoch: [266][  100/  217]    Overall Loss 0.197759    Objective Loss 0.197759                                        LR 0.000016    Time 0.142694    
2024-05-12 23:24:40,164 - Epoch: [266][  200/  217]    Overall Loss 0.205591    Objective Loss 0.205591                                        LR 0.000016    Time 0.114126    
2024-05-12 23:24:41,382 - Epoch: [266][  217/  217]    Overall Loss 0.205977    Objective Loss 0.205977    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.110781    
2024-05-12 23:24:41,889 - 

2024-05-12 23:24:41,891 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:24:54,755 - Epoch: [267][  100/  217]    Overall Loss 0.187932    Objective Loss 0.187932                                        LR 0.000016    Time 0.128370    
2024-05-12 23:25:04,740 - Epoch: [267][  200/  217]    Overall Loss 0.200625    Objective Loss 0.200625                                        LR 0.000016    Time 0.113977    
2024-05-12 23:25:06,126 - Epoch: [267][  217/  217]    Overall Loss 0.200542    Objective Loss 0.200542    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.111411    
2024-05-12 23:25:06,587 - 

2024-05-12 23:25:06,588 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:25:19,163 - Epoch: [268][  100/  217]    Overall Loss 0.188257    Objective Loss 0.188257                                        LR 0.000016    Time 0.125470    
2024-05-12 23:25:27,548 - Epoch: [268][  200/  217]    Overall Loss 0.201173    Objective Loss 0.201173                                        LR 0.000016    Time 0.104539    
2024-05-12 23:25:28,692 - Epoch: [268][  217/  217]    Overall Loss 0.200702    Objective Loss 0.200702    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.101597    
2024-05-12 23:25:29,282 - 

2024-05-12 23:25:29,283 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:25:42,044 - Epoch: [269][  100/  217]    Overall Loss 0.181412    Objective Loss 0.181412                                        LR 0.000016    Time 0.127329    
2024-05-12 23:25:52,367 - Epoch: [269][  200/  217]    Overall Loss 0.187173    Objective Loss 0.187173                                        LR 0.000016    Time 0.115143    
2024-05-12 23:25:53,905 - Epoch: [269][  217/  217]    Overall Loss 0.187369    Objective Loss 0.187369    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.113186    
2024-05-12 23:25:54,319 - 

2024-05-12 23:25:54,320 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:26:06,948 - Epoch: [270][  100/  217]    Overall Loss 0.192235    Objective Loss 0.192235                                        LR 0.000016    Time 0.125962    
2024-05-12 23:26:15,599 - Epoch: [270][  200/  217]    Overall Loss 0.197692    Objective Loss 0.197692                                        LR 0.000016    Time 0.106115    
2024-05-12 23:26:17,149 - Epoch: [270][  217/  217]    Overall Loss 0.196920    Objective Loss 0.196920    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.104916    
2024-05-12 23:26:17,788 - --- validate (epoch=270)-----------
2024-05-12 23:26:17,790 - 1736 samples (32 per mini-batch)
2024-05-12 23:26:25,787 - Epoch: [270][   55/   55]    Loss 2.267148    Top1 52.592166    Top5 70.449309    
2024-05-12 23:26:26,214 - ==> Top1: 52.592    Top5: 70.449    Loss: 2.267

2024-05-12 23:26:26,224 - ==> Best [Top1: 54.378   Top5: 71.717   Sparsity:0.00   Params: 382208 on epoch: 260]
2024-05-12 23:26:26,225 - Saving checkpoint to: logs/2024.05.12-214239/qat_checkpoint.pth.tar
2024-05-12 23:26:26,285 - 

2024-05-12 23:26:26,285 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:26:36,156 - Epoch: [271][  100/  217]    Overall Loss 0.188256    Objective Loss 0.188256                                        LR 0.000016    Time 0.098499    
2024-05-12 23:26:45,446 - Epoch: [271][  200/  217]    Overall Loss 0.194305    Objective Loss 0.194305                                        LR 0.000016    Time 0.095568    
2024-05-12 23:26:46,653 - Epoch: [271][  217/  217]    Overall Loss 0.194002    Objective Loss 0.194002    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.093619    
2024-05-12 23:26:47,109 - 

2024-05-12 23:26:47,110 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:27:00,290 - Epoch: [272][  100/  217]    Overall Loss 0.178909    Objective Loss 0.178909                                        LR 0.000016    Time 0.131516    
2024-05-12 23:27:08,348 - Epoch: [272][  200/  217]    Overall Loss 0.183778    Objective Loss 0.183778                                        LR 0.000016    Time 0.105929    
2024-05-12 23:27:09,620 - Epoch: [272][  217/  217]    Overall Loss 0.184126    Objective Loss 0.184126    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.103471    
2024-05-12 23:27:10,270 - 

2024-05-12 23:27:10,271 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:27:21,575 - Epoch: [273][  100/  217]    Overall Loss 0.174615    Objective Loss 0.174615                                        LR 0.000016    Time 0.112812    
2024-05-12 23:27:32,097 - Epoch: [273][  200/  217]    Overall Loss 0.179736    Objective Loss 0.179736                                        LR 0.000016    Time 0.108873    
2024-05-12 23:27:33,088 - Epoch: [273][  217/  217]    Overall Loss 0.181556    Objective Loss 0.181556    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.104891    
2024-05-12 23:27:33,428 - 

2024-05-12 23:27:33,429 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:27:45,635 - Epoch: [274][  100/  217]    Overall Loss 0.164346    Objective Loss 0.164346                                        LR 0.000016    Time 0.121779    
2024-05-12 23:27:53,463 - Epoch: [274][  200/  217]    Overall Loss 0.171612    Objective Loss 0.171612                                        LR 0.000016    Time 0.099919    
2024-05-12 23:27:55,469 - Epoch: [274][  217/  217]    Overall Loss 0.171692    Objective Loss 0.171692    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.101303    
2024-05-12 23:27:56,108 - 

2024-05-12 23:27:56,111 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:28:08,186 - Epoch: [275][  100/  217]    Overall Loss 0.174448    Objective Loss 0.174448                                        LR 0.000016    Time 0.120495    
2024-05-12 23:28:18,847 - Epoch: [275][  200/  217]    Overall Loss 0.175178    Objective Loss 0.175178                                        LR 0.000016    Time 0.113413    
2024-05-12 23:28:19,861 - Epoch: [275][  217/  217]    Overall Loss 0.173750    Objective Loss 0.173750    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.109186    
2024-05-12 23:28:20,248 - 

2024-05-12 23:28:20,250 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:28:33,777 - Epoch: [276][  100/  217]    Overall Loss 0.163436    Objective Loss 0.163436                                        LR 0.000016    Time 0.134952    
2024-05-12 23:28:41,478 - Epoch: [276][  200/  217]    Overall Loss 0.171316    Objective Loss 0.171316                                        LR 0.000016    Time 0.105869    
2024-05-12 23:28:43,205 - Epoch: [276][  217/  217]    Overall Loss 0.172325    Objective Loss 0.172325    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.105510    
2024-05-12 23:28:43,729 - 

2024-05-12 23:28:43,731 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:28:54,893 - Epoch: [277][  100/  217]    Overall Loss 0.169915    Objective Loss 0.169915                                        LR 0.000016    Time 0.111363    
2024-05-12 23:29:05,180 - Epoch: [277][  200/  217]    Overall Loss 0.162664    Objective Loss 0.162664                                        LR 0.000016    Time 0.106989    
2024-05-12 23:29:06,060 - Epoch: [277][  217/  217]    Overall Loss 0.165596    Objective Loss 0.165596    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.102645    
2024-05-12 23:29:06,316 - 

2024-05-12 23:29:06,316 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:29:16,936 - Epoch: [278][  100/  217]    Overall Loss 0.163954    Objective Loss 0.163954                                        LR 0.000016    Time 0.105960    
2024-05-12 23:29:25,279 - Epoch: [278][  200/  217]    Overall Loss 0.169581    Objective Loss 0.169581                                        LR 0.000016    Time 0.094577    
2024-05-12 23:29:26,552 - Epoch: [278][  217/  217]    Overall Loss 0.170578    Objective Loss 0.170578    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.093016    
2024-05-12 23:29:27,027 - 

2024-05-12 23:29:27,028 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:29:39,093 - Epoch: [279][  100/  217]    Overall Loss 0.172446    Objective Loss 0.172446                                        LR 0.000016    Time 0.120377    
2024-05-12 23:29:46,139 - Epoch: [279][  200/  217]    Overall Loss 0.173037    Objective Loss 0.173037                                        LR 0.000016    Time 0.095319    
2024-05-12 23:29:47,756 - Epoch: [279][  217/  217]    Overall Loss 0.175869    Objective Loss 0.175869    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.095277    
2024-05-12 23:29:48,352 - 

2024-05-12 23:29:48,353 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:30:00,825 - Epoch: [280][  100/  217]    Overall Loss 0.148389    Objective Loss 0.148389                                        LR 0.000016    Time 0.124465    
2024-05-12 23:30:11,397 - Epoch: [280][  200/  217]    Overall Loss 0.168946    Objective Loss 0.168946                                        LR 0.000016    Time 0.114966    
2024-05-12 23:30:12,511 - Epoch: [280][  217/  217]    Overall Loss 0.172118    Objective Loss 0.172118    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.111072    
2024-05-12 23:30:12,853 - --- validate (epoch=280)-----------
2024-05-12 23:30:12,854 - 1736 samples (32 per mini-batch)
2024-05-12 23:30:20,270 - Epoch: [280][   55/   55]    Loss 2.218272    Top1 53.859447    Top5 72.119816    
2024-05-12 23:30:20,910 - ==> Top1: 53.859    Top5: 72.120    Loss: 2.218

2024-05-12 23:30:20,923 - ==> Best [Top1: 54.378   Top5: 71.717   Sparsity:0.00   Params: 382208 on epoch: 260]
2024-05-12 23:30:20,924 - Saving checkpoint to: logs/2024.05.12-214239/qat_checkpoint.pth.tar
2024-05-12 23:30:21,008 - 

2024-05-12 23:30:21,009 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:30:32,936 - Epoch: [281][  100/  217]    Overall Loss 0.155002    Objective Loss 0.155002                                        LR 0.000016    Time 0.118996    
2024-05-12 23:30:43,074 - Epoch: [281][  200/  217]    Overall Loss 0.169390    Objective Loss 0.169390                                        LR 0.000016    Time 0.110058    
2024-05-12 23:30:44,139 - Epoch: [281][  217/  217]    Overall Loss 0.171316    Objective Loss 0.171316    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.106327    
2024-05-12 23:30:44,525 - 

2024-05-12 23:30:44,527 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:30:58,261 - Epoch: [282][  100/  217]    Overall Loss 0.153778    Objective Loss 0.153778                                        LR 0.000016    Time 0.136988    
2024-05-12 23:31:07,255 - Epoch: [282][  200/  217]    Overall Loss 0.163009    Objective Loss 0.163009                                        LR 0.000016    Time 0.113345    
2024-05-12 23:31:08,723 - Epoch: [282][  217/  217]    Overall Loss 0.164426    Objective Loss 0.164426    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.111209    
2024-05-12 23:31:09,311 - 

2024-05-12 23:31:09,313 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:31:20,587 - Epoch: [283][  100/  217]    Overall Loss 0.160086    Objective Loss 0.160086                                        LR 0.000016    Time 0.112459    
2024-05-12 23:31:29,626 - Epoch: [283][  200/  217]    Overall Loss 0.169280    Objective Loss 0.169280                                        LR 0.000016    Time 0.101300    
2024-05-12 23:31:30,661 - Epoch: [283][  217/  217]    Overall Loss 0.168035    Objective Loss 0.168035    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.098116    
2024-05-12 23:31:31,022 - 

2024-05-12 23:31:31,023 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:31:43,127 - Epoch: [284][  100/  217]    Overall Loss 0.150591    Objective Loss 0.150591                                        LR 0.000016    Time 0.120791    
2024-05-12 23:31:50,749 - Epoch: [284][  200/  217]    Overall Loss 0.155420    Objective Loss 0.155420                                        LR 0.000016    Time 0.098398    
2024-05-12 23:31:52,030 - Epoch: [284][  217/  217]    Overall Loss 0.159279    Objective Loss 0.159279    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.096571    
2024-05-12 23:31:52,563 - 

2024-05-12 23:31:52,567 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:32:03,693 - Epoch: [285][  100/  217]    Overall Loss 0.135182    Objective Loss 0.135182                                        LR 0.000016    Time 0.111005    
2024-05-12 23:32:12,442 - Epoch: [285][  200/  217]    Overall Loss 0.140754    Objective Loss 0.140754                                        LR 0.000016    Time 0.099118    
2024-05-12 23:32:14,032 - Epoch: [285][  217/  217]    Overall Loss 0.143480    Objective Loss 0.143480    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.098654    
2024-05-12 23:32:14,572 - 

2024-05-12 23:32:14,574 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:32:25,288 - Epoch: [286][  100/  217]    Overall Loss 0.153165    Objective Loss 0.153165                                        LR 0.000016    Time 0.106883    
2024-05-12 23:32:34,988 - Epoch: [286][  200/  217]    Overall Loss 0.153463    Objective Loss 0.153463                                        LR 0.000016    Time 0.101812    
2024-05-12 23:32:36,058 - Epoch: [286][  217/  217]    Overall Loss 0.157260    Objective Loss 0.157260    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.098753    
2024-05-12 23:32:36,396 - 

2024-05-12 23:32:36,397 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:32:48,570 - Epoch: [287][  100/  217]    Overall Loss 0.138675    Objective Loss 0.138675                                        LR 0.000016    Time 0.121478    
2024-05-12 23:32:56,169 - Epoch: [287][  200/  217]    Overall Loss 0.158202    Objective Loss 0.158202                                        LR 0.000016    Time 0.098623    
2024-05-12 23:32:56,949 - Epoch: [287][  217/  217]    Overall Loss 0.157758    Objective Loss 0.157758    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.094482    
2024-05-12 23:32:57,189 - 

2024-05-12 23:32:57,190 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:33:07,617 - Epoch: [288][  100/  217]    Overall Loss 0.145244    Objective Loss 0.145244                                        LR 0.000016    Time 0.104038    
2024-05-12 23:33:16,935 - Epoch: [288][  200/  217]    Overall Loss 0.158709    Objective Loss 0.158709                                        LR 0.000016    Time 0.098484    
2024-05-12 23:33:18,244 - Epoch: [288][  217/  217]    Overall Loss 0.159035    Objective Loss 0.159035    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.096775    
2024-05-12 23:33:18,891 - 

2024-05-12 23:33:18,893 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:33:31,934 - Epoch: [289][  100/  217]    Overall Loss 0.131206    Objective Loss 0.131206                                        LR 0.000016    Time 0.130141    
2024-05-12 23:33:41,544 - Epoch: [289][  200/  217]    Overall Loss 0.151815    Objective Loss 0.151815                                        LR 0.000016    Time 0.112993    
2024-05-12 23:33:43,163 - Epoch: [289][  217/  217]    Overall Loss 0.151502    Objective Loss 0.151502    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.111576    
2024-05-12 23:33:43,766 - 

2024-05-12 23:33:43,768 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:33:56,469 - Epoch: [290][  100/  217]    Overall Loss 0.142659    Objective Loss 0.142659                                        LR 0.000016    Time 0.126753    
2024-05-12 23:34:06,251 - Epoch: [290][  200/  217]    Overall Loss 0.151100    Objective Loss 0.151100                                        LR 0.000016    Time 0.112160    
2024-05-12 23:34:07,344 - Epoch: [290][  217/  217]    Overall Loss 0.152609    Objective Loss 0.152609    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.108390    
2024-05-12 23:34:07,783 - --- validate (epoch=290)-----------
2024-05-12 23:34:07,784 - 1736 samples (32 per mini-batch)
2024-05-12 23:34:14,606 - Epoch: [290][   55/   55]    Loss 2.272029    Top1 54.089862    Top5 71.370968    
2024-05-12 23:34:15,136 - ==> Top1: 54.090    Top5: 71.371    Loss: 2.272

2024-05-12 23:34:15,145 - ==> Best [Top1: 54.378   Top5: 71.717   Sparsity:0.00   Params: 382208 on epoch: 260]
2024-05-12 23:34:15,146 - Saving checkpoint to: logs/2024.05.12-214239/qat_checkpoint.pth.tar
2024-05-12 23:34:15,204 - 

2024-05-12 23:34:15,205 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:34:26,295 - Epoch: [291][  100/  217]    Overall Loss 0.130130    Objective Loss 0.130130                                        LR 0.000016    Time 0.110665    
2024-05-12 23:34:35,766 - Epoch: [291][  200/  217]    Overall Loss 0.144967    Objective Loss 0.144967                                        LR 0.000016    Time 0.102571    
2024-05-12 23:34:37,322 - Epoch: [291][  217/  217]    Overall Loss 0.147234    Objective Loss 0.147234    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.101684    
2024-05-12 23:34:37,890 - 

2024-05-12 23:34:37,892 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:34:49,048 - Epoch: [292][  100/  217]    Overall Loss 0.145804    Objective Loss 0.145804                                        LR 0.000016    Time 0.111275    
2024-05-12 23:34:56,901 - Epoch: [292][  200/  217]    Overall Loss 0.155442    Objective Loss 0.155442                                        LR 0.000016    Time 0.094789    
2024-05-12 23:34:58,732 - Epoch: [292][  217/  217]    Overall Loss 0.155130    Objective Loss 0.155130    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.095776    
2024-05-12 23:34:59,313 - 

2024-05-12 23:34:59,315 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:35:11,353 - Epoch: [293][  100/  217]    Overall Loss 0.137105    Objective Loss 0.137105                                        LR 0.000016    Time 0.120109    
2024-05-12 23:35:20,311 - Epoch: [293][  200/  217]    Overall Loss 0.138515    Objective Loss 0.138515                                        LR 0.000016    Time 0.104719    
2024-05-12 23:35:21,468 - Epoch: [293][  217/  217]    Overall Loss 0.143450    Objective Loss 0.143450    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.101823    
2024-05-12 23:35:21,918 - 

2024-05-12 23:35:21,920 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:35:34,248 - Epoch: [294][  100/  217]    Overall Loss 0.144367    Objective Loss 0.144367                                        LR 0.000016    Time 0.123027    
2024-05-12 23:35:42,538 - Epoch: [294][  200/  217]    Overall Loss 0.148193    Objective Loss 0.148193                                        LR 0.000016    Time 0.102851    
2024-05-12 23:35:44,255 - Epoch: [294][  217/  217]    Overall Loss 0.148484    Objective Loss 0.148484    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.102681    
2024-05-12 23:35:44,727 - 

2024-05-12 23:35:44,728 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:35:56,132 - Epoch: [295][  100/  217]    Overall Loss 0.136204    Objective Loss 0.136204                                        LR 0.000016    Time 0.113782    
2024-05-12 23:36:03,973 - Epoch: [295][  200/  217]    Overall Loss 0.145127    Objective Loss 0.145127                                        LR 0.000016    Time 0.095977    
2024-05-12 23:36:05,471 - Epoch: [295][  217/  217]    Overall Loss 0.145894    Objective Loss 0.145894    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.095340    
2024-05-12 23:36:06,077 - 

2024-05-12 23:36:06,080 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:36:15,891 - Epoch: [296][  100/  217]    Overall Loss 0.135723    Objective Loss 0.135723                                        LR 0.000016    Time 0.097916    
2024-05-12 23:36:23,759 - Epoch: [296][  200/  217]    Overall Loss 0.142363    Objective Loss 0.142363                                        LR 0.000016    Time 0.088191    
2024-05-12 23:36:25,283 - Epoch: [296][  217/  217]    Overall Loss 0.144188    Objective Loss 0.144188    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.088281    
2024-05-12 23:36:25,787 - 

2024-05-12 23:36:25,788 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:36:36,894 - Epoch: [297][  100/  217]    Overall Loss 0.150943    Objective Loss 0.150943                                        LR 0.000016    Time 0.110813    
2024-05-12 23:36:45,487 - Epoch: [297][  200/  217]    Overall Loss 0.151096    Objective Loss 0.151096                                        LR 0.000016    Time 0.098251    
2024-05-12 23:36:46,491 - Epoch: [297][  217/  217]    Overall Loss 0.150929    Objective Loss 0.150929    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.095163    
2024-05-12 23:36:46,897 - 

2024-05-12 23:36:46,899 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:36:57,174 - Epoch: [298][  100/  217]    Overall Loss 0.132461    Objective Loss 0.132461                                        LR 0.000016    Time 0.102524    
2024-05-12 23:37:05,977 - Epoch: [298][  200/  217]    Overall Loss 0.143276    Objective Loss 0.143276                                        LR 0.000016    Time 0.095154    
2024-05-12 23:37:07,198 - Epoch: [298][  217/  217]    Overall Loss 0.144677    Objective Loss 0.144677    Top1 96.721311    Top5 98.360656    LR 0.000016    Time 0.093307    
2024-05-12 23:37:07,720 - 

2024-05-12 23:37:07,722 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-12 23:37:19,013 - Epoch: [299][  100/  217]    Overall Loss 0.140526    Objective Loss 0.140526                                        LR 0.000016    Time 0.112658    
2024-05-12 23:37:26,324 - Epoch: [299][  200/  217]    Overall Loss 0.152029    Objective Loss 0.152029                                        LR 0.000016    Time 0.092777    
2024-05-12 23:37:28,023 - Epoch: [299][  217/  217]    Overall Loss 0.153965    Objective Loss 0.153965    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.093315    
2024-05-12 23:37:28,585 - --- test ---------------------
2024-05-12 23:37:28,587 - 1736 samples (32 per mini-batch)
2024-05-12 23:37:33,293 - Test: [   55/   55]    Loss 2.258776    Top1 53.744240    Top5 72.062212    
2024-05-12 23:37:33,584 - ==> Top1: 53.744    Top5: 72.062    Loss: 2.259

2024-05-12 23:37:33,589 - 
2024-05-12 23:37:33,589 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.12-214239/2024.05.12-214239.log
