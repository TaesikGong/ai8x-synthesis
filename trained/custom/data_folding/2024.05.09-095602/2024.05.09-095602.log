2024-05-09 09:56:02,799 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.09-095602/2024.05.09-095602.log
2024-05-09 09:56:08,045 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2024-05-09 09:56:08,046 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2024-05-09 09:56:08,282 - Dataset sizes:
	training=6941
	validation=1736
	test=1736
2024-05-09 09:56:08,282 - Reading compression schedule from: policies/schedule-cifar100.yaml
2024-05-09 09:56:08,288 - 

2024-05-09 09:56:08,289 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 09:56:32,647 - Epoch: [0][  100/  217]    Overall Loss 4.092563    Objective Loss 4.092563                                        LR 0.001000    Time 0.243491    
2024-05-09 09:57:00,081 - Epoch: [0][  200/  217]    Overall Loss 3.801544    Objective Loss 3.801544                                        LR 0.001000    Time 0.258859    
2024-05-09 09:57:04,283 - Epoch: [0][  217/  217]    Overall Loss 3.771691    Objective Loss 3.771691    Top1 34.426230    Top5 42.622951    LR 0.001000    Time 0.257930    
2024-05-09 09:57:04,561 - --- validate (epoch=0)-----------
2024-05-09 09:57:04,561 - 1736 samples (32 per mini-batch)
2024-05-09 09:57:20,274 - Epoch: [0][   55/   55]    Loss 3.505429    Top1 27.073733    Top5 37.845622    
2024-05-09 09:57:20,485 - ==> Top1: 27.074    Top5: 37.846    Loss: 3.505

2024-05-09 09:57:20,492 - ==> Best [Top1: 27.074   Top5: 37.846   Sparsity:0.00   Params: 386672 on epoch: 0]
2024-05-09 09:57:20,492 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 09:57:20,546 - 

2024-05-09 09:57:20,546 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 09:57:49,626 - Epoch: [1][  100/  217]    Overall Loss 3.325378    Objective Loss 3.325378                                        LR 0.001000    Time 0.290666    
2024-05-09 09:58:19,760 - Epoch: [1][  200/  217]    Overall Loss 3.246713    Objective Loss 3.246713                                        LR 0.001000    Time 0.295943    
2024-05-09 09:58:24,683 - Epoch: [1][  217/  217]    Overall Loss 3.229691    Objective Loss 3.229691    Top1 29.508197    Top5 34.426230    LR 0.001000    Time 0.295433    
2024-05-09 09:58:24,990 - 

2024-05-09 09:58:24,991 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 09:58:55,695 - Epoch: [2][  100/  217]    Overall Loss 2.947156    Objective Loss 2.947156                                        LR 0.001000    Time 0.306850    
2024-05-09 09:59:24,382 - Epoch: [2][  200/  217]    Overall Loss 2.916782    Objective Loss 2.916782                                        LR 0.001000    Time 0.296800    
2024-05-09 09:59:29,506 - Epoch: [2][  217/  217]    Overall Loss 2.907472    Objective Loss 2.907472    Top1 40.983607    Top5 49.180328    LR 0.001000    Time 0.297148    
2024-05-09 09:59:29,730 - 

2024-05-09 09:59:29,731 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 09:59:58,254 - Epoch: [3][  100/  217]    Overall Loss 2.650435    Objective Loss 2.650435                                        LR 0.001000    Time 0.285117    
2024-05-09 10:00:25,451 - Epoch: [3][  200/  217]    Overall Loss 2.610917    Objective Loss 2.610917                                        LR 0.001000    Time 0.278473    
2024-05-09 10:00:30,733 - Epoch: [3][  217/  217]    Overall Loss 2.595732    Objective Loss 2.595732    Top1 39.344262    Top5 55.737705    LR 0.001000    Time 0.280984    
2024-05-09 10:00:30,989 - 

2024-05-09 10:00:30,990 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:01:04,101 - Epoch: [4][  100/  217]    Overall Loss 2.365038    Objective Loss 2.365038                                        LR 0.001000    Time 0.330982    
2024-05-09 10:01:25,994 - Epoch: [4][  200/  217]    Overall Loss 2.309688    Objective Loss 2.309688                                        LR 0.001000    Time 0.274904    
2024-05-09 10:01:31,019 - Epoch: [4][  217/  217]    Overall Loss 2.291529    Objective Loss 2.291529    Top1 49.180328    Top5 72.131148    LR 0.001000    Time 0.276512    
2024-05-09 10:01:31,258 - 

2024-05-09 10:01:31,259 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:02:02,249 - Epoch: [5][  100/  217]    Overall Loss 2.004099    Objective Loss 2.004099                                        LR 0.001000    Time 0.309783    
2024-05-09 10:02:25,741 - Epoch: [5][  200/  217]    Overall Loss 1.998082    Objective Loss 1.998082                                        LR 0.001000    Time 0.272296    
2024-05-09 10:02:31,885 - Epoch: [5][  217/  217]    Overall Loss 2.006698    Objective Loss 2.006698    Top1 50.819672    Top5 72.131148    LR 0.001000    Time 0.279269    
2024-05-09 10:02:32,413 - 

2024-05-09 10:02:32,413 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:02:56,379 - Epoch: [6][  100/  217]    Overall Loss 1.718183    Objective Loss 1.718183                                        LR 0.001000    Time 0.239549    
2024-05-09 10:03:26,322 - Epoch: [6][  200/  217]    Overall Loss 1.733421    Objective Loss 1.733421                                        LR 0.001000    Time 0.269431    
2024-05-09 10:03:30,562 - Epoch: [6][  217/  217]    Overall Loss 1.738846    Objective Loss 1.738846    Top1 67.213115    Top5 88.524590    LR 0.001000    Time 0.267851    
2024-05-09 10:03:30,773 - 

2024-05-09 10:03:30,774 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:04:03,095 - Epoch: [7][  100/  217]    Overall Loss 1.511221    Objective Loss 1.511221                                        LR 0.001000    Time 0.323096    
2024-05-09 10:04:28,072 - Epoch: [7][  200/  217]    Overall Loss 1.531897    Objective Loss 1.531897                                        LR 0.001000    Time 0.286384    
2024-05-09 10:04:32,940 - Epoch: [7][  217/  217]    Overall Loss 1.536318    Objective Loss 1.536318    Top1 62.295082    Top5 85.245902    LR 0.001000    Time 0.286373    
2024-05-09 10:04:33,345 - 

2024-05-09 10:04:33,347 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:05:02,205 - Epoch: [8][  100/  217]    Overall Loss 1.315783    Objective Loss 1.315783                                        LR 0.001000    Time 0.288479    
2024-05-09 10:05:29,365 - Epoch: [8][  200/  217]    Overall Loss 1.328299    Objective Loss 1.328299                                        LR 0.001000    Time 0.279984    
2024-05-09 10:05:32,493 - Epoch: [8][  217/  217]    Overall Loss 1.332608    Objective Loss 1.332608    Top1 62.295082    Top5 86.885246    LR 0.001000    Time 0.272454    
2024-05-09 10:05:32,961 - 

2024-05-09 10:05:32,962 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:06:01,640 - Epoch: [9][  100/  217]    Overall Loss 1.117235    Objective Loss 1.117235                                        LR 0.001000    Time 0.286671    
2024-05-09 10:06:30,681 - Epoch: [9][  200/  217]    Overall Loss 1.125536    Objective Loss 1.125536                                        LR 0.001000    Time 0.288492    
2024-05-09 10:06:36,228 - Epoch: [9][  217/  217]    Overall Loss 1.131386    Objective Loss 1.131386    Top1 62.295082    Top5 85.245902    LR 0.001000    Time 0.291446    
2024-05-09 10:06:36,783 - 

2024-05-09 10:06:36,784 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:07:08,000 - Epoch: [10][  100/  217]    Overall Loss 0.899579    Objective Loss 0.899579                                        LR 0.001000    Time 0.312034    
2024-05-09 10:07:36,083 - Epoch: [10][  200/  217]    Overall Loss 0.938056    Objective Loss 0.938056                                        LR 0.001000    Time 0.296370    
2024-05-09 10:07:38,836 - Epoch: [10][  217/  217]    Overall Loss 0.944193    Objective Loss 0.944193    Top1 75.409836    Top5 95.081967    LR 0.001000    Time 0.285827    
2024-05-09 10:07:39,409 - --- validate (epoch=10)-----------
2024-05-09 10:07:39,410 - 1736 samples (32 per mini-batch)
2024-05-09 10:07:56,877 - Epoch: [10][   55/   55]    Loss 2.044911    Top1 53.225806    Top5 72.292627    
2024-05-09 10:07:57,231 - ==> Top1: 53.226    Top5: 72.293    Loss: 2.045

2024-05-09 10:07:57,235 - ==> Best [Top1: 53.226   Top5: 72.293   Sparsity:0.00   Params: 386672 on epoch: 10]
2024-05-09 10:07:57,235 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 10:07:57,286 - 

2024-05-09 10:07:57,287 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:08:27,121 - Epoch: [11][  100/  217]    Overall Loss 0.726544    Objective Loss 0.726544                                        LR 0.001000    Time 0.298206    
2024-05-09 10:08:51,646 - Epoch: [11][  200/  217]    Overall Loss 0.774227    Objective Loss 0.774227                                        LR 0.001000    Time 0.271669    
2024-05-09 10:08:55,087 - Epoch: [11][  217/  217]    Overall Loss 0.777487    Objective Loss 0.777487    Top1 81.967213    Top5 90.163934    LR 0.001000    Time 0.266232    
2024-05-09 10:08:55,820 - 

2024-05-09 10:08:55,821 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:09:28,293 - Epoch: [12][  100/  217]    Overall Loss 0.581117    Objective Loss 0.581117                                        LR 0.001000    Time 0.324566    
2024-05-09 10:09:54,233 - Epoch: [12][  200/  217]    Overall Loss 0.607920    Objective Loss 0.607920                                        LR 0.001000    Time 0.291924    
2024-05-09 10:09:56,914 - Epoch: [12][  217/  217]    Overall Loss 0.619759    Objective Loss 0.619759    Top1 80.327869    Top5 98.360656    LR 0.001000    Time 0.281401    
2024-05-09 10:09:57,148 - 

2024-05-09 10:09:57,149 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:10:28,393 - Epoch: [13][  100/  217]    Overall Loss 0.463564    Objective Loss 0.463564                                        LR 0.001000    Time 0.312319    
2024-05-09 10:10:54,229 - Epoch: [13][  200/  217]    Overall Loss 0.475732    Objective Loss 0.475732                                        LR 0.001000    Time 0.285279    
2024-05-09 10:10:58,968 - Epoch: [13][  217/  217]    Overall Loss 0.479104    Objective Loss 0.479104    Top1 90.163934    Top5 96.721311    LR 0.001000    Time 0.284755    
2024-05-09 10:10:59,338 - 

2024-05-09 10:10:59,339 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:11:30,873 - Epoch: [14][  100/  217]    Overall Loss 0.348941    Objective Loss 0.348941                                        LR 0.001000    Time 0.315205    
2024-05-09 10:11:55,146 - Epoch: [14][  200/  217]    Overall Loss 0.368328    Objective Loss 0.368328                                        LR 0.001000    Time 0.278909    
2024-05-09 10:12:00,337 - Epoch: [14][  217/  217]    Overall Loss 0.375022    Objective Loss 0.375022    Top1 86.885246    Top5 100.000000    LR 0.001000    Time 0.280967    
2024-05-09 10:12:01,225 - 

2024-05-09 10:12:01,227 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:12:35,281 - Epoch: [15][  100/  217]    Overall Loss 0.269662    Objective Loss 0.269662                                        LR 0.001000    Time 0.340414    
2024-05-09 10:13:01,571 - Epoch: [15][  200/  217]    Overall Loss 0.278273    Objective Loss 0.278273                                        LR 0.001000    Time 0.301595    
2024-05-09 10:13:06,740 - Epoch: [15][  217/  217]    Overall Loss 0.282705    Objective Loss 0.282705    Top1 90.163934    Top5 100.000000    LR 0.001000    Time 0.301777    
2024-05-09 10:13:07,375 - 

2024-05-09 10:13:07,376 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:13:36,625 - Epoch: [16][  100/  217]    Overall Loss 0.175824    Objective Loss 0.175824                                        LR 0.001000    Time 0.292359    
2024-05-09 10:14:02,861 - Epoch: [16][  200/  217]    Overall Loss 0.192993    Objective Loss 0.192993                                        LR 0.001000    Time 0.277297    
2024-05-09 10:14:08,078 - Epoch: [16][  217/  217]    Overall Loss 0.197549    Objective Loss 0.197549    Top1 93.442623    Top5 100.000000    LR 0.001000    Time 0.279606    
2024-05-09 10:14:08,375 - 

2024-05-09 10:14:08,376 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:14:37,368 - Epoch: [17][  100/  217]    Overall Loss 0.145911    Objective Loss 0.145911                                        LR 0.001000    Time 0.289796    
2024-05-09 10:15:06,478 - Epoch: [17][  200/  217]    Overall Loss 0.156281    Objective Loss 0.156281                                        LR 0.001000    Time 0.290383    
2024-05-09 10:15:11,667 - Epoch: [17][  217/  217]    Overall Loss 0.159164    Objective Loss 0.159164    Top1 95.081967    Top5 98.360656    LR 0.001000    Time 0.291535    
2024-05-09 10:15:12,140 - 

2024-05-09 10:15:12,141 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:15:45,701 - Epoch: [18][  100/  217]    Overall Loss 0.109507    Objective Loss 0.109507                                        LR 0.001000    Time 0.335474    
2024-05-09 10:16:09,369 - Epoch: [18][  200/  217]    Overall Loss 0.117116    Objective Loss 0.117116                                        LR 0.001000    Time 0.286017    
2024-05-09 10:16:14,951 - Epoch: [18][  217/  217]    Overall Loss 0.120040    Objective Loss 0.120040    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.289320    
2024-05-09 10:16:15,271 - 

2024-05-09 10:16:15,272 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:16:52,493 - Epoch: [19][  100/  217]    Overall Loss 0.086961    Objective Loss 0.086961                                        LR 0.001000    Time 0.372083    
2024-05-09 10:17:18,533 - Epoch: [19][  200/  217]    Overall Loss 0.089277    Objective Loss 0.089277                                        LR 0.001000    Time 0.316180    
2024-05-09 10:17:21,940 - Epoch: [19][  217/  217]    Overall Loss 0.091397    Objective Loss 0.091397    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.307098    
2024-05-09 10:17:22,276 - 

2024-05-09 10:17:22,277 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:17:54,372 - Epoch: [20][  100/  217]    Overall Loss 0.088304    Objective Loss 0.088304                                        LR 0.001000    Time 0.320795    
2024-05-09 10:18:18,497 - Epoch: [20][  200/  217]    Overall Loss 0.098329    Objective Loss 0.098329                                        LR 0.001000    Time 0.280948    
2024-05-09 10:18:23,422 - Epoch: [20][  217/  217]    Overall Loss 0.103906    Objective Loss 0.103906    Top1 95.081967    Top5 98.360656    LR 0.001000    Time 0.281616    
2024-05-09 10:18:23,746 - --- validate (epoch=20)-----------
2024-05-09 10:18:23,746 - 1736 samples (32 per mini-batch)
2024-05-09 10:18:41,316 - Epoch: [20][   55/   55]    Loss 2.608850    Top1 51.267281    Top5 68.894009    
2024-05-09 10:18:41,704 - ==> Top1: 51.267    Top5: 68.894    Loss: 2.609

2024-05-09 10:18:41,710 - ==> Best [Top1: 53.226   Top5: 72.293   Sparsity:0.00   Params: 386672 on epoch: 10]
2024-05-09 10:18:41,711 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 10:18:41,767 - 

2024-05-09 10:18:41,767 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:19:12,831 - Epoch: [21][  100/  217]    Overall Loss 0.112575    Objective Loss 0.112575                                        LR 0.001000    Time 0.310500    
2024-05-09 10:19:38,257 - Epoch: [21][  200/  217]    Overall Loss 0.132361    Objective Loss 0.132361                                        LR 0.001000    Time 0.282322    
2024-05-09 10:19:41,814 - Epoch: [21][  217/  217]    Overall Loss 0.140012    Objective Loss 0.140012    Top1 95.081967    Top5 100.000000    LR 0.001000    Time 0.276588    
2024-05-09 10:19:42,089 - 

2024-05-09 10:19:42,090 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:20:16,593 - Epoch: [22][  100/  217]    Overall Loss 0.183603    Objective Loss 0.183603                                        LR 0.001000    Time 0.344883    
2024-05-09 10:20:42,980 - Epoch: [22][  200/  217]    Overall Loss 0.245092    Objective Loss 0.245092                                        LR 0.001000    Time 0.304305    
2024-05-09 10:20:48,258 - Epoch: [22][  217/  217]    Overall Loss 0.253411    Objective Loss 0.253411    Top1 93.442623    Top5 98.360656    LR 0.001000    Time 0.304769    
2024-05-09 10:20:48,735 - 

2024-05-09 10:20:48,736 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:21:19,023 - Epoch: [23][  100/  217]    Overall Loss 0.219279    Objective Loss 0.219279                                        LR 0.001000    Time 0.302725    
2024-05-09 10:21:46,276 - Epoch: [23][  200/  217]    Overall Loss 0.225060    Objective Loss 0.225060                                        LR 0.001000    Time 0.287559    
2024-05-09 10:21:50,767 - Epoch: [23][  217/  217]    Overall Loss 0.226029    Objective Loss 0.226029    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.285716    
2024-05-09 10:21:51,264 - 

2024-05-09 10:21:51,265 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:22:19,367 - Epoch: [24][  100/  217]    Overall Loss 0.116323    Objective Loss 0.116323                                        LR 0.001000    Time 0.280893    
2024-05-09 10:22:44,202 - Epoch: [24][  200/  217]    Overall Loss 0.104925    Objective Loss 0.104925                                        LR 0.001000    Time 0.264560    
2024-05-09 10:22:47,406 - Epoch: [24][  217/  217]    Overall Loss 0.102819    Objective Loss 0.102819    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.258585    
2024-05-09 10:22:47,868 - 

2024-05-09 10:22:47,869 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:23:18,777 - Epoch: [25][  100/  217]    Overall Loss 0.046619    Objective Loss 0.046619                                        LR 0.001000    Time 0.308947    
2024-05-09 10:23:45,207 - Epoch: [25][  200/  217]    Overall Loss 0.048507    Objective Loss 0.048507                                        LR 0.001000    Time 0.286554    
2024-05-09 10:23:50,742 - Epoch: [25][  217/  217]    Overall Loss 0.047560    Objective Loss 0.047560    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.289600    
2024-05-09 10:23:51,144 - 

2024-05-09 10:23:51,146 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:24:20,005 - Epoch: [26][  100/  217]    Overall Loss 0.025468    Objective Loss 0.025468                                        LR 0.001000    Time 0.288444    
2024-05-09 10:24:52,185 - Epoch: [26][  200/  217]    Overall Loss 0.028018    Objective Loss 0.028018                                        LR 0.001000    Time 0.305045    
2024-05-09 10:24:55,657 - Epoch: [26][  217/  217]    Overall Loss 0.027699    Objective Loss 0.027699    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.297135    
2024-05-09 10:24:55,948 - 

2024-05-09 10:24:55,949 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:25:23,205 - Epoch: [27][  100/  217]    Overall Loss 0.017807    Objective Loss 0.017807                                        LR 0.001000    Time 0.272416    
2024-05-09 10:25:51,085 - Epoch: [27][  200/  217]    Overall Loss 0.017981    Objective Loss 0.017981                                        LR 0.001000    Time 0.275540    
2024-05-09 10:25:56,925 - Epoch: [27][  217/  217]    Overall Loss 0.018197    Objective Loss 0.018197    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.280855    
2024-05-09 10:25:57,310 - 

2024-05-09 10:25:57,310 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:26:27,218 - Epoch: [28][  100/  217]    Overall Loss 0.012137    Objective Loss 0.012137                                        LR 0.001000    Time 0.298946    
2024-05-09 10:26:55,160 - Epoch: [28][  200/  217]    Overall Loss 0.013291    Objective Loss 0.013291                                        LR 0.001000    Time 0.289129    
2024-05-09 10:27:00,026 - Epoch: [28][  217/  217]    Overall Loss 0.013175    Objective Loss 0.013175    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.288890    
2024-05-09 10:27:00,336 - 

2024-05-09 10:27:00,336 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:27:27,244 - Epoch: [29][  100/  217]    Overall Loss 0.014247    Objective Loss 0.014247                                        LR 0.001000    Time 0.268939    
2024-05-09 10:27:51,895 - Epoch: [29][  200/  217]    Overall Loss 0.014268    Objective Loss 0.014268                                        LR 0.001000    Time 0.257666    
2024-05-09 10:27:57,128 - Epoch: [29][  217/  217]    Overall Loss 0.013996    Objective Loss 0.013996    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.261586    
2024-05-09 10:27:57,425 - 

2024-05-09 10:27:57,426 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:28:24,022 - Epoch: [30][  100/  217]    Overall Loss 0.010203    Objective Loss 0.010203                                        LR 0.001000    Time 0.265843    
2024-05-09 10:28:49,559 - Epoch: [30][  200/  217]    Overall Loss 0.009940    Objective Loss 0.009940                                        LR 0.001000    Time 0.260543    
2024-05-09 10:28:52,614 - Epoch: [30][  217/  217]    Overall Loss 0.010357    Objective Loss 0.010357    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.254201    
2024-05-09 10:28:52,915 - --- validate (epoch=30)-----------
2024-05-09 10:28:52,915 - 1736 samples (32 per mini-batch)
2024-05-09 10:29:09,965 - Epoch: [30][   55/   55]    Loss 2.301862    Top1 58.525346    Top5 74.366359    
2024-05-09 10:29:10,378 - ==> Top1: 58.525    Top5: 74.366    Loss: 2.302

2024-05-09 10:29:10,389 - ==> Best [Top1: 58.525   Top5: 74.366   Sparsity:0.00   Params: 386672 on epoch: 30]
2024-05-09 10:29:10,390 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 10:29:10,437 - 

2024-05-09 10:29:10,438 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:29:37,066 - Epoch: [31][  100/  217]    Overall Loss 0.008809    Objective Loss 0.008809                                        LR 0.001000    Time 0.266150    
2024-05-09 10:30:05,327 - Epoch: [31][  200/  217]    Overall Loss 0.007931    Objective Loss 0.007931                                        LR 0.001000    Time 0.274310    
2024-05-09 10:30:10,401 - Epoch: [31][  217/  217]    Overall Loss 0.008380    Objective Loss 0.008380    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.276189    
2024-05-09 10:30:10,721 - 

2024-05-09 10:30:10,722 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:30:37,484 - Epoch: [32][  100/  217]    Overall Loss 0.005938    Objective Loss 0.005938                                        LR 0.001000    Time 0.267502    
2024-05-09 10:31:02,436 - Epoch: [32][  200/  217]    Overall Loss 0.007856    Objective Loss 0.007856                                        LR 0.001000    Time 0.258449    
2024-05-09 10:31:07,562 - Epoch: [32][  217/  217]    Overall Loss 0.007794    Objective Loss 0.007794    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.261812    
2024-05-09 10:31:08,007 - 

2024-05-09 10:31:08,008 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:31:39,634 - Epoch: [33][  100/  217]    Overall Loss 0.008382    Objective Loss 0.008382                                        LR 0.001000    Time 0.316123    
2024-05-09 10:32:09,341 - Epoch: [33][  200/  217]    Overall Loss 0.008197    Objective Loss 0.008197                                        LR 0.001000    Time 0.306506    
2024-05-09 10:32:14,242 - Epoch: [33][  217/  217]    Overall Loss 0.007935    Objective Loss 0.007935    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.305069    
2024-05-09 10:32:14,492 - 

2024-05-09 10:32:14,493 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:32:43,176 - Epoch: [34][  100/  217]    Overall Loss 0.005180    Objective Loss 0.005180                                        LR 0.001000    Time 0.286701    
2024-05-09 10:33:08,604 - Epoch: [34][  200/  217]    Overall Loss 0.005489    Objective Loss 0.005489                                        LR 0.001000    Time 0.270431    
2024-05-09 10:33:11,607 - Epoch: [34][  217/  217]    Overall Loss 0.005973    Objective Loss 0.005973    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.263079    
2024-05-09 10:33:11,883 - 

2024-05-09 10:33:11,884 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:33:39,224 - Epoch: [35][  100/  217]    Overall Loss 0.007397    Objective Loss 0.007397                                        LR 0.001000    Time 0.273283    
2024-05-09 10:34:06,085 - Epoch: [35][  200/  217]    Overall Loss 0.096928    Objective Loss 0.096928                                        LR 0.001000    Time 0.270886    
2024-05-09 10:34:10,652 - Epoch: [35][  217/  217]    Overall Loss 0.211580    Objective Loss 0.211580    Top1 59.016393    Top5 78.688525    LR 0.001000    Time 0.270701    
2024-05-09 10:34:11,017 - 

2024-05-09 10:34:11,018 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:34:40,311 - Epoch: [36][  100/  217]    Overall Loss 1.376739    Objective Loss 1.376739                                        LR 0.001000    Time 0.292809    
2024-05-09 10:35:05,349 - Epoch: [36][  200/  217]    Overall Loss 1.117772    Objective Loss 1.117772                                        LR 0.001000    Time 0.271536    
2024-05-09 10:35:08,597 - Epoch: [36][  217/  217]    Overall Loss 1.088002    Objective Loss 1.088002    Top1 77.049180    Top5 95.081967    LR 0.001000    Time 0.265219    
2024-05-09 10:35:08,877 - 

2024-05-09 10:35:08,877 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:35:38,164 - Epoch: [37][  100/  217]    Overall Loss 0.296888    Objective Loss 0.296888                                        LR 0.001000    Time 0.292726    
2024-05-09 10:36:08,022 - Epoch: [37][  200/  217]    Overall Loss 0.265370    Objective Loss 0.265370                                        LR 0.001000    Time 0.295587    
2024-05-09 10:36:11,244 - Epoch: [37][  217/  217]    Overall Loss 0.264370    Objective Loss 0.264370    Top1 91.803279    Top5 100.000000    LR 0.001000    Time 0.287260    
2024-05-09 10:36:11,512 - 

2024-05-09 10:36:11,513 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:36:39,844 - Epoch: [38][  100/  217]    Overall Loss 0.095941    Objective Loss 0.095941                                        LR 0.001000    Time 0.283190    
2024-05-09 10:37:06,425 - Epoch: [38][  200/  217]    Overall Loss 0.090747    Objective Loss 0.090747                                        LR 0.001000    Time 0.274437    
2024-05-09 10:37:10,902 - Epoch: [38][  217/  217]    Overall Loss 0.092622    Objective Loss 0.092622    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.273558    
2024-05-09 10:37:11,259 - 

2024-05-09 10:37:11,260 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:37:44,040 - Epoch: [39][  100/  217]    Overall Loss 0.043186    Objective Loss 0.043186                                        LR 0.001000    Time 0.327658    
2024-05-09 10:38:11,825 - Epoch: [39][  200/  217]    Overall Loss 0.039627    Objective Loss 0.039627                                        LR 0.001000    Time 0.302688    
2024-05-09 10:38:15,161 - Epoch: [39][  217/  217]    Overall Loss 0.038774    Objective Loss 0.038774    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.294334    
2024-05-09 10:38:15,782 - 

2024-05-09 10:38:15,783 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:38:44,278 - Epoch: [40][  100/  217]    Overall Loss 0.022261    Objective Loss 0.022261                                        LR 0.001000    Time 0.284796    
2024-05-09 10:39:06,624 - Epoch: [40][  200/  217]    Overall Loss 0.023178    Objective Loss 0.023178                                        LR 0.001000    Time 0.254046    
2024-05-09 10:39:09,967 - Epoch: [40][  217/  217]    Overall Loss 0.023070    Objective Loss 0.023070    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.249537    
2024-05-09 10:39:10,377 - --- validate (epoch=40)-----------
2024-05-09 10:39:10,378 - 1736 samples (32 per mini-batch)
2024-05-09 10:39:28,438 - Epoch: [40][   55/   55]    Loss 2.280954    Top1 58.698157    Top5 75.864055    
2024-05-09 10:39:28,834 - ==> Top1: 58.698    Top5: 75.864    Loss: 2.281

2024-05-09 10:39:28,839 - ==> Best [Top1: 58.698   Top5: 75.864   Sparsity:0.00   Params: 386672 on epoch: 40]
2024-05-09 10:39:28,840 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 10:39:28,889 - 

2024-05-09 10:39:28,889 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:40:02,318 - Epoch: [41][  100/  217]    Overall Loss 0.016920    Objective Loss 0.016920                                        LR 0.001000    Time 0.334162    
2024-05-09 10:40:26,769 - Epoch: [41][  200/  217]    Overall Loss 0.017109    Objective Loss 0.017109                                        LR 0.001000    Time 0.289278    
2024-05-09 10:40:32,099 - Epoch: [41][  217/  217]    Overall Loss 0.017358    Objective Loss 0.017358    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.291165    
2024-05-09 10:40:32,469 - 

2024-05-09 10:40:32,470 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:41:02,964 - Epoch: [42][  100/  217]    Overall Loss 0.014414    Objective Loss 0.014414                                        LR 0.001000    Time 0.304808    
2024-05-09 10:41:27,481 - Epoch: [42][  200/  217]    Overall Loss 0.013566    Objective Loss 0.013566                                        LR 0.001000    Time 0.274932    
2024-05-09 10:41:30,384 - Epoch: [42][  217/  217]    Overall Loss 0.013794    Objective Loss 0.013794    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.266763    
2024-05-09 10:41:30,713 - 

2024-05-09 10:41:30,714 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:41:59,493 - Epoch: [43][  100/  217]    Overall Loss 0.010005    Objective Loss 0.010005                                        LR 0.001000    Time 0.287663    
2024-05-09 10:42:25,928 - Epoch: [43][  200/  217]    Overall Loss 0.010306    Objective Loss 0.010306                                        LR 0.001000    Time 0.275946    
2024-05-09 10:42:28,894 - Epoch: [43][  217/  217]    Overall Loss 0.010594    Objective Loss 0.010594    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.267984    
2024-05-09 10:42:29,255 - 

2024-05-09 10:42:29,256 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:43:00,523 - Epoch: [44][  100/  217]    Overall Loss 0.012068    Objective Loss 0.012068                                        LR 0.001000    Time 0.312538    
2024-05-09 10:43:27,206 - Epoch: [44][  200/  217]    Overall Loss 0.010709    Objective Loss 0.010709                                        LR 0.001000    Time 0.289622    
2024-05-09 10:43:30,707 - Epoch: [44][  217/  217]    Overall Loss 0.010403    Objective Loss 0.010403    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.283055    
2024-05-09 10:43:31,050 - 

2024-05-09 10:43:31,051 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:44:00,374 - Epoch: [45][  100/  217]    Overall Loss 0.008585    Objective Loss 0.008585                                        LR 0.001000    Time 0.293082    
2024-05-09 10:44:26,389 - Epoch: [45][  200/  217]    Overall Loss 0.009055    Objective Loss 0.009055                                        LR 0.001000    Time 0.276547    
2024-05-09 10:44:30,269 - Epoch: [45][  217/  217]    Overall Loss 0.009167    Objective Loss 0.009167    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.272749    
2024-05-09 10:44:30,611 - 

2024-05-09 10:44:30,612 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:44:59,115 - Epoch: [46][  100/  217]    Overall Loss 0.006501    Objective Loss 0.006501                                        LR 0.001000    Time 0.284898    
2024-05-09 10:45:26,860 - Epoch: [46][  200/  217]    Overall Loss 0.007264    Objective Loss 0.007264                                        LR 0.001000    Time 0.281119    
2024-05-09 10:45:30,123 - Epoch: [46][  217/  217]    Overall Loss 0.007466    Objective Loss 0.007466    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.274120    
2024-05-09 10:45:30,488 - 

2024-05-09 10:45:30,489 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:46:02,785 - Epoch: [47][  100/  217]    Overall Loss 0.006411    Objective Loss 0.006411                                        LR 0.001000    Time 0.322825    
2024-05-09 10:46:29,980 - Epoch: [47][  200/  217]    Overall Loss 0.007197    Objective Loss 0.007197                                        LR 0.001000    Time 0.297326    
2024-05-09 10:46:35,116 - Epoch: [47][  217/  217]    Overall Loss 0.007538    Objective Loss 0.007538    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.297690    
2024-05-09 10:46:35,566 - 

2024-05-09 10:46:35,567 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:47:05,280 - Epoch: [48][  100/  217]    Overall Loss 0.007997    Objective Loss 0.007997                                        LR 0.001000    Time 0.296975    
2024-05-09 10:47:34,312 - Epoch: [48][  200/  217]    Overall Loss 0.007434    Objective Loss 0.007434                                        LR 0.001000    Time 0.293593    
2024-05-09 10:47:38,319 - Epoch: [48][  217/  217]    Overall Loss 0.007219    Objective Loss 0.007219    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.289047    
2024-05-09 10:47:38,638 - 

2024-05-09 10:47:38,640 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:48:09,810 - Epoch: [49][  100/  217]    Overall Loss 0.038040    Objective Loss 0.038040                                        LR 0.001000    Time 0.311571    
2024-05-09 10:48:35,141 - Epoch: [49][  200/  217]    Overall Loss 0.372242    Objective Loss 0.372242                                        LR 0.001000    Time 0.282373    
2024-05-09 10:48:38,872 - Epoch: [49][  217/  217]    Overall Loss 0.418529    Objective Loss 0.418529    Top1 68.852459    Top5 91.803279    LR 0.001000    Time 0.277430    
2024-05-09 10:48:40,171 - 

2024-05-09 10:48:40,171 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:49:10,221 - Epoch: [50][  100/  217]    Overall Loss 0.541917    Objective Loss 0.541917                                        LR 0.001000    Time 0.300367    
2024-05-09 10:49:38,303 - Epoch: [50][  200/  217]    Overall Loss 0.471573    Objective Loss 0.471573                                        LR 0.001000    Time 0.290525    
2024-05-09 10:49:43,334 - Epoch: [50][  217/  217]    Overall Loss 0.465814    Objective Loss 0.465814    Top1 91.803279    Top5 100.000000    LR 0.001000    Time 0.290937    
2024-05-09 10:49:43,649 - --- validate (epoch=50)-----------
2024-05-09 10:49:43,649 - 1736 samples (32 per mini-batch)
2024-05-09 10:50:02,458 - Epoch: [50][   55/   55]    Loss 2.404807    Top1 55.875576    Top5 73.502304    
2024-05-09 10:50:02,774 - ==> Top1: 55.876    Top5: 73.502    Loss: 2.405

2024-05-09 10:50:02,780 - ==> Best [Top1: 58.698   Top5: 75.864   Sparsity:0.00   Params: 386672 on epoch: 40]
2024-05-09 10:50:02,781 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 10:50:02,829 - 

2024-05-09 10:50:02,830 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:50:35,156 - Epoch: [51][  100/  217]    Overall Loss 0.124719    Objective Loss 0.124719                                        LR 0.001000    Time 0.323130    
2024-05-09 10:51:02,576 - Epoch: [51][  200/  217]    Overall Loss 0.109175    Objective Loss 0.109175                                        LR 0.001000    Time 0.298602    
2024-05-09 10:51:06,304 - Epoch: [51][  217/  217]    Overall Loss 0.108541    Objective Loss 0.108541    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.292382    
2024-05-09 10:51:06,596 - 

2024-05-09 10:51:06,597 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:51:36,136 - Epoch: [52][  100/  217]    Overall Loss 0.046984    Objective Loss 0.046984                                        LR 0.001000    Time 0.295276    
2024-05-09 10:52:01,005 - Epoch: [52][  200/  217]    Overall Loss 0.042034    Objective Loss 0.042034                                        LR 0.001000    Time 0.271919    
2024-05-09 10:52:05,353 - Epoch: [52][  217/  217]    Overall Loss 0.042836    Objective Loss 0.042836    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.270644    
2024-05-09 10:52:05,688 - 

2024-05-09 10:52:05,688 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:52:38,443 - Epoch: [53][  100/  217]    Overall Loss 0.024395    Objective Loss 0.024395                                        LR 0.001000    Time 0.327425    
2024-05-09 10:53:06,443 - Epoch: [53][  200/  217]    Overall Loss 0.024384    Objective Loss 0.024384                                        LR 0.001000    Time 0.303650    
2024-05-09 10:53:11,690 - Epoch: [53][  217/  217]    Overall Loss 0.024240    Objective Loss 0.024240    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.304026    
2024-05-09 10:53:12,145 - 

2024-05-09 10:53:12,145 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:53:45,570 - Epoch: [54][  100/  217]    Overall Loss 0.014304    Objective Loss 0.014304                                        LR 0.001000    Time 0.334123    
2024-05-09 10:54:12,836 - Epoch: [54][  200/  217]    Overall Loss 0.014226    Objective Loss 0.014226                                        LR 0.001000    Time 0.303332    
2024-05-09 10:54:17,556 - Epoch: [54][  217/  217]    Overall Loss 0.014196    Objective Loss 0.014196    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.301306    
2024-05-09 10:54:18,604 - 

2024-05-09 10:54:18,605 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:54:51,194 - Epoch: [55][  100/  217]    Overall Loss 0.010327    Objective Loss 0.010327                                        LR 0.001000    Time 0.325760    
2024-05-09 10:55:18,605 - Epoch: [55][  200/  217]    Overall Loss 0.011529    Objective Loss 0.011529                                        LR 0.001000    Time 0.299870    
2024-05-09 10:55:23,886 - Epoch: [55][  217/  217]    Overall Loss 0.011811    Objective Loss 0.011811    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.300702    
2024-05-09 10:55:24,234 - 

2024-05-09 10:55:24,235 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:55:53,010 - Epoch: [56][  100/  217]    Overall Loss 0.009314    Objective Loss 0.009314                                        LR 0.001000    Time 0.287612    
2024-05-09 10:56:18,734 - Epoch: [56][  200/  217]    Overall Loss 0.009391    Objective Loss 0.009391                                        LR 0.001000    Time 0.272372    
2024-05-09 10:56:21,846 - Epoch: [56][  217/  217]    Overall Loss 0.009528    Objective Loss 0.009528    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.265367    
2024-05-09 10:56:22,217 - 

2024-05-09 10:56:22,218 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:56:53,450 - Epoch: [57][  100/  217]    Overall Loss 0.009536    Objective Loss 0.009536                                        LR 0.001000    Time 0.312185    
2024-05-09 10:57:20,537 - Epoch: [57][  200/  217]    Overall Loss 0.009744    Objective Loss 0.009744                                        LR 0.001000    Time 0.291463    
2024-05-09 10:57:24,439 - Epoch: [57][  217/  217]    Overall Loss 0.009380    Objective Loss 0.009380    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.286595    
2024-05-09 10:57:24,761 - 

2024-05-09 10:57:24,762 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:57:55,365 - Epoch: [58][  100/  217]    Overall Loss 0.005973    Objective Loss 0.005973                                        LR 0.001000    Time 0.305908    
2024-05-09 10:58:22,721 - Epoch: [58][  200/  217]    Overall Loss 0.006633    Objective Loss 0.006633                                        LR 0.001000    Time 0.289668    
2024-05-09 10:58:28,652 - Epoch: [58][  217/  217]    Overall Loss 0.006484    Objective Loss 0.006484    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.294294    
2024-05-09 10:58:29,010 - 

2024-05-09 10:58:29,011 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:58:58,070 - Epoch: [59][  100/  217]    Overall Loss 0.005523    Objective Loss 0.005523                                        LR 0.001000    Time 0.290439    
2024-05-09 10:59:24,421 - Epoch: [59][  200/  217]    Overall Loss 0.005998    Objective Loss 0.005998                                        LR 0.001000    Time 0.276912    
2024-05-09 10:59:29,998 - Epoch: [59][  217/  217]    Overall Loss 0.005914    Objective Loss 0.005914    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.280912    
2024-05-09 10:59:30,513 - 

2024-05-09 10:59:30,514 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:00:01,686 - Epoch: [60][  100/  217]    Overall Loss 0.006065    Objective Loss 0.006065                                        LR 0.001000    Time 0.311590    
2024-05-09 11:00:29,118 - Epoch: [60][  200/  217]    Overall Loss 0.010355    Objective Loss 0.010355                                        LR 0.001000    Time 0.292896    
2024-05-09 11:00:34,020 - Epoch: [60][  217/  217]    Overall Loss 0.010384    Objective Loss 0.010384    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.292531    
2024-05-09 11:00:35,542 - --- validate (epoch=60)-----------
2024-05-09 11:00:35,542 - 1736 samples (32 per mini-batch)
2024-05-09 11:00:53,184 - Epoch: [60][   55/   55]    Loss 2.419871    Top1 58.640553    Top5 75.172811    
2024-05-09 11:00:53,699 - ==> Top1: 58.641    Top5: 75.173    Loss: 2.420

2024-05-09 11:00:53,707 - ==> Best [Top1: 58.698   Top5: 75.864   Sparsity:0.00   Params: 386672 on epoch: 40]
2024-05-09 11:00:53,708 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 11:00:53,758 - 

2024-05-09 11:00:53,759 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:01:24,723 - Epoch: [61][  100/  217]    Overall Loss 0.040865    Objective Loss 0.040865                                        LR 0.001000    Time 0.309522    
2024-05-09 11:01:53,547 - Epoch: [61][  200/  217]    Overall Loss 0.163464    Objective Loss 0.163464                                        LR 0.001000    Time 0.298820    
2024-05-09 11:01:58,557 - Epoch: [61][  217/  217]    Overall Loss 0.183917    Objective Loss 0.183917    Top1 95.081967    Top5 100.000000    LR 0.001000    Time 0.298482    
2024-05-09 11:01:58,930 - 

2024-05-09 11:01:58,931 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:02:29,063 - Epoch: [62][  100/  217]    Overall Loss 0.384969    Objective Loss 0.384969                                        LR 0.001000    Time 0.301199    
2024-05-09 11:02:56,957 - Epoch: [62][  200/  217]    Overall Loss 0.337973    Objective Loss 0.337973                                        LR 0.001000    Time 0.290010    
2024-05-09 11:02:59,904 - Epoch: [62][  217/  217]    Overall Loss 0.329208    Objective Loss 0.329208    Top1 91.803279    Top5 100.000000    LR 0.001000    Time 0.280858    
2024-05-09 11:03:00,191 - 

2024-05-09 11:03:00,192 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:03:30,663 - Epoch: [63][  100/  217]    Overall Loss 0.098313    Objective Loss 0.098313                                        LR 0.001000    Time 0.304576    
2024-05-09 11:03:56,299 - Epoch: [63][  200/  217]    Overall Loss 0.086952    Objective Loss 0.086952                                        LR 0.001000    Time 0.280408    
2024-05-09 11:04:03,352 - Epoch: [63][  217/  217]    Overall Loss 0.084705    Objective Loss 0.084705    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.290928    
2024-05-09 11:04:03,694 - 

2024-05-09 11:04:03,695 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:04:35,951 - Epoch: [64][  100/  217]    Overall Loss 0.030249    Objective Loss 0.030249                                        LR 0.001000    Time 0.322424    
2024-05-09 11:05:01,557 - Epoch: [64][  200/  217]    Overall Loss 0.028935    Objective Loss 0.028935                                        LR 0.001000    Time 0.289174    
2024-05-09 11:05:06,261 - Epoch: [64][  217/  217]    Overall Loss 0.028774    Objective Loss 0.028774    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.288183    
2024-05-09 11:05:06,874 - 

2024-05-09 11:05:06,875 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:05:40,194 - Epoch: [65][  100/  217]    Overall Loss 0.014742    Objective Loss 0.014742                                        LR 0.001000    Time 0.333045    
2024-05-09 11:06:06,046 - Epoch: [65][  200/  217]    Overall Loss 0.016102    Objective Loss 0.016102                                        LR 0.001000    Time 0.295716    
2024-05-09 11:06:11,559 - Epoch: [65][  217/  217]    Overall Loss 0.016212    Objective Loss 0.016212    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.297939    
2024-05-09 11:06:11,973 - 

2024-05-09 11:06:11,974 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:06:43,245 - Epoch: [66][  100/  217]    Overall Loss 0.010446    Objective Loss 0.010446                                        LR 0.001000    Time 0.312584    
2024-05-09 11:07:10,394 - Epoch: [66][  200/  217]    Overall Loss 0.010283    Objective Loss 0.010283                                        LR 0.001000    Time 0.291974    
2024-05-09 11:07:14,312 - Epoch: [66][  217/  217]    Overall Loss 0.010316    Objective Loss 0.010316    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.287141    
2024-05-09 11:07:14,656 - 

2024-05-09 11:07:14,656 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:07:43,996 - Epoch: [67][  100/  217]    Overall Loss 0.007524    Objective Loss 0.007524                                        LR 0.001000    Time 0.293275    
2024-05-09 11:08:10,387 - Epoch: [67][  200/  217]    Overall Loss 0.007428    Objective Loss 0.007428                                        LR 0.001000    Time 0.278531    
2024-05-09 11:08:15,527 - Epoch: [67][  217/  217]    Overall Loss 0.007506    Objective Loss 0.007506    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.280386    
2024-05-09 11:08:15,966 - 

2024-05-09 11:08:15,967 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:08:48,612 - Epoch: [68][  100/  217]    Overall Loss 0.005937    Objective Loss 0.005937                                        LR 0.001000    Time 0.326315    
2024-05-09 11:09:14,602 - Epoch: [68][  200/  217]    Overall Loss 0.005831    Objective Loss 0.005831                                        LR 0.001000    Time 0.293045    
2024-05-09 11:09:18,226 - Epoch: [68][  217/  217]    Overall Loss 0.006003    Objective Loss 0.006003    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.286774    
2024-05-09 11:09:18,961 - 

2024-05-09 11:09:18,962 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:09:49,611 - Epoch: [69][  100/  217]    Overall Loss 0.004573    Objective Loss 0.004573                                        LR 0.001000    Time 0.306361    
2024-05-09 11:10:16,474 - Epoch: [69][  200/  217]    Overall Loss 0.004776    Objective Loss 0.004776                                        LR 0.001000    Time 0.287425    
2024-05-09 11:10:20,572 - Epoch: [69][  217/  217]    Overall Loss 0.005158    Objective Loss 0.005158    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.283777    
2024-05-09 11:10:21,187 - 

2024-05-09 11:10:21,188 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:10:51,750 - Epoch: [70][  100/  217]    Overall Loss 0.004914    Objective Loss 0.004914                                        LR 0.001000    Time 0.305477    
2024-05-09 11:11:17,627 - Epoch: [70][  200/  217]    Overall Loss 0.004781    Objective Loss 0.004781                                        LR 0.001000    Time 0.282061    
2024-05-09 11:11:22,574 - Epoch: [70][  217/  217]    Overall Loss 0.005084    Objective Loss 0.005084    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.282746    
2024-05-09 11:11:22,980 - --- validate (epoch=70)-----------
2024-05-09 11:11:22,981 - 1736 samples (32 per mini-batch)
2024-05-09 11:11:39,389 - Epoch: [70][   55/   55]    Loss 2.403995    Top1 60.138249    Top5 76.900922    
2024-05-09 11:11:40,104 - ==> Top1: 60.138    Top5: 76.901    Loss: 2.404

2024-05-09 11:11:40,112 - ==> Best [Top1: 60.138   Top5: 76.901   Sparsity:0.00   Params: 386672 on epoch: 70]
2024-05-09 11:11:40,112 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 11:11:40,184 - 

2024-05-09 11:11:40,185 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:12:11,323 - Epoch: [71][  100/  217]    Overall Loss 0.004657    Objective Loss 0.004657                                        LR 0.001000    Time 0.311243    
2024-05-09 11:12:37,196 - Epoch: [71][  200/  217]    Overall Loss 0.004679    Objective Loss 0.004679                                        LR 0.001000    Time 0.284924    
2024-05-09 11:12:41,742 - Epoch: [71][  217/  217]    Overall Loss 0.004523    Objective Loss 0.004523    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.283541    
2024-05-09 11:12:42,253 - 

2024-05-09 11:12:42,254 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:13:09,252 - Epoch: [72][  100/  217]    Overall Loss 0.003875    Objective Loss 0.003875                                        LR 0.001000    Time 0.269841    
2024-05-09 11:13:36,719 - Epoch: [72][  200/  217]    Overall Loss 0.003729    Objective Loss 0.003729                                        LR 0.001000    Time 0.272192    
2024-05-09 11:13:42,203 - Epoch: [72][  217/  217]    Overall Loss 0.003880    Objective Loss 0.003880    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.276131    
2024-05-09 11:13:42,886 - 

2024-05-09 11:13:42,887 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:14:13,902 - Epoch: [73][  100/  217]    Overall Loss 0.004232    Objective Loss 0.004232                                        LR 0.001000    Time 0.310016    
2024-05-09 11:14:43,979 - Epoch: [73][  200/  217]    Overall Loss 0.004480    Objective Loss 0.004480                                        LR 0.001000    Time 0.305334    
2024-05-09 11:14:47,213 - Epoch: [73][  217/  217]    Overall Loss 0.004385    Objective Loss 0.004385    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.296296    
2024-05-09 11:14:47,624 - 

2024-05-09 11:14:47,624 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:15:17,138 - Epoch: [74][  100/  217]    Overall Loss 0.007299    Objective Loss 0.007299                                        LR 0.001000    Time 0.295013    
2024-05-09 11:15:44,219 - Epoch: [74][  200/  217]    Overall Loss 0.027213    Objective Loss 0.027213                                        LR 0.001000    Time 0.282852    
2024-05-09 11:15:47,768 - Epoch: [74][  217/  217]    Overall Loss 0.043092    Objective Loss 0.043092    Top1 90.163934    Top5 98.360656    LR 0.001000    Time 0.277034    
2024-05-09 11:15:48,017 - 

2024-05-09 11:15:48,018 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:16:17,660 - Epoch: [75][  100/  217]    Overall Loss 0.556214    Objective Loss 0.556214                                        LR 0.001000    Time 0.296296    
2024-05-09 11:16:44,474 - Epoch: [75][  200/  217]    Overall Loss 0.492045    Objective Loss 0.492045                                        LR 0.001000    Time 0.282163    
2024-05-09 11:16:47,736 - Epoch: [75][  217/  217]    Overall Loss 0.481961    Objective Loss 0.481961    Top1 85.245902    Top5 98.360656    LR 0.001000    Time 0.275077    
2024-05-09 11:16:48,267 - 

2024-05-09 11:16:48,268 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:17:18,753 - Epoch: [76][  100/  217]    Overall Loss 0.127011    Objective Loss 0.127011                                        LR 0.001000    Time 0.304735    
2024-05-09 11:17:45,736 - Epoch: [76][  200/  217]    Overall Loss 0.112940    Objective Loss 0.112940                                        LR 0.001000    Time 0.287204    
2024-05-09 11:17:48,898 - Epoch: [76][  217/  217]    Overall Loss 0.110323    Objective Loss 0.110323    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.279262    
2024-05-09 11:17:49,231 - 

2024-05-09 11:17:49,231 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:18:19,991 - Epoch: [77][  100/  217]    Overall Loss 0.028927    Objective Loss 0.028927                                        LR 0.001000    Time 0.307474    
2024-05-09 11:18:46,756 - Epoch: [77][  200/  217]    Overall Loss 0.029047    Objective Loss 0.029047                                        LR 0.001000    Time 0.287501    
2024-05-09 11:18:50,410 - Epoch: [77][  217/  217]    Overall Loss 0.029471    Objective Loss 0.029471    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.281804    
2024-05-09 11:18:50,676 - 

2024-05-09 11:18:50,677 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:19:21,137 - Epoch: [78][  100/  217]    Overall Loss 0.014082    Objective Loss 0.014082                                        LR 0.001000    Time 0.304466    
2024-05-09 11:19:46,261 - Epoch: [78][  200/  217]    Overall Loss 0.014523    Objective Loss 0.014523                                        LR 0.001000    Time 0.277795    
2024-05-09 11:19:50,713 - Epoch: [78][  217/  217]    Overall Loss 0.014083    Objective Loss 0.014083    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.276535    
2024-05-09 11:19:50,964 - 

2024-05-09 11:19:50,966 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:20:21,423 - Epoch: [79][  100/  217]    Overall Loss 0.009411    Objective Loss 0.009411                                        LR 0.001000    Time 0.304373    
2024-05-09 11:20:49,235 - Epoch: [79][  200/  217]    Overall Loss 0.009497    Objective Loss 0.009497                                        LR 0.001000    Time 0.291186    
2024-05-09 11:20:53,967 - Epoch: [79][  217/  217]    Overall Loss 0.009521    Objective Loss 0.009521    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.290169    
2024-05-09 11:20:54,256 - 

2024-05-09 11:20:54,257 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:21:24,155 - Epoch: [80][  100/  217]    Overall Loss 0.008345    Objective Loss 0.008345                                        LR 0.001000    Time 0.298841    
2024-05-09 11:21:50,905 - Epoch: [80][  200/  217]    Overall Loss 0.007171    Objective Loss 0.007171                                        LR 0.001000    Time 0.283107    
2024-05-09 11:21:57,633 - Epoch: [80][  217/  217]    Overall Loss 0.007260    Objective Loss 0.007260    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.291921    
2024-05-09 11:21:57,999 - --- validate (epoch=80)-----------
2024-05-09 11:21:58,000 - 1736 samples (32 per mini-batch)
2024-05-09 11:22:12,604 - Epoch: [80][   55/   55]    Loss 2.436245    Top1 59.965438    Top5 77.131336    
2024-05-09 11:22:12,893 - ==> Top1: 59.965    Top5: 77.131    Loss: 2.436

2024-05-09 11:22:12,900 - ==> Best [Top1: 60.138   Top5: 76.901   Sparsity:0.00   Params: 386672 on epoch: 70]
2024-05-09 11:22:12,901 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 11:22:12,947 - 

2024-05-09 11:22:12,949 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:22:43,263 - Epoch: [81][  100/  217]    Overall Loss 0.006300    Objective Loss 0.006300                                        LR 0.001000    Time 0.303014    
2024-05-09 11:23:09,011 - Epoch: [81][  200/  217]    Overall Loss 0.005905    Objective Loss 0.005905                                        LR 0.001000    Time 0.280194    
2024-05-09 11:23:12,073 - Epoch: [81][  217/  217]    Overall Loss 0.005794    Objective Loss 0.005794    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.272338    
2024-05-09 11:23:12,395 - 

2024-05-09 11:23:12,396 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:23:43,965 - Epoch: [82][  100/  217]    Overall Loss 0.004626    Objective Loss 0.004626                                        LR 0.001000    Time 0.315543    
2024-05-09 11:24:08,881 - Epoch: [82][  200/  217]    Overall Loss 0.004716    Objective Loss 0.004716                                        LR 0.001000    Time 0.282294    
2024-05-09 11:24:14,215 - Epoch: [82][  217/  217]    Overall Loss 0.004887    Objective Loss 0.004887    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.284753    
2024-05-09 11:24:14,567 - 

2024-05-09 11:24:14,568 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:24:41,523 - Epoch: [83][  100/  217]    Overall Loss 0.004320    Objective Loss 0.004320                                        LR 0.001000    Time 0.269425    
2024-05-09 11:25:08,635 - Epoch: [83][  200/  217]    Overall Loss 0.003900    Objective Loss 0.003900                                        LR 0.001000    Time 0.270216    
2024-05-09 11:25:12,808 - Epoch: [83][  217/  217]    Overall Loss 0.004213    Objective Loss 0.004213    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.268261    
2024-05-09 11:25:13,282 - 

2024-05-09 11:25:13,282 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:25:42,173 - Epoch: [84][  100/  217]    Overall Loss 0.005410    Objective Loss 0.005410                                        LR 0.001000    Time 0.288771    
2024-05-09 11:26:10,325 - Epoch: [84][  200/  217]    Overall Loss 0.005678    Objective Loss 0.005678                                        LR 0.001000    Time 0.285096    
2024-05-09 11:26:14,304 - Epoch: [84][  217/  217]    Overall Loss 0.005514    Objective Loss 0.005514    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.281085    
2024-05-09 11:26:14,653 - 

2024-05-09 11:26:14,654 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:26:43,593 - Epoch: [85][  100/  217]    Overall Loss 0.009149    Objective Loss 0.009149                                        LR 0.001000    Time 0.289250    
2024-05-09 11:27:13,491 - Epoch: [85][  200/  217]    Overall Loss 0.012369    Objective Loss 0.012369                                        LR 0.001000    Time 0.294049    
2024-05-09 11:27:16,829 - Epoch: [85][  217/  217]    Overall Loss 0.012773    Objective Loss 0.012773    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.286386    
2024-05-09 11:27:17,175 - 

2024-05-09 11:27:17,175 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:27:47,141 - Epoch: [86][  100/  217]    Overall Loss 0.017151    Objective Loss 0.017151                                        LR 0.001000    Time 0.299531    
2024-05-09 11:28:15,536 - Epoch: [86][  200/  217]    Overall Loss 0.025036    Objective Loss 0.025036                                        LR 0.001000    Time 0.291678    
2024-05-09 11:28:18,497 - Epoch: [86][  217/  217]    Overall Loss 0.026001    Objective Loss 0.026001    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.282465    
2024-05-09 11:28:18,972 - 

2024-05-09 11:28:18,973 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:28:50,668 - Epoch: [87][  100/  217]    Overall Loss 0.104384    Objective Loss 0.104384                                        LR 0.001000    Time 0.316826    
2024-05-09 11:29:15,284 - Epoch: [87][  200/  217]    Overall Loss 0.222009    Objective Loss 0.222009                                        LR 0.001000    Time 0.281430    
2024-05-09 11:29:20,217 - Epoch: [87][  217/  217]    Overall Loss 0.227580    Objective Loss 0.227580    Top1 93.442623    Top5 100.000000    LR 0.001000    Time 0.282095    
2024-05-09 11:29:20,690 - 

2024-05-09 11:29:20,691 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:29:52,523 - Epoch: [88][  100/  217]    Overall Loss 0.163202    Objective Loss 0.163202                                        LR 0.001000    Time 0.318182    
2024-05-09 11:30:19,921 - Epoch: [88][  200/  217]    Overall Loss 0.141150    Objective Loss 0.141150                                        LR 0.001000    Time 0.296016    
2024-05-09 11:30:25,066 - Epoch: [88][  217/  217]    Overall Loss 0.138052    Objective Loss 0.138052    Top1 95.081967    Top5 100.000000    LR 0.001000    Time 0.296520    
2024-05-09 11:30:25,589 - 

2024-05-09 11:30:25,589 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:30:50,244 - Epoch: [89][  100/  217]    Overall Loss 0.048052    Objective Loss 0.048052                                        LR 0.001000    Time 0.246419    
2024-05-09 11:31:19,215 - Epoch: [89][  200/  217]    Overall Loss 0.039858    Objective Loss 0.039858                                        LR 0.001000    Time 0.268007    
2024-05-09 11:31:24,142 - Epoch: [89][  217/  217]    Overall Loss 0.039910    Objective Loss 0.039910    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.269705    
2024-05-09 11:31:25,304 - 

2024-05-09 11:31:25,305 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:31:51,956 - Epoch: [90][  100/  217]    Overall Loss 0.017712    Objective Loss 0.017712                                        LR 0.001000    Time 0.266381    
2024-05-09 11:32:24,534 - Epoch: [90][  200/  217]    Overall Loss 0.015494    Objective Loss 0.015494                                        LR 0.001000    Time 0.296020    
2024-05-09 11:32:27,572 - Epoch: [90][  217/  217]    Overall Loss 0.015160    Objective Loss 0.015160    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.286818    
2024-05-09 11:32:27,879 - --- validate (epoch=90)-----------
2024-05-09 11:32:27,879 - 1736 samples (32 per mini-batch)
2024-05-09 11:32:45,005 - Epoch: [90][   55/   55]    Loss 2.524436    Top1 59.101382    Top5 76.267281    
2024-05-09 11:32:45,537 - ==> Top1: 59.101    Top5: 76.267    Loss: 2.524

2024-05-09 11:32:45,548 - ==> Best [Top1: 60.138   Top5: 76.901   Sparsity:0.00   Params: 386672 on epoch: 70]
2024-05-09 11:32:45,549 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 11:32:45,609 - 

2024-05-09 11:32:45,610 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:33:14,447 - Epoch: [91][  100/  217]    Overall Loss 0.007644    Objective Loss 0.007644                                        LR 0.001000    Time 0.288241    
2024-05-09 11:33:40,492 - Epoch: [91][  200/  217]    Overall Loss 0.009346    Objective Loss 0.009346                                        LR 0.001000    Time 0.274288    
2024-05-09 11:33:43,141 - Epoch: [91][  217/  217]    Overall Loss 0.009449    Objective Loss 0.009449    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.264993    
2024-05-09 11:33:43,440 - 

2024-05-09 11:33:43,440 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:34:13,863 - Epoch: [92][  100/  217]    Overall Loss 0.006514    Objective Loss 0.006514                                        LR 0.001000    Time 0.304100    
2024-05-09 11:34:39,830 - Epoch: [92][  200/  217]    Overall Loss 0.006807    Objective Loss 0.006807                                        LR 0.001000    Time 0.281829    
2024-05-09 11:34:42,825 - Epoch: [92][  217/  217]    Overall Loss 0.007310    Objective Loss 0.007310    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.273541    
2024-05-09 11:34:43,197 - 

2024-05-09 11:34:43,197 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:35:12,453 - Epoch: [93][  100/  217]    Overall Loss 0.009784    Objective Loss 0.009784                                        LR 0.001000    Time 0.292431    
2024-05-09 11:35:42,935 - Epoch: [93][  200/  217]    Overall Loss 0.008271    Objective Loss 0.008271                                        LR 0.001000    Time 0.298573    
2024-05-09 11:35:45,589 - Epoch: [93][  217/  217]    Overall Loss 0.008048    Objective Loss 0.008048    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.287400    
2024-05-09 11:35:46,043 - 

2024-05-09 11:35:46,044 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:36:11,324 - Epoch: [94][  100/  217]    Overall Loss 0.005137    Objective Loss 0.005137                                        LR 0.001000    Time 0.252673    
2024-05-09 11:36:40,964 - Epoch: [94][  200/  217]    Overall Loss 0.005319    Objective Loss 0.005319                                        LR 0.001000    Time 0.274475    
2024-05-09 11:36:44,584 - Epoch: [94][  217/  217]    Overall Loss 0.005511    Objective Loss 0.005511    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.269643    
2024-05-09 11:36:44,940 - 

2024-05-09 11:36:44,941 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:37:13,635 - Epoch: [95][  100/  217]    Overall Loss 0.003615    Objective Loss 0.003615                                        LR 0.001000    Time 0.286826    
2024-05-09 11:37:43,008 - Epoch: [95][  200/  217]    Overall Loss 0.004029    Objective Loss 0.004029                                        LR 0.001000    Time 0.290214    
2024-05-09 11:37:45,823 - Epoch: [95][  217/  217]    Overall Loss 0.004462    Objective Loss 0.004462    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.280438    
2024-05-09 11:37:46,065 - 

2024-05-09 11:37:46,066 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:38:15,743 - Epoch: [96][  100/  217]    Overall Loss 0.003822    Objective Loss 0.003822                                        LR 0.001000    Time 0.296648    
2024-05-09 11:38:45,274 - Epoch: [96][  200/  217]    Overall Loss 0.004079    Objective Loss 0.004079                                        LR 0.001000    Time 0.295922    
2024-05-09 11:38:49,442 - Epoch: [96][  217/  217]    Overall Loss 0.004002    Objective Loss 0.004002    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.291931    
2024-05-09 11:38:49,727 - 

2024-05-09 11:38:49,728 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:39:20,030 - Epoch: [97][  100/  217]    Overall Loss 0.003233    Objective Loss 0.003233                                        LR 0.001000    Time 0.302881    
2024-05-09 11:39:48,696 - Epoch: [97][  200/  217]    Overall Loss 0.004169    Objective Loss 0.004169                                        LR 0.001000    Time 0.294710    
2024-05-09 11:39:52,478 - Epoch: [97][  217/  217]    Overall Loss 0.003955    Objective Loss 0.003955    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.289036    
2024-05-09 11:39:52,796 - 

2024-05-09 11:39:52,797 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:40:18,925 - Epoch: [98][  100/  217]    Overall Loss 0.004154    Objective Loss 0.004154                                        LR 0.001000    Time 0.261144    
2024-05-09 11:40:47,414 - Epoch: [98][  200/  217]    Overall Loss 0.004083    Objective Loss 0.004083                                        LR 0.001000    Time 0.272962    
2024-05-09 11:40:52,315 - Epoch: [98][  217/  217]    Overall Loss 0.003928    Objective Loss 0.003928    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.274150    
2024-05-09 11:40:52,813 - 

2024-05-09 11:40:52,814 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:41:22,275 - Epoch: [99][  100/  217]    Overall Loss 0.003302    Objective Loss 0.003302                                        LR 0.001000    Time 0.294408    
2024-05-09 11:41:48,598 - Epoch: [99][  200/  217]    Overall Loss 0.002842    Objective Loss 0.002842                                        LR 0.001000    Time 0.278762    
2024-05-09 11:41:52,965 - Epoch: [99][  217/  217]    Overall Loss 0.003443    Objective Loss 0.003443    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.277036    
2024-05-09 11:41:53,538 - 

2024-05-09 11:41:53,539 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:42:20,339 - Epoch: [100][  100/  217]    Overall Loss 0.003592    Objective Loss 0.003592                                        LR 0.000250    Time 0.267863    
2024-05-09 11:42:49,267 - Epoch: [100][  200/  217]    Overall Loss 0.002947    Objective Loss 0.002947                                        LR 0.000250    Time 0.278511    
2024-05-09 11:42:54,578 - Epoch: [100][  217/  217]    Overall Loss 0.002842    Objective Loss 0.002842    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.281157    
2024-05-09 11:42:55,504 - --- validate (epoch=100)-----------
2024-05-09 11:42:55,505 - 1736 samples (32 per mini-batch)
2024-05-09 11:43:12,017 - Epoch: [100][   55/   55]    Loss 2.579342    Top1 59.562212    Top5 76.728111    
2024-05-09 11:43:12,299 - ==> Top1: 59.562    Top5: 76.728    Loss: 2.579

2024-05-09 11:43:12,303 - ==> Best [Top1: 60.138   Top5: 76.901   Sparsity:0.00   Params: 386672 on epoch: 70]
2024-05-09 11:43:12,304 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 11:43:12,338 - 

2024-05-09 11:43:12,339 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:43:43,531 - Epoch: [101][  100/  217]    Overall Loss 0.002571    Objective Loss 0.002571                                        LR 0.000250    Time 0.311762    
2024-05-09 11:44:09,161 - Epoch: [101][  200/  217]    Overall Loss 0.002444    Objective Loss 0.002444                                        LR 0.000250    Time 0.283973    
2024-05-09 11:44:12,631 - Epoch: [101][  217/  217]    Overall Loss 0.002527    Objective Loss 0.002527    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.277706    
2024-05-09 11:44:13,165 - 

2024-05-09 11:44:13,165 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:44:42,367 - Epoch: [102][  100/  217]    Overall Loss 0.003070    Objective Loss 0.003070                                        LR 0.000250    Time 0.291893    
2024-05-09 11:45:09,426 - Epoch: [102][  200/  217]    Overall Loss 0.002442    Objective Loss 0.002442                                        LR 0.000250    Time 0.281187    
2024-05-09 11:45:13,546 - Epoch: [102][  217/  217]    Overall Loss 0.002449    Objective Loss 0.002449    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.278132    
2024-05-09 11:45:14,567 - 

2024-05-09 11:45:14,568 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:45:47,384 - Epoch: [103][  100/  217]    Overall Loss 0.002283    Objective Loss 0.002283                                        LR 0.000250    Time 0.328026    
2024-05-09 11:46:14,913 - Epoch: [103][  200/  217]    Overall Loss 0.002202    Objective Loss 0.002202                                        LR 0.000250    Time 0.301598    
2024-05-09 11:46:19,374 - Epoch: [103][  217/  217]    Overall Loss 0.002124    Objective Loss 0.002124    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.298516    
2024-05-09 11:46:19,833 - 

2024-05-09 11:46:19,834 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:46:49,673 - Epoch: [104][  100/  217]    Overall Loss 0.003150    Objective Loss 0.003150                                        LR 0.000250    Time 0.298259    
2024-05-09 11:47:16,324 - Epoch: [104][  200/  217]    Overall Loss 0.002279    Objective Loss 0.002279                                        LR 0.000250    Time 0.282327    
2024-05-09 11:47:20,220 - Epoch: [104][  217/  217]    Overall Loss 0.002165    Objective Loss 0.002165    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.278151    
2024-05-09 11:47:20,607 - 

2024-05-09 11:47:20,608 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:47:55,133 - Epoch: [105][  100/  217]    Overall Loss 0.001920    Objective Loss 0.001920                                        LR 0.000250    Time 0.345107    
2024-05-09 11:48:21,776 - Epoch: [105][  200/  217]    Overall Loss 0.002203    Objective Loss 0.002203                                        LR 0.000250    Time 0.305714    
2024-05-09 11:48:25,603 - Epoch: [105][  217/  217]    Overall Loss 0.002125    Objective Loss 0.002125    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.299388    
2024-05-09 11:48:26,052 - 

2024-05-09 11:48:26,053 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:49:00,507 - Epoch: [106][  100/  217]    Overall Loss 0.002260    Objective Loss 0.002260                                        LR 0.000250    Time 0.344415    
2024-05-09 11:49:24,156 - Epoch: [106][  200/  217]    Overall Loss 0.002398    Objective Loss 0.002398                                        LR 0.000250    Time 0.290394    
2024-05-09 11:49:30,922 - Epoch: [106][  217/  217]    Overall Loss 0.002280    Objective Loss 0.002280    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.298813    
2024-05-09 11:49:31,298 - 

2024-05-09 11:49:31,299 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:50:04,461 - Epoch: [107][  100/  217]    Overall Loss 0.002348    Objective Loss 0.002348                                        LR 0.000250    Time 0.331484    
2024-05-09 11:50:32,325 - Epoch: [107][  200/  217]    Overall Loss 0.002112    Objective Loss 0.002112                                        LR 0.000250    Time 0.304998    
2024-05-09 11:50:35,951 - Epoch: [107][  217/  217]    Overall Loss 0.002015    Objective Loss 0.002015    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.297803    
2024-05-09 11:50:36,292 - 

2024-05-09 11:50:36,293 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:51:04,729 - Epoch: [108][  100/  217]    Overall Loss 0.002654    Objective Loss 0.002654                                        LR 0.000250    Time 0.284230    
2024-05-09 11:51:32,271 - Epoch: [108][  200/  217]    Overall Loss 0.002124    Objective Loss 0.002124                                        LR 0.000250    Time 0.279761    
2024-05-09 11:51:37,484 - Epoch: [108][  217/  217]    Overall Loss 0.002172    Objective Loss 0.002172    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.281857    
2024-05-09 11:51:37,980 - 

2024-05-09 11:51:37,980 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:52:08,472 - Epoch: [109][  100/  217]    Overall Loss 0.001547    Objective Loss 0.001547                                        LR 0.000250    Time 0.304783    
2024-05-09 11:52:32,395 - Epoch: [109][  200/  217]    Overall Loss 0.002038    Objective Loss 0.002038                                        LR 0.000250    Time 0.271953    
2024-05-09 11:52:36,771 - Epoch: [109][  217/  217]    Overall Loss 0.001944    Objective Loss 0.001944    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.270803    
2024-05-09 11:52:37,066 - 

2024-05-09 11:52:37,067 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:53:08,585 - Epoch: [110][  100/  217]    Overall Loss 0.001957    Objective Loss 0.001957                                        LR 0.000250    Time 0.315043    
2024-05-09 11:53:36,911 - Epoch: [110][  200/  217]    Overall Loss 0.001824    Objective Loss 0.001824                                        LR 0.000250    Time 0.299087    
2024-05-09 11:53:40,721 - Epoch: [110][  217/  217]    Overall Loss 0.001792    Objective Loss 0.001792    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.293198    
2024-05-09 11:53:40,990 - --- validate (epoch=110)-----------
2024-05-09 11:53:40,990 - 1736 samples (32 per mini-batch)
2024-05-09 11:53:55,977 - Epoch: [110][   55/   55]    Loss 2.562183    Top1 59.907834    Top5 76.497696    
2024-05-09 11:53:56,282 - ==> Top1: 59.908    Top5: 76.498    Loss: 2.562

2024-05-09 11:53:56,286 - ==> Best [Top1: 60.138   Top5: 76.901   Sparsity:0.00   Params: 386672 on epoch: 70]
2024-05-09 11:53:56,286 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 11:53:56,326 - 

2024-05-09 11:53:56,328 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:54:29,736 - Epoch: [111][  100/  217]    Overall Loss 0.001732    Objective Loss 0.001732                                        LR 0.000250    Time 0.333951    
2024-05-09 11:54:57,247 - Epoch: [111][  200/  217]    Overall Loss 0.001739    Objective Loss 0.001739                                        LR 0.000250    Time 0.304469    
2024-05-09 11:55:01,412 - Epoch: [111][  217/  217]    Overall Loss 0.001654    Objective Loss 0.001654    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.299798    
2024-05-09 11:55:01,720 - 

2024-05-09 11:55:01,721 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:55:31,043 - Epoch: [112][  100/  217]    Overall Loss 0.002449    Objective Loss 0.002449                                        LR 0.000250    Time 0.293097    
2024-05-09 11:55:56,819 - Epoch: [112][  200/  217]    Overall Loss 0.001711    Objective Loss 0.001711                                        LR 0.000250    Time 0.275367    
2024-05-09 11:56:02,188 - Epoch: [112][  217/  217]    Overall Loss 0.001637    Objective Loss 0.001637    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.278521    
2024-05-09 11:56:02,551 - 

2024-05-09 11:56:02,551 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:56:33,982 - Epoch: [113][  100/  217]    Overall Loss 0.001527    Objective Loss 0.001527                                        LR 0.000250    Time 0.314193    
2024-05-09 11:57:00,192 - Epoch: [113][  200/  217]    Overall Loss 0.001695    Objective Loss 0.001695                                        LR 0.000250    Time 0.288086    
2024-05-09 11:57:04,673 - Epoch: [113][  217/  217]    Overall Loss 0.001609    Objective Loss 0.001609    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.286154    
2024-05-09 11:57:05,195 - 

2024-05-09 11:57:05,197 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:57:37,466 - Epoch: [114][  100/  217]    Overall Loss 0.001198    Objective Loss 0.001198                                        LR 0.000250    Time 0.322536    
2024-05-09 11:58:05,602 - Epoch: [114][  200/  217]    Overall Loss 0.001575    Objective Loss 0.001575                                        LR 0.000250    Time 0.301882    
2024-05-09 11:58:09,642 - Epoch: [114][  217/  217]    Overall Loss 0.001667    Objective Loss 0.001667    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.296839    
2024-05-09 11:58:10,325 - 

2024-05-09 11:58:10,326 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:58:41,297 - Epoch: [115][  100/  217]    Overall Loss 0.001321    Objective Loss 0.001321                                        LR 0.000250    Time 0.309585    
2024-05-09 11:59:11,145 - Epoch: [115][  200/  217]    Overall Loss 0.001647    Objective Loss 0.001647                                        LR 0.000250    Time 0.303964    
2024-05-09 11:59:15,258 - Epoch: [115][  217/  217]    Overall Loss 0.001588    Objective Loss 0.001588    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.299096    
2024-05-09 11:59:15,570 - 

2024-05-09 11:59:15,570 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:59:46,182 - Epoch: [116][  100/  217]    Overall Loss 0.001965    Objective Loss 0.001965                                        LR 0.000250    Time 0.305982    
2024-05-09 12:00:11,716 - Epoch: [116][  200/  217]    Overall Loss 0.003369    Objective Loss 0.003369                                        LR 0.000250    Time 0.280599    
2024-05-09 12:00:16,813 - Epoch: [116][  217/  217]    Overall Loss 0.003246    Objective Loss 0.003246    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.282094    
2024-05-09 12:00:17,631 - 

2024-05-09 12:00:17,632 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:00:48,708 - Epoch: [117][  100/  217]    Overall Loss 0.002899    Objective Loss 0.002899                                        LR 0.000250    Time 0.310643    
2024-05-09 12:01:15,161 - Epoch: [117][  200/  217]    Overall Loss 0.004801    Objective Loss 0.004801                                        LR 0.000250    Time 0.287520    
2024-05-09 12:01:18,088 - Epoch: [117][  217/  217]    Overall Loss 0.004917    Objective Loss 0.004917    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.278475    
2024-05-09 12:01:18,485 - 

2024-05-09 12:01:18,486 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:01:49,542 - Epoch: [118][  100/  217]    Overall Loss 0.002046    Objective Loss 0.002046                                        LR 0.000250    Time 0.310410    
2024-05-09 12:02:16,052 - Epoch: [118][  200/  217]    Overall Loss 0.002655    Objective Loss 0.002655                                        LR 0.000250    Time 0.287696    
2024-05-09 12:02:20,892 - Epoch: [118][  217/  217]    Overall Loss 0.002521    Objective Loss 0.002521    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.287443    
2024-05-09 12:02:21,339 - 

2024-05-09 12:02:21,341 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:02:56,582 - Epoch: [119][  100/  217]    Overall Loss 0.002292    Objective Loss 0.002292                                        LR 0.000250    Time 0.352214    
2024-05-09 12:03:22,328 - Epoch: [119][  200/  217]    Overall Loss 0.001997    Objective Loss 0.001997                                        LR 0.000250    Time 0.304768    
2024-05-09 12:03:26,089 - Epoch: [119][  217/  217]    Overall Loss 0.002046    Objective Loss 0.002046    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.298211    
2024-05-09 12:03:26,376 - 

2024-05-09 12:03:26,377 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:03:55,758 - Epoch: [120][  100/  217]    Overall Loss 0.001409    Objective Loss 0.001409                                        LR 0.000250    Time 0.293681    
2024-05-09 12:04:24,178 - Epoch: [120][  200/  217]    Overall Loss 0.001835    Objective Loss 0.001835                                        LR 0.000250    Time 0.288883    
2024-05-09 12:04:30,238 - Epoch: [120][  217/  217]    Overall Loss 0.001765    Objective Loss 0.001765    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.294166    
2024-05-09 12:04:30,511 - --- validate (epoch=120)-----------
2024-05-09 12:04:30,514 - 1736 samples (32 per mini-batch)
2024-05-09 12:04:48,839 - Epoch: [120][   55/   55]    Loss 2.637730    Top1 59.735023    Top5 76.324885    
2024-05-09 12:04:49,358 - ==> Top1: 59.735    Top5: 76.325    Loss: 2.638

2024-05-09 12:04:49,363 - ==> Best [Top1: 60.138   Top5: 76.901   Sparsity:0.00   Params: 386672 on epoch: 70]
2024-05-09 12:04:49,363 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 12:04:49,414 - 

2024-05-09 12:04:49,414 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:05:19,046 - Epoch: [121][  100/  217]    Overall Loss 0.002103    Objective Loss 0.002103                                        LR 0.000250    Time 0.296180    
2024-05-09 12:05:47,031 - Epoch: [121][  200/  217]    Overall Loss 0.002432    Objective Loss 0.002432                                        LR 0.000250    Time 0.287958    
2024-05-09 12:05:52,048 - Epoch: [121][  217/  217]    Overall Loss 0.002509    Objective Loss 0.002509    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.288503    
2024-05-09 12:05:53,265 - 

2024-05-09 12:05:53,266 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:06:26,706 - Epoch: [122][  100/  217]    Overall Loss 0.002224    Objective Loss 0.002224                                        LR 0.000250    Time 0.334279    
2024-05-09 12:06:52,682 - Epoch: [122][  200/  217]    Overall Loss 0.003093    Objective Loss 0.003093                                        LR 0.000250    Time 0.296961    
2024-05-09 12:06:57,201 - Epoch: [122][  217/  217]    Overall Loss 0.003078    Objective Loss 0.003078    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.294512    
2024-05-09 12:06:57,525 - 

2024-05-09 12:06:57,525 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:07:28,390 - Epoch: [123][  100/  217]    Overall Loss 0.002949    Objective Loss 0.002949                                        LR 0.000250    Time 0.308519    
2024-05-09 12:07:57,529 - Epoch: [123][  200/  217]    Overall Loss 0.004517    Objective Loss 0.004517                                        LR 0.000250    Time 0.299899    
2024-05-09 12:08:01,064 - Epoch: [123][  217/  217]    Overall Loss 0.004262    Objective Loss 0.004262    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.292686    
2024-05-09 12:08:01,512 - 

2024-05-09 12:08:01,513 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:08:36,087 - Epoch: [124][  100/  217]    Overall Loss 0.001808    Objective Loss 0.001808                                        LR 0.000250    Time 0.345618    
2024-05-09 12:09:04,779 - Epoch: [124][  200/  217]    Overall Loss 0.002338    Objective Loss 0.002338                                        LR 0.000250    Time 0.316207    
2024-05-09 12:09:09,121 - Epoch: [124][  217/  217]    Overall Loss 0.002233    Objective Loss 0.002233    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.311430    
2024-05-09 12:09:09,529 - 

2024-05-09 12:09:09,530 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:09:41,700 - Epoch: [125][  100/  217]    Overall Loss 0.001109    Objective Loss 0.001109                                        LR 0.000250    Time 0.321569    
2024-05-09 12:10:10,195 - Epoch: [125][  200/  217]    Overall Loss 0.001515    Objective Loss 0.001515                                        LR 0.000250    Time 0.303198    
2024-05-09 12:10:12,820 - Epoch: [125][  217/  217]    Overall Loss 0.001768    Objective Loss 0.001768    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.291532    
2024-05-09 12:10:13,167 - 

2024-05-09 12:10:13,167 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:10:48,163 - Epoch: [126][  100/  217]    Overall Loss 0.002242    Objective Loss 0.002242                                        LR 0.000250    Time 0.349826    
2024-05-09 12:11:16,198 - Epoch: [126][  200/  217]    Overall Loss 0.001924    Objective Loss 0.001924                                        LR 0.000250    Time 0.315022    
2024-05-09 12:11:18,967 - Epoch: [126][  217/  217]    Overall Loss 0.001834    Objective Loss 0.001834    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.303089    
2024-05-09 12:11:19,326 - 

2024-05-09 12:11:19,327 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:11:50,677 - Epoch: [127][  100/  217]    Overall Loss 0.002081    Objective Loss 0.002081                                        LR 0.000250    Time 0.313381    
2024-05-09 12:12:14,754 - Epoch: [127][  200/  217]    Overall Loss 0.001878    Objective Loss 0.001878                                        LR 0.000250    Time 0.277014    
2024-05-09 12:12:17,784 - Epoch: [127][  217/  217]    Overall Loss 0.001779    Objective Loss 0.001779    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.269261    
2024-05-09 12:12:18,182 - 

2024-05-09 12:12:18,184 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:12:51,696 - Epoch: [128][  100/  217]    Overall Loss 0.002323    Objective Loss 0.002323                                        LR 0.000250    Time 0.334963    
2024-05-09 12:13:19,350 - Epoch: [128][  200/  217]    Overall Loss 0.001733    Objective Loss 0.001733                                        LR 0.000250    Time 0.305681    
2024-05-09 12:13:24,826 - Epoch: [128][  217/  217]    Overall Loss 0.001715    Objective Loss 0.001715    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.306951    
2024-05-09 12:13:25,415 - 

2024-05-09 12:13:25,416 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:13:55,045 - Epoch: [129][  100/  217]    Overall Loss 0.001464    Objective Loss 0.001464                                        LR 0.000250    Time 0.296151    
2024-05-09 12:14:23,321 - Epoch: [129][  200/  217]    Overall Loss 0.001370    Objective Loss 0.001370                                        LR 0.000250    Time 0.289403    
2024-05-09 12:14:28,342 - Epoch: [129][  217/  217]    Overall Loss 0.001406    Objective Loss 0.001406    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.289861    
2024-05-09 12:14:29,485 - 

2024-05-09 12:14:29,487 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:15:00,062 - Epoch: [130][  100/  217]    Overall Loss 0.002149    Objective Loss 0.002149                                        LR 0.000250    Time 0.305638    
2024-05-09 12:15:22,987 - Epoch: [130][  200/  217]    Overall Loss 0.001497    Objective Loss 0.001497                                        LR 0.000250    Time 0.267376    
2024-05-09 12:15:27,387 - Epoch: [130][  217/  217]    Overall Loss 0.001426    Objective Loss 0.001426    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.266694    
2024-05-09 12:15:28,149 - --- validate (epoch=130)-----------
2024-05-09 12:15:28,151 - 1736 samples (32 per mini-batch)
2024-05-09 12:15:44,701 - Epoch: [130][   55/   55]    Loss 2.684398    Top1 59.907834    Top5 76.152074    
2024-05-09 12:15:45,553 - ==> Top1: 59.908    Top5: 76.152    Loss: 2.684

2024-05-09 12:15:45,559 - ==> Best [Top1: 60.138   Top5: 76.901   Sparsity:0.00   Params: 386672 on epoch: 70]
2024-05-09 12:15:45,560 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 12:15:45,606 - 

2024-05-09 12:15:45,607 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:16:16,588 - Epoch: [131][  100/  217]    Overall Loss 0.001354    Objective Loss 0.001354                                        LR 0.000250    Time 0.309668    
2024-05-09 12:16:47,026 - Epoch: [131][  200/  217]    Overall Loss 0.001367    Objective Loss 0.001367                                        LR 0.000250    Time 0.306969    
2024-05-09 12:16:50,414 - Epoch: [131][  217/  217]    Overall Loss 0.001449    Objective Loss 0.001449    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.298511    
2024-05-09 12:16:51,094 - 

2024-05-09 12:16:51,095 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:17:22,438 - Epoch: [132][  100/  217]    Overall Loss 0.000484    Objective Loss 0.000484                                        LR 0.000250    Time 0.313280    
2024-05-09 12:17:48,903 - Epoch: [132][  200/  217]    Overall Loss 0.001310    Objective Loss 0.001310                                        LR 0.000250    Time 0.288907    
2024-05-09 12:17:52,824 - Epoch: [132][  217/  217]    Overall Loss 0.001509    Objective Loss 0.001509    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.284333    
2024-05-09 12:17:53,283 - 

2024-05-09 12:17:53,284 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:18:26,182 - Epoch: [133][  100/  217]    Overall Loss 0.001760    Objective Loss 0.001760                                        LR 0.000250    Time 0.328868    
2024-05-09 12:18:55,049 - Epoch: [133][  200/  217]    Overall Loss 0.001460    Objective Loss 0.001460                                        LR 0.000250    Time 0.308709    
2024-05-09 12:18:57,801 - Epoch: [133][  217/  217]    Overall Loss 0.001735    Objective Loss 0.001735    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.297198    
2024-05-09 12:18:58,244 - 

2024-05-09 12:18:58,245 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:19:30,034 - Epoch: [134][  100/  217]    Overall Loss 0.001049    Objective Loss 0.001049                                        LR 0.000250    Time 0.317745    
2024-05-09 12:19:58,894 - Epoch: [134][  200/  217]    Overall Loss 0.001962    Objective Loss 0.001962                                        LR 0.000250    Time 0.303116    
2024-05-09 12:20:02,993 - Epoch: [134][  217/  217]    Overall Loss 0.001838    Objective Loss 0.001838    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.298245    
2024-05-09 12:20:03,246 - 

2024-05-09 12:20:03,246 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:20:36,629 - Epoch: [135][  100/  217]    Overall Loss 0.000804    Objective Loss 0.000804                                        LR 0.000250    Time 0.333699    
2024-05-09 12:21:04,957 - Epoch: [135][  200/  217]    Overall Loss 0.001511    Objective Loss 0.001511                                        LR 0.000250    Time 0.308425    
2024-05-09 12:21:07,867 - Epoch: [135][  217/  217]    Overall Loss 0.001646    Objective Loss 0.001646    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.297660    
2024-05-09 12:21:08,214 - 

2024-05-09 12:21:08,215 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:21:39,564 - Epoch: [136][  100/  217]    Overall Loss 0.001708    Objective Loss 0.001708                                        LR 0.000250    Time 0.313355    
2024-05-09 12:22:06,305 - Epoch: [136][  200/  217]    Overall Loss 0.001417    Objective Loss 0.001417                                        LR 0.000250    Time 0.290323    
2024-05-09 12:22:09,114 - Epoch: [136][  217/  217]    Overall Loss 0.001625    Objective Loss 0.001625    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.280517    
2024-05-09 12:22:09,405 - 

2024-05-09 12:22:09,406 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:22:37,218 - Epoch: [137][  100/  217]    Overall Loss 0.004736    Objective Loss 0.004736                                        LR 0.000250    Time 0.277986    
2024-05-09 12:23:06,526 - Epoch: [137][  200/  217]    Overall Loss 0.005484    Objective Loss 0.005484                                        LR 0.000250    Time 0.285467    
2024-05-09 12:23:09,794 - Epoch: [137][  217/  217]    Overall Loss 0.005699    Objective Loss 0.005699    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.278155    
2024-05-09 12:23:10,128 - 

2024-05-09 12:23:10,129 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:23:41,300 - Epoch: [138][  100/  217]    Overall Loss 0.006133    Objective Loss 0.006133                                        LR 0.000250    Time 0.311517    
2024-05-09 12:24:10,575 - Epoch: [138][  200/  217]    Overall Loss 0.005254    Objective Loss 0.005254                                        LR 0.000250    Time 0.302069    
2024-05-09 12:24:14,192 - Epoch: [138][  217/  217]    Overall Loss 0.005081    Objective Loss 0.005081    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.295062    
2024-05-09 12:24:14,635 - 

2024-05-09 12:24:14,636 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:24:46,826 - Epoch: [139][  100/  217]    Overall Loss 0.002751    Objective Loss 0.002751                                        LR 0.000250    Time 0.321770    
2024-05-09 12:25:10,743 - Epoch: [139][  200/  217]    Overall Loss 0.003470    Objective Loss 0.003470                                        LR 0.000250    Time 0.280402    
2024-05-09 12:25:15,865 - Epoch: [139][  217/  217]    Overall Loss 0.003340    Objective Loss 0.003340    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.282026    
2024-05-09 12:25:16,314 - 

2024-05-09 12:25:16,315 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:25:48,309 - Epoch: [140][  100/  217]    Overall Loss 0.002650    Objective Loss 0.002650                                        LR 0.000250    Time 0.319816    
2024-05-09 12:26:14,090 - Epoch: [140][  200/  217]    Overall Loss 0.002604    Objective Loss 0.002604                                        LR 0.000250    Time 0.288748    
2024-05-09 12:26:19,685 - Epoch: [140][  217/  217]    Overall Loss 0.002600    Objective Loss 0.002600    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.291894    
2024-05-09 12:26:20,063 - --- validate (epoch=140)-----------
2024-05-09 12:26:20,063 - 1736 samples (32 per mini-batch)
2024-05-09 12:26:38,754 - Epoch: [140][   55/   55]    Loss 2.864709    Top1 59.101382    Top5 74.942396    
2024-05-09 12:26:39,000 - ==> Top1: 59.101    Top5: 74.942    Loss: 2.865

2024-05-09 12:26:39,006 - ==> Best [Top1: 60.138   Top5: 76.901   Sparsity:0.00   Params: 386672 on epoch: 70]
2024-05-09 12:26:39,006 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 12:26:39,054 - 

2024-05-09 12:26:39,054 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:27:10,461 - Epoch: [141][  100/  217]    Overall Loss 0.001807    Objective Loss 0.001807                                        LR 0.000250    Time 0.313928    
2024-05-09 12:27:38,365 - Epoch: [141][  200/  217]    Overall Loss 0.001877    Objective Loss 0.001877                                        LR 0.000250    Time 0.296429    
2024-05-09 12:27:43,701 - Epoch: [141][  217/  217]    Overall Loss 0.001773    Objective Loss 0.001773    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.297786    
2024-05-09 12:27:44,189 - 

2024-05-09 12:27:44,190 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:28:16,926 - Epoch: [142][  100/  217]    Overall Loss 0.001904    Objective Loss 0.001904                                        LR 0.000250    Time 0.327214    
2024-05-09 12:28:41,555 - Epoch: [142][  200/  217]    Overall Loss 0.001606    Objective Loss 0.001606                                        LR 0.000250    Time 0.286694    
2024-05-09 12:28:45,318 - Epoch: [142][  217/  217]    Overall Loss 0.001646    Objective Loss 0.001646    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.281563    
2024-05-09 12:28:45,644 - 

2024-05-09 12:28:45,644 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:29:17,021 - Epoch: [143][  100/  217]    Overall Loss 0.001005    Objective Loss 0.001005                                        LR 0.000250    Time 0.313624    
2024-05-09 12:29:47,362 - Epoch: [143][  200/  217]    Overall Loss 0.001805    Objective Loss 0.001805                                        LR 0.000250    Time 0.308460    
2024-05-09 12:29:52,037 - Epoch: [143][  217/  217]    Overall Loss 0.001699    Objective Loss 0.001699    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.305825    
2024-05-09 12:29:52,599 - 

2024-05-09 12:29:52,600 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:30:27,010 - Epoch: [144][  100/  217]    Overall Loss 0.001751    Objective Loss 0.001751                                        LR 0.000250    Time 0.343958    
2024-05-09 12:30:54,024 - Epoch: [144][  200/  217]    Overall Loss 0.001598    Objective Loss 0.001598                                        LR 0.000250    Time 0.306983    
2024-05-09 12:30:59,108 - Epoch: [144][  217/  217]    Overall Loss 0.001504    Objective Loss 0.001504    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.306349    
2024-05-09 12:30:59,478 - 

2024-05-09 12:30:59,479 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:31:29,591 - Epoch: [145][  100/  217]    Overall Loss 0.002182    Objective Loss 0.002182                                        LR 0.000250    Time 0.300997    
2024-05-09 12:31:55,113 - Epoch: [145][  200/  217]    Overall Loss 0.001417    Objective Loss 0.001417                                        LR 0.000250    Time 0.278042    
2024-05-09 12:32:00,280 - Epoch: [145][  217/  217]    Overall Loss 0.001500    Objective Loss 0.001500    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.280059    
2024-05-09 12:32:00,557 - 

2024-05-09 12:32:00,557 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:32:31,431 - Epoch: [146][  100/  217]    Overall Loss 0.001573    Objective Loss 0.001573                                        LR 0.000250    Time 0.308603    
2024-05-09 12:32:55,973 - Epoch: [146][  200/  217]    Overall Loss 0.001421    Objective Loss 0.001421                                        LR 0.000250    Time 0.276953    
2024-05-09 12:33:00,884 - Epoch: [146][  217/  217]    Overall Loss 0.001341    Objective Loss 0.001341    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.277874    
2024-05-09 12:33:01,282 - 

2024-05-09 12:33:01,283 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:33:33,381 - Epoch: [147][  100/  217]    Overall Loss 0.001309    Objective Loss 0.001309                                        LR 0.000250    Time 0.320839    
2024-05-09 12:33:59,358 - Epoch: [147][  200/  217]    Overall Loss 0.001475    Objective Loss 0.001475                                        LR 0.000250    Time 0.290241    
2024-05-09 12:34:04,497 - Epoch: [147][  217/  217]    Overall Loss 0.001401    Objective Loss 0.001401    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.291178    
2024-05-09 12:34:04,955 - 

2024-05-09 12:34:04,956 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:34:31,961 - Epoch: [148][  100/  217]    Overall Loss 0.001224    Objective Loss 0.001224                                        LR 0.000250    Time 0.269907    
2024-05-09 12:34:55,925 - Epoch: [148][  200/  217]    Overall Loss 0.000978    Objective Loss 0.000978                                        LR 0.000250    Time 0.254708    
2024-05-09 12:35:00,181 - Epoch: [148][  217/  217]    Overall Loss 0.001248    Objective Loss 0.001248    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.254352    
2024-05-09 12:35:00,486 - 

2024-05-09 12:35:00,487 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:35:31,945 - Epoch: [149][  100/  217]    Overall Loss 0.000987    Objective Loss 0.000987                                        LR 0.000250    Time 0.314440    
2024-05-09 12:35:53,764 - Epoch: [149][  200/  217]    Overall Loss 0.001417    Objective Loss 0.001417                                        LR 0.000250    Time 0.266259    
2024-05-09 12:35:56,816 - Epoch: [149][  217/  217]    Overall Loss 0.001487    Objective Loss 0.001487    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.259453    
2024-05-09 12:35:57,083 - 

2024-05-09 12:35:57,084 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:36:28,929 - Epoch: [150][  100/  217]    Overall Loss 0.000515    Objective Loss 0.000515                                        LR 0.000063    Time 0.318322    
2024-05-09 12:36:58,477 - Epoch: [150][  200/  217]    Overall Loss 0.001214    Objective Loss 0.001214                                        LR 0.000063    Time 0.306834    
2024-05-09 12:37:01,399 - Epoch: [150][  217/  217]    Overall Loss 0.001226    Objective Loss 0.001226    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.296250    
2024-05-09 12:37:01,803 - --- validate (epoch=150)-----------
2024-05-09 12:37:01,803 - 1736 samples (32 per mini-batch)
2024-05-09 12:37:18,261 - Epoch: [150][   55/   55]    Loss 2.802056    Top1 59.907834    Top5 75.403226    
2024-05-09 12:37:18,589 - ==> Top1: 59.908    Top5: 75.403    Loss: 2.802

2024-05-09 12:37:18,594 - ==> Best [Top1: 60.138   Top5: 76.901   Sparsity:0.00   Params: 386672 on epoch: 70]
2024-05-09 12:37:18,595 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 12:37:18,647 - 

2024-05-09 12:37:18,648 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:37:49,800 - Epoch: [151][  100/  217]    Overall Loss 0.001275    Objective Loss 0.001275                                        LR 0.000063    Time 0.311371    
2024-05-09 12:38:16,282 - Epoch: [151][  200/  217]    Overall Loss 0.001203    Objective Loss 0.001203                                        LR 0.000063    Time 0.288032    
2024-05-09 12:38:20,538 - Epoch: [151][  217/  217]    Overall Loss 0.001142    Objective Loss 0.001142    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.285057    
2024-05-09 12:38:20,854 - 

2024-05-09 12:38:20,856 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:38:50,601 - Epoch: [152][  100/  217]    Overall Loss 0.001888    Objective Loss 0.001888                                        LR 0.000063    Time 0.297314    
2024-05-09 12:39:19,886 - Epoch: [152][  200/  217]    Overall Loss 0.001240    Objective Loss 0.001240                                        LR 0.000063    Time 0.295018    
2024-05-09 12:39:25,510 - Epoch: [152][  217/  217]    Overall Loss 0.001163    Objective Loss 0.001163    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.297810    
2024-05-09 12:39:25,922 - 

2024-05-09 12:39:25,923 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:39:59,127 - Epoch: [153][  100/  217]    Overall Loss 0.001150    Objective Loss 0.001150                                        LR 0.000063    Time 0.331913    
2024-05-09 12:40:22,686 - Epoch: [153][  200/  217]    Overall Loss 0.001161    Objective Loss 0.001161                                        LR 0.000063    Time 0.283687    
2024-05-09 12:40:26,033 - Epoch: [153][  217/  217]    Overall Loss 0.001092    Objective Loss 0.001092    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.276879    
2024-05-09 12:40:26,341 - 

2024-05-09 12:40:26,342 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:40:58,432 - Epoch: [154][  100/  217]    Overall Loss 0.001120    Objective Loss 0.001120                                        LR 0.000063    Time 0.320764    
2024-05-09 12:41:24,687 - Epoch: [154][  200/  217]    Overall Loss 0.001217    Objective Loss 0.001217                                        LR 0.000063    Time 0.291588    
2024-05-09 12:41:29,453 - Epoch: [154][  217/  217]    Overall Loss 0.001141    Objective Loss 0.001141    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.290696    
2024-05-09 12:41:29,753 - 

2024-05-09 12:41:29,754 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:42:00,736 - Epoch: [155][  100/  217]    Overall Loss 0.001364    Objective Loss 0.001364                                        LR 0.000063    Time 0.309701    
2024-05-09 12:42:25,000 - Epoch: [155][  200/  217]    Overall Loss 0.000953    Objective Loss 0.000953                                        LR 0.000063    Time 0.276106    
2024-05-09 12:42:28,315 - Epoch: [155][  217/  217]    Overall Loss 0.001081    Objective Loss 0.001081    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.269740    
2024-05-09 12:42:28,574 - 

2024-05-09 12:42:28,575 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:43:02,179 - Epoch: [156][  100/  217]    Overall Loss 0.000952    Objective Loss 0.000952                                        LR 0.000063    Time 0.335914    
2024-05-09 12:43:30,249 - Epoch: [156][  200/  217]    Overall Loss 0.001055    Objective Loss 0.001055                                        LR 0.000063    Time 0.308242    
2024-05-09 12:43:34,206 - Epoch: [156][  217/  217]    Overall Loss 0.001079    Objective Loss 0.001079    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.302314    
2024-05-09 12:43:34,978 - 

2024-05-09 12:43:34,980 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:44:07,431 - Epoch: [157][  100/  217]    Overall Loss 0.001115    Objective Loss 0.001115                                        LR 0.000063    Time 0.324384    
2024-05-09 12:44:34,557 - Epoch: [157][  200/  217]    Overall Loss 0.000929    Objective Loss 0.000929                                        LR 0.000063    Time 0.297765    
2024-05-09 12:44:38,969 - Epoch: [157][  217/  217]    Overall Loss 0.001098    Objective Loss 0.001098    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.294756    
2024-05-09 12:44:39,314 - 

2024-05-09 12:44:39,315 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:45:09,265 - Epoch: [158][  100/  217]    Overall Loss 0.000789    Objective Loss 0.000789                                        LR 0.000063    Time 0.299382    
2024-05-09 12:45:38,537 - Epoch: [158][  200/  217]    Overall Loss 0.001018    Objective Loss 0.001018                                        LR 0.000063    Time 0.295989    
2024-05-09 12:45:42,005 - Epoch: [158][  217/  217]    Overall Loss 0.001072    Objective Loss 0.001072    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.288769    
2024-05-09 12:45:42,362 - 

2024-05-09 12:45:42,363 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:46:13,329 - Epoch: [159][  100/  217]    Overall Loss 0.001171    Objective Loss 0.001171                                        LR 0.000063    Time 0.309533    
2024-05-09 12:46:41,284 - Epoch: [159][  200/  217]    Overall Loss 0.001183    Objective Loss 0.001183                                        LR 0.000063    Time 0.294480    
2024-05-09 12:46:46,972 - Epoch: [159][  217/  217]    Overall Loss 0.001106    Objective Loss 0.001106    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.297608    
2024-05-09 12:46:47,308 - 

2024-05-09 12:46:47,309 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:47:16,138 - Epoch: [160][  100/  217]    Overall Loss 0.001166    Objective Loss 0.001166                                        LR 0.000063    Time 0.288162    
2024-05-09 12:47:39,963 - Epoch: [160][  200/  217]    Overall Loss 0.001163    Objective Loss 0.001163                                        LR 0.000063    Time 0.263142    
2024-05-09 12:47:44,045 - Epoch: [160][  217/  217]    Overall Loss 0.001090    Objective Loss 0.001090    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.261327    
2024-05-09 12:47:44,301 - --- validate (epoch=160)-----------
2024-05-09 12:47:44,302 - 1736 samples (32 per mini-batch)
2024-05-09 12:47:58,985 - Epoch: [160][   55/   55]    Loss 2.838835    Top1 59.792627    Top5 75.230415    
2024-05-09 12:47:59,278 - ==> Top1: 59.793    Top5: 75.230    Loss: 2.839

2024-05-09 12:47:59,286 - ==> Best [Top1: 60.138   Top5: 76.901   Sparsity:0.00   Params: 386672 on epoch: 70]
2024-05-09 12:47:59,286 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 12:47:59,337 - 

2024-05-09 12:47:59,338 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:48:25,925 - Epoch: [161][  100/  217]    Overall Loss 0.001180    Objective Loss 0.001180                                        LR 0.000063    Time 0.265743    
2024-05-09 12:48:52,400 - Epoch: [161][  200/  217]    Overall Loss 0.000927    Objective Loss 0.000927                                        LR 0.000063    Time 0.265181    
2024-05-09 12:48:57,685 - Epoch: [161][  217/  217]    Overall Loss 0.001103    Objective Loss 0.001103    Top1 98.360656    Top5 100.000000    LR 0.000063    Time 0.268747    
2024-05-09 12:48:57,973 - 

2024-05-09 12:48:57,974 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:49:26,290 - Epoch: [162][  100/  217]    Overall Loss 0.001329    Objective Loss 0.001329                                        LR 0.000063    Time 0.283034    
2024-05-09 12:49:54,161 - Epoch: [162][  200/  217]    Overall Loss 0.001143    Objective Loss 0.001143                                        LR 0.000063    Time 0.280811    
2024-05-09 12:49:59,308 - Epoch: [162][  217/  217]    Overall Loss 0.001072    Objective Loss 0.001072    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.282523    
2024-05-09 12:49:59,554 - 

2024-05-09 12:49:59,554 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:50:30,992 - Epoch: [163][  100/  217]    Overall Loss 0.000918    Objective Loss 0.000918                                        LR 0.000063    Time 0.314246    
2024-05-09 12:50:57,138 - Epoch: [163][  200/  217]    Overall Loss 0.001043    Objective Loss 0.001043                                        LR 0.000063    Time 0.287793    
2024-05-09 12:51:02,344 - Epoch: [163][  217/  217]    Overall Loss 0.001077    Objective Loss 0.001077    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.289227    
2024-05-09 12:51:02,571 - 

2024-05-09 12:51:02,571 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:51:29,902 - Epoch: [164][  100/  217]    Overall Loss 0.000200    Objective Loss 0.000200                                        LR 0.000063    Time 0.273185    
2024-05-09 12:51:57,559 - Epoch: [164][  200/  217]    Overall Loss 0.000961    Objective Loss 0.000961                                        LR 0.000063    Time 0.274812    
2024-05-09 12:52:02,519 - Epoch: [164][  217/  217]    Overall Loss 0.001076    Objective Loss 0.001076    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.276133    
2024-05-09 12:52:02,932 - 

2024-05-09 12:52:02,933 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:52:32,110 - Epoch: [165][  100/  217]    Overall Loss 0.000802    Objective Loss 0.000802                                        LR 0.000063    Time 0.291644    
2024-05-09 12:52:55,383 - Epoch: [165][  200/  217]    Overall Loss 0.000945    Objective Loss 0.000945                                        LR 0.000063    Time 0.262124    
2024-05-09 12:52:59,434 - Epoch: [165][  217/  217]    Overall Loss 0.001015    Objective Loss 0.001015    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.260242    
2024-05-09 12:52:59,995 - 

2024-05-09 12:52:59,996 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:53:28,953 - Epoch: [166][  100/  217]    Overall Loss 0.001333    Objective Loss 0.001333                                        LR 0.000063    Time 0.289434    
2024-05-09 12:53:58,476 - Epoch: [166][  200/  217]    Overall Loss 0.001126    Objective Loss 0.001126                                        LR 0.000063    Time 0.292264    
2024-05-09 12:54:03,632 - Epoch: [166][  217/  217]    Overall Loss 0.001073    Objective Loss 0.001073    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.293116    
2024-05-09 12:54:03,903 - 

2024-05-09 12:54:03,905 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:54:32,225 - Epoch: [167][  100/  217]    Overall Loss 0.001170    Objective Loss 0.001170                                        LR 0.000063    Time 0.283059    
2024-05-09 12:55:00,184 - Epoch: [167][  200/  217]    Overall Loss 0.001123    Objective Loss 0.001123                                        LR 0.000063    Time 0.281264    
2024-05-09 12:55:03,735 - Epoch: [167][  217/  217]    Overall Loss 0.001046    Objective Loss 0.001046    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.275580    
2024-05-09 12:55:04,032 - 

2024-05-09 12:55:04,033 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:55:34,578 - Epoch: [168][  100/  217]    Overall Loss 0.001254    Objective Loss 0.001254                                        LR 0.000063    Time 0.305327    
2024-05-09 12:55:59,991 - Epoch: [168][  200/  217]    Overall Loss 0.000963    Objective Loss 0.000963                                        LR 0.000063    Time 0.279673    
2024-05-09 12:56:05,222 - Epoch: [168][  217/  217]    Overall Loss 0.001001    Objective Loss 0.001001    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.281858    
2024-05-09 12:56:05,644 - 

2024-05-09 12:56:05,646 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:56:35,420 - Epoch: [169][  100/  217]    Overall Loss 0.001128    Objective Loss 0.001128                                        LR 0.000063    Time 0.297610    
2024-05-09 12:57:00,852 - Epoch: [169][  200/  217]    Overall Loss 0.000979    Objective Loss 0.000979                                        LR 0.000063    Time 0.275902    
2024-05-09 12:57:04,451 - Epoch: [169][  217/  217]    Overall Loss 0.001030    Objective Loss 0.001030    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.270862    
2024-05-09 12:57:04,714 - 

2024-05-09 12:57:04,715 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:57:35,541 - Epoch: [170][  100/  217]    Overall Loss 0.001564    Objective Loss 0.001564                                        LR 0.000063    Time 0.308145    
2024-05-09 12:57:59,579 - Epoch: [170][  200/  217]    Overall Loss 0.001218    Objective Loss 0.001218                                        LR 0.000063    Time 0.274208    
2024-05-09 12:58:04,149 - Epoch: [170][  217/  217]    Overall Loss 0.001134    Objective Loss 0.001134    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.273770    
2024-05-09 12:58:04,484 - --- validate (epoch=170)-----------
2024-05-09 12:58:04,485 - 1736 samples (32 per mini-batch)
2024-05-09 12:58:24,352 - Epoch: [170][   55/   55]    Loss 2.859044    Top1 59.792627    Top5 75.288018    
2024-05-09 12:58:24,670 - ==> Top1: 59.793    Top5: 75.288    Loss: 2.859

2024-05-09 12:58:24,674 - ==> Best [Top1: 60.138   Top5: 76.901   Sparsity:0.00   Params: 386672 on epoch: 70]
2024-05-09 12:58:24,674 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 12:58:24,717 - 

2024-05-09 12:58:24,718 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:58:54,685 - Epoch: [171][  100/  217]    Overall Loss 0.001296    Objective Loss 0.001296                                        LR 0.000063    Time 0.299527    
2024-05-09 12:59:18,410 - Epoch: [171][  200/  217]    Overall Loss 0.001013    Objective Loss 0.001013                                        LR 0.000063    Time 0.268332    
2024-05-09 12:59:22,264 - Epoch: [171][  217/  217]    Overall Loss 0.001055    Objective Loss 0.001055    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.265061    
2024-05-09 12:59:22,602 - 

2024-05-09 12:59:22,604 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:59:52,169 - Epoch: [172][  100/  217]    Overall Loss 0.000769    Objective Loss 0.000769                                        LR 0.000063    Time 0.295516    
2024-05-09 13:00:17,896 - Epoch: [172][  200/  217]    Overall Loss 0.000973    Objective Loss 0.000973                                        LR 0.000063    Time 0.276330    
2024-05-09 13:00:22,442 - Epoch: [172][  217/  217]    Overall Loss 0.001049    Objective Loss 0.001049    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.275624    
2024-05-09 13:00:22,734 - 

2024-05-09 13:00:22,735 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:00:54,505 - Epoch: [173][  100/  217]    Overall Loss 0.000356    Objective Loss 0.000356                                        LR 0.000063    Time 0.317562    
2024-05-09 13:01:21,589 - Epoch: [173][  200/  217]    Overall Loss 0.000974    Objective Loss 0.000974                                        LR 0.000063    Time 0.294134    
2024-05-09 13:01:28,366 - Epoch: [173][  217/  217]    Overall Loss 0.001007    Objective Loss 0.001007    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.302314    
2024-05-09 13:01:28,655 - 

2024-05-09 13:01:28,656 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:01:57,532 - Epoch: [174][  100/  217]    Overall Loss 0.000791    Objective Loss 0.000791                                        LR 0.000063    Time 0.288625    
2024-05-09 13:02:24,377 - Epoch: [174][  200/  217]    Overall Loss 0.001166    Objective Loss 0.001166                                        LR 0.000063    Time 0.278474    
2024-05-09 13:02:29,020 - Epoch: [174][  217/  217]    Overall Loss 0.001087    Objective Loss 0.001087    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.278045    
2024-05-09 13:02:29,266 - 

2024-05-09 13:02:29,267 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:02:59,413 - Epoch: [175][  100/  217]    Overall Loss 0.000904    Objective Loss 0.000904                                        LR 0.000063    Time 0.301330    
2024-05-09 13:03:25,984 - Epoch: [175][  200/  217]    Overall Loss 0.001169    Objective Loss 0.001169                                        LR 0.000063    Time 0.283459    
2024-05-09 13:03:30,833 - Epoch: [175][  217/  217]    Overall Loss 0.001086    Objective Loss 0.001086    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.283586    
2024-05-09 13:03:31,167 - 

2024-05-09 13:03:31,167 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:04:03,061 - Epoch: [176][  100/  217]    Overall Loss 0.001174    Objective Loss 0.001174                                        LR 0.000063    Time 0.318798    
2024-05-09 13:04:30,042 - Epoch: [176][  200/  217]    Overall Loss 0.001249    Objective Loss 0.001249                                        LR 0.000063    Time 0.294246    
2024-05-09 13:04:35,263 - Epoch: [176][  217/  217]    Overall Loss 0.001162    Objective Loss 0.001162    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.295243    
2024-05-09 13:04:35,662 - 

2024-05-09 13:04:35,662 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:05:05,767 - Epoch: [177][  100/  217]    Overall Loss 0.001654    Objective Loss 0.001654                                        LR 0.000063    Time 0.300916    
2024-05-09 13:05:31,291 - Epoch: [177][  200/  217]    Overall Loss 0.001018    Objective Loss 0.001018                                        LR 0.000063    Time 0.278016    
2024-05-09 13:05:34,978 - Epoch: [177][  217/  217]    Overall Loss 0.001029    Objective Loss 0.001029    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.273216    
2024-05-09 13:05:35,331 - 

2024-05-09 13:05:35,332 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:06:05,903 - Epoch: [178][  100/  217]    Overall Loss 0.000813    Objective Loss 0.000813                                        LR 0.000063    Time 0.305575    
2024-05-09 13:06:33,075 - Epoch: [178][  200/  217]    Overall Loss 0.000849    Objective Loss 0.000849                                        LR 0.000063    Time 0.288582    
2024-05-09 13:06:38,438 - Epoch: [178][  217/  217]    Overall Loss 0.001051    Objective Loss 0.001051    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.290677    
2024-05-09 13:06:38,792 - 

2024-05-09 13:06:38,793 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:07:06,932 - Epoch: [179][  100/  217]    Overall Loss 0.001141    Objective Loss 0.001141                                        LR 0.000063    Time 0.281236    
2024-05-09 13:07:32,308 - Epoch: [179][  200/  217]    Overall Loss 0.001040    Objective Loss 0.001040                                        LR 0.000063    Time 0.267433    
2024-05-09 13:07:36,973 - Epoch: [179][  217/  217]    Overall Loss 0.000970    Objective Loss 0.000970    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.267969    
2024-05-09 13:07:37,319 - 

2024-05-09 13:07:37,320 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:08:09,375 - Epoch: [180][  100/  217]    Overall Loss 0.000578    Objective Loss 0.000578                                        LR 0.000063    Time 0.320435    
2024-05-09 13:08:34,103 - Epoch: [180][  200/  217]    Overall Loss 0.000974    Objective Loss 0.000974                                        LR 0.000063    Time 0.283800    
2024-05-09 13:08:38,751 - Epoch: [180][  217/  217]    Overall Loss 0.000989    Objective Loss 0.000989    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.282966    
2024-05-09 13:08:39,231 - --- validate (epoch=180)-----------
2024-05-09 13:08:39,232 - 1736 samples (32 per mini-batch)
2024-05-09 13:08:59,490 - Epoch: [180][   55/   55]    Loss 2.858399    Top1 60.138249    Top5 75.403226    
2024-05-09 13:09:00,135 - ==> Top1: 60.138    Top5: 75.403    Loss: 2.858

2024-05-09 13:09:00,141 - ==> Best [Top1: 60.138   Top5: 76.901   Sparsity:0.00   Params: 386672 on epoch: 70]
2024-05-09 13:09:00,141 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 13:09:00,193 - 

2024-05-09 13:09:00,194 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:09:30,863 - Epoch: [181][  100/  217]    Overall Loss 0.000926    Objective Loss 0.000926                                        LR 0.000063    Time 0.306564    
2024-05-09 13:09:56,833 - Epoch: [181][  200/  217]    Overall Loss 0.001099    Objective Loss 0.001099                                        LR 0.000063    Time 0.283072    
2024-05-09 13:10:01,117 - Epoch: [181][  217/  217]    Overall Loss 0.001023    Objective Loss 0.001023    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.280624    
2024-05-09 13:10:01,431 - 

2024-05-09 13:10:01,432 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:10:33,392 - Epoch: [182][  100/  217]    Overall Loss 0.000600    Objective Loss 0.000600                                        LR 0.000063    Time 0.319469    
2024-05-09 13:11:00,343 - Epoch: [182][  200/  217]    Overall Loss 0.000974    Objective Loss 0.000974                                        LR 0.000063    Time 0.294421    
2024-05-09 13:11:02,898 - Epoch: [182][  217/  217]    Overall Loss 0.001041    Objective Loss 0.001041    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.283119    
2024-05-09 13:11:03,216 - 

2024-05-09 13:11:03,216 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:11:32,025 - Epoch: [183][  100/  217]    Overall Loss 0.001279    Objective Loss 0.001279                                        LR 0.000063    Time 0.287969    
2024-05-09 13:11:58,156 - Epoch: [183][  200/  217]    Overall Loss 0.001122    Objective Loss 0.001122                                        LR 0.000063    Time 0.274571    
2024-05-09 13:12:01,701 - Epoch: [183][  217/  217]    Overall Loss 0.001042    Objective Loss 0.001042    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.269387    
2024-05-09 13:12:02,138 - 

2024-05-09 13:12:02,143 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:12:33,555 - Epoch: [184][  100/  217]    Overall Loss 0.001593    Objective Loss 0.001593                                        LR 0.000063    Time 0.313959    
2024-05-09 13:13:01,807 - Epoch: [184][  200/  217]    Overall Loss 0.001106    Objective Loss 0.001106                                        LR 0.000063    Time 0.298174    
2024-05-09 13:13:05,061 - Epoch: [184][  217/  217]    Overall Loss 0.001029    Objective Loss 0.001029    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.289797    
2024-05-09 13:13:05,302 - 

2024-05-09 13:13:05,303 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:13:34,458 - Epoch: [185][  100/  217]    Overall Loss 0.000938    Objective Loss 0.000938                                        LR 0.000063    Time 0.291412    
2024-05-09 13:14:00,670 - Epoch: [185][  200/  217]    Overall Loss 0.001020    Objective Loss 0.001020                                        LR 0.000063    Time 0.276710    
2024-05-09 13:14:04,530 - Epoch: [185][  217/  217]    Overall Loss 0.000947    Objective Loss 0.000947    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.272804    
2024-05-09 13:14:04,885 - 

2024-05-09 13:14:04,886 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:14:36,442 - Epoch: [186][  100/  217]    Overall Loss 0.001010    Objective Loss 0.001010                                        LR 0.000063    Time 0.315431    
2024-05-09 13:15:02,933 - Epoch: [186][  200/  217]    Overall Loss 0.001041    Objective Loss 0.001041                                        LR 0.000063    Time 0.290109    
2024-05-09 13:15:06,625 - Epoch: [186][  217/  217]    Overall Loss 0.000969    Objective Loss 0.000969    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.284380    
2024-05-09 13:15:06,895 - 

2024-05-09 13:15:06,896 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:15:37,349 - Epoch: [187][  100/  217]    Overall Loss 0.001276    Objective Loss 0.001276                                        LR 0.000063    Time 0.304388    
2024-05-09 13:16:04,549 - Epoch: [187][  200/  217]    Overall Loss 0.001077    Objective Loss 0.001077                                        LR 0.000063    Time 0.288137    
2024-05-09 13:16:08,609 - Epoch: [187][  217/  217]    Overall Loss 0.001004    Objective Loss 0.001004    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.284261    
2024-05-09 13:16:08,874 - 

2024-05-09 13:16:08,874 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:16:39,512 - Epoch: [188][  100/  217]    Overall Loss 0.000383    Objective Loss 0.000383                                        LR 0.000063    Time 0.306255    
2024-05-09 13:17:05,804 - Epoch: [188][  200/  217]    Overall Loss 0.000921    Objective Loss 0.000921                                        LR 0.000063    Time 0.284528    
2024-05-09 13:17:10,001 - Epoch: [188][  217/  217]    Overall Loss 0.001088    Objective Loss 0.001088    Top1 98.360656    Top5 100.000000    LR 0.000063    Time 0.281568    
2024-05-09 13:17:10,348 - 

2024-05-09 13:17:10,348 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:17:45,080 - Epoch: [189][  100/  217]    Overall Loss 0.000885    Objective Loss 0.000885                                        LR 0.000063    Time 0.347203    
2024-05-09 13:18:10,962 - Epoch: [189][  200/  217]    Overall Loss 0.001205    Objective Loss 0.001205                                        LR 0.000063    Time 0.302956    
2024-05-09 13:18:14,455 - Epoch: [189][  217/  217]    Overall Loss 0.001118    Objective Loss 0.001118    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.295303    
2024-05-09 13:18:14,741 - 

2024-05-09 13:18:14,742 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:18:48,289 - Epoch: [190][  100/  217]    Overall Loss 0.000590    Objective Loss 0.000590                                        LR 0.000063    Time 0.335322    
2024-05-09 13:19:13,122 - Epoch: [190][  200/  217]    Overall Loss 0.000958    Objective Loss 0.000958                                        LR 0.000063    Time 0.291767    
2024-05-09 13:19:18,248 - Epoch: [190][  217/  217]    Overall Loss 0.001111    Objective Loss 0.001111    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.292518    
2024-05-09 13:19:18,747 - --- validate (epoch=190)-----------
2024-05-09 13:19:18,748 - 1736 samples (32 per mini-batch)
2024-05-09 13:19:38,726 - Epoch: [190][   55/   55]    Loss 2.881166    Top1 60.483871    Top5 75.576037    
2024-05-09 13:19:39,040 - ==> Top1: 60.484    Top5: 75.576    Loss: 2.881

2024-05-09 13:19:39,046 - ==> Best [Top1: 60.484   Top5: 75.576   Sparsity:0.00   Params: 386672 on epoch: 190]
2024-05-09 13:19:39,046 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 13:19:39,116 - 

2024-05-09 13:19:39,117 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:20:07,811 - Epoch: [191][  100/  217]    Overall Loss 0.001645    Objective Loss 0.001645                                        LR 0.000063    Time 0.286797    
2024-05-09 13:20:37,514 - Epoch: [191][  200/  217]    Overall Loss 0.001493    Objective Loss 0.001493                                        LR 0.000063    Time 0.291857    
2024-05-09 13:20:41,534 - Epoch: [191][  217/  217]    Overall Loss 0.001387    Objective Loss 0.001387    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.287503    
2024-05-09 13:20:41,840 - 

2024-05-09 13:20:41,840 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:21:12,428 - Epoch: [192][  100/  217]    Overall Loss 0.000325    Objective Loss 0.000325                                        LR 0.000063    Time 0.305759    
2024-05-09 13:21:39,452 - Epoch: [192][  200/  217]    Overall Loss 0.001238    Objective Loss 0.001238                                        LR 0.000063    Time 0.287941    
2024-05-09 13:21:43,957 - Epoch: [192][  217/  217]    Overall Loss 0.001242    Objective Loss 0.001242    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.286130    
2024-05-09 13:21:44,329 - 

2024-05-09 13:21:44,330 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:22:17,895 - Epoch: [193][  100/  217]    Overall Loss 0.000756    Objective Loss 0.000756                                        LR 0.000063    Time 0.335519    
2024-05-09 13:22:46,148 - Epoch: [193][  200/  217]    Overall Loss 0.001187    Objective Loss 0.001187                                        LR 0.000063    Time 0.308958    
2024-05-09 13:22:50,970 - Epoch: [193][  217/  217]    Overall Loss 0.001101    Objective Loss 0.001101    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.306941    
2024-05-09 13:22:51,428 - 

2024-05-09 13:22:51,430 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:23:24,901 - Epoch: [194][  100/  217]    Overall Loss 0.000941    Objective Loss 0.000941                                        LR 0.000063    Time 0.334578    
2024-05-09 13:23:53,180 - Epoch: [194][  200/  217]    Overall Loss 0.001017    Objective Loss 0.001017                                        LR 0.000063    Time 0.308625    
2024-05-09 13:23:57,021 - Epoch: [194][  217/  217]    Overall Loss 0.001047    Objective Loss 0.001047    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.302131    
2024-05-09 13:23:57,401 - 

2024-05-09 13:23:57,401 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:24:25,233 - Epoch: [195][  100/  217]    Overall Loss 0.000770    Objective Loss 0.000770                                        LR 0.000063    Time 0.278178    
2024-05-09 13:24:52,032 - Epoch: [195][  200/  217]    Overall Loss 0.000911    Objective Loss 0.000911                                        LR 0.000063    Time 0.273018    
2024-05-09 13:24:56,755 - Epoch: [195][  217/  217]    Overall Loss 0.001040    Objective Loss 0.001040    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.273383    
2024-05-09 13:24:57,269 - 

2024-05-09 13:24:57,270 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:25:27,034 - Epoch: [196][  100/  217]    Overall Loss 0.000539    Objective Loss 0.000539                                        LR 0.000063    Time 0.297506    
2024-05-09 13:25:51,624 - Epoch: [196][  200/  217]    Overall Loss 0.001077    Objective Loss 0.001077                                        LR 0.000063    Time 0.271638    
2024-05-09 13:25:55,505 - Epoch: [196][  217/  217]    Overall Loss 0.001000    Objective Loss 0.001000    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.268227    
2024-05-09 13:25:56,107 - 

2024-05-09 13:25:56,108 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:26:29,369 - Epoch: [197][  100/  217]    Overall Loss 0.000798    Objective Loss 0.000798                                        LR 0.000063    Time 0.332468    
2024-05-09 13:26:58,219 - Epoch: [197][  200/  217]    Overall Loss 0.001075    Objective Loss 0.001075                                        LR 0.000063    Time 0.310427    
2024-05-09 13:27:01,431 - Epoch: [197][  217/  217]    Overall Loss 0.000996    Objective Loss 0.000996    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.300895    
2024-05-09 13:27:01,928 - 

2024-05-09 13:27:01,930 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:27:31,201 - Epoch: [198][  100/  217]    Overall Loss 0.000651    Objective Loss 0.000651                                        LR 0.000063    Time 0.292547    
2024-05-09 13:28:02,449 - Epoch: [198][  200/  217]    Overall Loss 0.000772    Objective Loss 0.000772                                        LR 0.000063    Time 0.302455    
2024-05-09 13:28:05,407 - Epoch: [198][  217/  217]    Overall Loss 0.000968    Objective Loss 0.000968    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.292378    
2024-05-09 13:28:05,731 - 

2024-05-09 13:28:05,732 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:28:40,136 - Epoch: [199][  100/  217]    Overall Loss 0.000620    Objective Loss 0.000620                                        LR 0.000063    Time 0.343916    
2024-05-09 13:29:06,607 - Epoch: [199][  200/  217]    Overall Loss 0.001064    Objective Loss 0.001064                                        LR 0.000063    Time 0.304253    
2024-05-09 13:29:11,595 - Epoch: [199][  217/  217]    Overall Loss 0.000987    Objective Loss 0.000987    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.303392    
2024-05-09 13:29:12,004 - 

2024-05-09 13:29:12,005 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:29:42,313 - Epoch: [200][  100/  217]    Overall Loss 0.001237    Objective Loss 0.001237                                        LR 0.000016    Time 0.302941    
2024-05-09 13:30:05,963 - Epoch: [200][  200/  217]    Overall Loss 0.001007    Objective Loss 0.001007                                        LR 0.000016    Time 0.269655    
2024-05-09 13:30:09,799 - Epoch: [200][  217/  217]    Overall Loss 0.000935    Objective Loss 0.000935    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.266192    
2024-05-09 13:30:10,087 - --- validate (epoch=200)-----------
2024-05-09 13:30:10,087 - 1736 samples (32 per mini-batch)
2024-05-09 13:30:29,644 - Epoch: [200][   55/   55]    Loss 2.885626    Top1 60.311060    Top5 75.691244    
2024-05-09 13:30:29,968 - ==> Top1: 60.311    Top5: 75.691    Loss: 2.886

2024-05-09 13:30:29,975 - ==> Best [Top1: 60.484   Top5: 75.576   Sparsity:0.00   Params: 386672 on epoch: 190]
2024-05-09 13:30:29,976 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 13:30:30,021 - 

2024-05-09 13:30:30,021 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:31:03,364 - Epoch: [201][  100/  217]    Overall Loss 0.000986    Objective Loss 0.000986                                        LR 0.000016    Time 0.333288    
2024-05-09 13:31:29,287 - Epoch: [201][  200/  217]    Overall Loss 0.000868    Objective Loss 0.000868                                        LR 0.000016    Time 0.296203    
2024-05-09 13:31:32,314 - Epoch: [201][  217/  217]    Overall Loss 0.000896    Objective Loss 0.000896    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.286936    
2024-05-09 13:31:32,633 - 

2024-05-09 13:31:32,634 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:32:02,276 - Epoch: [202][  100/  217]    Overall Loss 0.001276    Objective Loss 0.001276                                        LR 0.000016    Time 0.296314    
2024-05-09 13:32:29,121 - Epoch: [202][  200/  217]    Overall Loss 0.000998    Objective Loss 0.000998                                        LR 0.000016    Time 0.282324    
2024-05-09 13:32:32,890 - Epoch: [202][  217/  217]    Overall Loss 0.000925    Objective Loss 0.000925    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.277566    
2024-05-09 13:32:33,206 - 

2024-05-09 13:32:33,206 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:33:03,144 - Epoch: [203][  100/  217]    Overall Loss 0.000780    Objective Loss 0.000780                                        LR 0.000016    Time 0.299265    
2024-05-09 13:33:27,758 - Epoch: [203][  200/  217]    Overall Loss 0.000737    Objective Loss 0.000737                                        LR 0.000016    Time 0.272644    
2024-05-09 13:33:31,887 - Epoch: [203][  217/  217]    Overall Loss 0.000887    Objective Loss 0.000887    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.270305    
2024-05-09 13:33:32,187 - 

2024-05-09 13:33:32,188 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:34:03,316 - Epoch: [204][  100/  217]    Overall Loss 0.001166    Objective Loss 0.001166                                        LR 0.000016    Time 0.311151    
2024-05-09 13:34:27,780 - Epoch: [204][  200/  217]    Overall Loss 0.000846    Objective Loss 0.000846                                        LR 0.000016    Time 0.277844    
2024-05-09 13:34:32,073 - Epoch: [204][  217/  217]    Overall Loss 0.000901    Objective Loss 0.000901    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.275850    
2024-05-09 13:34:32,379 - 

2024-05-09 13:34:32,380 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:35:06,074 - Epoch: [205][  100/  217]    Overall Loss 0.001022    Objective Loss 0.001022                                        LR 0.000016    Time 0.336824    
2024-05-09 13:35:31,046 - Epoch: [205][  200/  217]    Overall Loss 0.000876    Objective Loss 0.000876                                        LR 0.000016    Time 0.293209    
2024-05-09 13:35:36,160 - Epoch: [205][  217/  217]    Overall Loss 0.000884    Objective Loss 0.000884    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.293795    
2024-05-09 13:35:36,483 - 

2024-05-09 13:35:36,484 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:36:07,513 - Epoch: [206][  100/  217]    Overall Loss 0.000929    Objective Loss 0.000929                                        LR 0.000016    Time 0.310173    
2024-05-09 13:36:34,757 - Epoch: [206][  200/  217]    Overall Loss 0.000813    Objective Loss 0.000813                                        LR 0.000016    Time 0.291241    
2024-05-09 13:36:39,778 - Epoch: [206][  217/  217]    Overall Loss 0.000891    Objective Loss 0.000891    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.291551    
2024-05-09 13:36:40,022 - 

2024-05-09 13:36:40,024 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:37:13,897 - Epoch: [207][  100/  217]    Overall Loss 0.000930    Objective Loss 0.000930                                        LR 0.000016    Time 0.338599    
2024-05-09 13:37:39,212 - Epoch: [207][  200/  217]    Overall Loss 0.000961    Objective Loss 0.000961                                        LR 0.000016    Time 0.295819    
2024-05-09 13:37:42,674 - Epoch: [207][  217/  217]    Overall Loss 0.000896    Objective Loss 0.000896    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.288586    
2024-05-09 13:37:43,083 - 

2024-05-09 13:37:43,084 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:38:13,839 - Epoch: [208][  100/  217]    Overall Loss 0.000303    Objective Loss 0.000303                                        LR 0.000016    Time 0.307430    
2024-05-09 13:38:40,590 - Epoch: [208][  200/  217]    Overall Loss 0.000777    Objective Loss 0.000777                                        LR 0.000016    Time 0.287411    
2024-05-09 13:38:45,103 - Epoch: [208][  217/  217]    Overall Loss 0.000906    Objective Loss 0.000906    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.285678    
2024-05-09 13:38:45,488 - 

2024-05-09 13:38:45,488 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:39:16,121 - Epoch: [209][  100/  217]    Overall Loss 0.000957    Objective Loss 0.000957                                        LR 0.000016    Time 0.306204    
2024-05-09 13:39:45,574 - Epoch: [209][  200/  217]    Overall Loss 0.000976    Objective Loss 0.000976                                        LR 0.000016    Time 0.300303    
2024-05-09 13:39:49,550 - Epoch: [209][  217/  217]    Overall Loss 0.000906    Objective Loss 0.000906    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.295089    
2024-05-09 13:39:49,902 - 

2024-05-09 13:39:49,903 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:40:18,718 - Epoch: [210][  100/  217]    Overall Loss 0.001413    Objective Loss 0.001413                                        LR 0.000016    Time 0.288005    
2024-05-09 13:40:47,220 - Epoch: [210][  200/  217]    Overall Loss 0.000978    Objective Loss 0.000978                                        LR 0.000016    Time 0.286454    
2024-05-09 13:40:51,692 - Epoch: [210][  217/  217]    Overall Loss 0.000907    Objective Loss 0.000907    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.284603    
2024-05-09 13:40:52,326 - --- validate (epoch=210)-----------
2024-05-09 13:40:52,327 - 1736 samples (32 per mini-batch)
2024-05-09 13:41:10,016 - Epoch: [210][   55/   55]    Loss 2.890362    Top1 60.541475    Top5 75.748848    
2024-05-09 13:41:10,785 - ==> Top1: 60.541    Top5: 75.749    Loss: 2.890

2024-05-09 13:41:10,791 - ==> Best [Top1: 60.541   Top5: 75.749   Sparsity:0.00   Params: 386672 on epoch: 210]
2024-05-09 13:41:10,791 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 13:41:10,847 - 

2024-05-09 13:41:10,848 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:41:44,917 - Epoch: [211][  100/  217]    Overall Loss 0.001491    Objective Loss 0.001491                                        LR 0.000016    Time 0.340554    
2024-05-09 13:42:09,984 - Epoch: [211][  200/  217]    Overall Loss 0.000993    Objective Loss 0.000993                                        LR 0.000016    Time 0.295556    
2024-05-09 13:42:15,076 - Epoch: [211][  217/  217]    Overall Loss 0.000920    Objective Loss 0.000920    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.295843    
2024-05-09 13:42:15,332 - 

2024-05-09 13:42:15,332 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:42:49,669 - Epoch: [212][  100/  217]    Overall Loss 0.000944    Objective Loss 0.000944                                        LR 0.000016    Time 0.343239    
2024-05-09 13:43:14,037 - Epoch: [212][  200/  217]    Overall Loss 0.000847    Objective Loss 0.000847                                        LR 0.000016    Time 0.293396    
2024-05-09 13:43:19,177 - Epoch: [212][  217/  217]    Overall Loss 0.000903    Objective Loss 0.000903    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.294084    
2024-05-09 13:43:20,043 - 

2024-05-09 13:43:20,044 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:43:49,384 - Epoch: [213][  100/  217]    Overall Loss 0.000657    Objective Loss 0.000657                                        LR 0.000016    Time 0.293278    
2024-05-09 13:44:12,322 - Epoch: [213][  200/  217]    Overall Loss 0.000957    Objective Loss 0.000957                                        LR 0.000016    Time 0.261249    
2024-05-09 13:44:15,956 - Epoch: [213][  217/  217]    Overall Loss 0.000886    Objective Loss 0.000886    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.257519    
2024-05-09 13:44:16,321 - 

2024-05-09 13:44:16,322 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:44:44,980 - Epoch: [214][  100/  217]    Overall Loss 0.000620    Objective Loss 0.000620                                        LR 0.000016    Time 0.286449    
2024-05-09 13:45:11,209 - Epoch: [214][  200/  217]    Overall Loss 0.000970    Objective Loss 0.000970                                        LR 0.000016    Time 0.274315    
2024-05-09 13:45:15,152 - Epoch: [214][  217/  217]    Overall Loss 0.000901    Objective Loss 0.000901    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.270982    
2024-05-09 13:45:15,435 - 

2024-05-09 13:45:15,436 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:45:47,305 - Epoch: [215][  100/  217]    Overall Loss 0.000510    Objective Loss 0.000510                                        LR 0.000016    Time 0.318542    
2024-05-09 13:46:15,515 - Epoch: [215][  200/  217]    Overall Loss 0.000946    Objective Loss 0.000946                                        LR 0.000016    Time 0.300258    
2024-05-09 13:46:20,441 - Epoch: [215][  217/  217]    Overall Loss 0.000903    Objective Loss 0.000903    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.299422    
2024-05-09 13:46:20,846 - 

2024-05-09 13:46:20,847 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:46:57,194 - Epoch: [216][  100/  217]    Overall Loss 0.000492    Objective Loss 0.000492                                        LR 0.000016    Time 0.363331    
2024-05-09 13:47:23,004 - Epoch: [216][  200/  217]    Overall Loss 0.000856    Objective Loss 0.000856                                        LR 0.000016    Time 0.310654    
2024-05-09 13:47:27,287 - Epoch: [216][  217/  217]    Overall Loss 0.000891    Objective Loss 0.000891    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.306038    
2024-05-09 13:47:27,834 - 

2024-05-09 13:47:27,836 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:47:57,163 - Epoch: [217][  100/  217]    Overall Loss 0.001113    Objective Loss 0.001113                                        LR 0.000016    Time 0.293125    
2024-05-09 13:48:25,937 - Epoch: [217][  200/  217]    Overall Loss 0.000839    Objective Loss 0.000839                                        LR 0.000016    Time 0.290367    
2024-05-09 13:48:28,942 - Epoch: [217][  217/  217]    Overall Loss 0.000887    Objective Loss 0.000887    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.281451    
2024-05-09 13:48:29,276 - 

2024-05-09 13:48:29,276 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:48:59,641 - Epoch: [218][  100/  217]    Overall Loss 0.000972    Objective Loss 0.000972                                        LR 0.000016    Time 0.303501    
2024-05-09 13:49:26,145 - Epoch: [218][  200/  217]    Overall Loss 0.000866    Objective Loss 0.000866                                        LR 0.000016    Time 0.284204    
2024-05-09 13:49:31,493 - Epoch: [218][  217/  217]    Overall Loss 0.000907    Objective Loss 0.000907    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.286571    
2024-05-09 13:49:31,834 - 

2024-05-09 13:49:31,836 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:50:01,649 - Epoch: [219][  100/  217]    Overall Loss 0.000955    Objective Loss 0.000955                                        LR 0.000016    Time 0.297976    
2024-05-09 13:50:27,638 - Epoch: [219][  200/  217]    Overall Loss 0.000961    Objective Loss 0.000961                                        LR 0.000016    Time 0.278864    
2024-05-09 13:50:32,180 - Epoch: [219][  217/  217]    Overall Loss 0.000890    Objective Loss 0.000890    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.277939    
2024-05-09 13:50:32,480 - 

2024-05-09 13:50:32,481 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:50:59,714 - Epoch: [220][  100/  217]    Overall Loss 0.000315    Objective Loss 0.000315                                        LR 0.000016    Time 0.272198    
2024-05-09 13:51:24,058 - Epoch: [220][  200/  217]    Overall Loss 0.000864    Objective Loss 0.000864                                        LR 0.000016    Time 0.257755    
2024-05-09 13:51:27,657 - Epoch: [220][  217/  217]    Overall Loss 0.000914    Objective Loss 0.000914    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.254139    
2024-05-09 13:51:28,002 - --- validate (epoch=220)-----------
2024-05-09 13:51:28,002 - 1736 samples (32 per mini-batch)
2024-05-09 13:51:47,678 - Epoch: [220][   55/   55]    Loss 2.925897    Top1 60.195853    Top5 75.576037    
2024-05-09 13:51:47,985 - ==> Top1: 60.196    Top5: 75.576    Loss: 2.926

2024-05-09 13:51:47,990 - ==> Best [Top1: 60.541   Top5: 75.749   Sparsity:0.00   Params: 386672 on epoch: 210]
2024-05-09 13:51:47,991 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 13:51:48,042 - 

2024-05-09 13:51:48,043 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:52:16,409 - Epoch: [221][  100/  217]    Overall Loss 0.000737    Objective Loss 0.000737                                        LR 0.000016    Time 0.283529    
2024-05-09 13:52:42,244 - Epoch: [221][  200/  217]    Overall Loss 0.000633    Objective Loss 0.000633                                        LR 0.000016    Time 0.270881    
2024-05-09 13:52:46,044 - Epoch: [221][  217/  217]    Overall Loss 0.000904    Objective Loss 0.000904    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.267160    
2024-05-09 13:52:46,360 - 

2024-05-09 13:52:46,361 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:53:16,712 - Epoch: [222][  100/  217]    Overall Loss 0.000488    Objective Loss 0.000488                                        LR 0.000016    Time 0.303364    
2024-05-09 13:53:42,645 - Epoch: [222][  200/  217]    Overall Loss 0.000948    Objective Loss 0.000948                                        LR 0.000016    Time 0.281283    
2024-05-09 13:53:47,409 - Epoch: [222][  217/  217]    Overall Loss 0.000878    Objective Loss 0.000878    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.281190    
2024-05-09 13:53:47,661 - 

2024-05-09 13:53:47,661 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:54:15,687 - Epoch: [223][  100/  217]    Overall Loss 0.000725    Objective Loss 0.000725                                        LR 0.000016    Time 0.280131    
2024-05-09 13:54:44,692 - Epoch: [223][  200/  217]    Overall Loss 0.000850    Objective Loss 0.000850                                        LR 0.000016    Time 0.285031    
2024-05-09 13:54:49,396 - Epoch: [223][  217/  217]    Overall Loss 0.000907    Objective Loss 0.000907    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.284365    
2024-05-09 13:54:49,680 - 

2024-05-09 13:54:49,681 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:55:19,845 - Epoch: [224][  100/  217]    Overall Loss 0.001205    Objective Loss 0.001205                                        LR 0.000016    Time 0.301523    
2024-05-09 13:55:46,055 - Epoch: [224][  200/  217]    Overall Loss 0.000846    Objective Loss 0.000846                                        LR 0.000016    Time 0.281715    
2024-05-09 13:55:51,248 - Epoch: [224][  217/  217]    Overall Loss 0.000889    Objective Loss 0.000889    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.283561    
2024-05-09 13:55:51,567 - 

2024-05-09 13:55:51,568 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:56:24,086 - Epoch: [225][  100/  217]    Overall Loss 0.001144    Objective Loss 0.001144                                        LR 0.000016    Time 0.325037    
2024-05-09 13:56:50,045 - Epoch: [225][  200/  217]    Overall Loss 0.000811    Objective Loss 0.000811                                        LR 0.000016    Time 0.292256    
2024-05-09 13:56:53,131 - Epoch: [225][  217/  217]    Overall Loss 0.000860    Objective Loss 0.000860    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.283571    
2024-05-09 13:56:53,457 - 

2024-05-09 13:56:53,459 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:57:22,426 - Epoch: [226][  100/  217]    Overall Loss 0.000292    Objective Loss 0.000292                                        LR 0.000016    Time 0.289527    
2024-05-09 13:57:50,217 - Epoch: [226][  200/  217]    Overall Loss 0.000963    Objective Loss 0.000963                                        LR 0.000016    Time 0.283640    
2024-05-09 13:57:55,442 - Epoch: [226][  217/  217]    Overall Loss 0.000893    Objective Loss 0.000893    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.285487    
2024-05-09 13:57:55,824 - 

2024-05-09 13:57:55,825 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:58:23,073 - Epoch: [227][  100/  217]    Overall Loss 0.001206    Objective Loss 0.001206                                        LR 0.000016    Time 0.272371    
2024-05-09 13:58:48,883 - Epoch: [227][  200/  217]    Overall Loss 0.000961    Objective Loss 0.000961                                        LR 0.000016    Time 0.265175    
2024-05-09 13:58:54,211 - Epoch: [227][  217/  217]    Overall Loss 0.000891    Objective Loss 0.000891    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.268942    
2024-05-09 13:58:54,577 - 

2024-05-09 13:58:54,577 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:59:25,468 - Epoch: [228][  100/  217]    Overall Loss 0.000944    Objective Loss 0.000944                                        LR 0.000016    Time 0.308762    
2024-05-09 13:59:51,225 - Epoch: [228][  200/  217]    Overall Loss 0.000972    Objective Loss 0.000972                                        LR 0.000016    Time 0.283112    
2024-05-09 13:59:56,661 - Epoch: [228][  217/  217]    Overall Loss 0.000902    Objective Loss 0.000902    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.285971    
2024-05-09 13:59:57,123 - 

2024-05-09 13:59:57,124 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:00:24,604 - Epoch: [229][  100/  217]    Overall Loss 0.000545    Objective Loss 0.000545                                        LR 0.000016    Time 0.274660    
2024-05-09 14:00:52,709 - Epoch: [229][  200/  217]    Overall Loss 0.000867    Objective Loss 0.000867                                        LR 0.000016    Time 0.277786    
2024-05-09 14:00:55,999 - Epoch: [229][  217/  217]    Overall Loss 0.000915    Objective Loss 0.000915    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.271176    
2024-05-09 14:00:56,292 - 

2024-05-09 14:00:56,294 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:01:25,813 - Epoch: [230][  100/  217]    Overall Loss 0.000911    Objective Loss 0.000911                                        LR 0.000016    Time 0.295036    
2024-05-09 14:01:52,446 - Epoch: [230][  200/  217]    Overall Loss 0.000947    Objective Loss 0.000947                                        LR 0.000016    Time 0.280618    
2024-05-09 14:01:56,280 - Epoch: [230][  217/  217]    Overall Loss 0.000878    Objective Loss 0.000878    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.276288    
2024-05-09 14:01:56,728 - --- validate (epoch=230)-----------
2024-05-09 14:01:56,729 - 1736 samples (32 per mini-batch)
2024-05-09 14:02:17,274 - Epoch: [230][   55/   55]    Loss 2.930009    Top1 60.426267    Top5 75.172811    
2024-05-09 14:02:17,605 - ==> Top1: 60.426    Top5: 75.173    Loss: 2.930

2024-05-09 14:02:17,610 - ==> Best [Top1: 60.541   Top5: 75.749   Sparsity:0.00   Params: 386672 on epoch: 210]
2024-05-09 14:02:17,610 - Saving checkpoint to: logs/2024.05.09-095602/checkpoint.pth.tar
2024-05-09 14:02:17,646 - 

2024-05-09 14:02:17,647 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:02:47,337 - Epoch: [231][  100/  217]    Overall Loss 0.001181    Objective Loss 0.001181                                        LR 0.000016    Time 0.296773    
2024-05-09 14:03:10,348 - Epoch: [231][  200/  217]    Overall Loss 0.000967    Objective Loss 0.000967                                        LR 0.000016    Time 0.263384    
2024-05-09 14:03:15,538 - Epoch: [231][  217/  217]    Overall Loss 0.000897    Objective Loss 0.000897    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.266644    
2024-05-09 14:03:15,940 - 

2024-05-09 14:03:15,942 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:03:49,205 - Epoch: [232][  100/  217]    Overall Loss 0.001131    Objective Loss 0.001131                                        LR 0.000016    Time 0.332481    
2024-05-09 14:04:16,480 - Epoch: [232][  200/  217]    Overall Loss 0.000703    Objective Loss 0.000703                                        LR 0.000016    Time 0.302555    
2024-05-09 14:04:21,906 - Epoch: [232][  217/  217]    Overall Loss 0.000859    Objective Loss 0.000859    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.303843    
2024-05-09 14:04:22,411 - 

2024-05-09 14:04:22,412 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:04:51,979 - Epoch: [233][  100/  217]    Overall Loss 0.001215    Objective Loss 0.001215                                        LR 0.000016    Time 0.295530    
2024-05-09 14:05:17,873 - Epoch: [233][  200/  217]    Overall Loss 0.000958    Objective Loss 0.000958                                        LR 0.000016    Time 0.277169    
2024-05-09 14:05:23,456 - Epoch: [233][  217/  217]    Overall Loss 0.000888    Objective Loss 0.000888    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.281161    
2024-05-09 14:05:23,940 - 

2024-05-09 14:05:23,941 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:05:53,018 - Epoch: [234][  100/  217]    Overall Loss 0.000292    Objective Loss 0.000292                                        LR 0.000016    Time 0.290611    
2024-05-09 14:06:17,789 - Epoch: [234][  200/  217]    Overall Loss 0.000830    Objective Loss 0.000830                                        LR 0.000016    Time 0.269096    
2024-05-09 14:06:22,245 - Epoch: [234][  217/  217]    Overall Loss 0.000893    Objective Loss 0.000893    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.268538    
2024-05-09 14:06:22,790 - 

2024-05-09 14:06:22,791 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:06:53,821 - Epoch: [235][  100/  217]    Overall Loss 0.000897    Objective Loss 0.000897                                        LR 0.000016    Time 0.310169    
2024-05-09 14:07:18,634 - Epoch: [235][  200/  217]    Overall Loss 0.000817    Objective Loss 0.000817                                        LR 0.000016    Time 0.279086    
2024-05-09 14:07:23,171 - Epoch: [235][  217/  217]    Overall Loss 0.000869    Objective Loss 0.000869    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.278115    
2024-05-09 14:07:23,525 - 

2024-05-09 14:07:23,526 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:07:54,852 - Epoch: [236][  100/  217]    Overall Loss 0.000374    Objective Loss 0.000374                                        LR 0.000016    Time 0.313129    
2024-05-09 14:08:20,928 - Epoch: [236][  200/  217]    Overall Loss 0.001009    Objective Loss 0.001009                                        LR 0.000016    Time 0.286877    
2024-05-09 14:08:25,244 - Epoch: [236][  217/  217]    Overall Loss 0.000935    Objective Loss 0.000935    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.284280    
2024-05-09 14:08:25,738 - 

2024-05-09 14:08:25,739 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:08:55,431 - Epoch: [237][  100/  217]    Overall Loss 0.001171    Objective Loss 0.001171                                        LR 0.000016    Time 0.296792    
2024-05-09 14:09:17,738 - Epoch: [237][  200/  217]    Overall Loss 0.000974    Objective Loss 0.000974                                        LR 0.000016    Time 0.259870    
2024-05-09 14:09:23,671 - Epoch: [237][  217/  217]    Overall Loss 0.000902    Objective Loss 0.000902    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.266845    
2024-05-09 14:09:24,046 - 

2024-05-09 14:09:24,048 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:09:56,509 - Epoch: [238][  100/  217]    Overall Loss 0.001347    Objective Loss 0.001347                                        LR 0.000016    Time 0.324469    
2024-05-09 14:10:20,796 - Epoch: [238][  200/  217]    Overall Loss 0.000911    Objective Loss 0.000911                                        LR 0.000016    Time 0.283618    
2024-05-09 14:10:26,052 - Epoch: [238][  217/  217]    Overall Loss 0.000844    Objective Loss 0.000844    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.285609    
2024-05-09 14:10:26,339 - 

2024-05-09 14:10:26,340 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:10:54,753 - Epoch: [239][  100/  217]    Overall Loss 0.000983    Objective Loss 0.000983                                        LR 0.000016    Time 0.283980    
2024-05-09 14:11:23,654 - Epoch: [239][  200/  217]    Overall Loss 0.000839    Objective Loss 0.000839                                        LR 0.000016    Time 0.286427    
2024-05-09 14:11:26,328 - Epoch: [239][  217/  217]    Overall Loss 0.000880    Objective Loss 0.000880    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.276301    
2024-05-09 14:11:26,766 - 

2024-05-09 14:11:26,767 - Initiating quantization aware training (QAT)...
2024-05-09 14:11:26,833 - 

2024-05-09 14:11:26,834 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:11:58,233 - Epoch: [240][  100/  217]    Overall Loss 1.615662    Objective Loss 1.615662                                        LR 0.000016    Time 0.313862    
2024-05-09 14:12:28,470 - Epoch: [240][  200/  217]    Overall Loss 1.251226    Objective Loss 1.251226                                        LR 0.000016    Time 0.308057    
2024-05-09 14:12:32,309 - Epoch: [240][  217/  217]    Overall Loss 1.218344    Objective Loss 1.218344    Top1 83.606557    Top5 95.081967    LR 0.000016    Time 0.301601    
2024-05-09 14:12:32,772 - --- validate (epoch=240)-----------
2024-05-09 14:12:32,774 - 1736 samples (32 per mini-batch)
2024-05-09 14:12:51,473 - Epoch: [240][   55/   55]    Loss 2.097723    Top1 53.917051    Top5 70.910138    
2024-05-09 14:12:51,760 - ==> Top1: 53.917    Top5: 70.910    Loss: 2.098

2024-05-09 14:12:51,765 - ==> Best [Top1: 53.917   Top5: 70.910   Sparsity:0.00   Params: 386672 on epoch: 240]
2024-05-09 14:12:51,765 - Saving checkpoint to: logs/2024.05.09-095602/qat_checkpoint.pth.tar
2024-05-09 14:12:51,797 - 

2024-05-09 14:12:51,797 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:13:21,457 - Epoch: [241][  100/  217]    Overall Loss 0.670621    Objective Loss 0.670621                                        LR 0.000016    Time 0.296474    
2024-05-09 14:13:49,184 - Epoch: [241][  200/  217]    Overall Loss 0.643885    Objective Loss 0.643885                                        LR 0.000016    Time 0.286810    
2024-05-09 14:13:53,283 - Epoch: [241][  217/  217]    Overall Loss 0.635135    Objective Loss 0.635135    Top1 91.803279    Top5 100.000000    LR 0.000016    Time 0.283218    
2024-05-09 14:13:53,694 - 

2024-05-09 14:13:53,695 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:14:27,492 - Epoch: [242][  100/  217]    Overall Loss 0.530128    Objective Loss 0.530128                                        LR 0.000016    Time 0.337850    
2024-05-09 14:14:53,581 - Epoch: [242][  200/  217]    Overall Loss 0.511833    Objective Loss 0.511833                                        LR 0.000016    Time 0.299312    
2024-05-09 14:14:56,919 - Epoch: [242][  217/  217]    Overall Loss 0.509562    Objective Loss 0.509562    Top1 86.885246    Top5 98.360656    LR 0.000016    Time 0.291231    
2024-05-09 14:14:57,179 - 

2024-05-09 14:14:57,180 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:15:26,815 - Epoch: [243][  100/  217]    Overall Loss 0.468561    Objective Loss 0.468561                                        LR 0.000016    Time 0.296213    
2024-05-09 14:15:55,115 - Epoch: [243][  200/  217]    Overall Loss 0.450643    Objective Loss 0.450643                                        LR 0.000016    Time 0.289547    
2024-05-09 14:15:58,774 - Epoch: [243][  217/  217]    Overall Loss 0.449846    Objective Loss 0.449846    Top1 91.803279    Top5 100.000000    LR 0.000016    Time 0.283711    
2024-05-09 14:15:59,149 - 

2024-05-09 14:15:59,150 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:16:30,121 - Epoch: [244][  100/  217]    Overall Loss 0.381067    Objective Loss 0.381067                                        LR 0.000016    Time 0.309568    
2024-05-09 14:16:58,997 - Epoch: [244][  200/  217]    Overall Loss 0.391461    Objective Loss 0.391461                                        LR 0.000016    Time 0.299103    
2024-05-09 14:17:04,465 - Epoch: [244][  217/  217]    Overall Loss 0.391341    Objective Loss 0.391341    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.300860    
2024-05-09 14:17:04,783 - 

2024-05-09 14:17:04,784 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:17:38,148 - Epoch: [245][  100/  217]    Overall Loss 0.343677    Objective Loss 0.343677                                        LR 0.000016    Time 0.333498    
2024-05-09 14:18:02,181 - Epoch: [245][  200/  217]    Overall Loss 0.346477    Objective Loss 0.346477                                        LR 0.000016    Time 0.286859    
2024-05-09 14:18:06,230 - Epoch: [245][  217/  217]    Overall Loss 0.343972    Objective Loss 0.343972    Top1 98.360656    Top5 98.360656    LR 0.000016    Time 0.283036    
2024-05-09 14:18:06,549 - 

2024-05-09 14:18:06,550 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:18:36,427 - Epoch: [246][  100/  217]    Overall Loss 0.319602    Objective Loss 0.319602                                        LR 0.000016    Time 0.298632    
2024-05-09 14:19:06,008 - Epoch: [246][  200/  217]    Overall Loss 0.323233    Objective Loss 0.323233                                        LR 0.000016    Time 0.297159    
2024-05-09 14:19:09,970 - Epoch: [246][  217/  217]    Overall Loss 0.320996    Objective Loss 0.320996    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.292125    
2024-05-09 14:19:10,257 - 

2024-05-09 14:19:10,257 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:19:38,636 - Epoch: [247][  100/  217]    Overall Loss 0.286358    Objective Loss 0.286358                                        LR 0.000016    Time 0.283657    
2024-05-09 14:20:04,532 - Epoch: [247][  200/  217]    Overall Loss 0.294505    Objective Loss 0.294505                                        LR 0.000016    Time 0.271248    
2024-05-09 14:20:09,929 - Epoch: [247][  217/  217]    Overall Loss 0.295816    Objective Loss 0.295816    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.274858    
2024-05-09 14:20:10,194 - 

2024-05-09 14:20:10,196 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:20:37,409 - Epoch: [248][  100/  217]    Overall Loss 0.282122    Objective Loss 0.282122                                        LR 0.000016    Time 0.272002    
2024-05-09 14:21:02,970 - Epoch: [248][  200/  217]    Overall Loss 0.276058    Objective Loss 0.276058                                        LR 0.000016    Time 0.263746    
2024-05-09 14:21:07,436 - Epoch: [248][  217/  217]    Overall Loss 0.275864    Objective Loss 0.275864    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.263652    
2024-05-09 14:21:07,703 - 

2024-05-09 14:21:07,703 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:21:40,219 - Epoch: [249][  100/  217]    Overall Loss 0.259380    Objective Loss 0.259380                                        LR 0.000016    Time 0.325041    
2024-05-09 14:22:06,583 - Epoch: [249][  200/  217]    Overall Loss 0.261105    Objective Loss 0.261105                                        LR 0.000016    Time 0.294286    
2024-05-09 14:22:11,693 - Epoch: [249][  217/  217]    Overall Loss 0.261218    Objective Loss 0.261218    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.294768    
2024-05-09 14:22:12,121 - 

2024-05-09 14:22:12,122 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:22:45,802 - Epoch: [250][  100/  217]    Overall Loss 0.239937    Objective Loss 0.239937                                        LR 0.000016    Time 0.336652    
2024-05-09 14:23:11,971 - Epoch: [250][  200/  217]    Overall Loss 0.247771    Objective Loss 0.247771                                        LR 0.000016    Time 0.299111    
2024-05-09 14:23:16,868 - Epoch: [250][  217/  217]    Overall Loss 0.247793    Objective Loss 0.247793    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.298236    
2024-05-09 14:23:17,171 - --- validate (epoch=250)-----------
2024-05-09 14:23:17,171 - 1736 samples (32 per mini-batch)
2024-05-09 14:23:35,701 - Epoch: [250][   55/   55]    Loss 2.025464    Top1 58.352535    Top5 73.732719    
2024-05-09 14:23:36,058 - ==> Top1: 58.353    Top5: 73.733    Loss: 2.025

2024-05-09 14:23:36,064 - ==> Best [Top1: 58.353   Top5: 73.733   Sparsity:0.00   Params: 386672 on epoch: 250]
2024-05-09 14:23:36,065 - Saving checkpoint to: logs/2024.05.09-095602/qat_checkpoint.pth.tar
2024-05-09 14:23:36,110 - 

2024-05-09 14:23:36,111 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:24:06,071 - Epoch: [251][  100/  217]    Overall Loss 0.228490    Objective Loss 0.228490                                        LR 0.000016    Time 0.299469    
2024-05-09 14:24:29,798 - Epoch: [251][  200/  217]    Overall Loss 0.235046    Objective Loss 0.235046                                        LR 0.000016    Time 0.268310    
2024-05-09 14:24:34,600 - Epoch: [251][  217/  217]    Overall Loss 0.233344    Objective Loss 0.233344    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.269407    
2024-05-09 14:24:35,032 - 

2024-05-09 14:24:35,034 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:25:05,419 - Epoch: [252][  100/  217]    Overall Loss 0.223843    Objective Loss 0.223843                                        LR 0.000016    Time 0.303707    
2024-05-09 14:25:30,797 - Epoch: [252][  200/  217]    Overall Loss 0.228029    Objective Loss 0.228029                                        LR 0.000016    Time 0.278686    
2024-05-09 14:25:34,818 - Epoch: [252][  217/  217]    Overall Loss 0.226154    Objective Loss 0.226154    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.275375    
2024-05-09 14:25:35,262 - 

2024-05-09 14:25:35,263 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:26:05,243 - Epoch: [253][  100/  217]    Overall Loss 0.182600    Objective Loss 0.182600                                        LR 0.000016    Time 0.299662    
2024-05-09 14:26:33,448 - Epoch: [253][  200/  217]    Overall Loss 0.207776    Objective Loss 0.207776                                        LR 0.000016    Time 0.290793    
2024-05-09 14:26:36,254 - Epoch: [253][  217/  217]    Overall Loss 0.211326    Objective Loss 0.211326    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.280927    
2024-05-09 14:26:36,807 - 

2024-05-09 14:26:36,808 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:27:06,213 - Epoch: [254][  100/  217]    Overall Loss 0.199843    Objective Loss 0.199843                                        LR 0.000016    Time 0.293931    
2024-05-09 14:27:32,189 - Epoch: [254][  200/  217]    Overall Loss 0.200573    Objective Loss 0.200573                                        LR 0.000016    Time 0.276768    
2024-05-09 14:27:37,122 - Epoch: [254][  217/  217]    Overall Loss 0.200350    Objective Loss 0.200350    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.277808    
2024-05-09 14:27:37,460 - 

2024-05-09 14:27:37,461 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:28:07,916 - Epoch: [255][  100/  217]    Overall Loss 0.190257    Objective Loss 0.190257                                        LR 0.000016    Time 0.304421    
2024-05-09 14:28:34,276 - Epoch: [255][  200/  217]    Overall Loss 0.197578    Objective Loss 0.197578                                        LR 0.000016    Time 0.283949    
2024-05-09 14:28:38,855 - Epoch: [255][  217/  217]    Overall Loss 0.197673    Objective Loss 0.197673    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.282793    
2024-05-09 14:28:39,101 - 

2024-05-09 14:28:39,101 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:29:09,844 - Epoch: [256][  100/  217]    Overall Loss 0.174183    Objective Loss 0.174183                                        LR 0.000016    Time 0.307310    
2024-05-09 14:29:34,779 - Epoch: [256][  200/  217]    Overall Loss 0.185774    Objective Loss 0.185774                                        LR 0.000016    Time 0.278270    
2024-05-09 14:29:39,458 - Epoch: [256][  217/  217]    Overall Loss 0.186142    Objective Loss 0.186142    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.278021    
2024-05-09 14:29:39,792 - 

2024-05-09 14:29:39,793 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:30:10,366 - Epoch: [257][  100/  217]    Overall Loss 0.166762    Objective Loss 0.166762                                        LR 0.000016    Time 0.305623    
2024-05-09 14:30:35,317 - Epoch: [257][  200/  217]    Overall Loss 0.186196    Objective Loss 0.186196                                        LR 0.000016    Time 0.277507    
2024-05-09 14:30:40,210 - Epoch: [257][  217/  217]    Overall Loss 0.186163    Objective Loss 0.186163    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.278304    
2024-05-09 14:30:40,558 - 

2024-05-09 14:30:40,559 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:31:11,410 - Epoch: [258][  100/  217]    Overall Loss 0.159415    Objective Loss 0.159415                                        LR 0.000016    Time 0.308395    
2024-05-09 14:31:37,826 - Epoch: [258][  200/  217]    Overall Loss 0.174012    Objective Loss 0.174012                                        LR 0.000016    Time 0.286219    
2024-05-09 14:31:41,350 - Epoch: [258][  217/  217]    Overall Loss 0.175396    Objective Loss 0.175396    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.280018    
2024-05-09 14:31:41,635 - 

2024-05-09 14:31:41,636 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:32:12,265 - Epoch: [259][  100/  217]    Overall Loss 0.167292    Objective Loss 0.167292                                        LR 0.000016    Time 0.306143    
2024-05-09 14:32:42,678 - Epoch: [259][  200/  217]    Overall Loss 0.170205    Objective Loss 0.170205                                        LR 0.000016    Time 0.305050    
2024-05-09 14:32:45,807 - Epoch: [259][  217/  217]    Overall Loss 0.173108    Objective Loss 0.173108    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.295561    
2024-05-09 14:32:46,348 - 

2024-05-09 14:32:46,348 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:33:16,436 - Epoch: [260][  100/  217]    Overall Loss 0.162108    Objective Loss 0.162108                                        LR 0.000016    Time 0.300746    
2024-05-09 14:33:41,918 - Epoch: [260][  200/  217]    Overall Loss 0.167313    Objective Loss 0.167313                                        LR 0.000016    Time 0.277724    
2024-05-09 14:33:44,931 - Epoch: [260][  217/  217]    Overall Loss 0.170192    Objective Loss 0.170192    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.269838    
2024-05-09 14:33:45,362 - --- validate (epoch=260)-----------
2024-05-09 14:33:45,363 - 1736 samples (32 per mini-batch)
2024-05-09 14:34:04,713 - Epoch: [260][   55/   55]    Loss 2.171606    Top1 55.529954    Top5 72.235023    
2024-05-09 14:34:05,162 - ==> Top1: 55.530    Top5: 72.235    Loss: 2.172

2024-05-09 14:34:05,165 - ==> Best [Top1: 58.353   Top5: 73.733   Sparsity:0.00   Params: 386672 on epoch: 250]
2024-05-09 14:34:05,166 - Saving checkpoint to: logs/2024.05.09-095602/qat_checkpoint.pth.tar
2024-05-09 14:34:05,203 - 

2024-05-09 14:34:05,203 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:34:34,616 - Epoch: [261][  100/  217]    Overall Loss 0.153064    Objective Loss 0.153064                                        LR 0.000016    Time 0.294012    
2024-05-09 14:35:02,678 - Epoch: [261][  200/  217]    Overall Loss 0.160077    Objective Loss 0.160077                                        LR 0.000016    Time 0.287264    
2024-05-09 14:35:06,318 - Epoch: [261][  217/  217]    Overall Loss 0.160568    Objective Loss 0.160568    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.281519    
2024-05-09 14:35:06,577 - 

2024-05-09 14:35:06,577 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:35:37,026 - Epoch: [262][  100/  217]    Overall Loss 0.154907    Objective Loss 0.154907                                        LR 0.000016    Time 0.304364    
2024-05-09 14:36:05,791 - Epoch: [262][  200/  217]    Overall Loss 0.159404    Objective Loss 0.159404                                        LR 0.000016    Time 0.295949    
2024-05-09 14:36:10,467 - Epoch: [262][  217/  217]    Overall Loss 0.157785    Objective Loss 0.157785    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.294303    
2024-05-09 14:36:10,715 - 

2024-05-09 14:36:10,715 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:36:42,099 - Epoch: [263][  100/  217]    Overall Loss 0.136185    Objective Loss 0.136185                                        LR 0.000016    Time 0.313709    
2024-05-09 14:37:09,821 - Epoch: [263][  200/  217]    Overall Loss 0.154726    Objective Loss 0.154726                                        LR 0.000016    Time 0.295402    
2024-05-09 14:37:13,977 - Epoch: [263][  217/  217]    Overall Loss 0.154984    Objective Loss 0.154984    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.291400    
2024-05-09 14:37:14,321 - 

2024-05-09 14:37:14,322 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:37:46,017 - Epoch: [264][  100/  217]    Overall Loss 0.144123    Objective Loss 0.144123                                        LR 0.000016    Time 0.316809    
2024-05-09 14:38:12,617 - Epoch: [264][  200/  217]    Overall Loss 0.158447    Objective Loss 0.158447                                        LR 0.000016    Time 0.291341    
2024-05-09 14:38:16,034 - Epoch: [264][  217/  217]    Overall Loss 0.159824    Objective Loss 0.159824    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.284255    
2024-05-09 14:38:16,386 - 

2024-05-09 14:38:16,387 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:38:43,756 - Epoch: [265][  100/  217]    Overall Loss 0.138104    Objective Loss 0.138104                                        LR 0.000016    Time 0.273572    
2024-05-09 14:39:10,752 - Epoch: [265][  200/  217]    Overall Loss 0.141242    Objective Loss 0.141242                                        LR 0.000016    Time 0.271706    
2024-05-09 14:39:15,059 - Epoch: [265][  217/  217]    Overall Loss 0.142522    Objective Loss 0.142522    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.270258    
2024-05-09 14:39:15,332 - 

2024-05-09 14:39:15,333 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:39:45,128 - Epoch: [266][  100/  217]    Overall Loss 0.138916    Objective Loss 0.138916                                        LR 0.000016    Time 0.297823    
2024-05-09 14:40:10,981 - Epoch: [266][  200/  217]    Overall Loss 0.143882    Objective Loss 0.143882                                        LR 0.000016    Time 0.278120    
2024-05-09 14:40:14,610 - Epoch: [266][  217/  217]    Overall Loss 0.145062    Objective Loss 0.145062    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.273047    
2024-05-09 14:40:15,026 - 

2024-05-09 14:40:15,027 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:40:45,143 - Epoch: [267][  100/  217]    Overall Loss 0.146367    Objective Loss 0.146367                                        LR 0.000016    Time 0.301029    
2024-05-09 14:41:12,513 - Epoch: [267][  200/  217]    Overall Loss 0.152120    Objective Loss 0.152120                                        LR 0.000016    Time 0.287299    
2024-05-09 14:41:17,523 - Epoch: [267][  217/  217]    Overall Loss 0.153182    Objective Loss 0.153182    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.287869    
2024-05-09 14:41:17,790 - 

2024-05-09 14:41:17,792 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:41:47,079 - Epoch: [268][  100/  217]    Overall Loss 0.131081    Objective Loss 0.131081                                        LR 0.000016    Time 0.292721    
2024-05-09 14:42:13,659 - Epoch: [268][  200/  217]    Overall Loss 0.139385    Objective Loss 0.139385                                        LR 0.000016    Time 0.279192    
2024-05-09 14:42:17,364 - Epoch: [268][  217/  217]    Overall Loss 0.139927    Objective Loss 0.139927    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.274383    
2024-05-09 14:42:17,669 - 

2024-05-09 14:42:17,669 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:42:47,362 - Epoch: [269][  100/  217]    Overall Loss 0.125358    Objective Loss 0.125358                                        LR 0.000016    Time 0.296814    
2024-05-09 14:43:13,758 - Epoch: [269][  200/  217]    Overall Loss 0.138569    Objective Loss 0.138569                                        LR 0.000016    Time 0.280321    
2024-05-09 14:43:18,448 - Epoch: [269][  217/  217]    Overall Loss 0.139654    Objective Loss 0.139654    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.279959    
2024-05-09 14:43:18,898 - 

2024-05-09 14:43:18,898 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:43:48,487 - Epoch: [270][  100/  217]    Overall Loss 0.122141    Objective Loss 0.122141                                        LR 0.000016    Time 0.295766    
2024-05-09 14:44:10,086 - Epoch: [270][  200/  217]    Overall Loss 0.137281    Objective Loss 0.137281                                        LR 0.000016    Time 0.255823    
2024-05-09 14:44:13,731 - Epoch: [270][  217/  217]    Overall Loss 0.136970    Objective Loss 0.136970    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.252569    
2024-05-09 14:44:14,053 - --- validate (epoch=270)-----------
2024-05-09 14:44:14,054 - 1736 samples (32 per mini-batch)
2024-05-09 14:44:30,288 - Epoch: [270][   55/   55]    Loss 2.063698    Top1 56.509217    Top5 73.847926    
2024-05-09 14:44:30,646 - ==> Top1: 56.509    Top5: 73.848    Loss: 2.064

2024-05-09 14:44:30,648 - ==> Best [Top1: 58.353   Top5: 73.733   Sparsity:0.00   Params: 386672 on epoch: 250]
2024-05-09 14:44:30,649 - Saving checkpoint to: logs/2024.05.09-095602/qat_checkpoint.pth.tar
2024-05-09 14:44:30,687 - 

2024-05-09 14:44:30,688 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:44:58,007 - Epoch: [271][  100/  217]    Overall Loss 0.131145    Objective Loss 0.131145                                        LR 0.000016    Time 0.273062    
2024-05-09 14:45:26,738 - Epoch: [271][  200/  217]    Overall Loss 0.131296    Objective Loss 0.131296                                        LR 0.000016    Time 0.280127    
2024-05-09 14:45:29,954 - Epoch: [271][  217/  217]    Overall Loss 0.132104    Objective Loss 0.132104    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.272990    
2024-05-09 14:45:30,322 - 

2024-05-09 14:45:30,323 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:45:59,432 - Epoch: [272][  100/  217]    Overall Loss 0.117012    Objective Loss 0.117012                                        LR 0.000016    Time 0.290975    
2024-05-09 14:46:25,823 - Epoch: [272][  200/  217]    Overall Loss 0.129582    Objective Loss 0.129582                                        LR 0.000016    Time 0.277381    
2024-05-09 14:46:30,910 - Epoch: [272][  217/  217]    Overall Loss 0.130039    Objective Loss 0.130039    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.279067    
2024-05-09 14:46:31,496 - 

2024-05-09 14:46:31,497 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:47:00,923 - Epoch: [273][  100/  217]    Overall Loss 0.115099    Objective Loss 0.115099                                        LR 0.000016    Time 0.294093    
2024-05-09 14:47:27,438 - Epoch: [273][  200/  217]    Overall Loss 0.117733    Objective Loss 0.117733                                        LR 0.000016    Time 0.279565    
2024-05-09 14:47:30,585 - Epoch: [273][  217/  217]    Overall Loss 0.118363    Objective Loss 0.118363    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.272136    
2024-05-09 14:47:31,586 - 

2024-05-09 14:47:31,586 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:47:59,405 - Epoch: [274][  100/  217]    Overall Loss 0.119132    Objective Loss 0.119132                                        LR 0.000016    Time 0.278052    
2024-05-09 14:48:27,009 - Epoch: [274][  200/  217]    Overall Loss 0.118155    Objective Loss 0.118155                                        LR 0.000016    Time 0.276993    
2024-05-09 14:48:30,174 - Epoch: [274][  217/  217]    Overall Loss 0.119234    Objective Loss 0.119234    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.269867    
2024-05-09 14:48:30,548 - 

2024-05-09 14:48:30,549 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:48:58,736 - Epoch: [275][  100/  217]    Overall Loss 0.108936    Objective Loss 0.108936                                        LR 0.000016    Time 0.281731    
2024-05-09 14:49:25,891 - Epoch: [275][  200/  217]    Overall Loss 0.117690    Objective Loss 0.117690                                        LR 0.000016    Time 0.276584    
2024-05-09 14:49:29,333 - Epoch: [275][  217/  217]    Overall Loss 0.123293    Objective Loss 0.123293    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.270766    
2024-05-09 14:49:29,680 - 

2024-05-09 14:49:29,681 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:49:58,310 - Epoch: [276][  100/  217]    Overall Loss 0.111933    Objective Loss 0.111933                                        LR 0.000016    Time 0.286161    
2024-05-09 14:50:27,745 - Epoch: [276][  200/  217]    Overall Loss 0.117377    Objective Loss 0.117377                                        LR 0.000016    Time 0.290198    
2024-05-09 14:50:33,020 - Epoch: [276][  217/  217]    Overall Loss 0.117918    Objective Loss 0.117918    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.291761    
2024-05-09 14:50:33,350 - 

2024-05-09 14:50:33,350 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:51:02,871 - Epoch: [277][  100/  217]    Overall Loss 0.107432    Objective Loss 0.107432                                        LR 0.000016    Time 0.295072    
2024-05-09 14:51:30,515 - Epoch: [277][  200/  217]    Overall Loss 0.120312    Objective Loss 0.120312                                        LR 0.000016    Time 0.285698    
2024-05-09 14:51:34,047 - Epoch: [277][  217/  217]    Overall Loss 0.120964    Objective Loss 0.120964    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.279580    
2024-05-09 14:51:34,749 - 

2024-05-09 14:51:34,750 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:52:04,377 - Epoch: [278][  100/  217]    Overall Loss 0.105159    Objective Loss 0.105159                                        LR 0.000016    Time 0.296125    
2024-05-09 14:52:31,549 - Epoch: [278][  200/  217]    Overall Loss 0.114422    Objective Loss 0.114422                                        LR 0.000016    Time 0.283854    
2024-05-09 14:52:34,878 - Epoch: [278][  217/  217]    Overall Loss 0.115391    Objective Loss 0.115391    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.276951    
2024-05-09 14:52:35,191 - 

2024-05-09 14:52:35,192 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:53:06,587 - Epoch: [279][  100/  217]    Overall Loss 0.105843    Objective Loss 0.105843                                        LR 0.000016    Time 0.313797    
2024-05-09 14:53:31,648 - Epoch: [279][  200/  217]    Overall Loss 0.115625    Objective Loss 0.115625                                        LR 0.000016    Time 0.282140    
2024-05-09 14:53:37,174 - Epoch: [279][  217/  217]    Overall Loss 0.117001    Objective Loss 0.117001    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.285489    
2024-05-09 14:53:37,434 - 

2024-05-09 14:53:37,435 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:54:08,229 - Epoch: [280][  100/  217]    Overall Loss 0.116432    Objective Loss 0.116432                                        LR 0.000016    Time 0.307759    
2024-05-09 14:54:37,068 - Epoch: [280][  200/  217]    Overall Loss 0.113830    Objective Loss 0.113830                                        LR 0.000016    Time 0.297994    
2024-05-09 14:54:42,296 - Epoch: [280][  217/  217]    Overall Loss 0.114187    Objective Loss 0.114187    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.298729    
2024-05-09 14:54:42,938 - --- validate (epoch=280)-----------
2024-05-09 14:54:42,939 - 1736 samples (32 per mini-batch)
2024-05-09 14:55:00,338 - Epoch: [280][   55/   55]    Loss 2.059342    Top1 57.834101    Top5 74.596774    
2024-05-09 14:55:00,770 - ==> Top1: 57.834    Top5: 74.597    Loss: 2.059

2024-05-09 14:55:00,778 - ==> Best [Top1: 58.353   Top5: 73.733   Sparsity:0.00   Params: 386672 on epoch: 250]
2024-05-09 14:55:00,778 - Saving checkpoint to: logs/2024.05.09-095602/qat_checkpoint.pth.tar
2024-05-09 14:55:00,827 - 

2024-05-09 14:55:00,827 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:55:31,621 - Epoch: [281][  100/  217]    Overall Loss 0.106049    Objective Loss 0.106049                                        LR 0.000016    Time 0.307784    
2024-05-09 14:55:56,016 - Epoch: [281][  200/  217]    Overall Loss 0.111315    Objective Loss 0.111315                                        LR 0.000016    Time 0.275805    
2024-05-09 14:55:59,505 - Epoch: [281][  217/  217]    Overall Loss 0.111164    Objective Loss 0.111164    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.270257    
2024-05-09 14:55:59,840 - 

2024-05-09 14:55:59,841 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:56:29,678 - Epoch: [282][  100/  217]    Overall Loss 0.102703    Objective Loss 0.102703                                        LR 0.000016    Time 0.298240    
2024-05-09 14:56:57,741 - Epoch: [282][  200/  217]    Overall Loss 0.104175    Objective Loss 0.104175                                        LR 0.000016    Time 0.289371    
2024-05-09 14:57:02,564 - Epoch: [282][  217/  217]    Overall Loss 0.105476    Objective Loss 0.105476    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.288917    
2024-05-09 14:57:02,852 - 

2024-05-09 14:57:02,853 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:57:32,109 - Epoch: [283][  100/  217]    Overall Loss 0.096858    Objective Loss 0.096858                                        LR 0.000016    Time 0.292443    
2024-05-09 14:57:57,721 - Epoch: [283][  200/  217]    Overall Loss 0.110150    Objective Loss 0.110150                                        LR 0.000016    Time 0.274215    
2024-05-09 14:58:01,603 - Epoch: [283][  217/  217]    Overall Loss 0.109512    Objective Loss 0.109512    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.270604    
2024-05-09 14:58:01,989 - 

2024-05-09 14:58:01,990 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:58:31,294 - Epoch: [284][  100/  217]    Overall Loss 0.101026    Objective Loss 0.101026                                        LR 0.000016    Time 0.292917    
2024-05-09 14:58:58,192 - Epoch: [284][  200/  217]    Overall Loss 0.107279    Objective Loss 0.107279                                        LR 0.000016    Time 0.280896    
2024-05-09 14:59:02,925 - Epoch: [284][  217/  217]    Overall Loss 0.109298    Objective Loss 0.109298    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.280689    
2024-05-09 14:59:03,320 - 

2024-05-09 14:59:03,320 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:59:34,726 - Epoch: [285][  100/  217]    Overall Loss 0.087974    Objective Loss 0.087974                                        LR 0.000016    Time 0.313923    
2024-05-09 15:00:04,236 - Epoch: [285][  200/  217]    Overall Loss 0.095765    Objective Loss 0.095765                                        LR 0.000016    Time 0.304444    
2024-05-09 15:00:08,670 - Epoch: [285][  217/  217]    Overall Loss 0.096401    Objective Loss 0.096401    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.301012    
2024-05-09 15:00:09,112 - 

2024-05-09 15:00:09,114 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:00:41,186 - Epoch: [286][  100/  217]    Overall Loss 0.080016    Objective Loss 0.080016                                        LR 0.000016    Time 0.320555    
2024-05-09 15:01:06,769 - Epoch: [286][  200/  217]    Overall Loss 0.095130    Objective Loss 0.095130                                        LR 0.000016    Time 0.288134    
2024-05-09 15:01:10,718 - Epoch: [286][  217/  217]    Overall Loss 0.097152    Objective Loss 0.097152    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.283745    
2024-05-09 15:01:11,222 - 

2024-05-09 15:01:11,224 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:01:37,730 - Epoch: [287][  100/  217]    Overall Loss 0.101437    Objective Loss 0.101437                                        LR 0.000016    Time 0.264919    
2024-05-09 15:02:07,952 - Epoch: [287][  200/  217]    Overall Loss 0.099389    Objective Loss 0.099389                                        LR 0.000016    Time 0.283508    
2024-05-09 15:02:13,049 - Epoch: [287][  217/  217]    Overall Loss 0.099645    Objective Loss 0.099645    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.284774    
2024-05-09 15:02:13,439 - 

2024-05-09 15:02:13,440 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:02:40,469 - Epoch: [288][  100/  217]    Overall Loss 0.084943    Objective Loss 0.084943                                        LR 0.000016    Time 0.270146    
2024-05-09 15:03:08,717 - Epoch: [288][  200/  217]    Overall Loss 0.094664    Objective Loss 0.094664                                        LR 0.000016    Time 0.276252    
2024-05-09 15:03:13,690 - Epoch: [288][  217/  217]    Overall Loss 0.098279    Objective Loss 0.098279    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.277515    
2024-05-09 15:03:14,158 - 

2024-05-09 15:03:14,159 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:03:44,172 - Epoch: [289][  100/  217]    Overall Loss 0.094264    Objective Loss 0.094264                                        LR 0.000016    Time 0.300012    
2024-05-09 15:04:09,473 - Epoch: [289][  200/  217]    Overall Loss 0.098268    Objective Loss 0.098268                                        LR 0.000016    Time 0.276452    
2024-05-09 15:04:14,876 - Epoch: [289][  217/  217]    Overall Loss 0.099096    Objective Loss 0.099096    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.279677    
2024-05-09 15:04:15,761 - 

2024-05-09 15:04:15,762 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:04:42,880 - Epoch: [290][  100/  217]    Overall Loss 0.096911    Objective Loss 0.096911                                        LR 0.000016    Time 0.271027    
2024-05-09 15:05:08,133 - Epoch: [290][  200/  217]    Overall Loss 0.097540    Objective Loss 0.097540                                        LR 0.000016    Time 0.261720    
2024-05-09 15:05:10,912 - Epoch: [290][  217/  217]    Overall Loss 0.098335    Objective Loss 0.098335    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.254007    
2024-05-09 15:05:11,821 - --- validate (epoch=290)-----------
2024-05-09 15:05:11,822 - 1736 samples (32 per mini-batch)
2024-05-09 15:05:28,277 - Epoch: [290][   55/   55]    Loss 2.075041    Top1 58.006912    Top5 74.135945    
2024-05-09 15:05:28,623 - ==> Top1: 58.007    Top5: 74.136    Loss: 2.075

2024-05-09 15:05:28,631 - ==> Best [Top1: 58.353   Top5: 73.733   Sparsity:0.00   Params: 386672 on epoch: 250]
2024-05-09 15:05:28,631 - Saving checkpoint to: logs/2024.05.09-095602/qat_checkpoint.pth.tar
2024-05-09 15:05:28,672 - 

2024-05-09 15:05:28,673 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:05:59,798 - Epoch: [291][  100/  217]    Overall Loss 0.095505    Objective Loss 0.095505                                        LR 0.000016    Time 0.311106    
2024-05-09 15:06:26,315 - Epoch: [291][  200/  217]    Overall Loss 0.095387    Objective Loss 0.095387                                        LR 0.000016    Time 0.288047    
2024-05-09 15:06:29,174 - Epoch: [291][  217/  217]    Overall Loss 0.096624    Objective Loss 0.096624    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.278644    
2024-05-09 15:06:29,437 - 

2024-05-09 15:06:29,437 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:06:57,848 - Epoch: [292][  100/  217]    Overall Loss 0.097318    Objective Loss 0.097318                                        LR 0.000016    Time 0.283964    
2024-05-09 15:07:20,483 - Epoch: [292][  200/  217]    Overall Loss 0.102486    Objective Loss 0.102486                                        LR 0.000016    Time 0.255095    
2024-05-09 15:07:25,191 - Epoch: [292][  217/  217]    Overall Loss 0.103205    Objective Loss 0.103205    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.256791    
2024-05-09 15:07:25,578 - 

2024-05-09 15:07:25,579 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:07:54,384 - Epoch: [293][  100/  217]    Overall Loss 0.096766    Objective Loss 0.096766                                        LR 0.000016    Time 0.287922    
2024-05-09 15:08:16,256 - Epoch: [293][  200/  217]    Overall Loss 0.093653    Objective Loss 0.093653                                        LR 0.000016    Time 0.253260    
2024-05-09 15:08:19,880 - Epoch: [293][  217/  217]    Overall Loss 0.094023    Objective Loss 0.094023    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.250110    
2024-05-09 15:08:20,330 - 

2024-05-09 15:08:20,330 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:08:46,247 - Epoch: [294][  100/  217]    Overall Loss 0.084853    Objective Loss 0.084853                                        LR 0.000016    Time 0.259041    
2024-05-09 15:09:04,739 - Epoch: [294][  200/  217]    Overall Loss 0.092180    Objective Loss 0.092180                                        LR 0.000016    Time 0.221921    
2024-05-09 15:09:07,897 - Epoch: [294][  217/  217]    Overall Loss 0.092925    Objective Loss 0.092925    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.219078    
2024-05-09 15:09:08,362 - 

2024-05-09 15:09:08,363 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:09:33,427 - Epoch: [295][  100/  217]    Overall Loss 0.087426    Objective Loss 0.087426                                        LR 0.000016    Time 0.250517    
2024-05-09 15:09:51,059 - Epoch: [295][  200/  217]    Overall Loss 0.089054    Objective Loss 0.089054                                        LR 0.000016    Time 0.213369    
2024-05-09 15:09:53,539 - Epoch: [295][  217/  217]    Overall Loss 0.091600    Objective Loss 0.091600    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.208071    
2024-05-09 15:09:53,795 - 

2024-05-09 15:09:53,796 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:10:14,226 - Epoch: [296][  100/  217]    Overall Loss 0.100441    Objective Loss 0.100441                                        LR 0.000016    Time 0.204185    
2024-05-09 15:10:34,772 - Epoch: [296][  200/  217]    Overall Loss 0.102437    Objective Loss 0.102437                                        LR 0.000016    Time 0.204766    
2024-05-09 15:10:37,962 - Epoch: [296][  217/  217]    Overall Loss 0.103386    Objective Loss 0.103386    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.203417    
2024-05-09 15:10:38,211 - 

2024-05-09 15:10:38,212 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:10:58,612 - Epoch: [297][  100/  217]    Overall Loss 0.084077    Objective Loss 0.084077                                        LR 0.000016    Time 0.203895    
2024-05-09 15:11:15,500 - Epoch: [297][  200/  217]    Overall Loss 0.089692    Objective Loss 0.089692                                        LR 0.000016    Time 0.186332    
2024-05-09 15:11:18,392 - Epoch: [297][  217/  217]    Overall Loss 0.090492    Objective Loss 0.090492    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.185053    
2024-05-09 15:11:18,722 - 

2024-05-09 15:11:18,723 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:11:45,227 - Epoch: [298][  100/  217]    Overall Loss 0.074972    Objective Loss 0.074972                                        LR 0.000016    Time 0.264917    
2024-05-09 15:12:02,280 - Epoch: [298][  200/  217]    Overall Loss 0.083721    Objective Loss 0.083721                                        LR 0.000016    Time 0.217671    
2024-05-09 15:12:04,823 - Epoch: [298][  217/  217]    Overall Loss 0.085578    Objective Loss 0.085578    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.212331    
2024-05-09 15:12:05,149 - 

2024-05-09 15:12:05,149 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:12:25,356 - Epoch: [299][  100/  217]    Overall Loss 0.091482    Objective Loss 0.091482                                        LR 0.000016    Time 0.201951    
2024-05-09 15:12:43,369 - Epoch: [299][  200/  217]    Overall Loss 0.089276    Objective Loss 0.089276                                        LR 0.000016    Time 0.190984    
2024-05-09 15:12:46,660 - Epoch: [299][  217/  217]    Overall Loss 0.090388    Objective Loss 0.090388    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.191178    
2024-05-09 15:12:46,934 - --- test ---------------------
2024-05-09 15:12:46,935 - 1736 samples (32 per mini-batch)
2024-05-09 15:12:58,938 - Test: [   55/   55]    Loss 2.109081    Top1 56.682028    Top5 73.790323    
2024-05-09 15:12:59,357 - ==> Top1: 56.682    Top5: 73.790    Loss: 2.109

2024-05-09 15:12:59,366 - 
2024-05-09 15:12:59,367 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.09-095602/2024.05.09-095602.log
