2024-05-09 09:55:56,114 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.09-095556/2024.05.09-095556.log
2024-05-09 09:56:00,301 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2024-05-09 09:56:00,301 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2024-05-09 09:56:00,453 - Dataset sizes:
	training=6941
	validation=1736
	test=1736
2024-05-09 09:56:00,453 - Reading compression schedule from: policies/schedule-cifar100.yaml
2024-05-09 09:56:00,457 - 

2024-05-09 09:56:00,458 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 09:56:22,133 - Epoch: [0][  100/  217]    Overall Loss 4.027363    Objective Loss 4.027363                                        LR 0.001000    Time 0.216676    
2024-05-09 09:56:50,286 - Epoch: [0][  200/  217]    Overall Loss 3.761111    Objective Loss 3.761111                                        LR 0.001000    Time 0.249044    
2024-05-09 09:56:55,143 - Epoch: [0][  217/  217]    Overall Loss 3.719914    Objective Loss 3.719914    Top1 31.147541    Top5 45.901639    LR 0.001000    Time 0.251905    
2024-05-09 09:56:55,708 - --- validate (epoch=0)-----------
2024-05-09 09:56:55,709 - 1736 samples (32 per mini-batch)
2024-05-09 09:57:12,696 - Epoch: [0][   55/   55]    Loss 3.564109    Top1 26.497696    Top5 34.447005    
2024-05-09 09:57:13,026 - ==> Top1: 26.498    Top5: 34.447    Loss: 3.564

2024-05-09 09:57:13,032 - ==> Best [Top1: 26.498   Top5: 34.447   Sparsity:0.00   Params: 382352 on epoch: 0]
2024-05-09 09:57:13,033 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 09:57:13,097 - 

2024-05-09 09:57:13,099 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 09:57:44,061 - Epoch: [1][  100/  217]    Overall Loss 3.281228    Objective Loss 3.281228                                        LR 0.001000    Time 0.309463    
2024-05-09 09:58:09,279 - Epoch: [1][  200/  217]    Overall Loss 3.224928    Objective Loss 3.224928                                        LR 0.001000    Time 0.280705    
2024-05-09 09:58:14,403 - Epoch: [1][  217/  217]    Overall Loss 3.218404    Objective Loss 3.218404    Top1 31.147541    Top5 37.704918    LR 0.001000    Time 0.282319    
2024-05-09 09:58:15,485 - 

2024-05-09 09:58:15,487 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 09:58:41,694 - Epoch: [2][  100/  217]    Overall Loss 2.972720    Objective Loss 2.972720                                        LR 0.001000    Time 0.261894    
2024-05-09 09:59:12,172 - Epoch: [2][  200/  217]    Overall Loss 2.930998    Objective Loss 2.930998                                        LR 0.001000    Time 0.283273    
2024-05-09 09:59:17,137 - Epoch: [2][  217/  217]    Overall Loss 2.919696    Objective Loss 2.919696    Top1 37.704918    Top5 50.819672    LR 0.001000    Time 0.283951    
2024-05-09 09:59:17,628 - 

2024-05-09 09:59:17,629 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 09:59:48,228 - Epoch: [3][  100/  217]    Overall Loss 2.608701    Objective Loss 2.608701                                        LR 0.001000    Time 0.305848    
2024-05-09 10:00:16,723 - Epoch: [3][  200/  217]    Overall Loss 2.606463    Objective Loss 2.606463                                        LR 0.001000    Time 0.295338    
2024-05-09 10:00:21,794 - Epoch: [3][  217/  217]    Overall Loss 2.602553    Objective Loss 2.602553    Top1 44.262295    Top5 59.016393    LR 0.001000    Time 0.295554    
2024-05-09 10:00:22,461 - 

2024-05-09 10:00:22,462 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:00:51,155 - Epoch: [4][  100/  217]    Overall Loss 2.304003    Objective Loss 2.304003                                        LR 0.001000    Time 0.286782    
2024-05-09 10:01:15,007 - Epoch: [4][  200/  217]    Overall Loss 2.309065    Objective Loss 2.309065                                        LR 0.001000    Time 0.262594    
2024-05-09 10:01:18,587 - Epoch: [4][  217/  217]    Overall Loss 2.293140    Objective Loss 2.293140    Top1 45.901639    Top5 75.409836    LR 0.001000    Time 0.258511    
2024-05-09 10:01:18,826 - 

2024-05-09 10:01:18,826 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:01:44,958 - Epoch: [5][  100/  217]    Overall Loss 2.029765    Objective Loss 2.029765                                        LR 0.001000    Time 0.261182    
2024-05-09 10:02:13,789 - Epoch: [5][  200/  217]    Overall Loss 2.012627    Objective Loss 2.012627                                        LR 0.001000    Time 0.274613    
2024-05-09 10:02:18,547 - Epoch: [5][  217/  217]    Overall Loss 2.004789    Objective Loss 2.004789    Top1 63.934426    Top5 75.409836    LR 0.001000    Time 0.275013    
2024-05-09 10:02:18,844 - 

2024-05-09 10:02:18,846 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:02:48,044 - Epoch: [6][  100/  217]    Overall Loss 1.716135    Objective Loss 1.716135                                        LR 0.001000    Time 0.291731    
2024-05-09 10:03:14,914 - Epoch: [6][  200/  217]    Overall Loss 1.745244    Objective Loss 1.745244                                        LR 0.001000    Time 0.280153    
2024-05-09 10:03:20,332 - Epoch: [6][  217/  217]    Overall Loss 1.752502    Objective Loss 1.752502    Top1 60.655738    Top5 77.049180    LR 0.001000    Time 0.283161    
2024-05-09 10:03:20,621 - 

2024-05-09 10:03:20,622 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:03:49,603 - Epoch: [7][  100/  217]    Overall Loss 1.523261    Objective Loss 1.523261                                        LR 0.001000    Time 0.289677    
2024-05-09 10:04:16,561 - Epoch: [7][  200/  217]    Overall Loss 1.531540    Objective Loss 1.531540                                        LR 0.001000    Time 0.279566    
2024-05-09 10:04:21,235 - Epoch: [7][  217/  217]    Overall Loss 1.532899    Objective Loss 1.532899    Top1 60.655738    Top5 86.885246    LR 0.001000    Time 0.279192    
2024-05-09 10:04:21,595 - 

2024-05-09 10:04:21,596 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:04:52,461 - Epoch: [8][  100/  217]    Overall Loss 1.326180    Objective Loss 1.326180                                        LR 0.001000    Time 0.308526    
2024-05-09 10:05:21,979 - Epoch: [8][  200/  217]    Overall Loss 1.321756    Objective Loss 1.321756                                        LR 0.001000    Time 0.301794    
2024-05-09 10:05:25,794 - Epoch: [8][  217/  217]    Overall Loss 1.327478    Objective Loss 1.327478    Top1 65.573770    Top5 86.885246    LR 0.001000    Time 0.295721    
2024-05-09 10:05:26,024 - 

2024-05-09 10:05:26,025 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:05:54,479 - Epoch: [9][  100/  217]    Overall Loss 1.091718    Objective Loss 1.091718                                        LR 0.001000    Time 0.284408    
2024-05-09 10:06:20,054 - Epoch: [9][  200/  217]    Overall Loss 1.124761    Objective Loss 1.124761                                        LR 0.001000    Time 0.270022    
2024-05-09 10:06:24,887 - Epoch: [9][  217/  217]    Overall Loss 1.123254    Objective Loss 1.123254    Top1 63.934426    Top5 90.163934    LR 0.001000    Time 0.271122    
2024-05-09 10:06:25,162 - 

2024-05-09 10:06:25,162 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:06:52,858 - Epoch: [10][  100/  217]    Overall Loss 0.927810    Objective Loss 0.927810                                        LR 0.001000    Time 0.276784    
2024-05-09 10:07:17,734 - Epoch: [10][  200/  217]    Overall Loss 0.961751    Objective Loss 0.961751                                        LR 0.001000    Time 0.262715    
2024-05-09 10:07:20,955 - Epoch: [10][  217/  217]    Overall Loss 0.963013    Objective Loss 0.963013    Top1 83.606557    Top5 93.442623    LR 0.001000    Time 0.256968    
2024-05-09 10:07:21,226 - --- validate (epoch=10)-----------
2024-05-09 10:07:21,227 - 1736 samples (32 per mini-batch)
2024-05-09 10:07:37,285 - Epoch: [10][   55/   55]    Loss 2.293088    Top1 50.460829    Top5 67.569124    
2024-05-09 10:07:37,526 - ==> Top1: 50.461    Top5: 67.569    Loss: 2.293

2024-05-09 10:07:37,529 - ==> Best [Top1: 50.461   Top5: 67.569   Sparsity:0.00   Params: 382352 on epoch: 10]
2024-05-09 10:07:37,530 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 10:07:37,574 - 

2024-05-09 10:07:37,574 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:08:08,057 - Epoch: [11][  100/  217]    Overall Loss 0.745451    Objective Loss 0.745451                                        LR 0.001000    Time 0.304692    
2024-05-09 10:08:32,955 - Epoch: [11][  200/  217]    Overall Loss 0.782782    Objective Loss 0.782782                                        LR 0.001000    Time 0.276782    
2024-05-09 10:08:37,212 - Epoch: [11][  217/  217]    Overall Loss 0.786826    Objective Loss 0.786826    Top1 75.409836    Top5 90.163934    LR 0.001000    Time 0.274703    
2024-05-09 10:08:37,504 - 

2024-05-09 10:08:37,504 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:09:06,858 - Epoch: [12][  100/  217]    Overall Loss 0.607961    Objective Loss 0.607961                                        LR 0.001000    Time 0.293416    
2024-05-09 10:09:36,887 - Epoch: [12][  200/  217]    Overall Loss 0.628261    Objective Loss 0.628261                                        LR 0.001000    Time 0.296796    
2024-05-09 10:09:40,515 - Epoch: [12][  217/  217]    Overall Loss 0.630243    Objective Loss 0.630243    Top1 88.524590    Top5 100.000000    LR 0.001000    Time 0.290255    
2024-05-09 10:09:40,891 - 

2024-05-09 10:09:40,892 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:10:10,052 - Epoch: [13][  100/  217]    Overall Loss 0.488251    Objective Loss 0.488251                                        LR 0.001000    Time 0.291480    
2024-05-09 10:10:34,638 - Epoch: [13][  200/  217]    Overall Loss 0.494090    Objective Loss 0.494090                                        LR 0.001000    Time 0.268612    
2024-05-09 10:10:39,313 - Epoch: [13][  217/  217]    Overall Loss 0.500650    Objective Loss 0.500650    Top1 81.967213    Top5 95.081967    LR 0.001000    Time 0.269102    
2024-05-09 10:10:40,216 - 

2024-05-09 10:10:40,217 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:11:08,167 - Epoch: [14][  100/  217]    Overall Loss 0.369181    Objective Loss 0.369181                                        LR 0.001000    Time 0.279375    
2024-05-09 10:11:34,660 - Epoch: [14][  200/  217]    Overall Loss 0.390850    Objective Loss 0.390850                                        LR 0.001000    Time 0.272093    
2024-05-09 10:11:39,254 - Epoch: [14][  217/  217]    Overall Loss 0.394908    Objective Loss 0.394908    Top1 91.803279    Top5 100.000000    LR 0.001000    Time 0.271934    
2024-05-09 10:11:39,586 - 

2024-05-09 10:11:39,587 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:12:07,416 - Epoch: [15][  100/  217]    Overall Loss 0.278687    Objective Loss 0.278687                                        LR 0.001000    Time 0.278164    
2024-05-09 10:12:31,185 - Epoch: [15][  200/  217]    Overall Loss 0.299777    Objective Loss 0.299777                                        LR 0.001000    Time 0.257875    
2024-05-09 10:12:36,320 - Epoch: [15][  217/  217]    Overall Loss 0.302358    Objective Loss 0.302358    Top1 93.442623    Top5 100.000000    LR 0.001000    Time 0.261298    
2024-05-09 10:12:36,562 - 

2024-05-09 10:12:36,562 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:13:02,717 - Epoch: [16][  100/  217]    Overall Loss 0.209939    Objective Loss 0.209939                                        LR 0.001000    Time 0.261424    
2024-05-09 10:13:24,162 - Epoch: [16][  200/  217]    Overall Loss 0.229281    Objective Loss 0.229281                                        LR 0.001000    Time 0.237884    
2024-05-09 10:13:28,373 - Epoch: [16][  217/  217]    Overall Loss 0.232845    Objective Loss 0.232845    Top1 93.442623    Top5 100.000000    LR 0.001000    Time 0.238637    
2024-05-09 10:13:28,712 - 

2024-05-09 10:13:28,713 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:13:56,933 - Epoch: [17][  100/  217]    Overall Loss 0.166730    Objective Loss 0.166730                                        LR 0.001000    Time 0.282067    
2024-05-09 10:14:24,405 - Epoch: [17][  200/  217]    Overall Loss 0.170423    Objective Loss 0.170423                                        LR 0.001000    Time 0.278331    
2024-05-09 10:14:28,340 - Epoch: [17][  217/  217]    Overall Loss 0.172999    Objective Loss 0.172999    Top1 93.442623    Top5 100.000000    LR 0.001000    Time 0.274651    
2024-05-09 10:14:28,606 - 

2024-05-09 10:14:28,607 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:14:56,658 - Epoch: [18][  100/  217]    Overall Loss 0.119605    Objective Loss 0.119605                                        LR 0.001000    Time 0.280368    
2024-05-09 10:15:23,247 - Epoch: [18][  200/  217]    Overall Loss 0.123838    Objective Loss 0.123838                                        LR 0.001000    Time 0.273066    
2024-05-09 10:15:28,515 - Epoch: [18][  217/  217]    Overall Loss 0.128367    Objective Loss 0.128367    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.275936    
2024-05-09 10:15:28,805 - 

2024-05-09 10:15:28,806 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:15:58,888 - Epoch: [19][  100/  217]    Overall Loss 0.100072    Objective Loss 0.100072                                        LR 0.001000    Time 0.300701    
2024-05-09 10:16:24,485 - Epoch: [19][  200/  217]    Overall Loss 0.106293    Objective Loss 0.106293                                        LR 0.001000    Time 0.278270    
2024-05-09 10:16:29,064 - Epoch: [19][  217/  217]    Overall Loss 0.107343    Objective Loss 0.107343    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.277561    
2024-05-09 10:16:29,387 - 

2024-05-09 10:16:29,388 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:17:00,706 - Epoch: [20][  100/  217]    Overall Loss 0.086415    Objective Loss 0.086415                                        LR 0.001000    Time 0.313050    
2024-05-09 10:17:25,647 - Epoch: [20][  200/  217]    Overall Loss 0.089559    Objective Loss 0.089559                                        LR 0.001000    Time 0.281175    
2024-05-09 10:17:29,471 - Epoch: [20][  217/  217]    Overall Loss 0.090715    Objective Loss 0.090715    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.276755    
2024-05-09 10:17:29,803 - --- validate (epoch=20)-----------
2024-05-09 10:17:29,804 - 1736 samples (32 per mini-batch)
2024-05-09 10:17:45,153 - Epoch: [20][   55/   55]    Loss 2.590508    Top1 52.419355    Top5 67.914747    
2024-05-09 10:17:45,367 - ==> Top1: 52.419    Top5: 67.915    Loss: 2.591

2024-05-09 10:17:45,372 - ==> Best [Top1: 52.419   Top5: 67.915   Sparsity:0.00   Params: 382352 on epoch: 20]
2024-05-09 10:17:45,372 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 10:17:45,427 - 

2024-05-09 10:17:45,428 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:18:11,827 - Epoch: [21][  100/  217]    Overall Loss 0.073319    Objective Loss 0.073319                                        LR 0.001000    Time 0.263885    
2024-05-09 10:18:35,906 - Epoch: [21][  200/  217]    Overall Loss 0.083358    Objective Loss 0.083358                                        LR 0.001000    Time 0.252281    
2024-05-09 10:18:41,043 - Epoch: [21][  217/  217]    Overall Loss 0.086251    Objective Loss 0.086251    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.256172    
2024-05-09 10:18:41,378 - 

2024-05-09 10:18:41,379 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:19:08,282 - Epoch: [22][  100/  217]    Overall Loss 0.079740    Objective Loss 0.079740                                        LR 0.001000    Time 0.268914    
2024-05-09 10:19:35,758 - Epoch: [22][  200/  217]    Overall Loss 0.085596    Objective Loss 0.085596                                        LR 0.001000    Time 0.271715    
2024-05-09 10:19:39,630 - Epoch: [22][  217/  217]    Overall Loss 0.089083    Objective Loss 0.089083    Top1 93.442623    Top5 98.360656    LR 0.001000    Time 0.268264    
2024-05-09 10:19:40,079 - 

2024-05-09 10:19:40,080 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:20:10,871 - Epoch: [23][  100/  217]    Overall Loss 0.121636    Objective Loss 0.121636                                        LR 0.001000    Time 0.307773    
2024-05-09 10:20:34,337 - Epoch: [23][  200/  217]    Overall Loss 0.192989    Objective Loss 0.192989                                        LR 0.001000    Time 0.271160    
2024-05-09 10:20:39,466 - Epoch: [23][  217/  217]    Overall Loss 0.210537    Objective Loss 0.210537    Top1 88.524590    Top5 98.360656    LR 0.001000    Time 0.273538    
2024-05-09 10:20:39,849 - 

2024-05-09 10:20:39,850 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:21:09,322 - Epoch: [24][  100/  217]    Overall Loss 0.350679    Objective Loss 0.350679                                        LR 0.001000    Time 0.294578    
2024-05-09 10:21:34,032 - Epoch: [24][  200/  217]    Overall Loss 0.380229    Objective Loss 0.380229                                        LR 0.001000    Time 0.270779    
2024-05-09 10:21:38,403 - Epoch: [24][  217/  217]    Overall Loss 0.391207    Objective Loss 0.391207    Top1 78.688525    Top5 100.000000    LR 0.001000    Time 0.269693    
2024-05-09 10:21:38,611 - 

2024-05-09 10:21:38,612 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:22:09,737 - Epoch: [25][  100/  217]    Overall Loss 0.218510    Objective Loss 0.218510                                        LR 0.001000    Time 0.311092    
2024-05-09 10:22:34,060 - Epoch: [25][  200/  217]    Overall Loss 0.193477    Objective Loss 0.193477                                        LR 0.001000    Time 0.277106    
2024-05-09 10:22:39,080 - Epoch: [25][  217/  217]    Overall Loss 0.190473    Objective Loss 0.190473    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.278518    
2024-05-09 10:22:39,878 - 

2024-05-09 10:22:39,880 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:23:08,511 - Epoch: [26][  100/  217]    Overall Loss 0.068488    Objective Loss 0.068488                                        LR 0.001000    Time 0.286190    
2024-05-09 10:23:34,304 - Epoch: [26][  200/  217]    Overall Loss 0.068634    Objective Loss 0.068634                                        LR 0.001000    Time 0.272004    
2024-05-09 10:23:38,037 - Epoch: [26][  217/  217]    Overall Loss 0.068398    Objective Loss 0.068398    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.267886    
2024-05-09 10:23:38,284 - 

2024-05-09 10:23:38,285 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:24:07,464 - Epoch: [27][  100/  217]    Overall Loss 0.029120    Objective Loss 0.029120                                        LR 0.001000    Time 0.291672    
2024-05-09 10:24:31,326 - Epoch: [27][  200/  217]    Overall Loss 0.029557    Objective Loss 0.029557                                        LR 0.001000    Time 0.265084    
2024-05-09 10:24:35,402 - Epoch: [27][  217/  217]    Overall Loss 0.030073    Objective Loss 0.030073    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.263093    
2024-05-09 10:24:35,716 - 

2024-05-09 10:24:35,717 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:25:03,726 - Epoch: [28][  100/  217]    Overall Loss 0.022204    Objective Loss 0.022204                                        LR 0.001000    Time 0.279973    
2024-05-09 10:25:26,346 - Epoch: [28][  200/  217]    Overall Loss 0.022191    Objective Loss 0.022191                                        LR 0.001000    Time 0.253028    
2024-05-09 10:25:30,646 - Epoch: [28][  217/  217]    Overall Loss 0.022355    Objective Loss 0.022355    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.253012    
2024-05-09 10:25:31,072 - 

2024-05-09 10:25:31,073 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:26:01,841 - Epoch: [29][  100/  217]    Overall Loss 0.013856    Objective Loss 0.013856                                        LR 0.001000    Time 0.307557    
2024-05-09 10:26:27,931 - Epoch: [29][  200/  217]    Overall Loss 0.015157    Objective Loss 0.015157                                        LR 0.001000    Time 0.284168    
2024-05-09 10:26:32,993 - Epoch: [29][  217/  217]    Overall Loss 0.015181    Objective Loss 0.015181    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.285219    
2024-05-09 10:26:33,315 - 

2024-05-09 10:26:33,317 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:27:03,579 - Epoch: [30][  100/  217]    Overall Loss 0.013155    Objective Loss 0.013155                                        LR 0.001000    Time 0.302486    
2024-05-09 10:27:26,165 - Epoch: [30][  200/  217]    Overall Loss 0.011898    Objective Loss 0.011898                                        LR 0.001000    Time 0.264115    
2024-05-09 10:27:30,427 - Epoch: [30][  217/  217]    Overall Loss 0.012426    Objective Loss 0.012426    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.263052    
2024-05-09 10:27:30,732 - --- validate (epoch=30)-----------
2024-05-09 10:27:30,733 - 1736 samples (32 per mini-batch)
2024-05-09 10:27:47,416 - Epoch: [30][   55/   55]    Loss 2.721496    Top1 53.398618    Top5 69.988479    
2024-05-09 10:27:47,771 - ==> Top1: 53.399    Top5: 69.988    Loss: 2.721

2024-05-09 10:27:47,775 - ==> Best [Top1: 53.399   Top5: 69.988   Sparsity:0.00   Params: 382352 on epoch: 30]
2024-05-09 10:27:47,775 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 10:27:47,832 - 

2024-05-09 10:27:47,833 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:28:15,759 - Epoch: [31][  100/  217]    Overall Loss 0.012856    Objective Loss 0.012856                                        LR 0.001000    Time 0.279128    
2024-05-09 10:28:46,363 - Epoch: [31][  200/  217]    Overall Loss 0.013511    Objective Loss 0.013511                                        LR 0.001000    Time 0.292509    
2024-05-09 10:28:50,081 - Epoch: [31][  217/  217]    Overall Loss 0.013200    Objective Loss 0.013200    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.286716    
2024-05-09 10:28:50,481 - 

2024-05-09 10:28:50,482 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:29:20,118 - Epoch: [32][  100/  217]    Overall Loss 0.013434    Objective Loss 0.013434                                        LR 0.001000    Time 0.296234    
2024-05-09 10:29:42,319 - Epoch: [32][  200/  217]    Overall Loss 0.012147    Objective Loss 0.012147                                        LR 0.001000    Time 0.259066    
2024-05-09 10:29:46,947 - Epoch: [32][  217/  217]    Overall Loss 0.011975    Objective Loss 0.011975    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.260088    
2024-05-09 10:29:47,220 - 

2024-05-09 10:29:47,221 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:30:20,193 - Epoch: [33][  100/  217]    Overall Loss 0.016763    Objective Loss 0.016763                                        LR 0.001000    Time 0.329588    
2024-05-09 10:30:44,780 - Epoch: [33][  200/  217]    Overall Loss 0.014297    Objective Loss 0.014297                                        LR 0.001000    Time 0.287674    
2024-05-09 10:30:49,743 - Epoch: [33][  217/  217]    Overall Loss 0.013844    Objective Loss 0.013844    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.288002    
2024-05-09 10:30:50,261 - 

2024-05-09 10:30:50,262 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:31:20,420 - Epoch: [34][  100/  217]    Overall Loss 0.010364    Objective Loss 0.010364                                        LR 0.001000    Time 0.301455    
2024-05-09 10:31:48,197 - Epoch: [34][  200/  217]    Overall Loss 0.010803    Objective Loss 0.010803                                        LR 0.001000    Time 0.289561    
2024-05-09 10:31:51,629 - Epoch: [34][  217/  217]    Overall Loss 0.010646    Objective Loss 0.010646    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.282678    
2024-05-09 10:31:52,254 - 

2024-05-09 10:31:52,255 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:32:20,058 - Epoch: [35][  100/  217]    Overall Loss 0.007168    Objective Loss 0.007168                                        LR 0.001000    Time 0.277892    
2024-05-09 10:32:44,205 - Epoch: [35][  200/  217]    Overall Loss 0.007277    Objective Loss 0.007277                                        LR 0.001000    Time 0.259617    
2024-05-09 10:32:48,545 - Epoch: [35][  217/  217]    Overall Loss 0.007080    Objective Loss 0.007080    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.259265    
2024-05-09 10:32:49,008 - 

2024-05-09 10:32:49,009 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:33:18,679 - Epoch: [36][  100/  217]    Overall Loss 0.005427    Objective Loss 0.005427                                        LR 0.001000    Time 0.296595    
2024-05-09 10:33:40,357 - Epoch: [36][  200/  217]    Overall Loss 0.005480    Objective Loss 0.005480                                        LR 0.001000    Time 0.256634    
2024-05-09 10:33:44,486 - Epoch: [36][  217/  217]    Overall Loss 0.005801    Objective Loss 0.005801    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.255541    
2024-05-09 10:33:44,813 - 

2024-05-09 10:33:44,813 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:34:17,207 - Epoch: [37][  100/  217]    Overall Loss 0.005015    Objective Loss 0.005015                                        LR 0.001000    Time 0.323822    
2024-05-09 10:34:43,364 - Epoch: [37][  200/  217]    Overall Loss 0.005765    Objective Loss 0.005765                                        LR 0.001000    Time 0.292639    
2024-05-09 10:34:46,190 - Epoch: [37][  217/  217]    Overall Loss 0.005626    Objective Loss 0.005626    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.282723    
2024-05-09 10:34:46,510 - 

2024-05-09 10:34:46,510 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:35:14,947 - Epoch: [38][  100/  217]    Overall Loss 0.005139    Objective Loss 0.005139                                        LR 0.001000    Time 0.284253    
2024-05-09 10:35:40,472 - Epoch: [38][  200/  217]    Overall Loss 0.004861    Objective Loss 0.004861                                        LR 0.001000    Time 0.269693    
2024-05-09 10:35:45,221 - Epoch: [38][  217/  217]    Overall Loss 0.004747    Objective Loss 0.004747    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.270444    
2024-05-09 10:35:45,473 - 

2024-05-09 10:35:45,473 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:36:16,999 - Epoch: [39][  100/  217]    Overall Loss 0.004007    Objective Loss 0.004007                                        LR 0.001000    Time 0.315092    
2024-05-09 10:36:41,666 - Epoch: [39][  200/  217]    Overall Loss 0.004558    Objective Loss 0.004558                                        LR 0.001000    Time 0.280820    
2024-05-09 10:36:45,310 - Epoch: [39][  217/  217]    Overall Loss 0.004499    Objective Loss 0.004499    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.275600    
2024-05-09 10:36:45,981 - 

2024-05-09 10:36:45,982 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:37:15,004 - Epoch: [40][  100/  217]    Overall Loss 0.004570    Objective Loss 0.004570                                        LR 0.001000    Time 0.290095    
2024-05-09 10:37:41,296 - Epoch: [40][  200/  217]    Overall Loss 0.004860    Objective Loss 0.004860                                        LR 0.001000    Time 0.276453    
2024-05-09 10:37:44,778 - Epoch: [40][  217/  217]    Overall Loss 0.004941    Objective Loss 0.004941    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.270827    
2024-05-09 10:37:45,109 - --- validate (epoch=40)-----------
2024-05-09 10:37:45,110 - 1736 samples (32 per mini-batch)
2024-05-09 10:38:01,978 - Epoch: [40][   55/   55]    Loss 2.805798    Top1 54.781106    Top5 70.506912    
2024-05-09 10:38:02,245 - ==> Top1: 54.781    Top5: 70.507    Loss: 2.806

2024-05-09 10:38:02,249 - ==> Best [Top1: 54.781   Top5: 70.507   Sparsity:0.00   Params: 382352 on epoch: 40]
2024-05-09 10:38:02,250 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 10:38:02,316 - 

2024-05-09 10:38:02,318 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:38:35,170 - Epoch: [41][  100/  217]    Overall Loss 0.004599    Objective Loss 0.004599                                        LR 0.001000    Time 0.328365    
2024-05-09 10:38:58,683 - Epoch: [41][  200/  217]    Overall Loss 0.340444    Objective Loss 0.340444                                        LR 0.001000    Time 0.281695    
2024-05-09 10:39:02,269 - Epoch: [41][  217/  217]    Overall Loss 0.464630    Objective Loss 0.464630    Top1 54.098361    Top5 77.049180    LR 0.001000    Time 0.276139    
2024-05-09 10:39:02,532 - 

2024-05-09 10:39:02,533 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:39:31,284 - Epoch: [42][  100/  217]    Overall Loss 1.395091    Objective Loss 1.395091                                        LR 0.001000    Time 0.287392    
2024-05-09 10:39:56,270 - Epoch: [42][  200/  217]    Overall Loss 1.210574    Objective Loss 1.210574                                        LR 0.001000    Time 0.268568    
2024-05-09 10:39:59,886 - Epoch: [42][  217/  217]    Overall Loss 1.187863    Objective Loss 1.187863    Top1 67.213115    Top5 93.442623    LR 0.001000    Time 0.264182    
2024-05-09 10:40:00,098 - 

2024-05-09 10:40:00,099 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:40:26,938 - Epoch: [43][  100/  217]    Overall Loss 0.407731    Objective Loss 0.407731                                        LR 0.001000    Time 0.268281    
2024-05-09 10:40:53,052 - Epoch: [43][  200/  217]    Overall Loss 0.389388    Objective Loss 0.389388                                        LR 0.001000    Time 0.264651    
2024-05-09 10:40:56,615 - Epoch: [43][  217/  217]    Overall Loss 0.380683    Objective Loss 0.380683    Top1 86.885246    Top5 100.000000    LR 0.001000    Time 0.260329    
2024-05-09 10:40:56,889 - 

2024-05-09 10:40:56,889 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:41:26,188 - Epoch: [44][  100/  217]    Overall Loss 0.141541    Objective Loss 0.141541                                        LR 0.001000    Time 0.292868    
2024-05-09 10:41:50,869 - Epoch: [44][  200/  217]    Overall Loss 0.132025    Objective Loss 0.132025                                        LR 0.001000    Time 0.269780    
2024-05-09 10:41:55,971 - Epoch: [44][  217/  217]    Overall Loss 0.131951    Objective Loss 0.131951    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.272144    
2024-05-09 10:41:56,892 - 

2024-05-09 10:41:56,894 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:42:23,385 - Epoch: [45][  100/  217]    Overall Loss 0.060671    Objective Loss 0.060671                                        LR 0.001000    Time 0.264752    
2024-05-09 10:42:53,184 - Epoch: [45][  200/  217]    Overall Loss 0.058243    Objective Loss 0.058243                                        LR 0.001000    Time 0.281264    
2024-05-09 10:42:56,746 - Epoch: [45][  217/  217]    Overall Loss 0.059764    Objective Loss 0.059764    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.275633    
2024-05-09 10:42:57,156 - 

2024-05-09 10:42:57,156 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:43:26,261 - Epoch: [46][  100/  217]    Overall Loss 0.029454    Objective Loss 0.029454                                        LR 0.001000    Time 0.290924    
2024-05-09 10:43:52,609 - Epoch: [46][  200/  217]    Overall Loss 0.030427    Objective Loss 0.030427                                        LR 0.001000    Time 0.277146    
2024-05-09 10:43:56,985 - Epoch: [46][  217/  217]    Overall Loss 0.030378    Objective Loss 0.030378    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.275586    
2024-05-09 10:43:57,381 - 

2024-05-09 10:43:57,382 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:44:27,795 - Epoch: [47][  100/  217]    Overall Loss 0.019855    Objective Loss 0.019855                                        LR 0.001000    Time 0.303985    
2024-05-09 10:44:54,598 - Epoch: [47][  200/  217]    Overall Loss 0.020581    Objective Loss 0.020581                                        LR 0.001000    Time 0.285942    
2024-05-09 10:44:59,160 - Epoch: [47][  217/  217]    Overall Loss 0.020226    Objective Loss 0.020226    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.284547    
2024-05-09 10:44:59,635 - 

2024-05-09 10:44:59,636 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:45:29,434 - Epoch: [48][  100/  217]    Overall Loss 0.012492    Objective Loss 0.012492                                        LR 0.001000    Time 0.297858    
2024-05-09 10:45:53,945 - Epoch: [48][  200/  217]    Overall Loss 0.013203    Objective Loss 0.013203                                        LR 0.001000    Time 0.271427    
2024-05-09 10:45:59,112 - Epoch: [48][  217/  217]    Overall Loss 0.013245    Objective Loss 0.013245    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.273961    
2024-05-09 10:45:59,613 - 

2024-05-09 10:45:59,614 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:46:26,815 - Epoch: [49][  100/  217]    Overall Loss 0.011042    Objective Loss 0.011042                                        LR 0.001000    Time 0.271848    
2024-05-09 10:46:55,174 - Epoch: [49][  200/  217]    Overall Loss 0.010770    Objective Loss 0.010770                                        LR 0.001000    Time 0.277647    
2024-05-09 10:47:00,247 - Epoch: [49][  217/  217]    Overall Loss 0.010990    Objective Loss 0.010990    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.279261    
2024-05-09 10:47:00,653 - 

2024-05-09 10:47:00,655 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:47:32,997 - Epoch: [50][  100/  217]    Overall Loss 0.008081    Objective Loss 0.008081                                        LR 0.001000    Time 0.323286    
2024-05-09 10:47:57,261 - Epoch: [50][  200/  217]    Overall Loss 0.008395    Objective Loss 0.008395                                        LR 0.001000    Time 0.282901    
2024-05-09 10:48:02,331 - Epoch: [50][  217/  217]    Overall Loss 0.008993    Objective Loss 0.008993    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.284090    
2024-05-09 10:48:02,954 - --- validate (epoch=50)-----------
2024-05-09 10:48:02,954 - 1736 samples (32 per mini-batch)
2024-05-09 10:48:22,372 - Epoch: [50][   55/   55]    Loss 2.630308    Top1 55.645161    Top5 72.119816    
2024-05-09 10:48:23,128 - ==> Top1: 55.645    Top5: 72.120    Loss: 2.630

2024-05-09 10:48:23,136 - ==> Best [Top1: 55.645   Top5: 72.120   Sparsity:0.00   Params: 382352 on epoch: 50]
2024-05-09 10:48:23,137 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 10:48:23,208 - 

2024-05-09 10:48:23,209 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:48:54,095 - Epoch: [51][  100/  217]    Overall Loss 0.007955    Objective Loss 0.007955                                        LR 0.001000    Time 0.308723    
2024-05-09 10:49:22,209 - Epoch: [51][  200/  217]    Overall Loss 0.008052    Objective Loss 0.008052                                        LR 0.001000    Time 0.294870    
2024-05-09 10:49:27,314 - Epoch: [51][  217/  217]    Overall Loss 0.008902    Objective Loss 0.008902    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.295278    
2024-05-09 10:49:27,997 - 

2024-05-09 10:49:27,998 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:50:00,856 - Epoch: [52][  100/  217]    Overall Loss 0.008301    Objective Loss 0.008301                                        LR 0.001000    Time 0.328447    
2024-05-09 10:50:28,532 - Epoch: [52][  200/  217]    Overall Loss 0.009263    Objective Loss 0.009263                                        LR 0.001000    Time 0.302546    
2024-05-09 10:50:33,620 - Epoch: [52][  217/  217]    Overall Loss 0.009420    Objective Loss 0.009420    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.302280    
2024-05-09 10:50:34,220 - 

2024-05-09 10:50:34,221 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:51:06,148 - Epoch: [53][  100/  217]    Overall Loss 0.006646    Objective Loss 0.006646                                        LR 0.001000    Time 0.319131    
2024-05-09 10:51:34,280 - Epoch: [53][  200/  217]    Overall Loss 0.007087    Objective Loss 0.007087                                        LR 0.001000    Time 0.300162    
2024-05-09 10:51:39,060 - Epoch: [53][  217/  217]    Overall Loss 0.007240    Objective Loss 0.007240    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.298661    
2024-05-09 10:51:39,694 - 

2024-05-09 10:51:39,695 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:52:08,934 - Epoch: [54][  100/  217]    Overall Loss 0.014477    Objective Loss 0.014477                                        LR 0.001000    Time 0.292249    
2024-05-09 10:52:35,156 - Epoch: [54][  200/  217]    Overall Loss 0.012217    Objective Loss 0.012217                                        LR 0.001000    Time 0.277165    
2024-05-09 10:52:39,597 - Epoch: [54][  217/  217]    Overall Loss 0.012372    Objective Loss 0.012372    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.275902    
2024-05-09 10:52:39,996 - 

2024-05-09 10:52:39,996 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:53:10,152 - Epoch: [55][  100/  217]    Overall Loss 0.009616    Objective Loss 0.009616                                        LR 0.001000    Time 0.301441    
2024-05-09 10:53:36,342 - Epoch: [55][  200/  217]    Overall Loss 0.008485    Objective Loss 0.008485                                        LR 0.001000    Time 0.281609    
2024-05-09 10:53:41,000 - Epoch: [55][  217/  217]    Overall Loss 0.008313    Objective Loss 0.008313    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.281001    
2024-05-09 10:53:41,368 - 

2024-05-09 10:53:41,370 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:54:11,744 - Epoch: [56][  100/  217]    Overall Loss 0.007159    Objective Loss 0.007159                                        LR 0.001000    Time 0.303606    
2024-05-09 10:54:36,406 - Epoch: [56][  200/  217]    Overall Loss 0.012849    Objective Loss 0.012849                                        LR 0.001000    Time 0.275052    
2024-05-09 10:54:41,058 - Epoch: [56][  217/  217]    Overall Loss 0.015459    Objective Loss 0.015459    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.274927    
2024-05-09 10:54:41,519 - 

2024-05-09 10:54:41,520 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:55:10,805 - Epoch: [57][  100/  217]    Overall Loss 0.441307    Objective Loss 0.441307                                        LR 0.001000    Time 0.292722    
2024-05-09 10:55:39,256 - Epoch: [57][  200/  217]    Overall Loss 0.702466    Objective Loss 0.702466                                        LR 0.001000    Time 0.288558    
2024-05-09 10:55:42,358 - Epoch: [57][  217/  217]    Overall Loss 0.698367    Objective Loss 0.698367    Top1 78.688525    Top5 88.524590    LR 0.001000    Time 0.280234    
2024-05-09 10:55:43,206 - 

2024-05-09 10:55:43,207 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:56:13,109 - Epoch: [58][  100/  217]    Overall Loss 0.330923    Objective Loss 0.330923                                        LR 0.001000    Time 0.298899    
2024-05-09 10:56:38,056 - Epoch: [58][  200/  217]    Overall Loss 0.292816    Objective Loss 0.292816                                        LR 0.001000    Time 0.274066    
2024-05-09 10:56:42,660 - Epoch: [58][  217/  217]    Overall Loss 0.291073    Objective Loss 0.291073    Top1 95.081967    Top5 100.000000    LR 0.001000    Time 0.273801    
2024-05-09 10:56:42,994 - 

2024-05-09 10:56:42,995 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:57:14,017 - Epoch: [59][  100/  217]    Overall Loss 0.096182    Objective Loss 0.096182                                        LR 0.001000    Time 0.310018    
2024-05-09 10:57:43,354 - Epoch: [59][  200/  217]    Overall Loss 0.082065    Objective Loss 0.082065                                        LR 0.001000    Time 0.301636    
2024-05-09 10:57:48,080 - Epoch: [59][  217/  217]    Overall Loss 0.081745    Objective Loss 0.081745    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.299769    
2024-05-09 10:57:48,502 - 

2024-05-09 10:57:48,503 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:58:19,162 - Epoch: [60][  100/  217]    Overall Loss 0.028290    Objective Loss 0.028290                                        LR 0.001000    Time 0.306405    
2024-05-09 10:58:46,069 - Epoch: [60][  200/  217]    Overall Loss 0.028398    Objective Loss 0.028398                                        LR 0.001000    Time 0.287673    
2024-05-09 10:58:51,017 - Epoch: [60][  217/  217]    Overall Loss 0.028135    Objective Loss 0.028135    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.287925    
2024-05-09 10:58:51,326 - --- validate (epoch=60)-----------
2024-05-09 10:58:51,327 - 1736 samples (32 per mini-batch)
2024-05-09 10:59:10,667 - Epoch: [60][   55/   55]    Loss 2.697129    Top1 54.435484    Top5 72.638249    
2024-05-09 10:59:11,062 - ==> Top1: 54.435    Top5: 72.638    Loss: 2.697

2024-05-09 10:59:11,067 - ==> Best [Top1: 55.645   Top5: 72.120   Sparsity:0.00   Params: 382352 on epoch: 50]
2024-05-09 10:59:11,068 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 10:59:11,118 - 

2024-05-09 10:59:11,119 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 10:59:42,451 - Epoch: [61][  100/  217]    Overall Loss 0.015326    Objective Loss 0.015326                                        LR 0.001000    Time 0.313185    
2024-05-09 11:00:07,468 - Epoch: [61][  200/  217]    Overall Loss 0.015742    Objective Loss 0.015742                                        LR 0.001000    Time 0.281618    
2024-05-09 11:00:11,526 - Epoch: [61][  217/  217]    Overall Loss 0.015644    Objective Loss 0.015644    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.278247    
2024-05-09 11:00:11,856 - 

2024-05-09 11:00:11,857 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:00:40,921 - Epoch: [62][  100/  217]    Overall Loss 0.011847    Objective Loss 0.011847                                        LR 0.001000    Time 0.290490    
2024-05-09 11:01:07,398 - Epoch: [62][  200/  217]    Overall Loss 0.011456    Objective Loss 0.011456                                        LR 0.001000    Time 0.277570    
2024-05-09 11:01:11,564 - Epoch: [62][  217/  217]    Overall Loss 0.011446    Objective Loss 0.011446    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.275016    
2024-05-09 11:01:11,936 - 

2024-05-09 11:01:11,937 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:01:42,132 - Epoch: [63][  100/  217]    Overall Loss 0.008662    Objective Loss 0.008662                                        LR 0.001000    Time 0.301805    
2024-05-09 11:02:05,435 - Epoch: [63][  200/  217]    Overall Loss 0.008809    Objective Loss 0.008809                                        LR 0.001000    Time 0.267356    
2024-05-09 11:02:08,919 - Epoch: [63][  217/  217]    Overall Loss 0.009027    Objective Loss 0.009027    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.262455    
2024-05-09 11:02:09,203 - 

2024-05-09 11:02:09,204 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:02:40,647 - Epoch: [64][  100/  217]    Overall Loss 0.006612    Objective Loss 0.006612                                        LR 0.001000    Time 0.314286    
2024-05-09 11:03:06,599 - Epoch: [64][  200/  217]    Overall Loss 0.007233    Objective Loss 0.007233                                        LR 0.001000    Time 0.286848    
2024-05-09 11:03:10,549 - Epoch: [64][  217/  217]    Overall Loss 0.007470    Objective Loss 0.007470    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.282566    
2024-05-09 11:03:10,875 - 

2024-05-09 11:03:10,876 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:03:41,183 - Epoch: [65][  100/  217]    Overall Loss 0.008173    Objective Loss 0.008173                                        LR 0.001000    Time 0.302936    
2024-05-09 11:04:07,219 - Epoch: [65][  200/  217]    Overall Loss 0.008884    Objective Loss 0.008884                                        LR 0.001000    Time 0.281582    
2024-05-09 11:04:11,440 - Epoch: [65][  217/  217]    Overall Loss 0.008595    Objective Loss 0.008595    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.278961    
2024-05-09 11:04:11,801 - 

2024-05-09 11:04:11,803 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:04:44,041 - Epoch: [66][  100/  217]    Overall Loss 0.006212    Objective Loss 0.006212                                        LR 0.001000    Time 0.322243    
2024-05-09 11:05:11,657 - Epoch: [66][  200/  217]    Overall Loss 0.007251    Objective Loss 0.007251                                        LR 0.001000    Time 0.299134    
2024-05-09 11:05:16,803 - Epoch: [66][  217/  217]    Overall Loss 0.007150    Objective Loss 0.007150    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.299405    
2024-05-09 11:05:17,177 - 

2024-05-09 11:05:17,179 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:05:46,771 - Epoch: [67][  100/  217]    Overall Loss 0.007186    Objective Loss 0.007186                                        LR 0.001000    Time 0.295778    
2024-05-09 11:06:11,923 - Epoch: [67][  200/  217]    Overall Loss 0.007721    Objective Loss 0.007721                                        LR 0.001000    Time 0.273580    
2024-05-09 11:06:16,261 - Epoch: [67][  217/  217]    Overall Loss 0.007594    Objective Loss 0.007594    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.272127    
2024-05-09 11:06:16,616 - 

2024-05-09 11:06:16,617 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:06:45,717 - Epoch: [68][  100/  217]    Overall Loss 0.006117    Objective Loss 0.006117                                        LR 0.001000    Time 0.290859    
2024-05-09 11:07:12,964 - Epoch: [68][  200/  217]    Overall Loss 0.005269    Objective Loss 0.005269                                        LR 0.001000    Time 0.281602    
2024-05-09 11:07:17,546 - Epoch: [68][  217/  217]    Overall Loss 0.005122    Objective Loss 0.005122    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.280646    
2024-05-09 11:07:17,829 - 

2024-05-09 11:07:17,830 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:07:48,561 - Epoch: [69][  100/  217]    Overall Loss 0.004107    Objective Loss 0.004107                                        LR 0.001000    Time 0.307183    
2024-05-09 11:08:14,773 - Epoch: [69][  200/  217]    Overall Loss 0.004486    Objective Loss 0.004486                                        LR 0.001000    Time 0.284582    
2024-05-09 11:08:19,295 - Epoch: [69][  217/  217]    Overall Loss 0.004381    Objective Loss 0.004381    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.283115    
2024-05-09 11:08:19,677 - 

2024-05-09 11:08:19,678 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:08:48,924 - Epoch: [70][  100/  217]    Overall Loss 0.004073    Objective Loss 0.004073                                        LR 0.001000    Time 0.292330    
2024-05-09 11:09:15,149 - Epoch: [70][  200/  217]    Overall Loss 0.004238    Objective Loss 0.004238                                        LR 0.001000    Time 0.277234    
2024-05-09 11:09:18,958 - Epoch: [70][  217/  217]    Overall Loss 0.004345    Objective Loss 0.004345    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.273054    
2024-05-09 11:09:19,352 - --- validate (epoch=70)-----------
2024-05-09 11:09:19,353 - 1736 samples (32 per mini-batch)
2024-05-09 11:09:39,090 - Epoch: [70][   55/   55]    Loss 2.808615    Top1 55.702765    Top5 72.350230    
2024-05-09 11:09:39,362 - ==> Top1: 55.703    Top5: 72.350    Loss: 2.809

2024-05-09 11:09:39,369 - ==> Best [Top1: 55.703   Top5: 72.350   Sparsity:0.00   Params: 382352 on epoch: 70]
2024-05-09 11:09:39,369 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 11:09:39,419 - 

2024-05-09 11:09:39,419 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:10:08,194 - Epoch: [71][  100/  217]    Overall Loss 0.002703    Objective Loss 0.002703                                        LR 0.001000    Time 0.287608    
2024-05-09 11:10:32,044 - Epoch: [71][  200/  217]    Overall Loss 0.003878    Objective Loss 0.003878                                        LR 0.001000    Time 0.262990    
2024-05-09 11:10:36,488 - Epoch: [71][  217/  217]    Overall Loss 0.003748    Objective Loss 0.003748    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.262859    
2024-05-09 11:10:37,043 - 

2024-05-09 11:10:37,045 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:11:09,756 - Epoch: [72][  100/  217]    Overall Loss 0.003041    Objective Loss 0.003041                                        LR 0.001000    Time 0.326963    
2024-05-09 11:11:32,791 - Epoch: [72][  200/  217]    Overall Loss 0.003219    Objective Loss 0.003219                                        LR 0.001000    Time 0.278592    
2024-05-09 11:11:36,254 - Epoch: [72][  217/  217]    Overall Loss 0.003651    Objective Loss 0.003651    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.272715    
2024-05-09 11:11:36,681 - 

2024-05-09 11:11:36,682 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:12:07,775 - Epoch: [73][  100/  217]    Overall Loss 0.002865    Objective Loss 0.002865                                        LR 0.001000    Time 0.310805    
2024-05-09 11:12:34,671 - Epoch: [73][  200/  217]    Overall Loss 0.004034    Objective Loss 0.004034                                        LR 0.001000    Time 0.289817    
2024-05-09 11:12:38,926 - Epoch: [73][  217/  217]    Overall Loss 0.004066    Objective Loss 0.004066    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.286711    
2024-05-09 11:12:39,408 - 

2024-05-09 11:12:39,409 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:13:10,143 - Epoch: [74][  100/  217]    Overall Loss 0.004909    Objective Loss 0.004909                                        LR 0.001000    Time 0.307187    
2024-05-09 11:13:35,588 - Epoch: [74][  200/  217]    Overall Loss 0.004500    Objective Loss 0.004500                                        LR 0.001000    Time 0.280754    
2024-05-09 11:13:40,449 - Epoch: [74][  217/  217]    Overall Loss 0.004563    Objective Loss 0.004563    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.281148    
2024-05-09 11:13:40,782 - 

2024-05-09 11:13:40,783 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:14:09,897 - Epoch: [75][  100/  217]    Overall Loss 0.003575    Objective Loss 0.003575                                        LR 0.001000    Time 0.291006    
2024-05-09 11:14:35,549 - Epoch: [75][  200/  217]    Overall Loss 0.005010    Objective Loss 0.005010                                        LR 0.001000    Time 0.273710    
2024-05-09 11:14:39,775 - Epoch: [75][  217/  217]    Overall Loss 0.005031    Objective Loss 0.005031    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.271730    
2024-05-09 11:14:40,425 - 

2024-05-09 11:14:40,425 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:15:10,853 - Epoch: [76][  100/  217]    Overall Loss 0.003744    Objective Loss 0.003744                                        LR 0.001000    Time 0.304141    
2024-05-09 11:15:37,499 - Epoch: [76][  200/  217]    Overall Loss 0.395400    Objective Loss 0.395400                                        LR 0.001000    Time 0.285242    
2024-05-09 11:15:41,635 - Epoch: [76][  217/  217]    Overall Loss 0.460337    Objective Loss 0.460337    Top1 63.934426    Top5 88.524590    LR 0.001000    Time 0.281945    
2024-05-09 11:15:42,087 - 

2024-05-09 11:15:42,088 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:16:10,022 - Epoch: [77][  100/  217]    Overall Loss 0.766703    Objective Loss 0.766703                                        LR 0.001000    Time 0.279212    
2024-05-09 11:16:35,373 - Epoch: [77][  200/  217]    Overall Loss 0.624724    Objective Loss 0.624724                                        LR 0.001000    Time 0.266304    
2024-05-09 11:16:39,407 - Epoch: [77][  217/  217]    Overall Loss 0.607434    Objective Loss 0.607434    Top1 88.524590    Top5 100.000000    LR 0.001000    Time 0.263985    
2024-05-09 11:16:39,768 - 

2024-05-09 11:16:39,769 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:17:10,131 - Epoch: [78][  100/  217]    Overall Loss 0.130936    Objective Loss 0.130936                                        LR 0.001000    Time 0.303483    
2024-05-09 11:17:37,601 - Epoch: [78][  200/  217]    Overall Loss 0.115764    Objective Loss 0.115764                                        LR 0.001000    Time 0.289026    
2024-05-09 11:17:42,730 - Epoch: [78][  217/  217]    Overall Loss 0.112990    Objective Loss 0.112990    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.290008    
2024-05-09 11:17:43,354 - 

2024-05-09 11:17:43,354 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:18:09,068 - Epoch: [79][  100/  217]    Overall Loss 0.036822    Objective Loss 0.036822                                        LR 0.001000    Time 0.256993    
2024-05-09 11:18:34,983 - Epoch: [79][  200/  217]    Overall Loss 0.034428    Objective Loss 0.034428                                        LR 0.001000    Time 0.258010    
2024-05-09 11:18:39,651 - Epoch: [79][  217/  217]    Overall Loss 0.034296    Objective Loss 0.034296    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.259299    
2024-05-09 11:18:40,167 - 

2024-05-09 11:18:40,168 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:19:11,162 - Epoch: [80][  100/  217]    Overall Loss 0.019099    Objective Loss 0.019099                                        LR 0.001000    Time 0.309816    
2024-05-09 11:19:35,684 - Epoch: [80][  200/  217]    Overall Loss 0.017860    Objective Loss 0.017860                                        LR 0.001000    Time 0.277464    
2024-05-09 11:19:40,215 - Epoch: [80][  217/  217]    Overall Loss 0.017469    Objective Loss 0.017469    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.276594    
2024-05-09 11:19:40,661 - --- validate (epoch=80)-----------
2024-05-09 11:19:40,662 - 1736 samples (32 per mini-batch)
2024-05-09 11:19:57,962 - Epoch: [80][   55/   55]    Loss 2.735638    Top1 55.472350    Top5 72.523041    
2024-05-09 11:19:58,773 - ==> Top1: 55.472    Top5: 72.523    Loss: 2.736

2024-05-09 11:19:58,778 - ==> Best [Top1: 55.703   Top5: 72.350   Sparsity:0.00   Params: 382352 on epoch: 70]
2024-05-09 11:19:58,778 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 11:19:58,828 - 

2024-05-09 11:19:58,829 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:20:28,470 - Epoch: [81][  100/  217]    Overall Loss 0.012290    Objective Loss 0.012290                                        LR 0.001000    Time 0.296287    
2024-05-09 11:20:57,358 - Epoch: [81][  200/  217]    Overall Loss 0.012701    Objective Loss 0.012701                                        LR 0.001000    Time 0.292519    
2024-05-09 11:21:01,587 - Epoch: [81][  217/  217]    Overall Loss 0.012588    Objective Loss 0.012588    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.289079    
2024-05-09 11:21:01,912 - 

2024-05-09 11:21:01,913 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:21:32,264 - Epoch: [82][  100/  217]    Overall Loss 0.008814    Objective Loss 0.008814                                        LR 0.001000    Time 0.303362    
2024-05-09 11:21:59,260 - Epoch: [82][  200/  217]    Overall Loss 0.009849    Objective Loss 0.009849                                        LR 0.001000    Time 0.286595    
2024-05-09 11:22:04,484 - Epoch: [82][  217/  217]    Overall Loss 0.009991    Objective Loss 0.009991    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.288206    
2024-05-09 11:22:04,994 - 

2024-05-09 11:22:04,996 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:22:38,553 - Epoch: [83][  100/  217]    Overall Loss 0.008167    Objective Loss 0.008167                                        LR 0.001000    Time 0.335434    
2024-05-09 11:23:05,715 - Epoch: [83][  200/  217]    Overall Loss 0.009201    Objective Loss 0.009201                                        LR 0.001000    Time 0.303471    
2024-05-09 11:23:10,358 - Epoch: [83][  217/  217]    Overall Loss 0.009105    Objective Loss 0.009105    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.301082    
2024-05-09 11:23:11,087 - 

2024-05-09 11:23:11,087 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:23:41,389 - Epoch: [84][  100/  217]    Overall Loss 0.006525    Objective Loss 0.006525                                        LR 0.001000    Time 0.302879    
2024-05-09 11:24:10,576 - Epoch: [84][  200/  217]    Overall Loss 0.007339    Objective Loss 0.007339                                        LR 0.001000    Time 0.297321    
2024-05-09 11:24:13,660 - Epoch: [84][  217/  217]    Overall Loss 0.007230    Objective Loss 0.007230    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.288231    
2024-05-09 11:24:14,092 - 

2024-05-09 11:24:14,094 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:24:45,878 - Epoch: [85][  100/  217]    Overall Loss 0.006136    Objective Loss 0.006136                                        LR 0.001000    Time 0.317699    
2024-05-09 11:25:13,335 - Epoch: [85][  200/  217]    Overall Loss 0.006329    Objective Loss 0.006329                                        LR 0.001000    Time 0.296078    
2024-05-09 11:25:16,530 - Epoch: [85][  217/  217]    Overall Loss 0.006289    Objective Loss 0.006289    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.287593    
2024-05-09 11:25:17,051 - 

2024-05-09 11:25:17,052 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:25:46,307 - Epoch: [86][  100/  217]    Overall Loss 0.006944    Objective Loss 0.006944                                        LR 0.001000    Time 0.292410    
2024-05-09 11:26:13,033 - Epoch: [86][  200/  217]    Overall Loss 0.006814    Objective Loss 0.006814                                        LR 0.001000    Time 0.279777    
2024-05-09 11:26:16,437 - Epoch: [86][  217/  217]    Overall Loss 0.006744    Objective Loss 0.006744    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.273536    
2024-05-09 11:26:16,802 - 

2024-05-09 11:26:16,803 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:26:46,550 - Epoch: [87][  100/  217]    Overall Loss 0.004604    Objective Loss 0.004604                                        LR 0.001000    Time 0.297325    
2024-05-09 11:27:13,273 - Epoch: [87][  200/  217]    Overall Loss 0.004880    Objective Loss 0.004880                                        LR 0.001000    Time 0.282206    
2024-05-09 11:27:16,250 - Epoch: [87][  217/  217]    Overall Loss 0.004909    Objective Loss 0.004909    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.273803    
2024-05-09 11:27:16,660 - 

2024-05-09 11:27:16,660 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:27:45,462 - Epoch: [88][  100/  217]    Overall Loss 0.003424    Objective Loss 0.003424                                        LR 0.001000    Time 0.287868    
2024-05-09 11:28:14,937 - Epoch: [88][  200/  217]    Overall Loss 0.004575    Objective Loss 0.004575                                        LR 0.001000    Time 0.291250    
2024-05-09 11:28:18,218 - Epoch: [88][  217/  217]    Overall Loss 0.004421    Objective Loss 0.004421    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.283547    
2024-05-09 11:28:18,826 - 

2024-05-09 11:28:18,828 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:28:48,372 - Epoch: [89][  100/  217]    Overall Loss 0.004729    Objective Loss 0.004729                                        LR 0.001000    Time 0.295299    
2024-05-09 11:29:14,372 - Epoch: [89][  200/  217]    Overall Loss 0.004320    Objective Loss 0.004320                                        LR 0.001000    Time 0.277598    
2024-05-09 11:29:17,317 - Epoch: [89][  217/  217]    Overall Loss 0.004211    Objective Loss 0.004211    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.269412    
2024-05-09 11:29:18,099 - 

2024-05-09 11:29:18,100 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:29:48,651 - Epoch: [90][  100/  217]    Overall Loss 0.003610    Objective Loss 0.003610                                        LR 0.001000    Time 0.305358    
2024-05-09 11:30:15,000 - Epoch: [90][  200/  217]    Overall Loss 0.003577    Objective Loss 0.003577                                        LR 0.001000    Time 0.284361    
2024-05-09 11:30:20,018 - Epoch: [90][  217/  217]    Overall Loss 0.003983    Objective Loss 0.003983    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.285197    
2024-05-09 11:30:20,402 - --- validate (epoch=90)-----------
2024-05-09 11:30:20,403 - 1736 samples (32 per mini-batch)
2024-05-09 11:30:36,248 - Epoch: [90][   55/   55]    Loss 2.870653    Top1 55.990783    Top5 72.292627    
2024-05-09 11:30:36,531 - ==> Top1: 55.991    Top5: 72.293    Loss: 2.871

2024-05-09 11:30:36,534 - ==> Best [Top1: 55.991   Top5: 72.293   Sparsity:0.00   Params: 382352 on epoch: 90]
2024-05-09 11:30:36,535 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 11:30:36,573 - 

2024-05-09 11:30:36,574 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:31:06,502 - Epoch: [91][  100/  217]    Overall Loss 0.003455    Objective Loss 0.003455                                        LR 0.001000    Time 0.299151    
2024-05-09 11:31:31,412 - Epoch: [91][  200/  217]    Overall Loss 0.003924    Objective Loss 0.003924                                        LR 0.001000    Time 0.274067    
2024-05-09 11:31:36,212 - Epoch: [91][  217/  217]    Overall Loss 0.003852    Objective Loss 0.003852    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.274707    
2024-05-09 11:31:36,667 - 

2024-05-09 11:31:36,668 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:32:08,076 - Epoch: [92][  100/  217]    Overall Loss 0.003256    Objective Loss 0.003256                                        LR 0.001000    Time 0.313933    
2024-05-09 11:32:35,713 - Epoch: [92][  200/  217]    Overall Loss 0.003527    Objective Loss 0.003527                                        LR 0.001000    Time 0.295086    
2024-05-09 11:32:42,116 - Epoch: [92][  217/  217]    Overall Loss 0.003547    Objective Loss 0.003547    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.301458    
2024-05-09 11:32:42,595 - 

2024-05-09 11:32:42,596 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:33:14,966 - Epoch: [93][  100/  217]    Overall Loss 0.002846    Objective Loss 0.002846                                        LR 0.001000    Time 0.323573    
2024-05-09 11:33:47,713 - Epoch: [93][  200/  217]    Overall Loss 0.003424    Objective Loss 0.003424                                        LR 0.001000    Time 0.325462    
2024-05-09 11:33:50,727 - Epoch: [93][  217/  217]    Overall Loss 0.003320    Objective Loss 0.003320    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.313846    
2024-05-09 11:33:51,201 - 

2024-05-09 11:33:51,201 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:34:23,608 - Epoch: [94][  100/  217]    Overall Loss 0.003198    Objective Loss 0.003198                                        LR 0.001000    Time 0.323952    
2024-05-09 11:34:51,205 - Epoch: [94][  200/  217]    Overall Loss 0.003154    Objective Loss 0.003154                                        LR 0.001000    Time 0.299900    
2024-05-09 11:34:54,324 - Epoch: [94][  217/  217]    Overall Loss 0.003233    Objective Loss 0.003233    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.290771    
2024-05-09 11:34:54,843 - 

2024-05-09 11:34:54,845 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:35:28,417 - Epoch: [95][  100/  217]    Overall Loss 0.003064    Objective Loss 0.003064                                        LR 0.001000    Time 0.335590    
2024-05-09 11:35:52,026 - Epoch: [95][  200/  217]    Overall Loss 0.002686    Objective Loss 0.002686                                        LR 0.001000    Time 0.285781    
2024-05-09 11:35:55,384 - Epoch: [95][  217/  217]    Overall Loss 0.002895    Objective Loss 0.002895    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.278854    
2024-05-09 11:35:55,781 - 

2024-05-09 11:35:55,782 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:36:24,018 - Epoch: [96][  100/  217]    Overall Loss 0.002832    Objective Loss 0.002832                                        LR 0.001000    Time 0.282216    
2024-05-09 11:36:51,129 - Epoch: [96][  200/  217]    Overall Loss 0.003484    Objective Loss 0.003484                                        LR 0.001000    Time 0.276602    
2024-05-09 11:36:56,009 - Epoch: [96][  217/  217]    Overall Loss 0.003371    Objective Loss 0.003371    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.277411    
2024-05-09 11:36:56,458 - 

2024-05-09 11:36:56,459 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:37:26,132 - Epoch: [97][  100/  217]    Overall Loss 0.003352    Objective Loss 0.003352                                        LR 0.001000    Time 0.296608    
2024-05-09 11:37:51,166 - Epoch: [97][  200/  217]    Overall Loss 0.003161    Objective Loss 0.003161                                        LR 0.001000    Time 0.273409    
2024-05-09 11:37:56,055 - Epoch: [97][  217/  217]    Overall Loss 0.003264    Objective Loss 0.003264    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.274507    
2024-05-09 11:37:56,456 - 

2024-05-09 11:37:56,456 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:38:28,450 - Epoch: [98][  100/  217]    Overall Loss 0.844817    Objective Loss 0.844817                                        LR 0.001000    Time 0.319818    
2024-05-09 11:38:52,392 - Epoch: [98][  200/  217]    Overall Loss 0.804198    Objective Loss 0.804198                                        LR 0.001000    Time 0.279557    
2024-05-09 11:38:57,367 - Epoch: [98][  217/  217]    Overall Loss 0.784434    Objective Loss 0.784434    Top1 78.688525    Top5 96.721311    LR 0.001000    Time 0.280571    
2024-05-09 11:38:57,707 - 

2024-05-09 11:38:57,708 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:39:28,206 - Epoch: [99][  100/  217]    Overall Loss 0.200698    Objective Loss 0.200698                                        LR 0.001000    Time 0.304860    
2024-05-09 11:39:54,457 - Epoch: [99][  200/  217]    Overall Loss 0.171297    Objective Loss 0.171297                                        LR 0.001000    Time 0.283620    
2024-05-09 11:39:58,791 - Epoch: [99][  217/  217]    Overall Loss 0.167877    Objective Loss 0.167877    Top1 95.081967    Top5 100.000000    LR 0.001000    Time 0.281359    
2024-05-09 11:39:59,070 - 

2024-05-09 11:39:59,071 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:40:26,705 - Epoch: [100][  100/  217]    Overall Loss 0.048674    Objective Loss 0.048674                                        LR 0.000250    Time 0.276195    
2024-05-09 11:40:53,612 - Epoch: [100][  200/  217]    Overall Loss 0.043042    Objective Loss 0.043042                                        LR 0.000250    Time 0.272564    
2024-05-09 11:40:57,887 - Epoch: [100][  217/  217]    Overall Loss 0.042425    Objective Loss 0.042425    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.270898    
2024-05-09 11:40:58,277 - --- validate (epoch=100)-----------
2024-05-09 11:40:58,277 - 1736 samples (32 per mini-batch)
2024-05-09 11:41:16,298 - Epoch: [100][   55/   55]    Loss 2.801646    Top1 55.645161    Top5 71.428571    
2024-05-09 11:41:16,706 - ==> Top1: 55.645    Top5: 71.429    Loss: 2.802

2024-05-09 11:41:16,713 - ==> Best [Top1: 55.991   Top5: 72.293   Sparsity:0.00   Params: 382352 on epoch: 90]
2024-05-09 11:41:16,714 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 11:41:16,768 - 

2024-05-09 11:41:16,769 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:41:48,609 - Epoch: [101][  100/  217]    Overall Loss 0.022832    Objective Loss 0.022832                                        LR 0.000250    Time 0.318284    
2024-05-09 11:42:12,925 - Epoch: [101][  200/  217]    Overall Loss 0.020549    Objective Loss 0.020549                                        LR 0.000250    Time 0.280662    
2024-05-09 11:42:16,908 - Epoch: [101][  217/  217]    Overall Loss 0.020587    Objective Loss 0.020587    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.277010    
2024-05-09 11:42:17,342 - 

2024-05-09 11:42:17,343 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:42:46,841 - Epoch: [102][  100/  217]    Overall Loss 0.016818    Objective Loss 0.016818                                        LR 0.000250    Time 0.294849    
2024-05-09 11:43:13,238 - Epoch: [102][  200/  217]    Overall Loss 0.016528    Objective Loss 0.016528                                        LR 0.000250    Time 0.279344    
2024-05-09 11:43:17,066 - Epoch: [102][  217/  217]    Overall Loss 0.016660    Objective Loss 0.016660    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.275087    
2024-05-09 11:43:17,355 - 

2024-05-09 11:43:17,355 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:43:44,998 - Epoch: [103][  100/  217]    Overall Loss 0.014143    Objective Loss 0.014143                                        LR 0.000250    Time 0.276274    
2024-05-09 11:44:13,708 - Epoch: [103][  200/  217]    Overall Loss 0.014057    Objective Loss 0.014057                                        LR 0.000250    Time 0.281627    
2024-05-09 11:44:18,898 - Epoch: [103][  217/  217]    Overall Loss 0.014295    Objective Loss 0.014295    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.283466    
2024-05-09 11:44:19,392 - 

2024-05-09 11:44:19,393 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:44:46,118 - Epoch: [104][  100/  217]    Overall Loss 0.011046    Objective Loss 0.011046                                        LR 0.000250    Time 0.267130    
2024-05-09 11:45:11,803 - Epoch: [104][  200/  217]    Overall Loss 0.011780    Objective Loss 0.011780                                        LR 0.000250    Time 0.261929    
2024-05-09 11:45:15,814 - Epoch: [104][  217/  217]    Overall Loss 0.011724    Objective Loss 0.011724    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.259875    
2024-05-09 11:45:16,172 - 

2024-05-09 11:45:16,172 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:45:47,500 - Epoch: [105][  100/  217]    Overall Loss 0.011371    Objective Loss 0.011371                                        LR 0.000250    Time 0.313138    
2024-05-09 11:46:12,608 - Epoch: [105][  200/  217]    Overall Loss 0.010818    Objective Loss 0.010818                                        LR 0.000250    Time 0.282051    
2024-05-09 11:46:15,606 - Epoch: [105][  217/  217]    Overall Loss 0.010717    Objective Loss 0.010717    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.273762    
2024-05-09 11:46:15,952 - 

2024-05-09 11:46:15,953 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:46:44,356 - Epoch: [106][  100/  217]    Overall Loss 0.009848    Objective Loss 0.009848                                        LR 0.000250    Time 0.283902    
2024-05-09 11:47:11,528 - Epoch: [106][  200/  217]    Overall Loss 0.009524    Objective Loss 0.009524                                        LR 0.000250    Time 0.277754    
2024-05-09 11:47:15,938 - Epoch: [106][  217/  217]    Overall Loss 0.009435    Objective Loss 0.009435    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.276305    
2024-05-09 11:47:16,273 - 

2024-05-09 11:47:16,274 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:47:46,927 - Epoch: [107][  100/  217]    Overall Loss 0.008486    Objective Loss 0.008486                                        LR 0.000250    Time 0.306384    
2024-05-09 11:48:13,964 - Epoch: [107][  200/  217]    Overall Loss 0.008459    Objective Loss 0.008459                                        LR 0.000250    Time 0.288317    
2024-05-09 11:48:18,994 - Epoch: [107][  217/  217]    Overall Loss 0.008412    Objective Loss 0.008412    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.288898    
2024-05-09 11:48:19,434 - 

2024-05-09 11:48:19,434 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:48:49,155 - Epoch: [108][  100/  217]    Overall Loss 0.006737    Objective Loss 0.006737                                        LR 0.000250    Time 0.297083    
2024-05-09 11:49:17,580 - Epoch: [108][  200/  217]    Overall Loss 0.007090    Objective Loss 0.007090                                        LR 0.000250    Time 0.290609    
2024-05-09 11:49:21,560 - Epoch: [108][  217/  217]    Overall Loss 0.007387    Objective Loss 0.007387    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.286170    
2024-05-09 11:49:22,407 - 

2024-05-09 11:49:22,408 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:49:54,495 - Epoch: [109][  100/  217]    Overall Loss 0.007499    Objective Loss 0.007499                                        LR 0.000250    Time 0.320723    
2024-05-09 11:50:21,754 - Epoch: [109][  200/  217]    Overall Loss 0.007142    Objective Loss 0.007142                                        LR 0.000250    Time 0.296595    
2024-05-09 11:50:25,378 - Epoch: [109][  217/  217]    Overall Loss 0.007293    Objective Loss 0.007293    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.290044    
2024-05-09 11:50:25,905 - 

2024-05-09 11:50:25,906 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:50:58,115 - Epoch: [110][  100/  217]    Overall Loss 0.007089    Objective Loss 0.007089                                        LR 0.000250    Time 0.321939    
2024-05-09 11:51:25,197 - Epoch: [110][  200/  217]    Overall Loss 0.006771    Objective Loss 0.006771                                        LR 0.000250    Time 0.296319    
2024-05-09 11:51:29,502 - Epoch: [110][  217/  217]    Overall Loss 0.006677    Objective Loss 0.006677    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.292928    
2024-05-09 11:51:30,163 - --- validate (epoch=110)-----------
2024-05-09 11:51:30,164 - 1736 samples (32 per mini-batch)
2024-05-09 11:51:46,826 - Epoch: [110][   55/   55]    Loss 2.865984    Top1 56.278802    Top5 71.831797    
2024-05-09 11:51:47,336 - ==> Top1: 56.279    Top5: 71.832    Loss: 2.866

2024-05-09 11:51:47,342 - ==> Best [Top1: 56.279   Top5: 71.832   Sparsity:0.00   Params: 382352 on epoch: 110]
2024-05-09 11:51:47,342 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 11:51:47,397 - 

2024-05-09 11:51:47,397 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:52:20,756 - Epoch: [111][  100/  217]    Overall Loss 0.005950    Objective Loss 0.005950                                        LR 0.000250    Time 0.333471    
2024-05-09 11:52:45,334 - Epoch: [111][  200/  217]    Overall Loss 0.006386    Objective Loss 0.006386                                        LR 0.000250    Time 0.289568    
2024-05-09 11:52:49,058 - Epoch: [111][  217/  217]    Overall Loss 0.006490    Objective Loss 0.006490    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.284032    
2024-05-09 11:52:49,580 - 

2024-05-09 11:52:49,581 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:53:19,027 - Epoch: [112][  100/  217]    Overall Loss 0.005895    Objective Loss 0.005895                                        LR 0.000250    Time 0.294344    
2024-05-09 11:53:45,602 - Epoch: [112][  200/  217]    Overall Loss 0.006205    Objective Loss 0.006205                                        LR 0.000250    Time 0.279985    
2024-05-09 11:53:48,692 - Epoch: [112][  217/  217]    Overall Loss 0.006065    Objective Loss 0.006065    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.272283    
2024-05-09 11:53:49,095 - 

2024-05-09 11:53:49,096 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:54:17,782 - Epoch: [113][  100/  217]    Overall Loss 0.005677    Objective Loss 0.005677                                        LR 0.000250    Time 0.286719    
2024-05-09 11:54:45,511 - Epoch: [113][  200/  217]    Overall Loss 0.005490    Objective Loss 0.005490                                        LR 0.000250    Time 0.281949    
2024-05-09 11:54:48,937 - Epoch: [113][  217/  217]    Overall Loss 0.005534    Objective Loss 0.005534    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.275639    
2024-05-09 11:54:49,394 - 

2024-05-09 11:54:49,395 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:55:16,764 - Epoch: [114][  100/  217]    Overall Loss 0.006213    Objective Loss 0.006213                                        LR 0.000250    Time 0.273569    
2024-05-09 11:55:41,562 - Epoch: [114][  200/  217]    Overall Loss 0.005252    Objective Loss 0.005252                                        LR 0.000250    Time 0.260717    
2024-05-09 11:55:45,532 - Epoch: [114][  217/  217]    Overall Loss 0.005213    Objective Loss 0.005213    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.258579    
2024-05-09 11:55:45,951 - 

2024-05-09 11:55:45,952 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:56:14,344 - Epoch: [115][  100/  217]    Overall Loss 0.005651    Objective Loss 0.005651                                        LR 0.000250    Time 0.283792    
2024-05-09 11:56:40,133 - Epoch: [115][  200/  217]    Overall Loss 0.005146    Objective Loss 0.005146                                        LR 0.000250    Time 0.270778    
2024-05-09 11:56:43,724 - Epoch: [115][  217/  217]    Overall Loss 0.005168    Objective Loss 0.005168    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.266103    
2024-05-09 11:56:44,156 - 

2024-05-09 11:56:44,157 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:57:12,029 - Epoch: [116][  100/  217]    Overall Loss 0.004343    Objective Loss 0.004343                                        LR 0.000250    Time 0.278589    
2024-05-09 11:57:38,386 - Epoch: [116][  200/  217]    Overall Loss 0.004210    Objective Loss 0.004210                                        LR 0.000250    Time 0.271021    
2024-05-09 11:57:43,465 - Epoch: [116][  217/  217]    Overall Loss 0.004346    Objective Loss 0.004346    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.273185    
2024-05-09 11:57:43,845 - 

2024-05-09 11:57:43,846 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:58:10,513 - Epoch: [117][  100/  217]    Overall Loss 0.003723    Objective Loss 0.003723                                        LR 0.000250    Time 0.266540    
2024-05-09 11:58:35,928 - Epoch: [117][  200/  217]    Overall Loss 0.004417    Objective Loss 0.004417                                        LR 0.000250    Time 0.260291    
2024-05-09 11:58:40,806 - Epoch: [117][  217/  217]    Overall Loss 0.004324    Objective Loss 0.004324    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.262368    
2024-05-09 11:58:41,173 - 

2024-05-09 11:58:41,175 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 11:59:10,096 - Epoch: [118][  100/  217]    Overall Loss 0.004079    Objective Loss 0.004079                                        LR 0.000250    Time 0.289080    
2024-05-09 11:59:32,328 - Epoch: [118][  200/  217]    Overall Loss 0.004401    Objective Loss 0.004401                                        LR 0.000250    Time 0.255650    
2024-05-09 11:59:36,860 - Epoch: [118][  217/  217]    Overall Loss 0.004322    Objective Loss 0.004322    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.256498    
2024-05-09 11:59:37,209 - 

2024-05-09 11:59:37,210 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:00:05,732 - Epoch: [119][  100/  217]    Overall Loss 0.004043    Objective Loss 0.004043                                        LR 0.000250    Time 0.285080    
2024-05-09 12:00:32,860 - Epoch: [119][  200/  217]    Overall Loss 0.004134    Objective Loss 0.004134                                        LR 0.000250    Time 0.278118    
2024-05-09 12:00:38,045 - Epoch: [119][  217/  217]    Overall Loss 0.004056    Objective Loss 0.004056    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.280211    
2024-05-09 12:00:38,445 - 

2024-05-09 12:00:38,446 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:01:06,424 - Epoch: [120][  100/  217]    Overall Loss 0.004221    Objective Loss 0.004221                                        LR 0.000250    Time 0.279646    
2024-05-09 12:01:32,195 - Epoch: [120][  200/  217]    Overall Loss 0.003707    Objective Loss 0.003707                                        LR 0.000250    Time 0.268620    
2024-05-09 12:01:37,204 - Epoch: [120][  217/  217]    Overall Loss 0.003586    Objective Loss 0.003586    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.270649    
2024-05-09 12:01:37,791 - --- validate (epoch=120)-----------
2024-05-09 12:01:37,791 - 1736 samples (32 per mini-batch)
2024-05-09 12:01:56,131 - Epoch: [120][   55/   55]    Loss 2.900654    Top1 55.875576    Top5 72.350230    
2024-05-09 12:01:56,474 - ==> Top1: 55.876    Top5: 72.350    Loss: 2.901

2024-05-09 12:01:56,480 - ==> Best [Top1: 56.279   Top5: 71.832   Sparsity:0.00   Params: 382352 on epoch: 110]
2024-05-09 12:01:56,481 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 12:01:56,532 - 

2024-05-09 12:01:56,533 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:02:22,294 - Epoch: [121][  100/  217]    Overall Loss 0.002886    Objective Loss 0.002886                                        LR 0.000250    Time 0.257461    
2024-05-09 12:02:51,414 - Epoch: [121][  200/  217]    Overall Loss 0.003653    Objective Loss 0.003653                                        LR 0.000250    Time 0.274270    
2024-05-09 12:02:56,498 - Epoch: [121][  217/  217]    Overall Loss 0.003569    Objective Loss 0.003569    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.276200    
2024-05-09 12:02:57,111 - 

2024-05-09 12:02:57,112 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:03:25,057 - Epoch: [122][  100/  217]    Overall Loss 0.003409    Objective Loss 0.003409                                        LR 0.000250    Time 0.279325    
2024-05-09 12:03:47,274 - Epoch: [122][  200/  217]    Overall Loss 0.003259    Objective Loss 0.003259                                        LR 0.000250    Time 0.250687    
2024-05-09 12:03:51,967 - Epoch: [122][  217/  217]    Overall Loss 0.003476    Objective Loss 0.003476    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.252660    
2024-05-09 12:03:52,385 - 

2024-05-09 12:03:52,386 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:04:23,434 - Epoch: [123][  100/  217]    Overall Loss 0.002998    Objective Loss 0.002998                                        LR 0.000250    Time 0.310310    
2024-05-09 12:04:48,948 - Epoch: [123][  200/  217]    Overall Loss 0.003183    Objective Loss 0.003183                                        LR 0.000250    Time 0.282656    
2024-05-09 12:04:51,925 - Epoch: [123][  217/  217]    Overall Loss 0.003086    Objective Loss 0.003086    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.274220    
2024-05-09 12:04:52,387 - 

2024-05-09 12:04:52,388 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:05:21,866 - Epoch: [124][  100/  217]    Overall Loss 0.002790    Objective Loss 0.002790                                        LR 0.000250    Time 0.294662    
2024-05-09 12:05:46,559 - Epoch: [124][  200/  217]    Overall Loss 0.002800    Objective Loss 0.002800                                        LR 0.000250    Time 0.270737    
2024-05-09 12:05:51,467 - Epoch: [124][  217/  217]    Overall Loss 0.003051    Objective Loss 0.003051    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.272134    
2024-05-09 12:05:51,886 - 

2024-05-09 12:05:51,887 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:06:20,189 - Epoch: [125][  100/  217]    Overall Loss 0.003417    Objective Loss 0.003417                                        LR 0.000250    Time 0.282900    
2024-05-09 12:06:44,410 - Epoch: [125][  200/  217]    Overall Loss 0.003421    Objective Loss 0.003421                                        LR 0.000250    Time 0.262501    
2024-05-09 12:06:48,702 - Epoch: [125][  217/  217]    Overall Loss 0.003713    Objective Loss 0.003713    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.261706    
2024-05-09 12:06:49,109 - 

2024-05-09 12:06:49,110 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:07:16,561 - Epoch: [126][  100/  217]    Overall Loss 0.003338    Objective Loss 0.003338                                        LR 0.000250    Time 0.274389    
2024-05-09 12:07:43,551 - Epoch: [126][  200/  217]    Overall Loss 0.003337    Objective Loss 0.003337                                        LR 0.000250    Time 0.272087    
2024-05-09 12:07:47,043 - Epoch: [126][  217/  217]    Overall Loss 0.003262    Objective Loss 0.003262    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.266858    
2024-05-09 12:07:47,354 - 

2024-05-09 12:07:47,355 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:08:14,784 - Epoch: [127][  100/  217]    Overall Loss 0.002492    Objective Loss 0.002492                                        LR 0.000250    Time 0.274171    
2024-05-09 12:08:40,279 - Epoch: [127][  200/  217]    Overall Loss 0.002347    Objective Loss 0.002347                                        LR 0.000250    Time 0.264500    
2024-05-09 12:08:45,437 - Epoch: [127][  217/  217]    Overall Loss 0.002659    Objective Loss 0.002659    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.267533    
2024-05-09 12:08:45,764 - 

2024-05-09 12:08:45,765 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:09:13,774 - Epoch: [128][  100/  217]    Overall Loss 0.004360    Objective Loss 0.004360                                        LR 0.000250    Time 0.279965    
2024-05-09 12:09:37,975 - Epoch: [128][  200/  217]    Overall Loss 0.003082    Objective Loss 0.003082                                        LR 0.000250    Time 0.260928    
2024-05-09 12:09:42,634 - Epoch: [128][  217/  217]    Overall Loss 0.002980    Objective Loss 0.002980    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.261949    
2024-05-09 12:09:43,065 - 

2024-05-09 12:09:43,065 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:10:11,065 - Epoch: [129][  100/  217]    Overall Loss 0.002301    Objective Loss 0.002301                                        LR 0.000250    Time 0.279848    
2024-05-09 12:10:32,805 - Epoch: [129][  200/  217]    Overall Loss 0.002725    Objective Loss 0.002725                                        LR 0.000250    Time 0.248562    
2024-05-09 12:10:37,212 - Epoch: [129][  217/  217]    Overall Loss 0.002622    Objective Loss 0.002622    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.249384    
2024-05-09 12:10:37,694 - 

2024-05-09 12:10:37,695 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:11:09,132 - Epoch: [130][  100/  217]    Overall Loss 0.002856    Objective Loss 0.002856                                        LR 0.000250    Time 0.314232    
2024-05-09 12:11:31,187 - Epoch: [130][  200/  217]    Overall Loss 0.002667    Objective Loss 0.002667                                        LR 0.000250    Time 0.267330    
2024-05-09 12:11:36,125 - Epoch: [130][  217/  217]    Overall Loss 0.002582    Objective Loss 0.002582    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.269132    
2024-05-09 12:11:36,451 - --- validate (epoch=130)-----------
2024-05-09 12:11:36,453 - 1736 samples (32 per mini-batch)
2024-05-09 12:11:54,641 - Epoch: [130][   55/   55]    Loss 2.989463    Top1 55.933180    Top5 72.465438    
2024-05-09 12:11:54,980 - ==> Top1: 55.933    Top5: 72.465    Loss: 2.989

2024-05-09 12:11:54,987 - ==> Best [Top1: 56.279   Top5: 71.832   Sparsity:0.00   Params: 382352 on epoch: 110]
2024-05-09 12:11:54,987 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 12:11:55,047 - 

2024-05-09 12:11:55,048 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:12:22,854 - Epoch: [131][  100/  217]    Overall Loss 0.002289    Objective Loss 0.002289                                        LR 0.000250    Time 0.277916    
2024-05-09 12:12:46,875 - Epoch: [131][  200/  217]    Overall Loss 0.001933    Objective Loss 0.001933                                        LR 0.000250    Time 0.259006    
2024-05-09 12:12:50,782 - Epoch: [131][  217/  217]    Overall Loss 0.002302    Objective Loss 0.002302    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.256709    
2024-05-09 12:12:51,160 - 

2024-05-09 12:12:51,160 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:13:21,444 - Epoch: [132][  100/  217]    Overall Loss 0.007304    Objective Loss 0.007304                                        LR 0.000250    Time 0.302706    
2024-05-09 12:13:45,718 - Epoch: [132][  200/  217]    Overall Loss 0.006319    Objective Loss 0.006319                                        LR 0.000250    Time 0.272663    
2024-05-09 12:13:49,823 - Epoch: [132][  217/  217]    Overall Loss 0.006159    Objective Loss 0.006159    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.270209    
2024-05-09 12:13:50,281 - 

2024-05-09 12:13:50,281 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:14:15,871 - Epoch: [133][  100/  217]    Overall Loss 0.002732    Objective Loss 0.002732                                        LR 0.000250    Time 0.255781    
2024-05-09 12:14:40,644 - Epoch: [133][  200/  217]    Overall Loss 0.003556    Objective Loss 0.003556                                        LR 0.000250    Time 0.251697    
2024-05-09 12:14:43,961 - Epoch: [133][  217/  217]    Overall Loss 0.003425    Objective Loss 0.003425    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.247255    
2024-05-09 12:14:44,513 - 

2024-05-09 12:14:44,514 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:15:10,706 - Epoch: [134][  100/  217]    Overall Loss 0.002991    Objective Loss 0.002991                                        LR 0.000250    Time 0.261789    
2024-05-09 12:15:33,970 - Epoch: [134][  200/  217]    Overall Loss 0.003845    Objective Loss 0.003845                                        LR 0.000250    Time 0.247163    
2024-05-09 12:15:37,767 - Epoch: [134][  217/  217]    Overall Loss 0.003856    Objective Loss 0.003856    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.245286    
2024-05-09 12:15:38,140 - 

2024-05-09 12:15:38,141 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:16:09,144 - Epoch: [135][  100/  217]    Overall Loss 0.003166    Objective Loss 0.003166                                        LR 0.000250    Time 0.309888    
2024-05-09 12:16:34,995 - Epoch: [135][  200/  217]    Overall Loss 0.003304    Objective Loss 0.003304                                        LR 0.000250    Time 0.284139    
2024-05-09 12:16:40,070 - Epoch: [135][  217/  217]    Overall Loss 0.003173    Objective Loss 0.003173    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.285254    
2024-05-09 12:16:40,723 - 

2024-05-09 12:16:40,724 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:17:09,235 - Epoch: [136][  100/  217]    Overall Loss 0.003565    Objective Loss 0.003565                                        LR 0.000250    Time 0.284996    
2024-05-09 12:17:35,061 - Epoch: [136][  200/  217]    Overall Loss 0.003547    Objective Loss 0.003547                                        LR 0.000250    Time 0.271572    
2024-05-09 12:17:39,375 - Epoch: [136][  217/  217]    Overall Loss 0.003415    Objective Loss 0.003415    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.270164    
2024-05-09 12:17:39,916 - 

2024-05-09 12:17:39,917 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:18:10,328 - Epoch: [137][  100/  217]    Overall Loss 0.002332    Objective Loss 0.002332                                        LR 0.000250    Time 0.303950    
2024-05-09 12:18:33,287 - Epoch: [137][  200/  217]    Overall Loss 0.002367    Objective Loss 0.002367                                        LR 0.000250    Time 0.266698    
2024-05-09 12:18:36,399 - Epoch: [137][  217/  217]    Overall Loss 0.002281    Objective Loss 0.002281    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.260134    
2024-05-09 12:18:36,887 - 

2024-05-09 12:18:36,888 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:19:06,360 - Epoch: [138][  100/  217]    Overall Loss 0.002340    Objective Loss 0.002340                                        LR 0.000250    Time 0.294591    
2024-05-09 12:19:36,640 - Epoch: [138][  200/  217]    Overall Loss 0.002338    Objective Loss 0.002338                                        LR 0.000250    Time 0.298639    
2024-05-09 12:19:39,539 - Epoch: [138][  217/  217]    Overall Loss 0.002245    Objective Loss 0.002245    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.288595    
2024-05-09 12:19:39,864 - 

2024-05-09 12:19:39,865 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:20:07,732 - Epoch: [139][  100/  217]    Overall Loss 0.001341    Objective Loss 0.001341                                        LR 0.000250    Time 0.278538    
2024-05-09 12:20:33,764 - Epoch: [139][  200/  217]    Overall Loss 0.001907    Objective Loss 0.001907                                        LR 0.000250    Time 0.269372    
2024-05-09 12:20:38,038 - Epoch: [139][  217/  217]    Overall Loss 0.001935    Objective Loss 0.001935    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.267958    
2024-05-09 12:20:38,431 - 

2024-05-09 12:20:38,432 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:21:07,642 - Epoch: [140][  100/  217]    Overall Loss 0.001958    Objective Loss 0.001958                                        LR 0.000250    Time 0.291961    
2024-05-09 12:21:31,740 - Epoch: [140][  200/  217]    Overall Loss 0.001775    Objective Loss 0.001775                                        LR 0.000250    Time 0.266409    
2024-05-09 12:21:34,410 - Epoch: [140][  217/  217]    Overall Loss 0.001690    Objective Loss 0.001690    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.257816    
2024-05-09 12:21:34,802 - --- validate (epoch=140)-----------
2024-05-09 12:21:34,802 - 1736 samples (32 per mini-batch)
2024-05-09 12:21:53,924 - Epoch: [140][   55/   55]    Loss 3.150591    Top1 55.587558    Top5 71.831797    
2024-05-09 12:21:54,755 - ==> Top1: 55.588    Top5: 71.832    Loss: 3.151

2024-05-09 12:21:54,762 - ==> Best [Top1: 56.279   Top5: 71.832   Sparsity:0.00   Params: 382352 on epoch: 110]
2024-05-09 12:21:54,762 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 12:21:54,809 - 

2024-05-09 12:21:54,809 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:22:26,060 - Epoch: [141][  100/  217]    Overall Loss 0.002002    Objective Loss 0.002002                                        LR 0.000250    Time 0.312371    
2024-05-09 12:22:51,581 - Epoch: [141][  200/  217]    Overall Loss 0.001869    Objective Loss 0.001869                                        LR 0.000250    Time 0.283730    
2024-05-09 12:22:54,612 - Epoch: [141][  217/  217]    Overall Loss 0.001794    Objective Loss 0.001794    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.275461    
2024-05-09 12:22:55,091 - 

2024-05-09 12:22:55,092 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:23:26,512 - Epoch: [142][  100/  217]    Overall Loss 0.001717    Objective Loss 0.001717                                        LR 0.000250    Time 0.313711    
2024-05-09 12:23:52,121 - Epoch: [142][  200/  217]    Overall Loss 0.001757    Objective Loss 0.001757                                        LR 0.000250    Time 0.284838    
2024-05-09 12:23:56,222 - Epoch: [142][  217/  217]    Overall Loss 0.001846    Objective Loss 0.001846    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.281408    
2024-05-09 12:23:56,607 - 

2024-05-09 12:23:56,608 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:24:24,049 - Epoch: [143][  100/  217]    Overall Loss 0.001506    Objective Loss 0.001506                                        LR 0.000250    Time 0.274278    
2024-05-09 12:24:51,459 - Epoch: [143][  200/  217]    Overall Loss 0.001713    Objective Loss 0.001713                                        LR 0.000250    Time 0.274128    
2024-05-09 12:24:57,494 - Epoch: [143][  217/  217]    Overall Loss 0.001620    Objective Loss 0.001620    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.280453    
2024-05-09 12:24:57,862 - 

2024-05-09 12:24:57,863 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:25:25,765 - Epoch: [144][  100/  217]    Overall Loss 0.001430    Objective Loss 0.001430                                        LR 0.000250    Time 0.278882    
2024-05-09 12:25:49,184 - Epoch: [144][  200/  217]    Overall Loss 0.001624    Objective Loss 0.001624                                        LR 0.000250    Time 0.256476    
2024-05-09 12:25:53,724 - Epoch: [144][  217/  217]    Overall Loss 0.001543    Objective Loss 0.001543    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.257291    
2024-05-09 12:25:54,094 - 

2024-05-09 12:25:54,095 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:26:22,766 - Epoch: [145][  100/  217]    Overall Loss 0.001002    Objective Loss 0.001002                                        LR 0.000250    Time 0.286580    
2024-05-09 12:26:48,383 - Epoch: [145][  200/  217]    Overall Loss 0.001710    Objective Loss 0.001710                                        LR 0.000250    Time 0.271299    
2024-05-09 12:26:53,325 - Epoch: [145][  217/  217]    Overall Loss 0.001616    Objective Loss 0.001616    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.272808    
2024-05-09 12:26:53,701 - 

2024-05-09 12:26:53,702 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:27:20,621 - Epoch: [146][  100/  217]    Overall Loss 0.000947    Objective Loss 0.000947                                        LR 0.000250    Time 0.268986    
2024-05-09 12:27:46,082 - Epoch: [146][  200/  217]    Overall Loss 0.001449    Objective Loss 0.001449                                        LR 0.000250    Time 0.261741    
2024-05-09 12:27:49,793 - Epoch: [146][  217/  217]    Overall Loss 0.001540    Objective Loss 0.001540    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.258322    
2024-05-09 12:27:50,190 - 

2024-05-09 12:27:50,191 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:28:19,537 - Epoch: [147][  100/  217]    Overall Loss 0.005942    Objective Loss 0.005942                                        LR 0.000250    Time 0.293339    
2024-05-09 12:28:43,615 - Epoch: [147][  200/  217]    Overall Loss 0.008093    Objective Loss 0.008093                                        LR 0.000250    Time 0.267001    
2024-05-09 12:28:47,217 - Epoch: [147][  217/  217]    Overall Loss 0.007940    Objective Loss 0.007940    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.262672    
2024-05-09 12:28:47,547 - 

2024-05-09 12:28:47,548 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:29:16,847 - Epoch: [148][  100/  217]    Overall Loss 0.004821    Objective Loss 0.004821                                        LR 0.000250    Time 0.292691    
2024-05-09 12:29:44,926 - Epoch: [148][  200/  217]    Overall Loss 0.005718    Objective Loss 0.005718                                        LR 0.000250    Time 0.286684    
2024-05-09 12:29:47,478 - Epoch: [148][  217/  217]    Overall Loss 0.005809    Objective Loss 0.005809    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.275971    
2024-05-09 12:29:47,782 - 

2024-05-09 12:29:47,784 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:30:15,614 - Epoch: [149][  100/  217]    Overall Loss 0.004776    Objective Loss 0.004776                                        LR 0.000250    Time 0.278160    
2024-05-09 12:30:40,029 - Epoch: [149][  200/  217]    Overall Loss 0.003789    Objective Loss 0.003789                                        LR 0.000250    Time 0.261098    
2024-05-09 12:30:45,031 - Epoch: [149][  217/  217]    Overall Loss 0.003634    Objective Loss 0.003634    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.263686    
2024-05-09 12:30:45,590 - 

2024-05-09 12:30:45,591 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:31:14,417 - Epoch: [150][  100/  217]    Overall Loss 0.002216    Objective Loss 0.002216                                        LR 0.000063    Time 0.288153    
2024-05-09 12:31:40,679 - Epoch: [150][  200/  217]    Overall Loss 0.002403    Objective Loss 0.002403                                        LR 0.000063    Time 0.275331    
2024-05-09 12:31:44,624 - Epoch: [150][  217/  217]    Overall Loss 0.002332    Objective Loss 0.002332    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.271930    
2024-05-09 12:31:45,036 - --- validate (epoch=150)-----------
2024-05-09 12:31:45,036 - 1736 samples (32 per mini-batch)
2024-05-09 12:32:01,697 - Epoch: [150][   55/   55]    Loss 3.263847    Top1 55.529954    Top5 71.313364    
2024-05-09 12:32:02,086 - ==> Top1: 55.530    Top5: 71.313    Loss: 3.264

2024-05-09 12:32:02,091 - ==> Best [Top1: 56.279   Top5: 71.832   Sparsity:0.00   Params: 382352 on epoch: 110]
2024-05-09 12:32:02,091 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 12:32:02,137 - 

2024-05-09 12:32:02,138 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:32:35,014 - Epoch: [151][  100/  217]    Overall Loss 0.002101    Objective Loss 0.002101                                        LR 0.000063    Time 0.328639    
2024-05-09 12:33:00,262 - Epoch: [151][  200/  217]    Overall Loss 0.001885    Objective Loss 0.001885                                        LR 0.000063    Time 0.290502    
2024-05-09 12:33:04,029 - Epoch: [151][  217/  217]    Overall Loss 0.002068    Objective Loss 0.002068    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.285090    
2024-05-09 12:33:04,453 - 

2024-05-09 12:33:04,454 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:33:29,829 - Epoch: [152][  100/  217]    Overall Loss 0.001931    Objective Loss 0.001931                                        LR 0.000063    Time 0.253640    
2024-05-09 12:34:00,406 - Epoch: [152][  200/  217]    Overall Loss 0.002007    Objective Loss 0.002007                                        LR 0.000063    Time 0.279643    
2024-05-09 12:34:05,220 - Epoch: [152][  217/  217]    Overall Loss 0.002066    Objective Loss 0.002066    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.279907    
2024-05-09 12:34:05,713 - 

2024-05-09 12:34:05,713 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:34:32,639 - Epoch: [153][  100/  217]    Overall Loss 0.002345    Objective Loss 0.002345                                        LR 0.000063    Time 0.269127    
2024-05-09 12:34:55,799 - Epoch: [153][  200/  217]    Overall Loss 0.002126    Objective Loss 0.002126                                        LR 0.000063    Time 0.250252    
2024-05-09 12:35:01,693 - Epoch: [153][  217/  217]    Overall Loss 0.002016    Objective Loss 0.002016    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.257794    
2024-05-09 12:35:02,474 - 

2024-05-09 12:35:02,475 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:35:34,853 - Epoch: [154][  100/  217]    Overall Loss 0.001863    Objective Loss 0.001863                                        LR 0.000063    Time 0.323653    
2024-05-09 12:35:59,132 - Epoch: [154][  200/  217]    Overall Loss 0.001726    Objective Loss 0.001726                                        LR 0.000063    Time 0.283161    
2024-05-09 12:36:03,349 - Epoch: [154][  217/  217]    Overall Loss 0.001651    Objective Loss 0.001651    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.280402    
2024-05-09 12:36:03,859 - 

2024-05-09 12:36:03,860 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:36:34,577 - Epoch: [155][  100/  217]    Overall Loss 0.001869    Objective Loss 0.001869                                        LR 0.000063    Time 0.307030    
2024-05-09 12:37:03,466 - Epoch: [155][  200/  217]    Overall Loss 0.001611    Objective Loss 0.001611                                        LR 0.000063    Time 0.297902    
2024-05-09 12:37:07,826 - Epoch: [155][  217/  217]    Overall Loss 0.001548    Objective Loss 0.001548    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.294643    
2024-05-09 12:37:08,181 - 

2024-05-09 12:37:08,182 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:37:37,125 - Epoch: [156][  100/  217]    Overall Loss 0.001557    Objective Loss 0.001557                                        LR 0.000063    Time 0.289313    
2024-05-09 12:38:02,845 - Epoch: [156][  200/  217]    Overall Loss 0.001659    Objective Loss 0.001659                                        LR 0.000063    Time 0.273192    
2024-05-09 12:38:06,897 - Epoch: [156][  217/  217]    Overall Loss 0.001699    Objective Loss 0.001699    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.270454    
2024-05-09 12:38:07,394 - 

2024-05-09 12:38:07,395 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:38:37,792 - Epoch: [157][  100/  217]    Overall Loss 0.001544    Objective Loss 0.001544                                        LR 0.000063    Time 0.303854    
2024-05-09 12:39:02,773 - Epoch: [157][  200/  217]    Overall Loss 0.001476    Objective Loss 0.001476                                        LR 0.000063    Time 0.276772    
2024-05-09 12:39:06,237 - Epoch: [157][  217/  217]    Overall Loss 0.001509    Objective Loss 0.001509    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.271039    
2024-05-09 12:39:06,571 - 

2024-05-09 12:39:06,572 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:39:35,703 - Epoch: [158][  100/  217]    Overall Loss 0.000971    Objective Loss 0.000971                                        LR 0.000063    Time 0.291194    
2024-05-09 12:40:02,909 - Epoch: [158][  200/  217]    Overall Loss 0.001242    Objective Loss 0.001242                                        LR 0.000063    Time 0.281567    
2024-05-09 12:40:07,140 - Epoch: [158][  217/  217]    Overall Loss 0.001400    Objective Loss 0.001400    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.278992    
2024-05-09 12:40:07,604 - 

2024-05-09 12:40:07,605 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:40:36,881 - Epoch: [159][  100/  217]    Overall Loss 0.001952    Objective Loss 0.001952                                        LR 0.000063    Time 0.292646    
2024-05-09 12:41:02,165 - Epoch: [159][  200/  217]    Overall Loss 0.001508    Objective Loss 0.001508                                        LR 0.000063    Time 0.272689    
2024-05-09 12:41:05,857 - Epoch: [159][  217/  217]    Overall Loss 0.001559    Objective Loss 0.001559    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.268321    
2024-05-09 12:41:06,236 - 

2024-05-09 12:41:06,236 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:41:36,043 - Epoch: [160][  100/  217]    Overall Loss 0.001392    Objective Loss 0.001392                                        LR 0.000063    Time 0.297939    
2024-05-09 12:42:01,864 - Epoch: [160][  200/  217]    Overall Loss 0.001357    Objective Loss 0.001357                                        LR 0.000063    Time 0.278023    
2024-05-09 12:42:06,753 - Epoch: [160][  217/  217]    Overall Loss 0.001297    Objective Loss 0.001297    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.278764    
2024-05-09 12:42:07,129 - --- validate (epoch=160)-----------
2024-05-09 12:42:07,131 - 1736 samples (32 per mini-batch)
2024-05-09 12:42:23,880 - Epoch: [160][   55/   55]    Loss 3.300592    Top1 55.645161    Top5 71.428571    
2024-05-09 12:42:24,475 - ==> Top1: 55.645    Top5: 71.429    Loss: 3.301

2024-05-09 12:42:24,485 - ==> Best [Top1: 56.279   Top5: 71.832   Sparsity:0.00   Params: 382352 on epoch: 110]
2024-05-09 12:42:24,485 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 12:42:24,535 - 

2024-05-09 12:42:24,536 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:42:58,195 - Epoch: [161][  100/  217]    Overall Loss 0.001679    Objective Loss 0.001679                                        LR 0.000063    Time 0.336454    
2024-05-09 12:43:22,627 - Epoch: [161][  200/  217]    Overall Loss 0.001446    Objective Loss 0.001446                                        LR 0.000063    Time 0.290328    
2024-05-09 12:43:27,826 - Epoch: [161][  217/  217]    Overall Loss 0.001372    Objective Loss 0.001372    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.291531    
2024-05-09 12:43:28,715 - 

2024-05-09 12:43:28,716 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:43:58,907 - Epoch: [162][  100/  217]    Overall Loss 0.000968    Objective Loss 0.000968                                        LR 0.000063    Time 0.301797    
2024-05-09 12:44:22,671 - Epoch: [162][  200/  217]    Overall Loss 0.001196    Objective Loss 0.001196                                        LR 0.000063    Time 0.269662    
2024-05-09 12:44:27,656 - Epoch: [162][  217/  217]    Overall Loss 0.001268    Objective Loss 0.001268    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.271497    
2024-05-09 12:44:28,054 - 

2024-05-09 12:44:28,056 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:44:56,523 - Epoch: [163][  100/  217]    Overall Loss 0.001084    Objective Loss 0.001084                                        LR 0.000063    Time 0.284538    
2024-05-09 12:45:22,466 - Epoch: [163][  200/  217]    Overall Loss 0.001399    Objective Loss 0.001399                                        LR 0.000063    Time 0.271924    
2024-05-09 12:45:29,230 - Epoch: [163][  217/  217]    Overall Loss 0.001340    Objective Loss 0.001340    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.281784    
2024-05-09 12:45:29,586 - 

2024-05-09 12:45:29,587 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:46:00,300 - Epoch: [164][  100/  217]    Overall Loss 0.001191    Objective Loss 0.001191                                        LR 0.000063    Time 0.307002    
2024-05-09 12:46:24,638 - Epoch: [164][  200/  217]    Overall Loss 0.001116    Objective Loss 0.001116                                        LR 0.000063    Time 0.275132    
2024-05-09 12:46:28,318 - Epoch: [164][  217/  217]    Overall Loss 0.001200    Objective Loss 0.001200    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.270503    
2024-05-09 12:46:28,827 - 

2024-05-09 12:46:28,828 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:46:59,529 - Epoch: [165][  100/  217]    Overall Loss 0.001007    Objective Loss 0.001007                                        LR 0.000063    Time 0.306874    
2024-05-09 12:47:26,406 - Epoch: [165][  200/  217]    Overall Loss 0.001230    Objective Loss 0.001230                                        LR 0.000063    Time 0.287763    
2024-05-09 12:47:31,617 - Epoch: [165][  217/  217]    Overall Loss 0.001199    Objective Loss 0.001199    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.289221    
2024-05-09 12:47:32,064 - 

2024-05-09 12:47:32,065 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:48:01,773 - Epoch: [166][  100/  217]    Overall Loss 0.000765    Objective Loss 0.000765                                        LR 0.000063    Time 0.296955    
2024-05-09 12:48:27,153 - Epoch: [166][  200/  217]    Overall Loss 0.001231    Objective Loss 0.001231                                        LR 0.000063    Time 0.275318    
2024-05-09 12:48:31,839 - Epoch: [166][  217/  217]    Overall Loss 0.001172    Objective Loss 0.001172    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.275336    
2024-05-09 12:48:32,488 - 

2024-05-09 12:48:32,489 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:49:02,936 - Epoch: [167][  100/  217]    Overall Loss 0.000762    Objective Loss 0.000762                                        LR 0.000063    Time 0.304320    
2024-05-09 12:49:27,093 - Epoch: [167][  200/  217]    Overall Loss 0.001222    Objective Loss 0.001222                                        LR 0.000063    Time 0.272891    
2024-05-09 12:49:32,121 - Epoch: [167][  217/  217]    Overall Loss 0.001167    Objective Loss 0.001167    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.274670    
2024-05-09 12:49:32,814 - 

2024-05-09 12:49:32,815 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:50:01,473 - Epoch: [168][  100/  217]    Overall Loss 0.001166    Objective Loss 0.001166                                        LR 0.000063    Time 0.286451    
2024-05-09 12:50:29,086 - Epoch: [168][  200/  217]    Overall Loss 0.001208    Objective Loss 0.001208                                        LR 0.000063    Time 0.281231    
2024-05-09 12:50:33,654 - Epoch: [168][  217/  217]    Overall Loss 0.001147    Objective Loss 0.001147    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.280239    
2024-05-09 12:50:33,985 - 

2024-05-09 12:50:33,986 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:51:07,290 - Epoch: [169][  100/  217]    Overall Loss 0.000841    Objective Loss 0.000841                                        LR 0.000063    Time 0.332906    
2024-05-09 12:51:28,850 - Epoch: [169][  200/  217]    Overall Loss 0.001339    Objective Loss 0.001339                                        LR 0.000063    Time 0.274201    
2024-05-09 12:51:35,725 - Epoch: [169][  217/  217]    Overall Loss 0.001293    Objective Loss 0.001293    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.284388    
2024-05-09 12:51:36,158 - 

2024-05-09 12:51:36,159 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:52:05,928 - Epoch: [170][  100/  217]    Overall Loss 0.001502    Objective Loss 0.001502                                        LR 0.000063    Time 0.297559    
2024-05-09 12:52:32,969 - Epoch: [170][  200/  217]    Overall Loss 0.001338    Objective Loss 0.001338                                        LR 0.000063    Time 0.283915    
2024-05-09 12:52:36,406 - Epoch: [170][  217/  217]    Overall Loss 0.001376    Objective Loss 0.001376    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.277499    
2024-05-09 12:52:36,883 - --- validate (epoch=170)-----------
2024-05-09 12:52:36,884 - 1736 samples (32 per mini-batch)
2024-05-09 12:52:52,818 - Epoch: [170][   55/   55]    Loss 3.315805    Top1 55.760369    Top5 71.428571    
2024-05-09 12:52:53,311 - ==> Top1: 55.760    Top5: 71.429    Loss: 3.316

2024-05-09 12:52:53,317 - ==> Best [Top1: 56.279   Top5: 71.832   Sparsity:0.00   Params: 382352 on epoch: 110]
2024-05-09 12:52:53,318 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 12:52:53,365 - 

2024-05-09 12:52:53,365 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:53:25,301 - Epoch: [171][  100/  217]    Overall Loss 0.000631    Objective Loss 0.000631                                        LR 0.000063    Time 0.319237    
2024-05-09 12:53:48,481 - Epoch: [171][  200/  217]    Overall Loss 0.001010    Objective Loss 0.001010                                        LR 0.000063    Time 0.275458    
2024-05-09 12:53:53,165 - Epoch: [171][  217/  217]    Overall Loss 0.001078    Objective Loss 0.001078    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.275452    
2024-05-09 12:53:53,496 - 

2024-05-09 12:53:53,496 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:54:22,050 - Epoch: [172][  100/  217]    Overall Loss 0.000950    Objective Loss 0.000950                                        LR 0.000063    Time 0.285422    
2024-05-09 12:54:49,130 - Epoch: [172][  200/  217]    Overall Loss 0.001178    Objective Loss 0.001178                                        LR 0.000063    Time 0.278050    
2024-05-09 12:54:53,482 - Epoch: [172][  217/  217]    Overall Loss 0.001129    Objective Loss 0.001129    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.276308    
2024-05-09 12:54:53,836 - 

2024-05-09 12:54:53,836 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:55:24,463 - Epoch: [173][  100/  217]    Overall Loss 0.000776    Objective Loss 0.000776                                        LR 0.000063    Time 0.306145    
2024-05-09 12:55:47,944 - Epoch: [173][  200/  217]    Overall Loss 0.001084    Objective Loss 0.001084                                        LR 0.000063    Time 0.270419    
2024-05-09 12:55:51,250 - Epoch: [173][  217/  217]    Overall Loss 0.001048    Objective Loss 0.001048    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.264457    
2024-05-09 12:55:51,730 - 

2024-05-09 12:55:51,730 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:56:20,474 - Epoch: [174][  100/  217]    Overall Loss 0.000806    Objective Loss 0.000806                                        LR 0.000063    Time 0.287312    
2024-05-09 12:56:47,009 - Epoch: [174][  200/  217]    Overall Loss 0.001094    Objective Loss 0.001094                                        LR 0.000063    Time 0.276271    
2024-05-09 12:56:51,147 - Epoch: [174][  217/  217]    Overall Loss 0.001051    Objective Loss 0.001051    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.273688    
2024-05-09 12:56:51,509 - 

2024-05-09 12:56:51,509 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:57:21,865 - Epoch: [175][  100/  217]    Overall Loss 0.001000    Objective Loss 0.001000                                        LR 0.000063    Time 0.303438    
2024-05-09 12:57:45,718 - Epoch: [175][  200/  217]    Overall Loss 0.000930    Objective Loss 0.000930                                        LR 0.000063    Time 0.270927    
2024-05-09 12:57:49,199 - Epoch: [175][  217/  217]    Overall Loss 0.000999    Objective Loss 0.000999    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.265733    
2024-05-09 12:57:49,659 - 

2024-05-09 12:57:49,660 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:58:18,816 - Epoch: [176][  100/  217]    Overall Loss 0.001422    Objective Loss 0.001422                                        LR 0.000063    Time 0.291444    
2024-05-09 12:58:45,658 - Epoch: [176][  200/  217]    Overall Loss 0.001172    Objective Loss 0.001172                                        LR 0.000063    Time 0.279870    
2024-05-09 12:58:49,518 - Epoch: [176][  217/  217]    Overall Loss 0.001143    Objective Loss 0.001143    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.275721    
2024-05-09 12:58:49,918 - 

2024-05-09 12:58:49,919 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 12:59:20,172 - Epoch: [177][  100/  217]    Overall Loss 0.001266    Objective Loss 0.001266                                        LR 0.000063    Time 0.302410    
2024-05-09 12:59:44,673 - Epoch: [177][  200/  217]    Overall Loss 0.001057    Objective Loss 0.001057                                        LR 0.000063    Time 0.273650    
2024-05-09 12:59:48,272 - Epoch: [177][  217/  217]    Overall Loss 0.000999    Objective Loss 0.000999    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.268785    
2024-05-09 12:59:48,749 - 

2024-05-09 12:59:48,750 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:00:16,907 - Epoch: [178][  100/  217]    Overall Loss 0.000778    Objective Loss 0.000778                                        LR 0.000063    Time 0.281439    
2024-05-09 13:00:44,078 - Epoch: [178][  200/  217]    Overall Loss 0.000912    Objective Loss 0.000912                                        LR 0.000063    Time 0.276506    
2024-05-09 13:00:47,579 - Epoch: [178][  217/  217]    Overall Loss 0.000991    Objective Loss 0.000991    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.270963    
2024-05-09 13:00:48,215 - 

2024-05-09 13:00:48,216 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:01:17,915 - Epoch: [179][  100/  217]    Overall Loss 0.000480    Objective Loss 0.000480                                        LR 0.000063    Time 0.296872    
2024-05-09 13:01:42,524 - Epoch: [179][  200/  217]    Overall Loss 0.000909    Objective Loss 0.000909                                        LR 0.000063    Time 0.271394    
2024-05-09 13:01:46,921 - Epoch: [179][  217/  217]    Overall Loss 0.001076    Objective Loss 0.001076    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.270382    
2024-05-09 13:01:47,857 - 

2024-05-09 13:01:47,859 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:02:12,899 - Epoch: [180][  100/  217]    Overall Loss 0.001361    Objective Loss 0.001361                                        LR 0.000063    Time 0.250277    
2024-05-09 13:02:42,207 - Epoch: [180][  200/  217]    Overall Loss 0.001075    Objective Loss 0.001075                                        LR 0.000063    Time 0.271616    
2024-05-09 13:02:46,708 - Epoch: [180][  217/  217]    Overall Loss 0.001018    Objective Loss 0.001018    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.271066    
2024-05-09 13:02:47,146 - --- validate (epoch=180)-----------
2024-05-09 13:02:47,148 - 1736 samples (32 per mini-batch)
2024-05-09 13:03:03,312 - Epoch: [180][   55/   55]    Loss 3.318340    Top1 55.529954    Top5 71.716590    
2024-05-09 13:03:03,602 - ==> Top1: 55.530    Top5: 71.717    Loss: 3.318

2024-05-09 13:03:03,607 - ==> Best [Top1: 56.279   Top5: 71.832   Sparsity:0.00   Params: 382352 on epoch: 110]
2024-05-09 13:03:03,607 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 13:03:03,644 - 

2024-05-09 13:03:03,645 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:03:36,368 - Epoch: [181][  100/  217]    Overall Loss 0.001069    Objective Loss 0.001069                                        LR 0.000063    Time 0.327096    
2024-05-09 13:04:00,582 - Epoch: [181][  200/  217]    Overall Loss 0.000873    Objective Loss 0.000873                                        LR 0.000063    Time 0.284558    
2024-05-09 13:04:03,827 - Epoch: [181][  217/  217]    Overall Loss 0.001034    Objective Loss 0.001034    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.277206    
2024-05-09 13:04:04,435 - 

2024-05-09 13:04:04,435 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:04:32,403 - Epoch: [182][  100/  217]    Overall Loss 0.001074    Objective Loss 0.001074                                        LR 0.000063    Time 0.279337    
2024-05-09 13:04:57,420 - Epoch: [182][  200/  217]    Overall Loss 0.001055    Objective Loss 0.001055                                        LR 0.000063    Time 0.264695    
2024-05-09 13:05:00,734 - Epoch: [182][  217/  217]    Overall Loss 0.000994    Objective Loss 0.000994    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.259219    
2024-05-09 13:05:01,170 - 

2024-05-09 13:05:01,170 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:05:31,806 - Epoch: [183][  100/  217]    Overall Loss 0.000996    Objective Loss 0.000996                                        LR 0.000063    Time 0.306213    
2024-05-09 13:06:01,947 - Epoch: [183][  200/  217]    Overall Loss 0.000971    Objective Loss 0.000971                                        LR 0.000063    Time 0.303758    
2024-05-09 13:06:05,021 - Epoch: [183][  217/  217]    Overall Loss 0.001032    Objective Loss 0.001032    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.294116    
2024-05-09 13:06:05,377 - 

2024-05-09 13:06:05,379 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:06:32,731 - Epoch: [184][  100/  217]    Overall Loss 0.001422    Objective Loss 0.001422                                        LR 0.000063    Time 0.273387    
2024-05-09 13:06:57,715 - Epoch: [184][  200/  217]    Overall Loss 0.001007    Objective Loss 0.001007                                        LR 0.000063    Time 0.261553    
2024-05-09 13:07:01,770 - Epoch: [184][  217/  217]    Overall Loss 0.000950    Objective Loss 0.000950    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.259743    
2024-05-09 13:07:02,110 - 

2024-05-09 13:07:02,111 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:07:31,682 - Epoch: [185][  100/  217]    Overall Loss 0.000569    Objective Loss 0.000569                                        LR 0.000063    Time 0.295573    
2024-05-09 13:07:57,162 - Epoch: [185][  200/  217]    Overall Loss 0.000882    Objective Loss 0.000882                                        LR 0.000063    Time 0.275128    
2024-05-09 13:08:01,763 - Epoch: [185][  217/  217]    Overall Loss 0.000926    Objective Loss 0.000926    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.274766    
2024-05-09 13:08:02,113 - 

2024-05-09 13:08:02,113 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:08:29,320 - Epoch: [186][  100/  217]    Overall Loss 0.000825    Objective Loss 0.000825                                        LR 0.000063    Time 0.271945    
2024-05-09 13:08:56,985 - Epoch: [186][  200/  217]    Overall Loss 0.001146    Objective Loss 0.001146                                        LR 0.000063    Time 0.274242    
2024-05-09 13:09:00,644 - Epoch: [186][  217/  217]    Overall Loss 0.001080    Objective Loss 0.001080    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.269609    
2024-05-09 13:09:01,117 - 

2024-05-09 13:09:01,117 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:09:31,505 - Epoch: [187][  100/  217]    Overall Loss 0.000553    Objective Loss 0.000553                                        LR 0.000063    Time 0.303628    
2024-05-09 13:09:57,592 - Epoch: [187][  200/  217]    Overall Loss 0.000892    Objective Loss 0.000892                                        LR 0.000063    Time 0.282192    
2024-05-09 13:10:00,538 - Epoch: [187][  217/  217]    Overall Loss 0.000964    Objective Loss 0.000964    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.273652    
2024-05-09 13:10:00,892 - 

2024-05-09 13:10:00,893 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:10:30,159 - Epoch: [188][  100/  217]    Overall Loss 0.000964    Objective Loss 0.000964                                        LR 0.000063    Time 0.292541    
2024-05-09 13:10:54,822 - Epoch: [188][  200/  217]    Overall Loss 0.000947    Objective Loss 0.000947                                        LR 0.000063    Time 0.269527    
2024-05-09 13:10:59,018 - Epoch: [188][  217/  217]    Overall Loss 0.000893    Objective Loss 0.000893    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.267737    
2024-05-09 13:10:59,492 - 

2024-05-09 13:10:59,493 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:11:26,684 - Epoch: [189][  100/  217]    Overall Loss 0.000927    Objective Loss 0.000927                                        LR 0.000063    Time 0.271792    
2024-05-09 13:11:52,669 - Epoch: [189][  200/  217]    Overall Loss 0.000866    Objective Loss 0.000866                                        LR 0.000063    Time 0.265760    
2024-05-09 13:11:57,045 - Epoch: [189][  217/  217]    Overall Loss 0.000944    Objective Loss 0.000944    Top1 98.360656    Top5 100.000000    LR 0.000063    Time 0.265093    
2024-05-09 13:11:57,826 - 

2024-05-09 13:11:57,827 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:12:27,882 - Epoch: [190][  100/  217]    Overall Loss 0.001193    Objective Loss 0.001193                                        LR 0.000063    Time 0.300396    
2024-05-09 13:12:51,898 - Epoch: [190][  200/  217]    Overall Loss 0.001022    Objective Loss 0.001022                                        LR 0.000063    Time 0.270216    
2024-05-09 13:12:55,954 - Epoch: [190][  217/  217]    Overall Loss 0.001055    Objective Loss 0.001055    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.267726    
2024-05-09 13:12:56,295 - --- validate (epoch=190)-----------
2024-05-09 13:12:56,296 - 1736 samples (32 per mini-batch)
2024-05-09 13:13:14,383 - Epoch: [190][   55/   55]    Loss 3.324104    Top1 55.817972    Top5 72.062212    
2024-05-09 13:13:14,806 - ==> Top1: 55.818    Top5: 72.062    Loss: 3.324

2024-05-09 13:13:14,811 - ==> Best [Top1: 56.279   Top5: 71.832   Sparsity:0.00   Params: 382352 on epoch: 110]
2024-05-09 13:13:14,811 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 13:13:14,846 - 

2024-05-09 13:13:14,847 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:13:45,357 - Epoch: [191][  100/  217]    Overall Loss 0.000677    Objective Loss 0.000677                                        LR 0.000063    Time 0.304973    
2024-05-09 13:14:09,705 - Epoch: [191][  200/  217]    Overall Loss 0.001089    Objective Loss 0.001089                                        LR 0.000063    Time 0.274167    
2024-05-09 13:14:14,663 - Epoch: [191][  217/  217]    Overall Loss 0.001022    Objective Loss 0.001022    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.275526    
2024-05-09 13:14:15,183 - 

2024-05-09 13:14:15,184 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:14:47,389 - Epoch: [192][  100/  217]    Overall Loss 0.001612    Objective Loss 0.001612                                        LR 0.000063    Time 0.321910    
2024-05-09 13:15:13,325 - Epoch: [192][  200/  217]    Overall Loss 0.001062    Objective Loss 0.001062                                        LR 0.000063    Time 0.290579    
2024-05-09 13:15:16,584 - Epoch: [192][  217/  217]    Overall Loss 0.001032    Objective Loss 0.001032    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.282822    
2024-05-09 13:15:17,156 - 

2024-05-09 13:15:17,157 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:15:48,657 - Epoch: [193][  100/  217]    Overall Loss 0.000832    Objective Loss 0.000832                                        LR 0.000063    Time 0.314876    
2024-05-09 13:16:11,100 - Epoch: [193][  200/  217]    Overall Loss 0.000778    Objective Loss 0.000778                                        LR 0.000063    Time 0.269599    
2024-05-09 13:16:14,550 - Epoch: [193][  217/  217]    Overall Loss 0.000998    Objective Loss 0.000998    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.264364    
2024-05-09 13:16:14,899 - 

2024-05-09 13:16:14,899 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:16:43,585 - Epoch: [194][  100/  217]    Overall Loss 0.000848    Objective Loss 0.000848                                        LR 0.000063    Time 0.286742    
2024-05-09 13:17:06,061 - Epoch: [194][  200/  217]    Overall Loss 0.000934    Objective Loss 0.000934                                        LR 0.000063    Time 0.255691    
2024-05-09 13:17:09,052 - Epoch: [194][  217/  217]    Overall Loss 0.000878    Objective Loss 0.000878    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.249434    
2024-05-09 13:17:09,557 - 

2024-05-09 13:17:09,558 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:17:39,092 - Epoch: [195][  100/  217]    Overall Loss 0.000960    Objective Loss 0.000960                                        LR 0.000063    Time 0.295225    
2024-05-09 13:18:03,387 - Epoch: [195][  200/  217]    Overall Loss 0.000927    Objective Loss 0.000927                                        LR 0.000063    Time 0.269028    
2024-05-09 13:18:08,456 - Epoch: [195][  217/  217]    Overall Loss 0.000874    Objective Loss 0.000874    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.271302    
2024-05-09 13:18:08,703 - 

2024-05-09 13:18:08,704 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:18:42,282 - Epoch: [196][  100/  217]    Overall Loss 0.001233    Objective Loss 0.001233                                        LR 0.000063    Time 0.335649    
2024-05-09 13:19:04,605 - Epoch: [196][  200/  217]    Overall Loss 0.000955    Objective Loss 0.000955                                        LR 0.000063    Time 0.279390    
2024-05-09 13:19:08,827 - Epoch: [196][  217/  217]    Overall Loss 0.000985    Objective Loss 0.000985    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.276944    
2024-05-09 13:19:09,345 - 

2024-05-09 13:19:09,346 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:19:39,528 - Epoch: [197][  100/  217]    Overall Loss 0.001264    Objective Loss 0.001264                                        LR 0.000063    Time 0.301689    
2024-05-09 13:20:01,730 - Epoch: [197][  200/  217]    Overall Loss 0.001035    Objective Loss 0.001035                                        LR 0.000063    Time 0.261801    
2024-05-09 13:20:06,891 - Epoch: [197][  217/  217]    Overall Loss 0.000976    Objective Loss 0.000976    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.265061    
2024-05-09 13:20:07,248 - 

2024-05-09 13:20:07,249 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:20:37,232 - Epoch: [198][  100/  217]    Overall Loss 0.001483    Objective Loss 0.001483                                        LR 0.000063    Time 0.299703    
2024-05-09 13:21:01,147 - Epoch: [198][  200/  217]    Overall Loss 0.000940    Objective Loss 0.000940                                        LR 0.000063    Time 0.269371    
2024-05-09 13:21:04,297 - Epoch: [198][  217/  217]    Overall Loss 0.000923    Objective Loss 0.000923    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.262770    
2024-05-09 13:21:04,654 - 

2024-05-09 13:21:04,655 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:21:37,117 - Epoch: [199][  100/  217]    Overall Loss 0.000382    Objective Loss 0.000382                                        LR 0.000063    Time 0.324503    
2024-05-09 13:22:03,331 - Epoch: [199][  200/  217]    Overall Loss 0.000879    Objective Loss 0.000879                                        LR 0.000063    Time 0.293263    
2024-05-09 13:22:07,230 - Epoch: [199][  217/  217]    Overall Loss 0.000945    Objective Loss 0.000945    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.288246    
2024-05-09 13:22:07,854 - 

2024-05-09 13:22:07,855 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:22:39,546 - Epoch: [200][  100/  217]    Overall Loss 0.001130    Objective Loss 0.001130                                        LR 0.000016    Time 0.316796    
2024-05-09 13:23:01,488 - Epoch: [200][  200/  217]    Overall Loss 0.000882    Objective Loss 0.000882                                        LR 0.000016    Time 0.268047    
2024-05-09 13:23:08,372 - Epoch: [200][  217/  217]    Overall Loss 0.000825    Objective Loss 0.000825    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.278755    
2024-05-09 13:23:09,018 - --- validate (epoch=200)-----------
2024-05-09 13:23:09,019 - 1736 samples (32 per mini-batch)
2024-05-09 13:23:27,698 - Epoch: [200][   55/   55]    Loss 3.389829    Top1 55.529954    Top5 71.486175    
2024-05-09 13:23:28,250 - ==> Top1: 55.530    Top5: 71.486    Loss: 3.390

2024-05-09 13:23:28,256 - ==> Best [Top1: 56.279   Top5: 71.832   Sparsity:0.00   Params: 382352 on epoch: 110]
2024-05-09 13:23:28,256 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 13:23:28,306 - 

2024-05-09 13:23:28,307 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:23:56,636 - Epoch: [201][  100/  217]    Overall Loss 0.000675    Objective Loss 0.000675                                        LR 0.000016    Time 0.283157    
2024-05-09 13:24:23,385 - Epoch: [201][  200/  217]    Overall Loss 0.000726    Objective Loss 0.000726                                        LR 0.000016    Time 0.275257    
2024-05-09 13:24:26,217 - Epoch: [201][  217/  217]    Overall Loss 0.000838    Objective Loss 0.000838    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.266729    
2024-05-09 13:24:26,790 - 

2024-05-09 13:24:26,791 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:24:58,633 - Epoch: [202][  100/  217]    Overall Loss 0.001043    Objective Loss 0.001043                                        LR 0.000016    Time 0.318278    
2024-05-09 13:25:23,801 - Epoch: [202][  200/  217]    Overall Loss 0.000824    Objective Loss 0.000824                                        LR 0.000016    Time 0.284915    
2024-05-09 13:25:27,563 - Epoch: [202][  217/  217]    Overall Loss 0.000815    Objective Loss 0.000815    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.279921    
2024-05-09 13:25:27,825 - 

2024-05-09 13:25:27,826 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:25:56,451 - Epoch: [203][  100/  217]    Overall Loss 0.001175    Objective Loss 0.001175                                        LR 0.000016    Time 0.286116    
2024-05-09 13:26:22,105 - Epoch: [203][  200/  217]    Overall Loss 0.000924    Objective Loss 0.000924                                        LR 0.000016    Time 0.271262    
2024-05-09 13:26:27,376 - Epoch: [203][  217/  217]    Overall Loss 0.000870    Objective Loss 0.000870    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.274287    
2024-05-09 13:26:27,722 - 

2024-05-09 13:26:27,723 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:26:57,590 - Epoch: [204][  100/  217]    Overall Loss 0.000634    Objective Loss 0.000634                                        LR 0.000016    Time 0.298527    
2024-05-09 13:27:23,455 - Epoch: [204][  200/  217]    Overall Loss 0.000777    Objective Loss 0.000777                                        LR 0.000016    Time 0.278517    
2024-05-09 13:27:28,569 - Epoch: [204][  217/  217]    Overall Loss 0.000846    Objective Loss 0.000846    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.280253    
2024-05-09 13:27:29,053 - 

2024-05-09 13:27:29,053 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:27:59,045 - Epoch: [205][  100/  217]    Overall Loss 0.000162    Objective Loss 0.000162                                        LR 0.000016    Time 0.299777    
2024-05-09 13:28:27,986 - Epoch: [205][  200/  217]    Overall Loss 0.000680    Objective Loss 0.000680                                        LR 0.000016    Time 0.294536    
2024-05-09 13:28:32,159 - Epoch: [205][  217/  217]    Overall Loss 0.000789    Objective Loss 0.000789    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.290684    
2024-05-09 13:28:32,668 - 

2024-05-09 13:28:32,668 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:29:00,614 - Epoch: [206][  100/  217]    Overall Loss 0.000464    Objective Loss 0.000464                                        LR 0.000016    Time 0.279324    
2024-05-09 13:29:27,712 - Epoch: [206][  200/  217]    Overall Loss 0.000837    Objective Loss 0.000837                                        LR 0.000016    Time 0.275085    
2024-05-09 13:29:32,698 - Epoch: [206][  217/  217]    Overall Loss 0.000786    Objective Loss 0.000786    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.276505    
2024-05-09 13:29:33,985 - 

2024-05-09 13:29:33,986 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:30:02,299 - Epoch: [207][  100/  217]    Overall Loss 0.001052    Objective Loss 0.001052                                        LR 0.000016    Time 0.282978    
2024-05-09 13:30:29,840 - Epoch: [207][  200/  217]    Overall Loss 0.000810    Objective Loss 0.000810                                        LR 0.000016    Time 0.279074    
2024-05-09 13:30:32,918 - Epoch: [207][  217/  217]    Overall Loss 0.000759    Objective Loss 0.000759    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.271380    
2024-05-09 13:30:33,315 - 

2024-05-09 13:30:33,315 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:31:04,978 - Epoch: [208][  100/  217]    Overall Loss 0.001144    Objective Loss 0.001144                                        LR 0.000016    Time 0.316503    
2024-05-09 13:31:33,370 - Epoch: [208][  200/  217]    Overall Loss 0.000891    Objective Loss 0.000891                                        LR 0.000016    Time 0.300149    
2024-05-09 13:31:36,624 - Epoch: [208][  217/  217]    Overall Loss 0.000835    Objective Loss 0.000835    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.291620    
2024-05-09 13:31:36,962 - 

2024-05-09 13:31:36,963 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:32:09,055 - Epoch: [209][  100/  217]    Overall Loss 0.000565    Objective Loss 0.000565                                        LR 0.000016    Time 0.320797    
2024-05-09 13:32:37,182 - Epoch: [209][  200/  217]    Overall Loss 0.000845    Objective Loss 0.000845                                        LR 0.000016    Time 0.300972    
2024-05-09 13:32:42,440 - Epoch: [209][  217/  217]    Overall Loss 0.000790    Objective Loss 0.000790    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.301613    
2024-05-09 13:32:42,762 - 

2024-05-09 13:32:42,764 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:33:13,427 - Epoch: [210][  100/  217]    Overall Loss 0.000432    Objective Loss 0.000432                                        LR 0.000016    Time 0.306471    
2024-05-09 13:33:37,203 - Epoch: [210][  200/  217]    Overall Loss 0.000862    Objective Loss 0.000862                                        LR 0.000016    Time 0.272053    
2024-05-09 13:33:44,159 - Epoch: [210][  217/  217]    Overall Loss 0.000810    Objective Loss 0.000810    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.282782    
2024-05-09 13:33:44,488 - --- validate (epoch=210)-----------
2024-05-09 13:33:44,489 - 1736 samples (32 per mini-batch)
2024-05-09 13:34:04,578 - Epoch: [210][   55/   55]    Loss 3.397061    Top1 55.299539    Top5 71.947005    
2024-05-09 13:34:04,987 - ==> Top1: 55.300    Top5: 71.947    Loss: 3.397

2024-05-09 13:34:04,994 - ==> Best [Top1: 56.279   Top5: 71.832   Sparsity:0.00   Params: 382352 on epoch: 110]
2024-05-09 13:34:04,995 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 13:34:05,053 - 

2024-05-09 13:34:05,054 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:34:34,900 - Epoch: [211][  100/  217]    Overall Loss 0.000725    Objective Loss 0.000725                                        LR 0.000016    Time 0.298333    
2024-05-09 13:35:00,567 - Epoch: [211][  200/  217]    Overall Loss 0.000873    Objective Loss 0.000873                                        LR 0.000016    Time 0.277439    
2024-05-09 13:35:04,478 - Epoch: [211][  217/  217]    Overall Loss 0.000816    Objective Loss 0.000816    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.273714    
2024-05-09 13:35:04,885 - 

2024-05-09 13:35:04,886 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:35:33,499 - Epoch: [212][  100/  217]    Overall Loss 0.000783    Objective Loss 0.000783                                        LR 0.000016    Time 0.285997    
2024-05-09 13:36:00,527 - Epoch: [212][  200/  217]    Overall Loss 0.000691    Objective Loss 0.000691                                        LR 0.000016    Time 0.278081    
2024-05-09 13:36:03,491 - Epoch: [212][  217/  217]    Overall Loss 0.000844    Objective Loss 0.000844    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.269944    
2024-05-09 13:36:04,528 - 

2024-05-09 13:36:04,529 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:36:33,771 - Epoch: [213][  100/  217]    Overall Loss 0.001261    Objective Loss 0.001261                                        LR 0.000016    Time 0.292297    
2024-05-09 13:36:59,054 - Epoch: [213][  200/  217]    Overall Loss 0.000709    Objective Loss 0.000709                                        LR 0.000016    Time 0.272508    
2024-05-09 13:37:03,656 - Epoch: [213][  217/  217]    Overall Loss 0.000792    Objective Loss 0.000792    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.272352    
2024-05-09 13:37:03,936 - 

2024-05-09 13:37:03,937 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:37:31,902 - Epoch: [214][  100/  217]    Overall Loss 0.000818    Objective Loss 0.000818                                        LR 0.000016    Time 0.279514    
2024-05-09 13:37:58,247 - Epoch: [214][  200/  217]    Overall Loss 0.000875    Objective Loss 0.000875                                        LR 0.000016    Time 0.271385    
2024-05-09 13:38:01,425 - Epoch: [214][  217/  217]    Overall Loss 0.000817    Objective Loss 0.000817    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.264752    
2024-05-09 13:38:01,871 - 

2024-05-09 13:38:01,872 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:38:33,858 - Epoch: [215][  100/  217]    Overall Loss 0.000822    Objective Loss 0.000822                                        LR 0.000016    Time 0.319704    
2024-05-09 13:39:00,343 - Epoch: [215][  200/  217]    Overall Loss 0.000982    Objective Loss 0.000982                                        LR 0.000016    Time 0.292217    
2024-05-09 13:39:04,452 - Epoch: [215][  217/  217]    Overall Loss 0.000920    Objective Loss 0.000920    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.288244    
2024-05-09 13:39:05,189 - 

2024-05-09 13:39:05,190 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:39:35,957 - Epoch: [216][  100/  217]    Overall Loss 0.000900    Objective Loss 0.000900                                        LR 0.000016    Time 0.307534    
2024-05-09 13:40:02,081 - Epoch: [216][  200/  217]    Overall Loss 0.000871    Objective Loss 0.000871                                        LR 0.000016    Time 0.284318    
2024-05-09 13:40:07,085 - Epoch: [216][  217/  217]    Overall Loss 0.000818    Objective Loss 0.000818    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.285092    
2024-05-09 13:40:07,451 - 

2024-05-09 13:40:07,452 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:40:40,062 - Epoch: [217][  100/  217]    Overall Loss 0.001261    Objective Loss 0.001261                                        LR 0.000016    Time 0.325954    
2024-05-09 13:41:05,900 - Epoch: [217][  200/  217]    Overall Loss 0.000812    Objective Loss 0.000812                                        LR 0.000016    Time 0.292113    
2024-05-09 13:41:10,098 - Epoch: [217][  217/  217]    Overall Loss 0.000761    Objective Loss 0.000761    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.288561    
2024-05-09 13:41:10,416 - 

2024-05-09 13:41:10,417 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:41:40,372 - Epoch: [218][  100/  217]    Overall Loss 0.000844    Objective Loss 0.000844                                        LR 0.000016    Time 0.299416    
2024-05-09 13:42:05,862 - Epoch: [218][  200/  217]    Overall Loss 0.000793    Objective Loss 0.000793                                        LR 0.000016    Time 0.277097    
2024-05-09 13:42:11,621 - Epoch: [218][  217/  217]    Overall Loss 0.000827    Objective Loss 0.000827    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.281914    
2024-05-09 13:42:12,612 - 

2024-05-09 13:42:12,613 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:42:46,173 - Epoch: [219][  100/  217]    Overall Loss 0.000764    Objective Loss 0.000764                                        LR 0.000016    Time 0.335467    
2024-05-09 13:43:11,800 - Epoch: [219][  200/  217]    Overall Loss 0.000695    Objective Loss 0.000695                                        LR 0.000016    Time 0.295767    
2024-05-09 13:43:16,714 - Epoch: [219][  217/  217]    Overall Loss 0.000807    Objective Loss 0.000807    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.295228    
2024-05-09 13:43:17,323 - 

2024-05-09 13:43:17,324 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:43:48,942 - Epoch: [220][  100/  217]    Overall Loss 0.000319    Objective Loss 0.000319                                        LR 0.000016    Time 0.316050    
2024-05-09 13:44:15,434 - Epoch: [220][  200/  217]    Overall Loss 0.000661    Objective Loss 0.000661                                        LR 0.000016    Time 0.290419    
2024-05-09 13:44:19,109 - Epoch: [220][  217/  217]    Overall Loss 0.000769    Objective Loss 0.000769    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.284588    
2024-05-09 13:44:20,289 - --- validate (epoch=220)-----------
2024-05-09 13:44:20,290 - 1736 samples (32 per mini-batch)
2024-05-09 13:44:38,545 - Epoch: [220][   55/   55]    Loss 3.439359    Top1 55.587558    Top5 72.004608    
2024-05-09 13:44:39,025 - ==> Top1: 55.588    Top5: 72.005    Loss: 3.439

2024-05-09 13:44:39,033 - ==> Best [Top1: 56.279   Top5: 71.832   Sparsity:0.00   Params: 382352 on epoch: 110]
2024-05-09 13:44:39,034 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 13:44:39,075 - 

2024-05-09 13:44:39,075 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:45:09,291 - Epoch: [221][  100/  217]    Overall Loss 0.000579    Objective Loss 0.000579                                        LR 0.000016    Time 0.302027    
2024-05-09 13:45:37,960 - Epoch: [221][  200/  217]    Overall Loss 0.000664    Objective Loss 0.000664                                        LR 0.000016    Time 0.294263    
2024-05-09 13:45:41,581 - Epoch: [221][  217/  217]    Overall Loss 0.000730    Objective Loss 0.000730    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.287885    
2024-05-09 13:45:41,965 - 

2024-05-09 13:45:41,966 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:46:12,973 - Epoch: [222][  100/  217]    Overall Loss 0.000545    Objective Loss 0.000545                                        LR 0.000016    Time 0.309939    
2024-05-09 13:46:40,259 - Epoch: [222][  200/  217]    Overall Loss 0.000779    Objective Loss 0.000779                                        LR 0.000016    Time 0.291333    
2024-05-09 13:46:45,085 - Epoch: [222][  217/  217]    Overall Loss 0.000730    Objective Loss 0.000730    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.290730    
2024-05-09 13:46:45,599 - 

2024-05-09 13:46:45,600 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:47:13,713 - Epoch: [223][  100/  217]    Overall Loss 0.001315    Objective Loss 0.001315                                        LR 0.000016    Time 0.281005    
2024-05-09 13:47:39,354 - Epoch: [223][  200/  217]    Overall Loss 0.000956    Objective Loss 0.000956                                        LR 0.000016    Time 0.268650    
2024-05-09 13:47:44,591 - Epoch: [223][  217/  217]    Overall Loss 0.000895    Objective Loss 0.000895    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.271721    
2024-05-09 13:47:44,949 - 

2024-05-09 13:47:44,950 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:48:14,087 - Epoch: [224][  100/  217]    Overall Loss 0.000541    Objective Loss 0.000541                                        LR 0.000016    Time 0.291229    
2024-05-09 13:48:39,126 - Epoch: [224][  200/  217]    Overall Loss 0.000720    Objective Loss 0.000720                                        LR 0.000016    Time 0.270692    
2024-05-09 13:48:43,527 - Epoch: [224][  217/  217]    Overall Loss 0.000771    Objective Loss 0.000771    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.269751    
2024-05-09 13:48:44,002 - 

2024-05-09 13:48:44,003 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:49:15,427 - Epoch: [225][  100/  217]    Overall Loss 0.000343    Objective Loss 0.000343                                        LR 0.000016    Time 0.314104    
2024-05-09 13:49:43,908 - Epoch: [225][  200/  217]    Overall Loss 0.000859    Objective Loss 0.000859                                        LR 0.000016    Time 0.299401    
2024-05-09 13:49:47,324 - Epoch: [225][  217/  217]    Overall Loss 0.000802    Objective Loss 0.000802    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.291675    
2024-05-09 13:49:47,782 - 

2024-05-09 13:49:47,782 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:50:17,414 - Epoch: [226][  100/  217]    Overall Loss 0.001100    Objective Loss 0.001100                                        LR 0.000016    Time 0.296183    
2024-05-09 13:50:46,695 - Epoch: [226][  200/  217]    Overall Loss 0.000779    Objective Loss 0.000779                                        LR 0.000016    Time 0.294444    
2024-05-09 13:50:50,529 - Epoch: [226][  217/  217]    Overall Loss 0.000730    Objective Loss 0.000730    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.289032    
2024-05-09 13:50:50,991 - 

2024-05-09 13:50:50,992 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:51:18,719 - Epoch: [227][  100/  217]    Overall Loss 0.000550    Objective Loss 0.000550                                        LR 0.000016    Time 0.277137    
2024-05-09 13:51:43,207 - Epoch: [227][  200/  217]    Overall Loss 0.000662    Objective Loss 0.000662                                        LR 0.000016    Time 0.260945    
2024-05-09 13:51:47,834 - Epoch: [227][  217/  217]    Overall Loss 0.000740    Objective Loss 0.000740    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.261813    
2024-05-09 13:51:48,264 - 

2024-05-09 13:51:48,266 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:52:17,208 - Epoch: [228][  100/  217]    Overall Loss 0.000719    Objective Loss 0.000719                                        LR 0.000016    Time 0.289279    
2024-05-09 13:52:43,319 - Epoch: [228][  200/  217]    Overall Loss 0.000684    Objective Loss 0.000684                                        LR 0.000016    Time 0.275134    
2024-05-09 13:52:46,665 - Epoch: [228][  217/  217]    Overall Loss 0.000726    Objective Loss 0.000726    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.268989    
2024-05-09 13:52:47,595 - 

2024-05-09 13:52:47,596 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:53:20,553 - Epoch: [229][  100/  217]    Overall Loss 0.000141    Objective Loss 0.000141                                        LR 0.000016    Time 0.329439    
2024-05-09 13:53:48,061 - Epoch: [229][  200/  217]    Overall Loss 0.000658    Objective Loss 0.000658                                        LR 0.000016    Time 0.302198    
2024-05-09 13:53:51,565 - Epoch: [229][  217/  217]    Overall Loss 0.000768    Objective Loss 0.000768    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.294657    
2024-05-09 13:53:52,232 - 

2024-05-09 13:53:52,233 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:54:20,417 - Epoch: [230][  100/  217]    Overall Loss 0.000563    Objective Loss 0.000563                                        LR 0.000016    Time 0.281719    
2024-05-09 13:54:48,912 - Epoch: [230][  200/  217]    Overall Loss 0.000824    Objective Loss 0.000824                                        LR 0.000016    Time 0.283275    
2024-05-09 13:54:52,966 - Epoch: [230][  217/  217]    Overall Loss 0.000770    Objective Loss 0.000770    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.279756    
2024-05-09 13:54:53,978 - --- validate (epoch=230)-----------
2024-05-09 13:54:53,979 - 1736 samples (32 per mini-batch)
2024-05-09 13:55:11,167 - Epoch: [230][   55/   55]    Loss 3.430299    Top1 55.587558    Top5 72.062212    
2024-05-09 13:55:11,711 - ==> Top1: 55.588    Top5: 72.062    Loss: 3.430

2024-05-09 13:55:11,716 - ==> Best [Top1: 56.279   Top5: 71.832   Sparsity:0.00   Params: 382352 on epoch: 110]
2024-05-09 13:55:11,717 - Saving checkpoint to: logs/2024.05.09-095556/checkpoint.pth.tar
2024-05-09 13:55:11,763 - 

2024-05-09 13:55:11,764 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:55:39,829 - Epoch: [231][  100/  217]    Overall Loss 0.001026    Objective Loss 0.001026                                        LR 0.000016    Time 0.280516    
2024-05-09 13:56:05,699 - Epoch: [231][  200/  217]    Overall Loss 0.000706    Objective Loss 0.000706                                        LR 0.000016    Time 0.269548    
2024-05-09 13:56:11,548 - Epoch: [231][  217/  217]    Overall Loss 0.000770    Objective Loss 0.000770    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.275380    
2024-05-09 13:56:12,066 - 

2024-05-09 13:56:12,068 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:56:44,466 - Epoch: [232][  100/  217]    Overall Loss 0.000422    Objective Loss 0.000422                                        LR 0.000016    Time 0.323856    
2024-05-09 13:57:07,520 - Epoch: [232][  200/  217]    Overall Loss 0.000846    Objective Loss 0.000846                                        LR 0.000016    Time 0.277143    
2024-05-09 13:57:13,689 - Epoch: [232][  217/  217]    Overall Loss 0.000791    Objective Loss 0.000791    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.283846    
2024-05-09 13:57:14,501 - 

2024-05-09 13:57:14,502 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:57:46,679 - Epoch: [233][  100/  217]    Overall Loss 0.000782    Objective Loss 0.000782                                        LR 0.000016    Time 0.321651    
2024-05-09 13:58:12,381 - Epoch: [233][  200/  217]    Overall Loss 0.000797    Objective Loss 0.000797                                        LR 0.000016    Time 0.289273    
2024-05-09 13:58:15,699 - Epoch: [233][  217/  217]    Overall Loss 0.000743    Objective Loss 0.000743    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.281896    
2024-05-09 13:58:16,269 - 

2024-05-09 13:58:16,269 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:58:43,478 - Epoch: [234][  100/  217]    Overall Loss 0.000663    Objective Loss 0.000663                                        LR 0.000016    Time 0.271972    
2024-05-09 13:59:10,293 - Epoch: [234][  200/  217]    Overall Loss 0.000873    Objective Loss 0.000873                                        LR 0.000016    Time 0.269992    
2024-05-09 13:59:14,809 - Epoch: [234][  217/  217]    Overall Loss 0.000819    Objective Loss 0.000819    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.269639    
2024-05-09 13:59:15,418 - 

2024-05-09 13:59:15,419 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 13:59:45,445 - Epoch: [235][  100/  217]    Overall Loss 0.000519    Objective Loss 0.000519                                        LR 0.000016    Time 0.300118    
2024-05-09 14:00:10,588 - Epoch: [235][  200/  217]    Overall Loss 0.000672    Objective Loss 0.000672                                        LR 0.000016    Time 0.275709    
2024-05-09 14:00:16,432 - Epoch: [235][  217/  217]    Overall Loss 0.000744    Objective Loss 0.000744    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.281030    
2024-05-09 14:00:17,187 - 

2024-05-09 14:00:17,188 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:00:49,171 - Epoch: [236][  100/  217]    Overall Loss 0.000821    Objective Loss 0.000821                                        LR 0.000016    Time 0.319695    
2024-05-09 14:01:15,699 - Epoch: [236][  200/  217]    Overall Loss 0.000822    Objective Loss 0.000822                                        LR 0.000016    Time 0.292425    
2024-05-09 14:01:19,211 - Epoch: [236][  217/  217]    Overall Loss 0.000768    Objective Loss 0.000768    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.285689    
2024-05-09 14:01:19,890 - 

2024-05-09 14:01:19,891 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:01:54,696 - Epoch: [237][  100/  217]    Overall Loss 0.000831    Objective Loss 0.000831                                        LR 0.000016    Time 0.347914    
2024-05-09 14:02:21,278 - Epoch: [237][  200/  217]    Overall Loss 0.000825    Objective Loss 0.000825                                        LR 0.000016    Time 0.306799    
2024-05-09 14:02:24,992 - Epoch: [237][  217/  217]    Overall Loss 0.000772    Objective Loss 0.000772    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.299864    
2024-05-09 14:02:25,544 - 

2024-05-09 14:02:25,544 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:02:51,915 - Epoch: [238][  100/  217]    Overall Loss 0.000586    Objective Loss 0.000586                                        LR 0.000016    Time 0.263591    
2024-05-09 14:03:18,869 - Epoch: [238][  200/  217]    Overall Loss 0.000573    Objective Loss 0.000573                                        LR 0.000016    Time 0.266499    
2024-05-09 14:03:23,691 - Epoch: [238][  217/  217]    Overall Loss 0.000795    Objective Loss 0.000795    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.267834    
2024-05-09 14:03:24,129 - 

2024-05-09 14:03:24,130 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:03:56,747 - Epoch: [239][  100/  217]    Overall Loss 0.000378    Objective Loss 0.000378                                        LR 0.000016    Time 0.326044    
2024-05-09 14:04:23,900 - Epoch: [239][  200/  217]    Overall Loss 0.000711    Objective Loss 0.000711                                        LR 0.000016    Time 0.298726    
2024-05-09 14:04:27,910 - Epoch: [239][  217/  217]    Overall Loss 0.000776    Objective Loss 0.000776    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.293789    
2024-05-09 14:04:28,309 - 

2024-05-09 14:04:28,310 - Initiating quantization aware training (QAT)...
2024-05-09 14:04:28,394 - 

2024-05-09 14:04:28,395 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:05:01,599 - Epoch: [240][  100/  217]    Overall Loss 1.911057    Objective Loss 1.911057                                        LR 0.000016    Time 0.331915    
2024-05-09 14:05:30,320 - Epoch: [240][  200/  217]    Overall Loss 1.506338    Objective Loss 1.506338                                        LR 0.000016    Time 0.309511    
2024-05-09 14:05:35,174 - Epoch: [240][  217/  217]    Overall Loss 1.461286    Objective Loss 1.461286    Top1 72.131148    Top5 96.721311    LR 0.000016    Time 0.307623    
2024-05-09 14:05:35,801 - --- validate (epoch=240)-----------
2024-05-09 14:05:35,802 - 1736 samples (32 per mini-batch)
2024-05-09 14:05:53,617 - Epoch: [240][   55/   55]    Loss 2.218531    Top1 50.979263    Top5 67.626728    
2024-05-09 14:05:54,038 - ==> Top1: 50.979    Top5: 67.627    Loss: 2.219

2024-05-09 14:05:54,041 - ==> Best [Top1: 50.979   Top5: 67.627   Sparsity:0.00   Params: 382352 on epoch: 240]
2024-05-09 14:05:54,041 - Saving checkpoint to: logs/2024.05.09-095556/qat_checkpoint.pth.tar
2024-05-09 14:05:54,065 - 

2024-05-09 14:05:54,065 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:06:24,351 - Epoch: [241][  100/  217]    Overall Loss 0.819886    Objective Loss 0.819886                                        LR 0.000016    Time 0.302739    
2024-05-09 14:06:49,673 - Epoch: [241][  200/  217]    Overall Loss 0.772792    Objective Loss 0.772792                                        LR 0.000016    Time 0.277924    
2024-05-09 14:06:54,456 - Epoch: [241][  217/  217]    Overall Loss 0.769119    Objective Loss 0.769119    Top1 83.606557    Top5 95.081967    LR 0.000016    Time 0.278181    
2024-05-09 14:06:54,767 - 

2024-05-09 14:06:54,768 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:07:24,163 - Epoch: [242][  100/  217]    Overall Loss 0.642487    Objective Loss 0.642487                                        LR 0.000016    Time 0.293827    
2024-05-09 14:07:50,740 - Epoch: [242][  200/  217]    Overall Loss 0.635010    Objective Loss 0.635010                                        LR 0.000016    Time 0.279744    
2024-05-09 14:07:54,129 - Epoch: [242][  217/  217]    Overall Loss 0.631779    Objective Loss 0.631779    Top1 91.803279    Top5 98.360656    LR 0.000016    Time 0.273437    
2024-05-09 14:07:54,728 - 

2024-05-09 14:07:54,729 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:08:28,155 - Epoch: [243][  100/  217]    Overall Loss 0.564601    Objective Loss 0.564601                                        LR 0.000016    Time 0.334136    
2024-05-09 14:08:51,274 - Epoch: [243][  200/  217]    Overall Loss 0.551664    Objective Loss 0.551664                                        LR 0.000016    Time 0.282609    
2024-05-09 14:08:55,695 - Epoch: [243][  217/  217]    Overall Loss 0.551598    Objective Loss 0.551598    Top1 81.967213    Top5 100.000000    LR 0.000016    Time 0.280835    
2024-05-09 14:08:56,413 - 

2024-05-09 14:08:56,413 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:09:28,938 - Epoch: [244][  100/  217]    Overall Loss 0.496951    Objective Loss 0.496951                                        LR 0.000016    Time 0.325114    
2024-05-09 14:09:53,992 - Epoch: [244][  200/  217]    Overall Loss 0.485108    Objective Loss 0.485108                                        LR 0.000016    Time 0.287770    
2024-05-09 14:09:59,024 - Epoch: [244][  217/  217]    Overall Loss 0.486999    Objective Loss 0.486999    Top1 86.885246    Top5 100.000000    LR 0.000016    Time 0.288406    
2024-05-09 14:09:59,540 - 

2024-05-09 14:09:59,541 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:10:25,690 - Epoch: [245][  100/  217]    Overall Loss 0.472117    Objective Loss 0.472117                                        LR 0.000016    Time 0.261366    
2024-05-09 14:10:51,995 - Epoch: [245][  200/  217]    Overall Loss 0.459061    Objective Loss 0.459061                                        LR 0.000016    Time 0.262153    
2024-05-09 14:10:55,162 - Epoch: [245][  217/  217]    Overall Loss 0.453627    Objective Loss 0.453627    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.256196    
2024-05-09 14:10:55,465 - 

2024-05-09 14:10:55,466 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:11:29,988 - Epoch: [246][  100/  217]    Overall Loss 0.409796    Objective Loss 0.409796                                        LR 0.000016    Time 0.345102    
2024-05-09 14:11:54,566 - Epoch: [246][  200/  217]    Overall Loss 0.412172    Objective Loss 0.412172                                        LR 0.000016    Time 0.295384    
2024-05-09 14:11:59,009 - Epoch: [246][  217/  217]    Overall Loss 0.413447    Objective Loss 0.413447    Top1 91.803279    Top5 100.000000    LR 0.000016    Time 0.292708    
2024-05-09 14:11:59,504 - 

2024-05-09 14:11:59,505 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:12:30,393 - Epoch: [247][  100/  217]    Overall Loss 0.383524    Objective Loss 0.383524                                        LR 0.000016    Time 0.308749    
2024-05-09 14:12:59,061 - Epoch: [247][  200/  217]    Overall Loss 0.392576    Objective Loss 0.392576                                        LR 0.000016    Time 0.297661    
2024-05-09 14:13:06,081 - Epoch: [247][  217/  217]    Overall Loss 0.390535    Objective Loss 0.390535    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.306679    
2024-05-09 14:13:06,333 - 

2024-05-09 14:13:06,334 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:13:36,215 - Epoch: [248][  100/  217]    Overall Loss 0.357045    Objective Loss 0.357045                                        LR 0.000016    Time 0.298682    
2024-05-09 14:14:03,625 - Epoch: [248][  200/  217]    Overall Loss 0.363584    Objective Loss 0.363584                                        LR 0.000016    Time 0.286328    
2024-05-09 14:14:07,619 - Epoch: [248][  217/  217]    Overall Loss 0.365200    Objective Loss 0.365200    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.282291    
2024-05-09 14:14:07,934 - 

2024-05-09 14:14:07,935 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:14:40,475 - Epoch: [249][  100/  217]    Overall Loss 0.338574    Objective Loss 0.338574                                        LR 0.000016    Time 0.325273    
2024-05-09 14:15:02,155 - Epoch: [249][  200/  217]    Overall Loss 0.346756    Objective Loss 0.346756                                        LR 0.000016    Time 0.270975    
2024-05-09 14:15:07,218 - Epoch: [249][  217/  217]    Overall Loss 0.345973    Objective Loss 0.345973    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.273071    
2024-05-09 14:15:07,634 - 

2024-05-09 14:15:07,634 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:15:38,689 - Epoch: [250][  100/  217]    Overall Loss 0.321888    Objective Loss 0.321888                                        LR 0.000016    Time 0.310428    
2024-05-09 14:16:06,541 - Epoch: [250][  200/  217]    Overall Loss 0.324841    Objective Loss 0.324841                                        LR 0.000016    Time 0.294418    
2024-05-09 14:16:11,551 - Epoch: [250][  217/  217]    Overall Loss 0.324996    Objective Loss 0.324996    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.294426    
2024-05-09 14:16:11,931 - --- validate (epoch=250)-----------
2024-05-09 14:16:11,932 - 1736 samples (32 per mini-batch)
2024-05-09 14:16:28,691 - Epoch: [250][   55/   55]    Loss 2.167342    Top1 55.011521    Top5 71.082949    
2024-05-09 14:16:28,945 - ==> Top1: 55.012    Top5: 71.083    Loss: 2.167

2024-05-09 14:16:28,950 - ==> Best [Top1: 55.012   Top5: 71.083   Sparsity:0.00   Params: 382352 on epoch: 250]
2024-05-09 14:16:28,950 - Saving checkpoint to: logs/2024.05.09-095556/qat_checkpoint.pth.tar
2024-05-09 14:16:28,998 - 

2024-05-09 14:16:28,999 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:17:00,057 - Epoch: [251][  100/  217]    Overall Loss 0.314002    Objective Loss 0.314002                                        LR 0.000016    Time 0.310449    
2024-05-09 14:17:26,933 - Epoch: [251][  200/  217]    Overall Loss 0.305183    Objective Loss 0.305183                                        LR 0.000016    Time 0.289553    
2024-05-09 14:17:31,875 - Epoch: [251][  217/  217]    Overall Loss 0.305436    Objective Loss 0.305436    Top1 91.803279    Top5 100.000000    LR 0.000016    Time 0.289629    
2024-05-09 14:17:32,722 - 

2024-05-09 14:17:32,723 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:18:06,050 - Epoch: [252][  100/  217]    Overall Loss 0.285491    Objective Loss 0.285491                                        LR 0.000016    Time 0.333140    
2024-05-09 14:18:33,998 - Epoch: [252][  200/  217]    Overall Loss 0.291150    Objective Loss 0.291150                                        LR 0.000016    Time 0.306248    
2024-05-09 14:18:39,122 - Epoch: [252][  217/  217]    Overall Loss 0.292921    Objective Loss 0.292921    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.305859    
2024-05-09 14:18:39,727 - 

2024-05-09 14:18:39,728 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:19:12,440 - Epoch: [253][  100/  217]    Overall Loss 0.279618    Objective Loss 0.279618                                        LR 0.000016    Time 0.327002    
2024-05-09 14:19:36,391 - Epoch: [253][  200/  217]    Overall Loss 0.284872    Objective Loss 0.284872                                        LR 0.000016    Time 0.283195    
2024-05-09 14:19:39,114 - Epoch: [253][  217/  217]    Overall Loss 0.286436    Objective Loss 0.286436    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.273549    
2024-05-09 14:19:39,510 - 

2024-05-09 14:19:39,510 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:20:11,970 - Epoch: [254][  100/  217]    Overall Loss 0.258281    Objective Loss 0.258281                                        LR 0.000016    Time 0.324477    
2024-05-09 14:20:37,727 - Epoch: [254][  200/  217]    Overall Loss 0.270416    Objective Loss 0.270416                                        LR 0.000016    Time 0.290968    
2024-05-09 14:20:40,582 - Epoch: [254][  217/  217]    Overall Loss 0.271054    Objective Loss 0.271054    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.281320    
2024-05-09 14:20:40,976 - 

2024-05-09 14:20:40,977 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:21:10,847 - Epoch: [255][  100/  217]    Overall Loss 0.250067    Objective Loss 0.250067                                        LR 0.000016    Time 0.298517    
2024-05-09 14:21:39,658 - Epoch: [255][  200/  217]    Overall Loss 0.256214    Objective Loss 0.256214                                        LR 0.000016    Time 0.293256    
2024-05-09 14:21:42,414 - Epoch: [255][  217/  217]    Overall Loss 0.254315    Objective Loss 0.254315    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.282971    
2024-05-09 14:21:42,695 - 

2024-05-09 14:21:42,696 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:22:11,523 - Epoch: [256][  100/  217]    Overall Loss 0.252721    Objective Loss 0.252721                                        LR 0.000016    Time 0.288152    
2024-05-09 14:22:37,682 - Epoch: [256][  200/  217]    Overall Loss 0.257673    Objective Loss 0.257673                                        LR 0.000016    Time 0.274817    
2024-05-09 14:22:41,253 - Epoch: [256][  217/  217]    Overall Loss 0.256008    Objective Loss 0.256008    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.269732    
2024-05-09 14:22:41,758 - 

2024-05-09 14:22:41,759 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:23:10,190 - Epoch: [257][  100/  217]    Overall Loss 0.230847    Objective Loss 0.230847                                        LR 0.000016    Time 0.284193    
2024-05-09 14:23:35,189 - Epoch: [257][  200/  217]    Overall Loss 0.237113    Objective Loss 0.237113                                        LR 0.000016    Time 0.267038    
2024-05-09 14:23:39,468 - Epoch: [257][  217/  217]    Overall Loss 0.238508    Objective Loss 0.238508    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.265823    
2024-05-09 14:23:39,949 - 

2024-05-09 14:23:39,951 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:24:12,859 - Epoch: [258][  100/  217]    Overall Loss 0.221470    Objective Loss 0.221470                                        LR 0.000016    Time 0.328957    
2024-05-09 14:24:38,300 - Epoch: [258][  200/  217]    Overall Loss 0.235342    Objective Loss 0.235342                                        LR 0.000016    Time 0.291625    
2024-05-09 14:24:41,100 - Epoch: [258][  217/  217]    Overall Loss 0.234398    Objective Loss 0.234398    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.281673    
2024-05-09 14:24:41,662 - 

2024-05-09 14:24:41,662 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:25:12,537 - Epoch: [259][  100/  217]    Overall Loss 0.219140    Objective Loss 0.219140                                        LR 0.000016    Time 0.308616    
2024-05-09 14:25:38,193 - Epoch: [259][  200/  217]    Overall Loss 0.226632    Objective Loss 0.226632                                        LR 0.000016    Time 0.282532    
2024-05-09 14:25:41,286 - Epoch: [259][  217/  217]    Overall Loss 0.226681    Objective Loss 0.226681    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.274638    
2024-05-09 14:25:41,614 - 

2024-05-09 14:25:41,615 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:26:11,009 - Epoch: [260][  100/  217]    Overall Loss 0.207769    Objective Loss 0.207769                                        LR 0.000016    Time 0.293816    
2024-05-09 14:26:37,774 - Epoch: [260][  200/  217]    Overall Loss 0.215978    Objective Loss 0.215978                                        LR 0.000016    Time 0.280658    
2024-05-09 14:26:41,176 - Epoch: [260][  217/  217]    Overall Loss 0.220622    Objective Loss 0.220622    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.274332    
2024-05-09 14:26:41,511 - --- validate (epoch=260)-----------
2024-05-09 14:26:41,512 - 1736 samples (32 per mini-batch)
2024-05-09 14:27:00,513 - Epoch: [260][   55/   55]    Loss 2.210714    Top1 53.513825    Top5 71.601382    
2024-05-09 14:27:00,922 - ==> Top1: 53.514    Top5: 71.601    Loss: 2.211

2024-05-09 14:27:00,927 - ==> Best [Top1: 55.012   Top5: 71.083   Sparsity:0.00   Params: 382352 on epoch: 250]
2024-05-09 14:27:00,927 - Saving checkpoint to: logs/2024.05.09-095556/qat_checkpoint.pth.tar
2024-05-09 14:27:00,962 - 

2024-05-09 14:27:00,963 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:27:30,123 - Epoch: [261][  100/  217]    Overall Loss 0.202568    Objective Loss 0.202568                                        LR 0.000016    Time 0.291471    
2024-05-09 14:28:01,850 - Epoch: [261][  200/  217]    Overall Loss 0.218182    Objective Loss 0.218182                                        LR 0.000016    Time 0.304309    
2024-05-09 14:28:05,293 - Epoch: [261][  217/  217]    Overall Loss 0.216698    Objective Loss 0.216698    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.296321    
2024-05-09 14:28:05,756 - 

2024-05-09 14:28:05,757 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:28:36,564 - Epoch: [262][  100/  217]    Overall Loss 0.206106    Objective Loss 0.206106                                        LR 0.000016    Time 0.307949    
2024-05-09 14:29:01,997 - Epoch: [262][  200/  217]    Overall Loss 0.212792    Objective Loss 0.212792                                        LR 0.000016    Time 0.281084    
2024-05-09 14:29:05,960 - Epoch: [262][  217/  217]    Overall Loss 0.213611    Objective Loss 0.213611    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.277309    
2024-05-09 14:29:06,618 - 

2024-05-09 14:29:06,619 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:29:37,738 - Epoch: [263][  100/  217]    Overall Loss 0.197243    Objective Loss 0.197243                                        LR 0.000016    Time 0.311078    
2024-05-09 14:30:04,628 - Epoch: [263][  200/  217]    Overall Loss 0.210008    Objective Loss 0.210008                                        LR 0.000016    Time 0.289929    
2024-05-09 14:30:07,899 - Epoch: [263][  217/  217]    Overall Loss 0.209953    Objective Loss 0.209953    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.282278    
2024-05-09 14:30:08,244 - 

2024-05-09 14:30:08,245 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:30:40,712 - Epoch: [264][  100/  217]    Overall Loss 0.184214    Objective Loss 0.184214                                        LR 0.000016    Time 0.324543    
2024-05-09 14:31:03,175 - Epoch: [264][  200/  217]    Overall Loss 0.197191    Objective Loss 0.197191                                        LR 0.000016    Time 0.274525    
2024-05-09 14:31:06,754 - Epoch: [264][  217/  217]    Overall Loss 0.198648    Objective Loss 0.198648    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.269498    
2024-05-09 14:31:07,155 - 

2024-05-09 14:31:07,156 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:31:41,353 - Epoch: [265][  100/  217]    Overall Loss 0.190368    Objective Loss 0.190368                                        LR 0.000016    Time 0.341847    
2024-05-09 14:32:05,257 - Epoch: [265][  200/  217]    Overall Loss 0.196985    Objective Loss 0.196985                                        LR 0.000016    Time 0.290388    
2024-05-09 14:32:09,260 - Epoch: [265][  217/  217]    Overall Loss 0.197066    Objective Loss 0.197066    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.286077    
2024-05-09 14:32:09,793 - 

2024-05-09 14:32:09,794 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:32:39,749 - Epoch: [266][  100/  217]    Overall Loss 0.184512    Objective Loss 0.184512                                        LR 0.000016    Time 0.299403    
2024-05-09 14:33:06,285 - Epoch: [266][  200/  217]    Overall Loss 0.189284    Objective Loss 0.189284                                        LR 0.000016    Time 0.282330    
2024-05-09 14:33:09,890 - Epoch: [266][  217/  217]    Overall Loss 0.190974    Objective Loss 0.190974    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.276811    
2024-05-09 14:33:10,336 - 

2024-05-09 14:33:10,336 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:33:39,462 - Epoch: [267][  100/  217]    Overall Loss 0.177815    Objective Loss 0.177815                                        LR 0.000016    Time 0.291124    
2024-05-09 14:34:02,928 - Epoch: [267][  200/  217]    Overall Loss 0.182619    Objective Loss 0.182619                                        LR 0.000016    Time 0.262832    
2024-05-09 14:34:07,923 - Epoch: [267][  217/  217]    Overall Loss 0.184489    Objective Loss 0.184489    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.265248    
2024-05-09 14:34:08,333 - 

2024-05-09 14:34:08,334 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:34:38,578 - Epoch: [268][  100/  217]    Overall Loss 0.175108    Objective Loss 0.175108                                        LR 0.000016    Time 0.302313    
2024-05-09 14:35:02,113 - Epoch: [268][  200/  217]    Overall Loss 0.180479    Objective Loss 0.180479                                        LR 0.000016    Time 0.268777    
2024-05-09 14:35:06,734 - Epoch: [268][  217/  217]    Overall Loss 0.181840    Objective Loss 0.181840    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.269006    
2024-05-09 14:35:07,083 - 

2024-05-09 14:35:07,085 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:35:40,289 - Epoch: [269][  100/  217]    Overall Loss 0.172106    Objective Loss 0.172106                                        LR 0.000016    Time 0.331900    
2024-05-09 14:36:02,804 - Epoch: [269][  200/  217]    Overall Loss 0.180448    Objective Loss 0.180448                                        LR 0.000016    Time 0.278468    
2024-05-09 14:36:06,264 - Epoch: [269][  217/  217]    Overall Loss 0.181553    Objective Loss 0.181553    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.272588    
2024-05-09 14:36:06,569 - 

2024-05-09 14:36:06,570 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:36:37,683 - Epoch: [270][  100/  217]    Overall Loss 0.180397    Objective Loss 0.180397                                        LR 0.000016    Time 0.311008    
2024-05-09 14:37:01,669 - Epoch: [270][  200/  217]    Overall Loss 0.179267    Objective Loss 0.179267                                        LR 0.000016    Time 0.275374    
2024-05-09 14:37:06,640 - Epoch: [270][  217/  217]    Overall Loss 0.181099    Objective Loss 0.181099    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.276701    
2024-05-09 14:37:07,029 - --- validate (epoch=270)-----------
2024-05-09 14:37:07,029 - 1736 samples (32 per mini-batch)
2024-05-09 14:37:23,786 - Epoch: [270][   55/   55]    Loss 2.256800    Top1 53.571429    Top5 70.276498    
2024-05-09 14:37:24,334 - ==> Top1: 53.571    Top5: 70.276    Loss: 2.257

2024-05-09 14:37:24,341 - ==> Best [Top1: 55.012   Top5: 71.083   Sparsity:0.00   Params: 382352 on epoch: 250]
2024-05-09 14:37:24,341 - Saving checkpoint to: logs/2024.05.09-095556/qat_checkpoint.pth.tar
2024-05-09 14:37:24,389 - 

2024-05-09 14:37:24,390 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:37:57,146 - Epoch: [271][  100/  217]    Overall Loss 0.167525    Objective Loss 0.167525                                        LR 0.000016    Time 0.327412    
2024-05-09 14:38:21,157 - Epoch: [271][  200/  217]    Overall Loss 0.174181    Objective Loss 0.174181                                        LR 0.000016    Time 0.283708    
2024-05-09 14:38:24,763 - Epoch: [271][  217/  217]    Overall Loss 0.174648    Objective Loss 0.174648    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.278090    
2024-05-09 14:38:25,373 - 

2024-05-09 14:38:25,374 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:38:56,393 - Epoch: [272][  100/  217]    Overall Loss 0.167165    Objective Loss 0.167165                                        LR 0.000016    Time 0.310050    
2024-05-09 14:39:23,362 - Epoch: [272][  200/  217]    Overall Loss 0.177775    Objective Loss 0.177775                                        LR 0.000016    Time 0.289813    
2024-05-09 14:39:27,467 - Epoch: [272][  217/  217]    Overall Loss 0.178545    Objective Loss 0.178545    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.286017    
2024-05-09 14:39:27,926 - 

2024-05-09 14:39:27,927 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:39:54,501 - Epoch: [273][  100/  217]    Overall Loss 0.162025    Objective Loss 0.162025                                        LR 0.000016    Time 0.265610    
2024-05-09 14:40:22,574 - Epoch: [273][  200/  217]    Overall Loss 0.175042    Objective Loss 0.175042                                        LR 0.000016    Time 0.273108    
2024-05-09 14:40:25,645 - Epoch: [273][  217/  217]    Overall Loss 0.173042    Objective Loss 0.173042    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.265854    
2024-05-09 14:40:25,976 - 

2024-05-09 14:40:25,977 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:40:54,917 - Epoch: [274][  100/  217]    Overall Loss 0.156183    Objective Loss 0.156183                                        LR 0.000016    Time 0.289284    
2024-05-09 14:41:17,246 - Epoch: [274][  200/  217]    Overall Loss 0.164846    Objective Loss 0.164846                                        LR 0.000016    Time 0.256231    
2024-05-09 14:41:20,801 - Epoch: [274][  217/  217]    Overall Loss 0.166784    Objective Loss 0.166784    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.252526    
2024-05-09 14:41:21,151 - 

2024-05-09 14:41:21,152 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:41:51,932 - Epoch: [275][  100/  217]    Overall Loss 0.155983    Objective Loss 0.155983                                        LR 0.000016    Time 0.307658    
2024-05-09 14:42:18,198 - Epoch: [275][  200/  217]    Overall Loss 0.168059    Objective Loss 0.168059                                        LR 0.000016    Time 0.285105    
2024-05-09 14:42:23,351 - Epoch: [275][  217/  217]    Overall Loss 0.167476    Objective Loss 0.167476    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.286502    
2024-05-09 14:42:23,733 - 

2024-05-09 14:42:23,734 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:42:51,290 - Epoch: [276][  100/  217]    Overall Loss 0.153598    Objective Loss 0.153598                                        LR 0.000016    Time 0.275416    
2024-05-09 14:43:19,368 - Epoch: [276][  200/  217]    Overall Loss 0.170502    Objective Loss 0.170502                                        LR 0.000016    Time 0.278032    
2024-05-09 14:43:22,895 - Epoch: [276][  217/  217]    Overall Loss 0.171187    Objective Loss 0.171187    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.272497    
2024-05-09 14:43:23,257 - 

2024-05-09 14:43:23,257 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:43:53,653 - Epoch: [277][  100/  217]    Overall Loss 0.142880    Objective Loss 0.142880                                        LR 0.000016    Time 0.303832    
2024-05-09 14:44:17,589 - Epoch: [277][  200/  217]    Overall Loss 0.152419    Objective Loss 0.152419                                        LR 0.000016    Time 0.271537    
2024-05-09 14:44:22,274 - Epoch: [277][  217/  217]    Overall Loss 0.155206    Objective Loss 0.155206    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.271845    
2024-05-09 14:44:22,680 - 

2024-05-09 14:44:22,680 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:44:52,416 - Epoch: [278][  100/  217]    Overall Loss 0.169806    Objective Loss 0.169806                                        LR 0.000016    Time 0.297223    
2024-05-09 14:45:21,625 - Epoch: [278][  200/  217]    Overall Loss 0.169290    Objective Loss 0.169290                                        LR 0.000016    Time 0.294599    
2024-05-09 14:45:25,120 - Epoch: [278][  217/  217]    Overall Loss 0.169557    Objective Loss 0.169557    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.287616    
2024-05-09 14:45:25,477 - 

2024-05-09 14:45:25,477 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:45:51,364 - Epoch: [279][  100/  217]    Overall Loss 0.145190    Objective Loss 0.145190                                        LR 0.000016    Time 0.258742    
2024-05-09 14:46:18,871 - Epoch: [279][  200/  217]    Overall Loss 0.150986    Objective Loss 0.150986                                        LR 0.000016    Time 0.266846    
2024-05-09 14:46:23,857 - Epoch: [279][  217/  217]    Overall Loss 0.151835    Objective Loss 0.151835    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.268908    
2024-05-09 14:46:24,244 - 

2024-05-09 14:46:24,245 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:46:56,218 - Epoch: [280][  100/  217]    Overall Loss 0.147552    Objective Loss 0.147552                                        LR 0.000016    Time 0.319578    
2024-05-09 14:47:24,250 - Epoch: [280][  200/  217]    Overall Loss 0.161405    Objective Loss 0.161405                                        LR 0.000016    Time 0.299884    
2024-05-09 14:47:29,268 - Epoch: [280][  217/  217]    Overall Loss 0.161230    Objective Loss 0.161230    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.299497    
2024-05-09 14:47:29,851 - --- validate (epoch=280)-----------
2024-05-09 14:47:29,853 - 1736 samples (32 per mini-batch)
2024-05-09 14:47:46,175 - Epoch: [280][   55/   55]    Loss 2.223256    Top1 54.032258    Top5 71.486175    
2024-05-09 14:47:46,610 - ==> Top1: 54.032    Top5: 71.486    Loss: 2.223

2024-05-09 14:47:46,614 - ==> Best [Top1: 55.012   Top5: 71.083   Sparsity:0.00   Params: 382352 on epoch: 250]
2024-05-09 14:47:46,614 - Saving checkpoint to: logs/2024.05.09-095556/qat_checkpoint.pth.tar
2024-05-09 14:47:46,644 - 

2024-05-09 14:47:46,644 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:48:22,447 - Epoch: [281][  100/  217]    Overall Loss 0.134427    Objective Loss 0.134427                                        LR 0.000016    Time 0.357895    
2024-05-09 14:48:47,893 - Epoch: [281][  200/  217]    Overall Loss 0.143979    Objective Loss 0.143979                                        LR 0.000016    Time 0.306112    
2024-05-09 14:48:50,811 - Epoch: [281][  217/  217]    Overall Loss 0.146258    Objective Loss 0.146258    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.295567    
2024-05-09 14:48:51,378 - 

2024-05-09 14:48:51,379 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:49:22,286 - Epoch: [282][  100/  217]    Overall Loss 0.158898    Objective Loss 0.158898                                        LR 0.000016    Time 0.308937    
2024-05-09 14:49:47,650 - Epoch: [282][  200/  217]    Overall Loss 0.157898    Objective Loss 0.157898                                        LR 0.000016    Time 0.281229    
2024-05-09 14:49:52,689 - Epoch: [282][  217/  217]    Overall Loss 0.156987    Objective Loss 0.156987    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.282406    
2024-05-09 14:49:53,334 - 

2024-05-09 14:49:53,335 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:50:23,099 - Epoch: [283][  100/  217]    Overall Loss 0.135797    Objective Loss 0.135797                                        LR 0.000016    Time 0.297498    
2024-05-09 14:50:48,955 - Epoch: [283][  200/  217]    Overall Loss 0.141540    Objective Loss 0.141540                                        LR 0.000016    Time 0.277972    
2024-05-09 14:50:54,924 - Epoch: [283][  217/  217]    Overall Loss 0.145150    Objective Loss 0.145150    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.283694    
2024-05-09 14:50:55,293 - 

2024-05-09 14:50:55,294 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:51:25,692 - Epoch: [284][  100/  217]    Overall Loss 0.142936    Objective Loss 0.142936                                        LR 0.000016    Time 0.303840    
2024-05-09 14:51:49,733 - Epoch: [284][  200/  217]    Overall Loss 0.150984    Objective Loss 0.150984                                        LR 0.000016    Time 0.272065    
2024-05-09 14:51:53,646 - Epoch: [284][  217/  217]    Overall Loss 0.149983    Objective Loss 0.149983    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.268774    
2024-05-09 14:51:54,203 - 

2024-05-09 14:51:54,203 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:52:24,675 - Epoch: [285][  100/  217]    Overall Loss 0.145653    Objective Loss 0.145653                                        LR 0.000016    Time 0.304584    
2024-05-09 14:52:52,349 - Epoch: [285][  200/  217]    Overall Loss 0.154843    Objective Loss 0.154843                                        LR 0.000016    Time 0.290600    
2024-05-09 14:52:57,427 - Epoch: [285][  217/  217]    Overall Loss 0.154170    Objective Loss 0.154170    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.291222    
2024-05-09 14:52:57,749 - 

2024-05-09 14:52:57,750 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:53:28,618 - Epoch: [286][  100/  217]    Overall Loss 0.128372    Objective Loss 0.128372                                        LR 0.000016    Time 0.308549    
2024-05-09 14:53:53,309 - Epoch: [286][  200/  217]    Overall Loss 0.135058    Objective Loss 0.135058                                        LR 0.000016    Time 0.277675    
2024-05-09 14:53:57,198 - Epoch: [286][  217/  217]    Overall Loss 0.140426    Objective Loss 0.140426    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.273833    
2024-05-09 14:53:57,660 - 

2024-05-09 14:53:57,660 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:54:27,028 - Epoch: [287][  100/  217]    Overall Loss 0.148531    Objective Loss 0.148531                                        LR 0.000016    Time 0.293387    
2024-05-09 14:54:51,301 - Epoch: [287][  200/  217]    Overall Loss 0.147545    Objective Loss 0.147545                                        LR 0.000016    Time 0.268002    
2024-05-09 14:54:56,261 - Epoch: [287][  217/  217]    Overall Loss 0.147520    Objective Loss 0.147520    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.269857    
2024-05-09 14:54:56,786 - 

2024-05-09 14:54:56,787 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:55:24,401 - Epoch: [288][  100/  217]    Overall Loss 0.131080    Objective Loss 0.131080                                        LR 0.000016    Time 0.276007    
2024-05-09 14:55:49,588 - Epoch: [288][  200/  217]    Overall Loss 0.139684    Objective Loss 0.139684                                        LR 0.000016    Time 0.263881    
2024-05-09 14:55:53,859 - Epoch: [288][  217/  217]    Overall Loss 0.140701    Objective Loss 0.140701    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.262879    
2024-05-09 14:55:54,304 - 

2024-05-09 14:55:54,304 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:56:23,053 - Epoch: [289][  100/  217]    Overall Loss 0.133052    Objective Loss 0.133052                                        LR 0.000016    Time 0.287369    
2024-05-09 14:56:45,172 - Epoch: [289][  200/  217]    Overall Loss 0.140577    Objective Loss 0.140577                                        LR 0.000016    Time 0.254225    
2024-05-09 14:56:49,192 - Epoch: [289][  217/  217]    Overall Loss 0.143542    Objective Loss 0.143542    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.252825    
2024-05-09 14:56:49,704 - 

2024-05-09 14:56:49,705 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:57:21,543 - Epoch: [290][  100/  217]    Overall Loss 0.129537    Objective Loss 0.129537                                        LR 0.000016    Time 0.318249    
2024-05-09 14:57:47,295 - Epoch: [290][  200/  217]    Overall Loss 0.143673    Objective Loss 0.143673                                        LR 0.000016    Time 0.287835    
2024-05-09 14:57:50,719 - Epoch: [290][  217/  217]    Overall Loss 0.143889    Objective Loss 0.143889    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.281053    
2024-05-09 14:57:51,342 - --- validate (epoch=290)-----------
2024-05-09 14:57:51,343 - 1736 samples (32 per mini-batch)
2024-05-09 14:58:07,156 - Epoch: [290][   55/   55]    Loss 2.256364    Top1 54.377880    Top5 72.695853    
2024-05-09 14:58:07,643 - ==> Top1: 54.378    Top5: 72.696    Loss: 2.256

2024-05-09 14:58:07,651 - ==> Best [Top1: 55.012   Top5: 71.083   Sparsity:0.00   Params: 382352 on epoch: 250]
2024-05-09 14:58:07,651 - Saving checkpoint to: logs/2024.05.09-095556/qat_checkpoint.pth.tar
2024-05-09 14:58:07,689 - 

2024-05-09 14:58:07,690 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:58:38,803 - Epoch: [291][  100/  217]    Overall Loss 0.122193    Objective Loss 0.122193                                        LR 0.000016    Time 0.311006    
2024-05-09 14:59:04,574 - Epoch: [291][  200/  217]    Overall Loss 0.135731    Objective Loss 0.135731                                        LR 0.000016    Time 0.284301    
2024-05-09 14:59:09,669 - Epoch: [291][  217/  217]    Overall Loss 0.136521    Objective Loss 0.136521    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.285498    
2024-05-09 14:59:10,213 - 

2024-05-09 14:59:10,214 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 14:59:41,106 - Epoch: [292][  100/  217]    Overall Loss 0.134403    Objective Loss 0.134403                                        LR 0.000016    Time 0.308788    
2024-05-09 15:00:03,618 - Epoch: [292][  200/  217]    Overall Loss 0.141276    Objective Loss 0.141276                                        LR 0.000016    Time 0.266902    
2024-05-09 15:00:06,476 - Epoch: [292][  217/  217]    Overall Loss 0.142208    Objective Loss 0.142208    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.259151    
2024-05-09 15:00:07,047 - 

2024-05-09 15:00:07,048 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:00:40,081 - Epoch: [293][  100/  217]    Overall Loss 0.123794    Objective Loss 0.123794                                        LR 0.000016    Time 0.330186    
2024-05-09 15:01:03,424 - Epoch: [293][  200/  217]    Overall Loss 0.134095    Objective Loss 0.134095                                        LR 0.000016    Time 0.281748    
2024-05-09 15:01:07,176 - Epoch: [293][  217/  217]    Overall Loss 0.135648    Objective Loss 0.135648    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.276957    
2024-05-09 15:01:07,504 - 

2024-05-09 15:01:07,505 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:01:34,480 - Epoch: [294][  100/  217]    Overall Loss 0.130756    Objective Loss 0.130756                                        LR 0.000016    Time 0.269631    
2024-05-09 15:02:06,096 - Epoch: [294][  200/  217]    Overall Loss 0.139535    Objective Loss 0.139535                                        LR 0.000016    Time 0.292840    
2024-05-09 15:02:11,282 - Epoch: [294][  217/  217]    Overall Loss 0.141839    Objective Loss 0.141839    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.293789    
2024-05-09 15:02:11,661 - 

2024-05-09 15:02:11,662 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:02:36,832 - Epoch: [295][  100/  217]    Overall Loss 0.120957    Objective Loss 0.120957                                        LR 0.000016    Time 0.251575    
2024-05-09 15:03:02,060 - Epoch: [295][  200/  217]    Overall Loss 0.134903    Objective Loss 0.134903                                        LR 0.000016    Time 0.251877    
2024-05-09 15:03:05,377 - Epoch: [295][  217/  217]    Overall Loss 0.137770    Objective Loss 0.137770    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.247416    
2024-05-09 15:03:05,777 - 

2024-05-09 15:03:05,777 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:03:35,214 - Epoch: [296][  100/  217]    Overall Loss 0.140379    Objective Loss 0.140379                                        LR 0.000016    Time 0.294250    
2024-05-09 15:04:01,604 - Epoch: [296][  200/  217]    Overall Loss 0.142247    Objective Loss 0.142247                                        LR 0.000016    Time 0.279025    
2024-05-09 15:04:05,107 - Epoch: [296][  217/  217]    Overall Loss 0.140633    Objective Loss 0.140633    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.273297    
2024-05-09 15:04:05,418 - 

2024-05-09 15:04:05,419 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:04:33,797 - Epoch: [297][  100/  217]    Overall Loss 0.114798    Objective Loss 0.114798                                        LR 0.000016    Time 0.283643    
2024-05-09 15:05:00,538 - Epoch: [297][  200/  217]    Overall Loss 0.125397    Objective Loss 0.125397                                        LR 0.000016    Time 0.275468    
2024-05-09 15:05:03,927 - Epoch: [297][  217/  217]    Overall Loss 0.127149    Objective Loss 0.127149    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.269490    
2024-05-09 15:05:04,385 - 

2024-05-09 15:05:04,385 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:05:33,976 - Epoch: [298][  100/  217]    Overall Loss 0.120522    Objective Loss 0.120522                                        LR 0.000016    Time 0.295774    
2024-05-09 15:05:58,674 - Epoch: [298][  200/  217]    Overall Loss 0.124494    Objective Loss 0.124494                                        LR 0.000016    Time 0.271320    
2024-05-09 15:06:02,145 - Epoch: [298][  217/  217]    Overall Loss 0.125281    Objective Loss 0.125281    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.266048    
2024-05-09 15:06:02,489 - 

2024-05-09 15:06:02,489 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-09 15:06:30,285 - Epoch: [299][  100/  217]    Overall Loss 0.108225    Objective Loss 0.108225                                        LR 0.000016    Time 0.277828    
2024-05-09 15:06:54,921 - Epoch: [299][  200/  217]    Overall Loss 0.120972    Objective Loss 0.120972                                        LR 0.000016    Time 0.262029    
2024-05-09 15:06:58,719 - Epoch: [299][  217/  217]    Overall Loss 0.129429    Objective Loss 0.129429    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.258996    
2024-05-09 15:06:59,036 - --- test ---------------------
2024-05-09 15:06:59,037 - 1736 samples (32 per mini-batch)
2024-05-09 15:07:15,476 - Test: [   55/   55]    Loss 2.293813    Top1 53.917051    Top5 70.506912    
2024-05-09 15:07:15,785 - ==> Top1: 53.917    Top5: 70.507    Loss: 2.294

2024-05-09 15:07:15,792 - 
2024-05-09 15:07:15,793 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.09-095556/2024.05.09-095556.log
