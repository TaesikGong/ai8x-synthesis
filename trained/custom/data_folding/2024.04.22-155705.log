2024-04-22 15:57:05,835 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.04.22-155705/2024.04.22-155705.log
2024-04-22 15:57:10,191 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2024-04-22 15:57:10,191 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}
2024-04-22 15:57:10,274 - Dataset sizes:
	training=8523
	validation=946
	test=3925
2024-04-22 15:57:10,275 - Reading compression schedule from: policies/schedule.yaml
2024-04-22 15:57:10,280 - 

2024-04-22 15:57:10,281 - Training epoch: 8523 samples (256 per mini-batch)
2024-04-22 15:57:23,104 - Epoch: [0][   10/   34]    Overall Loss 2.278143    Objective Loss 2.278143                                        LR 0.100000    Time 1.281982    
2024-04-22 15:57:34,254 - Epoch: [0][   20/   34]    Overall Loss 2.220081    Objective Loss 2.220081                                        LR 0.100000    Time 1.198329    
2024-04-22 15:57:48,651 - Epoch: [0][   30/   34]    Overall Loss 2.196646    Objective Loss 2.196646                                        LR 0.100000    Time 1.278674    
2024-04-22 15:57:51,337 - Epoch: [0][   34/   34]    Overall Loss 2.183425    Objective Loss 2.183425    Top1 24.773414    Top5 71.299094    LR 0.100000    Time 1.207215    
2024-04-22 15:57:51,451 - --- validate (epoch=0)-----------
2024-04-22 15:57:51,452 - 946 samples (256 per mini-batch)
2024-04-22 15:57:56,045 - Epoch: [0][    4/    4]    Loss 2.083327    Top1 25.369979    Top5 71.775899    
2024-04-22 15:57:56,153 - ==> Top1: 25.370    Top5: 71.776    Loss: 2.083

2024-04-22 15:57:56,155 - ==> Confusion:
[[53  1  2  0  2 15 10  1  1  2]
 [43  9  9  0  2 18  5  1  4  7]
 [37 11 12  0  5  9 12  2  4  7]
 [23  7  5  0  3 26  2  5  5  4]
 [26  7  0  0 20 16 12  3  2  7]
 [40  2  5  0  2 39  4  3  0  0]
 [27  5  8  0 17  9 30  2  1  5]
 [31 10  7  0  2 22  9  3  3  6]
 [46  5  3  0  4  6 10  1  8 14]
 [ 7  1  4  0 11  3  6  1  1 66]]

2024-04-22 15:57:56,159 - ==> Best [Top1: 25.370   Top5: 71.776   Sparsity:0.00   Params: 77088 on epoch: 0]
2024-04-22 15:57:56,159 - Saving checkpoint to: logs/2024.04.22-155705/checkpoint.pth.tar
2024-04-22 15:57:56,171 - 

2024-04-22 15:57:56,172 - Training epoch: 8523 samples (256 per mini-batch)
2024-04-22 15:58:10,216 - Epoch: [1][   10/   34]    Overall Loss 2.074440    Objective Loss 2.074440                                        LR 0.100000    Time 1.404138    
2024-04-22 15:58:21,128 - Epoch: [1][   20/   34]    Overall Loss 2.076510    Objective Loss 2.076510                                        LR 0.100000    Time 1.247500    
2024-04-22 15:58:34,981 - Epoch: [1][   30/   34]    Overall Loss 2.062583    Objective Loss 2.062583                                        LR 0.100000    Time 1.293343    
2024-04-22 15:58:40,617 - Epoch: [1][   34/   34]    Overall Loss 2.062883    Objective Loss 2.062883    Top1 27.794562    Top5 73.413897    LR 0.100000    Time 1.306884    
2024-04-22 15:58:40,866 - --- validate (epoch=1)-----------
2024-04-22 15:58:40,868 - 946 samples (256 per mini-batch)
2024-04-22 15:58:48,521 - Epoch: [1][    4/    4]    Loss 2.084788    Top1 25.581395    Top5 73.678647    
2024-04-22 15:58:48,692 - ==> Top1: 25.581    Top5: 73.679    Loss: 2.085

2024-04-22 15:58:48,694 - ==> Confusion:
[[14  3 20  3  2  4 24  6  6  5]
 [14  2 44  0  0  3  6  7  9 13]
 [ 6  0 41  0  6  2 12  6  7 19]
 [ 9  2 29  6  2  7 12  4  2  7]
 [12  1 15  1 16  6 25  2  2 13]
 [19  0 25  1  4 20 16  5  1  4]
 [ 7  0 32  0  8  1 25  2  6 23]
 [ 4  0 43  2  6  6 12  7  2 11]
 [ 5  0 19  0  4  2 10  4 31 22]
 [ 0  0  5  2  4  2  6  1  0 80]]

2024-04-22 15:58:48,699 - ==> Best [Top1: 25.581   Top5: 73.679   Sparsity:0.00   Params: 77088 on epoch: 1]
2024-04-22 15:58:48,700 - Saving checkpoint to: logs/2024.04.22-155705/checkpoint.pth.tar
2024-04-22 15:58:48,716 - 

2024-04-22 15:58:48,717 - Training epoch: 8523 samples (256 per mini-batch)
