2024-05-12 22:26:26,340 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.12-222626/2024.05.12-222626.log
2024-05-12 22:26:47,020 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2024-05-12 22:26:47,021 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2024-05-12 22:26:47,123 - Dataset sizes:
	training=6941
	validation=1736
	test=1736
2024-05-12 22:26:47,124 - Reading compression schedule from: policies/schedule-cifar100.yaml
2024-05-12 22:26:47,140 - 

2024-05-12 22:26:47,142 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:27:04,365 - Epoch: [0][   70/   70]    Overall Loss 3.893808    Objective Loss 3.893808    Top1 22.695035    Top5 31.914894    LR 0.001000    Time 0.245736    
2024-05-12 22:27:05,102 - --- validate (epoch=0)-----------
2024-05-12 22:27:05,105 - 1736 samples (100 per mini-batch)
2024-05-12 22:27:12,878 - Epoch: [0][   18/   18]    Loss 4.576467    Top1 2.304147    Top5 10.368664    
2024-05-12 22:27:13,634 - ==> Top1: 2.304    Top5: 10.369    Loss: 4.576

2024-05-12 22:27:13,648 - ==> Best [Top1: 2.304   Top5: 10.369   Sparsity:0.00   Params: 725752 on epoch: 0]
2024-05-12 22:27:13,650 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 22:27:13,797 - 

2024-05-12 22:27:13,798 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:27:29,510 - Epoch: [1][   70/   70]    Overall Loss 3.323643    Objective Loss 3.323643    Top1 31.914894    Top5 39.007092    LR 0.001000    Time 0.224185    
2024-05-12 22:27:30,121 - 

2024-05-12 22:27:30,123 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:27:47,447 - Epoch: [2][   70/   70]    Overall Loss 3.058195    Objective Loss 3.058195    Top1 31.205674    Top5 46.808511    LR 0.001000    Time 0.247138    
2024-05-12 22:27:48,384 - 

2024-05-12 22:27:48,385 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:28:04,915 - Epoch: [3][   70/   70]    Overall Loss 2.807708    Objective Loss 2.807708    Top1 43.971631    Top5 58.156028    LR 0.001000    Time 0.235803    
2024-05-12 22:28:05,750 - 

2024-05-12 22:28:05,753 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:28:23,681 - Epoch: [4][   70/   70]    Overall Loss 2.549499    Objective Loss 2.549499    Top1 47.517730    Top5 64.539007    LR 0.001000    Time 0.255769    
2024-05-12 22:28:24,518 - 

2024-05-12 22:28:24,520 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:28:40,291 - Epoch: [5][   70/   70]    Overall Loss 2.291524    Objective Loss 2.291524    Top1 57.446809    Top5 77.304965    LR 0.001000    Time 0.224958    
2024-05-12 22:28:41,049 - 

2024-05-12 22:28:41,051 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:29:01,750 - Epoch: [6][   70/   70]    Overall Loss 2.004358    Objective Loss 2.004358    Top1 56.737589    Top5 75.177305    LR 0.001000    Time 0.295369    
2024-05-12 22:29:02,669 - 

2024-05-12 22:29:02,672 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:29:16,138 - Epoch: [7][   70/   70]    Overall Loss 1.740854    Objective Loss 1.740854    Top1 60.992908    Top5 77.304965    LR 0.001000    Time 0.192043    
2024-05-12 22:29:16,847 - 

2024-05-12 22:29:16,848 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:29:32,473 - Epoch: [8][   70/   70]    Overall Loss 1.484742    Objective Loss 1.484742    Top1 65.248227    Top5 87.943262    LR 0.001000    Time 0.222916    
2024-05-12 22:29:33,311 - 

2024-05-12 22:29:33,312 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:29:49,203 - Epoch: [9][   70/   70]    Overall Loss 1.242264    Objective Loss 1.242264    Top1 68.794326    Top5 87.943262    LR 0.001000    Time 0.226616    
2024-05-12 22:29:50,198 - 

2024-05-12 22:29:50,200 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:30:05,446 - Epoch: [10][   70/   70]    Overall Loss 1.006114    Objective Loss 1.006114    Top1 81.560284    Top5 95.744681    LR 0.001000    Time 0.217487    
2024-05-12 22:30:06,221 - --- validate (epoch=10)-----------
2024-05-12 22:30:06,223 - 1736 samples (100 per mini-batch)
2024-05-12 22:30:13,846 - Epoch: [10][   18/   18]    Loss 2.153223    Top1 51.382488    Top5 69.182028    
2024-05-12 22:30:14,832 - ==> Top1: 51.382    Top5: 69.182    Loss: 2.153

2024-05-12 22:30:14,854 - ==> Best [Top1: 51.382   Top5: 69.182   Sparsity:0.00   Params: 725752 on epoch: 10]
2024-05-12 22:30:14,856 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 22:30:15,069 - 

2024-05-12 22:30:15,070 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:30:30,116 - Epoch: [11][   70/   70]    Overall Loss 0.778241    Objective Loss 0.778241    Top1 78.723404    Top5 94.326241    LR 0.001000    Time 0.214595    
2024-05-12 22:30:30,821 - 

2024-05-12 22:30:30,824 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:30:46,358 - Epoch: [12][   70/   70]    Overall Loss 0.579347    Objective Loss 0.579347    Top1 87.234043    Top5 97.872340    LR 0.001000    Time 0.221561    
2024-05-12 22:30:47,219 - 

2024-05-12 22:30:47,221 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:31:04,034 - Epoch: [13][   70/   70]    Overall Loss 0.387012    Objective Loss 0.387012    Top1 95.744681    Top5 100.000000    LR 0.001000    Time 0.239856    
2024-05-12 22:31:04,849 - 

2024-05-12 22:31:04,851 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:31:22,862 - Epoch: [14][   70/   70]    Overall Loss 0.228999    Objective Loss 0.228999    Top1 97.163121    Top5 99.290780    LR 0.001000    Time 0.256977    
2024-05-12 22:31:23,787 - 

2024-05-12 22:31:23,790 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:31:38,545 - Epoch: [15][   70/   70]    Overall Loss 0.129118    Objective Loss 0.129118    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.210455    
2024-05-12 22:31:39,474 - 

2024-05-12 22:31:39,477 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:31:52,717 - Epoch: [16][   70/   70]    Overall Loss 0.071645    Objective Loss 0.071645    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.188834    
2024-05-12 22:31:53,580 - 

2024-05-12 22:31:53,582 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:32:06,642 - Epoch: [17][   70/   70]    Overall Loss 0.045542    Objective Loss 0.045542    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.186236    
2024-05-12 22:32:07,488 - 

2024-05-12 22:32:07,489 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:32:21,125 - Epoch: [18][   70/   70]    Overall Loss 0.032992    Objective Loss 0.032992    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.194514    
2024-05-12 22:32:21,806 - 

2024-05-12 22:32:21,809 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:32:33,129 - Epoch: [19][   70/   70]    Overall Loss 0.025612    Objective Loss 0.025612    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.161380    
2024-05-12 22:32:34,074 - 

2024-05-12 22:32:34,077 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:32:47,391 - Epoch: [20][   70/   70]    Overall Loss 0.021850    Objective Loss 0.021850    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.189880    
2024-05-12 22:32:48,177 - --- validate (epoch=20)-----------
2024-05-12 22:32:48,179 - 1736 samples (100 per mini-batch)
2024-05-12 22:32:55,917 - Epoch: [20][   18/   18]    Loss 2.140720    Top1 56.739631    Top5 71.486175    
2024-05-12 22:32:56,651 - ==> Top1: 56.740    Top5: 71.486    Loss: 2.141

2024-05-12 22:32:56,664 - ==> Best [Top1: 56.740   Top5: 71.486   Sparsity:0.00   Params: 725752 on epoch: 20]
2024-05-12 22:32:56,665 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 22:32:56,827 - 

2024-05-12 22:32:56,829 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:33:09,470 - Epoch: [21][   70/   70]    Overall Loss 0.019914    Objective Loss 0.019914    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.180261    
2024-05-12 22:33:10,275 - 

2024-05-12 22:33:10,278 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:33:25,187 - Epoch: [22][   70/   70]    Overall Loss 0.015998    Objective Loss 0.015998    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.212668    
2024-05-12 22:33:26,002 - 

2024-05-12 22:33:26,004 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:33:39,750 - Epoch: [23][   70/   70]    Overall Loss 0.014205    Objective Loss 0.014205    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.196040    
2024-05-12 22:33:40,654 - 

2024-05-12 22:33:40,656 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:33:57,015 - Epoch: [24][   70/   70]    Overall Loss 0.012753    Objective Loss 0.012753    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.233356    
2024-05-12 22:33:57,868 - 

2024-05-12 22:33:57,871 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:34:13,797 - Epoch: [25][   70/   70]    Overall Loss 0.011547    Objective Loss 0.011547    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.227178    
2024-05-12 22:34:14,572 - 

2024-05-12 22:34:14,574 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:34:28,272 - Epoch: [26][   70/   70]    Overall Loss 0.010728    Objective Loss 0.010728    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.195360    
2024-05-12 22:34:29,568 - 

2024-05-12 22:34:29,570 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:34:43,362 - Epoch: [27][   70/   70]    Overall Loss 0.009667    Objective Loss 0.009667    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.196714    
2024-05-12 22:34:44,197 - 

2024-05-12 22:34:44,199 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:34:57,052 - Epoch: [28][   70/   70]    Overall Loss 0.008667    Objective Loss 0.008667    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.183284    
2024-05-12 22:34:57,773 - 

2024-05-12 22:34:57,777 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:35:10,283 - Epoch: [29][   70/   70]    Overall Loss 0.008392    Objective Loss 0.008392    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.178342    
2024-05-12 22:35:11,337 - 

2024-05-12 22:35:11,338 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:35:25,406 - Epoch: [30][   70/   70]    Overall Loss 0.008081    Objective Loss 0.008081    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.200627    
2024-05-12 22:35:26,176 - --- validate (epoch=30)-----------
2024-05-12 22:35:26,178 - 1736 samples (100 per mini-batch)
2024-05-12 22:35:33,865 - Epoch: [30][   18/   18]    Loss 2.099333    Top1 57.891705    Top5 72.638249    
2024-05-12 22:35:34,555 - ==> Top1: 57.892    Top5: 72.638    Loss: 2.099

2024-05-12 22:35:34,571 - ==> Best [Top1: 57.892   Top5: 72.638   Sparsity:0.00   Params: 725752 on epoch: 30]
2024-05-12 22:35:34,572 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 22:35:34,790 - 

2024-05-12 22:35:34,792 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:35:47,163 - Epoch: [31][   70/   70]    Overall Loss 0.007237    Objective Loss 0.007237    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.176382    
2024-05-12 22:35:47,934 - 

2024-05-12 22:35:47,937 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:36:01,615 - Epoch: [32][   70/   70]    Overall Loss 0.006794    Objective Loss 0.006794    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.195079    
2024-05-12 22:36:02,279 - 

2024-05-12 22:36:02,281 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:36:16,382 - Epoch: [33][   70/   70]    Overall Loss 0.006457    Objective Loss 0.006457    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.201143    
2024-05-12 22:36:17,240 - 

2024-05-12 22:36:17,242 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:36:31,883 - Epoch: [34][   70/   70]    Overall Loss 0.005875    Objective Loss 0.005875    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.208827    
2024-05-12 22:36:32,681 - 

2024-05-12 22:36:32,682 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:36:46,859 - Epoch: [35][   70/   70]    Overall Loss 0.005897    Objective Loss 0.005897    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.202241    
2024-05-12 22:36:47,630 - 

2024-05-12 22:36:47,631 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:37:02,914 - Epoch: [36][   70/   70]    Overall Loss 0.005204    Objective Loss 0.005204    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.217996    
2024-05-12 22:37:03,695 - 

2024-05-12 22:37:03,697 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:37:16,274 - Epoch: [37][   70/   70]    Overall Loss 0.004991    Objective Loss 0.004991    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.179355    
2024-05-12 22:37:17,095 - 

2024-05-12 22:37:17,097 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:37:31,948 - Epoch: [38][   70/   70]    Overall Loss 0.010168    Objective Loss 0.010168    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.211858    
2024-05-12 22:37:32,913 - 

2024-05-12 22:37:32,915 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:37:48,751 - Epoch: [39][   70/   70]    Overall Loss 0.006480    Objective Loss 0.006480    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.225886    
2024-05-12 22:37:49,599 - 

2024-05-12 22:37:49,600 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:38:03,832 - Epoch: [40][   70/   70]    Overall Loss 0.004820    Objective Loss 0.004820    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.202981    
2024-05-12 22:38:04,669 - --- validate (epoch=40)-----------
2024-05-12 22:38:04,670 - 1736 samples (100 per mini-batch)
2024-05-12 22:38:12,589 - Epoch: [40][   18/   18]    Loss 2.243372    Top1 58.064516    Top5 72.004608    
2024-05-12 22:38:13,314 - ==> Top1: 58.065    Top5: 72.005    Loss: 2.243

2024-05-12 22:38:13,322 - ==> Best [Top1: 58.065   Top5: 72.005   Sparsity:0.00   Params: 725752 on epoch: 40]
2024-05-12 22:38:13,323 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 22:38:13,430 - 

2024-05-12 22:38:13,431 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:38:28,273 - Epoch: [41][   70/   70]    Overall Loss 0.004673    Objective Loss 0.004673    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.211745    
2024-05-12 22:38:29,148 - 

2024-05-12 22:38:29,150 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:38:43,683 - Epoch: [42][   70/   70]    Overall Loss 0.004432    Objective Loss 0.004432    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.207316    
2024-05-12 22:38:44,659 - 

2024-05-12 22:38:44,661 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:38:58,469 - Epoch: [43][   70/   70]    Overall Loss 0.004296    Objective Loss 0.004296    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.196926    
2024-05-12 22:38:59,340 - 

2024-05-12 22:38:59,341 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:39:12,312 - Epoch: [44][   70/   70]    Overall Loss 0.024451    Objective Loss 0.024451    Top1 94.326241    Top5 98.581560    LR 0.001000    Time 0.185011    
2024-05-12 22:39:12,864 - 

2024-05-12 22:39:12,865 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:39:24,452 - Epoch: [45][   70/   70]    Overall Loss 1.798076    Objective Loss 1.798076    Top1 53.900709    Top5 81.560284    LR 0.001000    Time 0.165208    
2024-05-12 22:39:25,330 - 

2024-05-12 22:39:25,333 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:39:41,137 - Epoch: [46][   70/   70]    Overall Loss 1.066042    Objective Loss 1.066042    Top1 68.085106    Top5 86.524823    LR 0.001000    Time 0.225395    
2024-05-12 22:39:41,995 - 

2024-05-12 22:39:41,998 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:39:56,766 - Epoch: [47][   70/   70]    Overall Loss 0.596756    Objective Loss 0.596756    Top1 85.815603    Top5 95.744681    LR 0.001000    Time 0.210659    
2024-05-12 22:39:57,503 - 

2024-05-12 22:39:57,505 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:40:13,619 - Epoch: [48][   70/   70]    Overall Loss 0.283176    Objective Loss 0.283176    Top1 88.652482    Top5 97.872340    LR 0.001000    Time 0.229857    
2024-05-12 22:40:14,409 - 

2024-05-12 22:40:14,413 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:40:32,824 - Epoch: [49][   70/   70]    Overall Loss 0.108194    Objective Loss 0.108194    Top1 97.163121    Top5 100.000000    LR 0.001000    Time 0.262624    
2024-05-12 22:40:33,849 - 

2024-05-12 22:40:33,852 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:40:51,338 - Epoch: [50][   70/   70]    Overall Loss 0.038951    Objective Loss 0.038951    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.249432    
2024-05-12 22:40:52,629 - --- validate (epoch=50)-----------
2024-05-12 22:40:52,634 - 1736 samples (100 per mini-batch)
2024-05-12 22:41:01,523 - Epoch: [50][   18/   18]    Loss 2.068557    Top1 58.582949    Top5 74.481567    
2024-05-12 22:41:02,431 - ==> Top1: 58.583    Top5: 74.482    Loss: 2.069

2024-05-12 22:41:02,445 - ==> Best [Top1: 58.583   Top5: 74.482   Sparsity:0.00   Params: 725752 on epoch: 50]
2024-05-12 22:41:02,446 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 22:41:02,637 - 

2024-05-12 22:41:02,638 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:41:18,910 - Epoch: [51][   70/   70]    Overall Loss 0.019574    Objective Loss 0.019574    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.232143    
2024-05-12 22:41:19,698 - 

2024-05-12 22:41:19,701 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:41:38,506 - Epoch: [52][   70/   70]    Overall Loss 0.014910    Objective Loss 0.014910    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.268292    
2024-05-12 22:41:39,392 - 

2024-05-12 22:41:39,394 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:41:54,953 - Epoch: [53][   70/   70]    Overall Loss 0.012642    Objective Loss 0.012642    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.221915    
2024-05-12 22:41:55,590 - 

2024-05-12 22:41:55,593 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:42:10,228 - Epoch: [54][   70/   70]    Overall Loss 0.010724    Objective Loss 0.010724    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.208740    
2024-05-12 22:42:11,077 - 

2024-05-12 22:42:11,080 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:42:28,076 - Epoch: [55][   70/   70]    Overall Loss 0.008922    Objective Loss 0.008922    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.242438    
2024-05-12 22:42:28,703 - 

2024-05-12 22:42:28,705 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:42:45,628 - Epoch: [56][   70/   70]    Overall Loss 0.008044    Objective Loss 0.008044    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.241423    
2024-05-12 22:42:46,422 - 

2024-05-12 22:42:46,428 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:43:03,492 - Epoch: [57][   70/   70]    Overall Loss 0.007610    Objective Loss 0.007610    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.243298    
2024-05-12 22:43:04,109 - 

2024-05-12 22:43:04,112 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:43:21,396 - Epoch: [58][   70/   70]    Overall Loss 0.006920    Objective Loss 0.006920    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.246567    
2024-05-12 22:43:22,134 - 

2024-05-12 22:43:22,136 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:43:41,344 - Epoch: [59][   70/   70]    Overall Loss 0.006831    Objective Loss 0.006831    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.274029    
2024-05-12 22:43:42,084 - 

2024-05-12 22:43:42,085 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:43:59,750 - Epoch: [60][   70/   70]    Overall Loss 0.008293    Objective Loss 0.008293    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.252002    
2024-05-12 22:44:00,569 - --- validate (epoch=60)-----------
2024-05-12 22:44:00,571 - 1736 samples (100 per mini-batch)
2024-05-12 22:44:08,569 - Epoch: [60][   18/   18]    Loss 2.143269    Top1 59.216590    Top5 75.345622    
2024-05-12 22:44:09,296 - ==> Top1: 59.217    Top5: 75.346    Loss: 2.143

2024-05-12 22:44:09,321 - ==> Best [Top1: 59.217   Top5: 75.346   Sparsity:0.00   Params: 725752 on epoch: 60]
2024-05-12 22:44:09,322 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 22:44:09,554 - 

2024-05-12 22:44:09,556 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:44:28,415 - Epoch: [61][   70/   70]    Overall Loss 0.005877    Objective Loss 0.005877    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.269071    
2024-05-12 22:44:29,356 - 

2024-05-12 22:44:29,359 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:44:48,333 - Epoch: [62][   70/   70]    Overall Loss 0.005151    Objective Loss 0.005151    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.270653    
2024-05-12 22:44:49,145 - 

2024-05-12 22:44:49,147 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:45:05,796 - Epoch: [63][   70/   70]    Overall Loss 0.005129    Objective Loss 0.005129    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.237527    
2024-05-12 22:45:06,731 - 

2024-05-12 22:45:06,734 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:45:23,613 - Epoch: [64][   70/   70]    Overall Loss 0.004887    Objective Loss 0.004887    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.240792    
2024-05-12 22:45:24,400 - 

2024-05-12 22:45:24,403 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:45:40,642 - Epoch: [65][   70/   70]    Overall Loss 0.004548    Objective Loss 0.004548    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.231630    
2024-05-12 22:45:41,453 - 

2024-05-12 22:45:41,456 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:45:58,589 - Epoch: [66][   70/   70]    Overall Loss 0.004025    Objective Loss 0.004025    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.244404    
2024-05-12 22:45:59,387 - 

2024-05-12 22:45:59,389 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:46:16,060 - Epoch: [67][   70/   70]    Overall Loss 0.004093    Objective Loss 0.004093    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.237800    
2024-05-12 22:46:16,920 - 

2024-05-12 22:46:16,922 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:46:33,840 - Epoch: [68][   70/   70]    Overall Loss 0.003776    Objective Loss 0.003776    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.241315    
2024-05-12 22:46:34,504 - 

2024-05-12 22:46:34,505 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:46:48,700 - Epoch: [69][   70/   70]    Overall Loss 0.003771    Objective Loss 0.003771    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.202470    
2024-05-12 22:46:49,471 - 

2024-05-12 22:46:49,474 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:47:00,676 - Epoch: [70][   70/   70]    Overall Loss 0.003476    Objective Loss 0.003476    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.159703    
2024-05-12 22:47:01,366 - --- validate (epoch=70)-----------
2024-05-12 22:47:01,368 - 1736 samples (100 per mini-batch)
2024-05-12 22:47:09,268 - Epoch: [70][   18/   18]    Loss 2.204835    Top1 59.216590    Top5 75.403226    
2024-05-12 22:47:10,008 - ==> Top1: 59.217    Top5: 75.403    Loss: 2.205

2024-05-12 22:47:10,023 - ==> Best [Top1: 59.217   Top5: 75.403   Sparsity:0.00   Params: 725752 on epoch: 70]
2024-05-12 22:47:10,024 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 22:47:10,237 - 

2024-05-12 22:47:10,239 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:47:26,693 - Epoch: [71][   70/   70]    Overall Loss 0.003231    Objective Loss 0.003231    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.234743    
2024-05-12 22:47:27,389 - 

2024-05-12 22:47:27,392 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:47:42,423 - Epoch: [72][   70/   70]    Overall Loss 0.003127    Objective Loss 0.003127    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.214389    
2024-05-12 22:47:43,290 - 

2024-05-12 22:47:43,292 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:48:00,021 - Epoch: [73][   70/   70]    Overall Loss 0.003437    Objective Loss 0.003437    Top1 98.581560    Top5 100.000000    LR 0.001000    Time 0.238674    
2024-05-12 22:48:00,845 - 

2024-05-12 22:48:00,846 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:48:15,348 - Epoch: [74][   70/   70]    Overall Loss 0.003138    Objective Loss 0.003138    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.206840    
2024-05-12 22:48:16,107 - 

2024-05-12 22:48:16,109 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:48:32,405 - Epoch: [75][   70/   70]    Overall Loss 0.002902    Objective Loss 0.002902    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.232464    
2024-05-12 22:48:33,566 - 

2024-05-12 22:48:33,568 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:48:47,197 - Epoch: [76][   70/   70]    Overall Loss 0.002688    Objective Loss 0.002688    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.194363    
2024-05-12 22:48:48,011 - 

2024-05-12 22:48:48,014 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:49:00,397 - Epoch: [77][   70/   70]    Overall Loss 0.002669    Objective Loss 0.002669    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.176592    
2024-05-12 22:49:01,122 - 

2024-05-12 22:49:01,124 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:49:14,440 - Epoch: [78][   70/   70]    Overall Loss 0.002560    Objective Loss 0.002560    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.189915    
2024-05-12 22:49:15,289 - 

2024-05-12 22:49:15,293 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:49:28,087 - Epoch: [79][   70/   70]    Overall Loss 0.002920    Objective Loss 0.002920    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.182416    
2024-05-12 22:49:28,872 - 

2024-05-12 22:49:28,874 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:49:41,525 - Epoch: [80][   70/   70]    Overall Loss 0.002526    Objective Loss 0.002526    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.180459    
2024-05-12 22:49:42,213 - --- validate (epoch=80)-----------
2024-05-12 22:49:42,215 - 1736 samples (100 per mini-batch)
2024-05-12 22:49:49,217 - Epoch: [80][   18/   18]    Loss 2.262651    Top1 58.582949    Top5 75.518433    
2024-05-12 22:49:50,098 - ==> Top1: 58.583    Top5: 75.518    Loss: 2.263

2024-05-12 22:49:50,123 - ==> Best [Top1: 59.217   Top5: 75.403   Sparsity:0.00   Params: 725752 on epoch: 70]
2024-05-12 22:49:50,124 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 22:49:50,301 - 

2024-05-12 22:49:50,302 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:50:04,161 - Epoch: [81][   70/   70]    Overall Loss 0.002475    Objective Loss 0.002475    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.197633    
2024-05-12 22:50:04,930 - 

2024-05-12 22:50:04,933 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:50:19,848 - Epoch: [82][   70/   70]    Overall Loss 0.002347    Objective Loss 0.002347    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.212768    
2024-05-12 22:50:20,735 - 

2024-05-12 22:50:20,737 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:50:33,713 - Epoch: [83][   70/   70]    Overall Loss 0.011519    Objective Loss 0.011519    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.185052    
2024-05-12 22:50:34,424 - 

2024-05-12 22:50:34,426 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:50:49,262 - Epoch: [84][   70/   70]    Overall Loss 1.121839    Objective Loss 1.121839    Top1 60.992908    Top5 80.141844    LR 0.001000    Time 0.211650    
2024-05-12 22:50:50,052 - 

2024-05-12 22:50:50,054 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:51:04,599 - Epoch: [85][   70/   70]    Overall Loss 0.677843    Objective Loss 0.677843    Top1 86.524823    Top5 97.163121    LR 0.001000    Time 0.207487    
2024-05-12 22:51:05,450 - 

2024-05-12 22:51:05,452 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:51:21,112 - Epoch: [86][   70/   70]    Overall Loss 0.189737    Objective Loss 0.189737    Top1 97.163121    Top5 100.000000    LR 0.001000    Time 0.223385    
2024-05-12 22:51:21,835 - 

2024-05-12 22:51:21,838 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:51:37,221 - Epoch: [87][   70/   70]    Overall Loss 0.052620    Objective Loss 0.052620    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.219401    
2024-05-12 22:51:38,012 - 

2024-05-12 22:51:38,013 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:51:53,023 - Epoch: [88][   70/   70]    Overall Loss 0.018305    Objective Loss 0.018305    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.214090    
2024-05-12 22:51:53,800 - 

2024-05-12 22:51:53,805 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:52:08,562 - Epoch: [89][   70/   70]    Overall Loss 0.010300    Objective Loss 0.010300    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.210428    
2024-05-12 22:52:09,361 - 

2024-05-12 22:52:09,363 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:52:22,805 - Epoch: [90][   70/   70]    Overall Loss 0.008696    Objective Loss 0.008696    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.191736    
2024-05-12 22:52:23,583 - --- validate (epoch=90)-----------
2024-05-12 22:52:23,584 - 1736 samples (100 per mini-batch)
2024-05-12 22:52:31,084 - Epoch: [90][   18/   18]    Loss 2.162813    Top1 60.253456    Top5 75.979263    
2024-05-12 22:52:31,791 - ==> Top1: 60.253    Top5: 75.979    Loss: 2.163

2024-05-12 22:52:31,803 - ==> Best [Top1: 60.253   Top5: 75.979   Sparsity:0.00   Params: 725752 on epoch: 90]
2024-05-12 22:52:31,804 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 22:52:32,001 - 

2024-05-12 22:52:32,003 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:52:45,394 - Epoch: [91][   70/   70]    Overall Loss 0.007197    Objective Loss 0.007197    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.190975    
2024-05-12 22:52:46,182 - 

2024-05-12 22:52:46,184 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:53:01,449 - Epoch: [92][   70/   70]    Overall Loss 0.006238    Objective Loss 0.006238    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.217732    
2024-05-12 22:53:02,393 - 

2024-05-12 22:53:02,395 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:53:16,686 - Epoch: [93][   70/   70]    Overall Loss 0.005763    Objective Loss 0.005763    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.203807    
2024-05-12 22:53:17,406 - 

2024-05-12 22:53:17,409 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:53:33,100 - Epoch: [94][   70/   70]    Overall Loss 0.005098    Objective Loss 0.005098    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.223819    
2024-05-12 22:53:33,937 - 

2024-05-12 22:53:33,940 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:53:48,605 - Epoch: [95][   70/   70]    Overall Loss 0.004734    Objective Loss 0.004734    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.209132    
2024-05-12 22:53:49,348 - 

2024-05-12 22:53:49,351 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:54:05,547 - Epoch: [96][   70/   70]    Overall Loss 0.004373    Objective Loss 0.004373    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.231040    
2024-05-12 22:54:06,460 - 

2024-05-12 22:54:06,462 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:54:22,691 - Epoch: [97][   70/   70]    Overall Loss 0.004169    Objective Loss 0.004169    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.231481    
2024-05-12 22:54:23,477 - 

2024-05-12 22:54:23,478 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:54:36,380 - Epoch: [98][   70/   70]    Overall Loss 0.003902    Objective Loss 0.003902    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.184092    
2024-05-12 22:54:37,119 - 

2024-05-12 22:54:37,121 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:54:52,616 - Epoch: [99][   70/   70]    Overall Loss 0.003720    Objective Loss 0.003720    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.221000    
2024-05-12 22:54:53,394 - 

2024-05-12 22:54:53,397 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:55:09,052 - Epoch: [100][   70/   70]    Overall Loss 0.003297    Objective Loss 0.003297    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.223284    
2024-05-12 22:55:09,820 - --- validate (epoch=100)-----------
2024-05-12 22:55:09,822 - 1736 samples (100 per mini-batch)
2024-05-12 22:55:16,278 - Epoch: [100][   18/   18]    Loss 2.222351    Top1 60.023041    Top5 75.518433    
2024-05-12 22:55:16,928 - ==> Top1: 60.023    Top5: 75.518    Loss: 2.222

2024-05-12 22:55:16,942 - ==> Best [Top1: 60.253   Top5: 75.979   Sparsity:0.00   Params: 725752 on epoch: 90]
2024-05-12 22:55:16,943 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 22:55:17,067 - 

2024-05-12 22:55:17,069 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:55:36,713 - Epoch: [101][   70/   70]    Overall Loss 0.003246    Objective Loss 0.003246    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.280215    
2024-05-12 22:55:37,974 - 

2024-05-12 22:55:37,976 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:55:55,067 - Epoch: [102][   70/   70]    Overall Loss 0.003205    Objective Loss 0.003205    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.243821    
2024-05-12 22:55:55,867 - 

2024-05-12 22:55:55,870 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:56:09,557 - Epoch: [103][   70/   70]    Overall Loss 0.003166    Objective Loss 0.003166    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.195250    
2024-05-12 22:56:10,268 - 

2024-05-12 22:56:10,270 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:56:22,898 - Epoch: [104][   70/   70]    Overall Loss 0.003092    Objective Loss 0.003092    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.180125    
2024-05-12 22:56:23,610 - 

2024-05-12 22:56:23,612 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:56:37,131 - Epoch: [105][   70/   70]    Overall Loss 0.003081    Objective Loss 0.003081    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.192844    
2024-05-12 22:56:37,863 - 

2024-05-12 22:56:37,865 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:56:50,762 - Epoch: [106][   70/   70]    Overall Loss 0.002968    Objective Loss 0.002968    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.183927    
2024-05-12 22:56:51,561 - 

2024-05-12 22:56:51,563 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:57:05,236 - Epoch: [107][   70/   70]    Overall Loss 0.002890    Objective Loss 0.002890    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.195004    
2024-05-12 22:57:06,242 - 

2024-05-12 22:57:06,244 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:57:19,408 - Epoch: [108][   70/   70]    Overall Loss 0.002953    Objective Loss 0.002953    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.187737    
2024-05-12 22:57:20,160 - 

2024-05-12 22:57:20,162 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:57:34,211 - Epoch: [109][   70/   70]    Overall Loss 0.002832    Objective Loss 0.002832    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.200372    
2024-05-12 22:57:34,890 - 

2024-05-12 22:57:34,891 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:57:48,269 - Epoch: [110][   70/   70]    Overall Loss 0.002885    Objective Loss 0.002885    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.190812    
2024-05-12 22:57:49,029 - --- validate (epoch=110)-----------
2024-05-12 22:57:49,030 - 1736 samples (100 per mini-batch)
2024-05-12 22:57:55,441 - Epoch: [110][   18/   18]    Loss 2.261254    Top1 60.023041    Top5 75.403226    
2024-05-12 22:57:56,152 - ==> Top1: 60.023    Top5: 75.403    Loss: 2.261

2024-05-12 22:57:56,162 - ==> Best [Top1: 60.253   Top5: 75.979   Sparsity:0.00   Params: 725752 on epoch: 90]
2024-05-12 22:57:56,164 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 22:57:56,279 - 

2024-05-12 22:57:56,280 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:58:11,122 - Epoch: [111][   70/   70]    Overall Loss 0.002793    Objective Loss 0.002793    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.211697    
2024-05-12 22:58:11,913 - 

2024-05-12 22:58:11,915 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:58:28,046 - Epoch: [112][   70/   70]    Overall Loss 0.002824    Objective Loss 0.002824    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.230131    
2024-05-12 22:58:28,768 - 

2024-05-12 22:58:28,769 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:58:43,572 - Epoch: [113][   70/   70]    Overall Loss 0.002807    Objective Loss 0.002807    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.211173    
2024-05-12 22:58:44,326 - 

2024-05-12 22:58:44,328 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:58:58,976 - Epoch: [114][   70/   70]    Overall Loss 0.002973    Objective Loss 0.002973    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.208907    
2024-05-12 22:58:59,645 - 

2024-05-12 22:58:59,647 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:59:14,037 - Epoch: [115][   70/   70]    Overall Loss 0.002719    Objective Loss 0.002719    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.205240    
2024-05-12 22:59:14,912 - 

2024-05-12 22:59:14,913 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:59:28,070 - Epoch: [116][   70/   70]    Overall Loss 0.002664    Objective Loss 0.002664    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.187666    
2024-05-12 22:59:28,854 - 

2024-05-12 22:59:28,855 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:59:42,421 - Epoch: [117][   70/   70]    Overall Loss 0.002614    Objective Loss 0.002614    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.193483    
2024-05-12 22:59:43,274 - 

2024-05-12 22:59:43,276 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 22:59:59,058 - Epoch: [118][   70/   70]    Overall Loss 0.002595    Objective Loss 0.002595    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.225130    
2024-05-12 22:59:59,901 - 

2024-05-12 22:59:59,902 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:00:13,585 - Epoch: [119][   70/   70]    Overall Loss 0.002545    Objective Loss 0.002545    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.195157    
2024-05-12 23:00:14,310 - 

2024-05-12 23:00:14,312 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:00:27,018 - Epoch: [120][   70/   70]    Overall Loss 0.002505    Objective Loss 0.002505    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.181229    
2024-05-12 23:00:27,866 - --- validate (epoch=120)-----------
2024-05-12 23:00:27,867 - 1736 samples (100 per mini-batch)
2024-05-12 23:00:35,510 - Epoch: [120][   18/   18]    Loss 2.236477    Top1 60.195853    Top5 75.288018    
2024-05-12 23:00:36,231 - ==> Top1: 60.196    Top5: 75.288    Loss: 2.236

2024-05-12 23:00:36,241 - ==> Best [Top1: 60.253   Top5: 75.979   Sparsity:0.00   Params: 725752 on epoch: 90]
2024-05-12 23:00:36,242 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 23:00:36,336 - 

2024-05-12 23:00:36,337 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:00:50,362 - Epoch: [121][   70/   70]    Overall Loss 0.002682    Objective Loss 0.002682    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.200026    
2024-05-12 23:00:51,204 - 

2024-05-12 23:00:51,206 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:01:06,748 - Epoch: [122][   70/   70]    Overall Loss 0.002506    Objective Loss 0.002506    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.221671    
2024-05-12 23:01:07,499 - 

2024-05-12 23:01:07,500 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:01:22,514 - Epoch: [123][   70/   70]    Overall Loss 0.002416    Objective Loss 0.002416    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.214143    
2024-05-12 23:01:23,327 - 

2024-05-12 23:01:23,328 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:01:35,683 - Epoch: [124][   70/   70]    Overall Loss 0.002405    Objective Loss 0.002405    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.176157    
2024-05-12 23:01:36,406 - 

2024-05-12 23:01:36,408 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:01:50,115 - Epoch: [125][   70/   70]    Overall Loss 0.002359    Objective Loss 0.002359    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.195533    
2024-05-12 23:01:50,828 - 

2024-05-12 23:01:50,830 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:02:03,480 - Epoch: [126][   70/   70]    Overall Loss 0.002292    Objective Loss 0.002292    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.180424    
2024-05-12 23:02:04,261 - 

2024-05-12 23:02:04,264 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:02:19,213 - Epoch: [127][   70/   70]    Overall Loss 0.002223    Objective Loss 0.002223    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.213238    
2024-05-12 23:02:19,999 - 

2024-05-12 23:02:20,000 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:02:34,331 - Epoch: [128][   70/   70]    Overall Loss 0.002258    Objective Loss 0.002258    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.204419    
2024-05-12 23:02:35,162 - 

2024-05-12 23:02:35,164 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:02:49,419 - Epoch: [129][   70/   70]    Overall Loss 0.002248    Objective Loss 0.002248    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.203334    
2024-05-12 23:02:50,134 - 

2024-05-12 23:02:50,136 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:03:03,746 - Epoch: [130][   70/   70]    Overall Loss 0.002184    Objective Loss 0.002184    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.194110    
2024-05-12 23:03:04,626 - --- validate (epoch=130)-----------
2024-05-12 23:03:04,627 - 1736 samples (100 per mini-batch)
2024-05-12 23:03:11,938 - Epoch: [130][   18/   18]    Loss 2.277726    Top1 60.023041    Top5 75.115207    
2024-05-12 23:03:12,804 - ==> Top1: 60.023    Top5: 75.115    Loss: 2.278

2024-05-12 23:03:12,818 - ==> Best [Top1: 60.253   Top5: 75.979   Sparsity:0.00   Params: 725752 on epoch: 90]
2024-05-12 23:03:12,819 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 23:03:12,921 - 

2024-05-12 23:03:12,923 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:03:27,525 - Epoch: [131][   70/   70]    Overall Loss 0.002225    Objective Loss 0.002225    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.208307    
2024-05-12 23:03:28,393 - 

2024-05-12 23:03:28,396 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:03:43,001 - Epoch: [132][   70/   70]    Overall Loss 0.002107    Objective Loss 0.002107    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.208305    
2024-05-12 23:03:43,811 - 

2024-05-12 23:03:43,813 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:03:58,501 - Epoch: [133][   70/   70]    Overall Loss 0.002159    Objective Loss 0.002159    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.209503    
2024-05-12 23:03:59,376 - 

2024-05-12 23:03:59,378 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:04:14,125 - Epoch: [134][   70/   70]    Overall Loss 0.002319    Objective Loss 0.002319    Top1 98.581560    Top5 100.000000    LR 0.000250    Time 0.210372    
2024-05-12 23:04:14,972 - 

2024-05-12 23:04:14,975 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:04:29,665 - Epoch: [135][   70/   70]    Overall Loss 0.002137    Objective Loss 0.002137    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.209498    
2024-05-12 23:04:30,404 - 

2024-05-12 23:04:30,406 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:04:44,414 - Epoch: [136][   70/   70]    Overall Loss 0.002066    Objective Loss 0.002066    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.199803    
2024-05-12 23:04:45,236 - 

2024-05-12 23:04:45,238 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:04:56,551 - Epoch: [137][   70/   70]    Overall Loss 0.002049    Objective Loss 0.002049    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.161367    
2024-05-12 23:04:57,249 - 

2024-05-12 23:04:57,251 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:05:09,659 - Epoch: [138][   70/   70]    Overall Loss 0.001976    Objective Loss 0.001976    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.176838    
2024-05-12 23:05:10,482 - 

2024-05-12 23:05:10,483 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:05:25,496 - Epoch: [139][   70/   70]    Overall Loss 0.002036    Objective Loss 0.002036    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.214190    
2024-05-12 23:05:26,313 - 

2024-05-12 23:05:26,315 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:05:41,198 - Epoch: [140][   70/   70]    Overall Loss 0.001941    Objective Loss 0.001941    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.212283    
2024-05-12 23:05:42,065 - --- validate (epoch=140)-----------
2024-05-12 23:05:42,068 - 1736 samples (100 per mini-batch)
2024-05-12 23:05:49,019 - Epoch: [140][   18/   18]    Loss 2.287514    Top1 60.080645    Top5 74.711982    
2024-05-12 23:05:49,800 - ==> Top1: 60.081    Top5: 74.712    Loss: 2.288

2024-05-12 23:05:49,812 - ==> Best [Top1: 60.253   Top5: 75.979   Sparsity:0.00   Params: 725752 on epoch: 90]
2024-05-12 23:05:49,812 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 23:05:49,915 - 

2024-05-12 23:05:49,917 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:06:05,312 - Epoch: [141][   70/   70]    Overall Loss 0.002065    Objective Loss 0.002065    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.219592    
2024-05-12 23:06:06,159 - 

2024-05-12 23:06:06,161 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:06:20,835 - Epoch: [142][   70/   70]    Overall Loss 0.001931    Objective Loss 0.001931    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.209272    
2024-05-12 23:06:21,652 - 

2024-05-12 23:06:21,654 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:06:35,642 - Epoch: [143][   70/   70]    Overall Loss 0.002144    Objective Loss 0.002144    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.199522    
2024-05-12 23:06:36,492 - 

2024-05-12 23:06:36,494 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:06:49,158 - Epoch: [144][   70/   70]    Overall Loss 0.001899    Objective Loss 0.001899    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.180601    
2024-05-12 23:06:49,984 - 

2024-05-12 23:06:49,986 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:07:05,522 - Epoch: [145][   70/   70]    Overall Loss 0.001913    Objective Loss 0.001913    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.221601    
2024-05-12 23:07:06,266 - 

2024-05-12 23:07:06,268 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:07:19,956 - Epoch: [146][   70/   70]    Overall Loss 0.001824    Objective Loss 0.001824    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.195240    
2024-05-12 23:07:20,621 - 

2024-05-12 23:07:20,623 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:07:33,773 - Epoch: [147][   70/   70]    Overall Loss 0.001824    Objective Loss 0.001824    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.187519    
2024-05-12 23:07:34,573 - 

2024-05-12 23:07:34,575 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:07:47,489 - Epoch: [148][   70/   70]    Overall Loss 0.001754    Objective Loss 0.001754    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.184169    
2024-05-12 23:07:48,272 - 

2024-05-12 23:07:48,273 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:08:01,668 - Epoch: [149][   70/   70]    Overall Loss 0.001687    Objective Loss 0.001687    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.191058    
2024-05-12 23:08:02,505 - 

2024-05-12 23:08:02,507 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:08:15,949 - Epoch: [150][   70/   70]    Overall Loss 0.001594    Objective Loss 0.001594    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.191675    
2024-05-12 23:08:16,722 - --- validate (epoch=150)-----------
2024-05-12 23:08:16,723 - 1736 samples (100 per mini-batch)
2024-05-12 23:08:23,599 - Epoch: [150][   18/   18]    Loss 2.327379    Top1 60.311060    Top5 74.884793    
2024-05-12 23:08:24,232 - ==> Top1: 60.311    Top5: 74.885    Loss: 2.327

2024-05-12 23:08:24,242 - ==> Best [Top1: 60.311   Top5: 74.885   Sparsity:0.00   Params: 725752 on epoch: 150]
2024-05-12 23:08:24,242 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 23:08:24,312 - 

2024-05-12 23:08:24,313 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:08:37,859 - Epoch: [151][   70/   70]    Overall Loss 0.001608    Objective Loss 0.001608    Top1 98.581560    Top5 100.000000    LR 0.000063    Time 0.193210    
2024-05-12 23:08:38,616 - 

2024-05-12 23:08:38,618 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:08:52,486 - Epoch: [152][   70/   70]    Overall Loss 0.001593    Objective Loss 0.001593    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.197780    
2024-05-12 23:08:53,241 - 

2024-05-12 23:08:53,243 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:09:07,086 - Epoch: [153][   70/   70]    Overall Loss 0.001615    Objective Loss 0.001615    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.197428    
2024-05-12 23:09:07,831 - 

2024-05-12 23:09:07,833 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:09:22,841 - Epoch: [154][   70/   70]    Overall Loss 0.001537    Objective Loss 0.001537    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.214049    
2024-05-12 23:09:23,592 - 

2024-05-12 23:09:23,593 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:09:37,902 - Epoch: [155][   70/   70]    Overall Loss 0.001595    Objective Loss 0.001595    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.204096    
2024-05-12 23:09:38,578 - 

2024-05-12 23:09:38,581 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:09:51,481 - Epoch: [156][   70/   70]    Overall Loss 0.001549    Objective Loss 0.001549    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.184002    
2024-05-12 23:09:52,278 - 

2024-05-12 23:09:52,280 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:10:05,933 - Epoch: [157][   70/   70]    Overall Loss 0.001573    Objective Loss 0.001573    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.194745    
2024-05-12 23:10:06,683 - 

2024-05-12 23:10:06,687 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:10:20,628 - Epoch: [158][   70/   70]    Overall Loss 0.001572    Objective Loss 0.001572    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.198832    
2024-05-12 23:10:21,382 - 

2024-05-12 23:10:21,383 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:10:34,896 - Epoch: [159][   70/   70]    Overall Loss 0.001555    Objective Loss 0.001555    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.192745    
2024-05-12 23:10:35,660 - 

2024-05-12 23:10:35,662 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:10:48,475 - Epoch: [160][   70/   70]    Overall Loss 0.001546    Objective Loss 0.001546    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.182735    
2024-05-12 23:10:49,221 - --- validate (epoch=160)-----------
2024-05-12 23:10:49,222 - 1736 samples (100 per mini-batch)
2024-05-12 23:10:57,203 - Epoch: [160][   18/   18]    Loss 2.333565    Top1 60.080645    Top5 74.596774    
2024-05-12 23:10:58,118 - ==> Top1: 60.081    Top5: 74.597    Loss: 2.334

2024-05-12 23:10:58,131 - ==> Best [Top1: 60.311   Top5: 74.885   Sparsity:0.00   Params: 725752 on epoch: 150]
2024-05-12 23:10:58,132 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 23:10:58,236 - 

2024-05-12 23:10:58,238 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:11:12,363 - Epoch: [161][   70/   70]    Overall Loss 0.001578    Objective Loss 0.001578    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.201503    
2024-05-12 23:11:13,115 - 

2024-05-12 23:11:13,117 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:11:28,613 - Epoch: [162][   70/   70]    Overall Loss 0.001576    Objective Loss 0.001576    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.221057    
2024-05-12 23:11:29,374 - 

2024-05-12 23:11:29,375 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:11:44,032 - Epoch: [163][   70/   70]    Overall Loss 0.001536    Objective Loss 0.001536    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.209033    
2024-05-12 23:11:44,774 - 

2024-05-12 23:11:44,776 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:11:56,296 - Epoch: [164][   70/   70]    Overall Loss 0.001539    Objective Loss 0.001539    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.164279    
2024-05-12 23:11:57,160 - 

2024-05-12 23:11:57,161 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:12:11,728 - Epoch: [165][   70/   70]    Overall Loss 0.001545    Objective Loss 0.001545    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.207786    
2024-05-12 23:12:12,565 - 

2024-05-12 23:12:12,567 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:12:25,583 - Epoch: [166][   70/   70]    Overall Loss 0.001490    Objective Loss 0.001490    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.185647    
2024-05-12 23:12:26,523 - 

2024-05-12 23:12:26,526 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:12:38,995 - Epoch: [167][   70/   70]    Overall Loss 0.001492    Objective Loss 0.001492    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.177783    
2024-05-12 23:12:39,775 - 

2024-05-12 23:12:39,776 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:12:52,350 - Epoch: [168][   70/   70]    Overall Loss 0.001497    Objective Loss 0.001497    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.179335    
2024-05-12 23:12:53,062 - 

2024-05-12 23:12:53,064 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:13:06,975 - Epoch: [169][   70/   70]    Overall Loss 0.001487    Objective Loss 0.001487    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.198394    
2024-05-12 23:13:07,808 - 

2024-05-12 23:13:07,811 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:13:20,510 - Epoch: [170][   70/   70]    Overall Loss 0.001474    Objective Loss 0.001474    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.181049    
2024-05-12 23:13:21,294 - --- validate (epoch=170)-----------
2024-05-12 23:13:21,297 - 1736 samples (100 per mini-batch)
2024-05-12 23:13:28,516 - Epoch: [170][   18/   18]    Loss 2.367022    Top1 59.907834    Top5 74.539171    
2024-05-12 23:13:29,362 - ==> Top1: 59.908    Top5: 74.539    Loss: 2.367

2024-05-12 23:13:29,378 - ==> Best [Top1: 60.311   Top5: 74.885   Sparsity:0.00   Params: 725752 on epoch: 150]
2024-05-12 23:13:29,379 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 23:13:29,507 - 

2024-05-12 23:13:29,509 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:13:44,657 - Epoch: [171][   70/   70]    Overall Loss 0.001536    Objective Loss 0.001536    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.216039    
2024-05-12 23:13:45,587 - 

2024-05-12 23:13:45,589 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:13:59,834 - Epoch: [172][   70/   70]    Overall Loss 0.001501    Objective Loss 0.001501    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.203188    
2024-05-12 23:14:00,550 - 

2024-05-12 23:14:00,552 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:14:14,670 - Epoch: [173][   70/   70]    Overall Loss 0.001443    Objective Loss 0.001443    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.201346    
2024-05-12 23:14:15,498 - 

2024-05-12 23:14:15,499 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:14:30,214 - Epoch: [174][   70/   70]    Overall Loss 0.001492    Objective Loss 0.001492    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.209883    
2024-05-12 23:14:30,966 - 

2024-05-12 23:14:30,968 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:14:45,808 - Epoch: [175][   70/   70]    Overall Loss 0.001469    Objective Loss 0.001469    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.211646    
2024-05-12 23:14:46,667 - 

2024-05-12 23:14:46,671 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:15:00,196 - Epoch: [176][   70/   70]    Overall Loss 0.001441    Objective Loss 0.001441    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.192883    
2024-05-12 23:15:01,002 - 

2024-05-12 23:15:01,004 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:15:12,230 - Epoch: [177][   70/   70]    Overall Loss 0.001452    Objective Loss 0.001452    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.160133    
2024-05-12 23:15:12,942 - 

2024-05-12 23:15:12,944 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:15:26,173 - Epoch: [178][   70/   70]    Overall Loss 0.001507    Objective Loss 0.001507    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.188689    
2024-05-12 23:15:26,920 - 

2024-05-12 23:15:26,923 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:15:39,749 - Epoch: [179][   70/   70]    Overall Loss 0.001433    Objective Loss 0.001433    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.182893    
2024-05-12 23:15:40,590 - 

2024-05-12 23:15:40,592 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:15:54,031 - Epoch: [180][   70/   70]    Overall Loss 0.001411    Objective Loss 0.001411    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.191702    
2024-05-12 23:15:54,806 - --- validate (epoch=180)-----------
2024-05-12 23:15:54,807 - 1736 samples (100 per mini-batch)
2024-05-12 23:16:01,930 - Epoch: [180][   18/   18]    Loss 2.396404    Top1 59.735023    Top5 74.654378    
2024-05-12 23:16:02,737 - ==> Top1: 59.735    Top5: 74.654    Loss: 2.396

2024-05-12 23:16:02,747 - ==> Best [Top1: 60.311   Top5: 74.885   Sparsity:0.00   Params: 725752 on epoch: 150]
2024-05-12 23:16:02,748 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 23:16:02,848 - 

2024-05-12 23:16:02,849 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:16:17,163 - Epoch: [181][   70/   70]    Overall Loss 0.001460    Objective Loss 0.001460    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.204126    
2024-05-12 23:16:17,995 - 

2024-05-12 23:16:17,996 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:16:35,226 - Epoch: [182][   70/   70]    Overall Loss 0.001381    Objective Loss 0.001381    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.245837    
2024-05-12 23:16:36,005 - 

2024-05-12 23:16:36,009 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:16:50,035 - Epoch: [183][   70/   70]    Overall Loss 0.001425    Objective Loss 0.001425    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.200063    
2024-05-12 23:16:50,784 - 

2024-05-12 23:16:50,786 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:17:05,283 - Epoch: [184][   70/   70]    Overall Loss 0.001457    Objective Loss 0.001457    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.206816    
2024-05-12 23:17:05,934 - 

2024-05-12 23:17:05,936 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:17:19,558 - Epoch: [185][   70/   70]    Overall Loss 0.001380    Objective Loss 0.001380    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.194283    
2024-05-12 23:17:20,232 - 

2024-05-12 23:17:20,234 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:17:32,779 - Epoch: [186][   70/   70]    Overall Loss 0.001397    Objective Loss 0.001397    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.178917    
2024-05-12 23:17:33,609 - 

2024-05-12 23:17:33,611 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:17:46,726 - Epoch: [187][   70/   70]    Overall Loss 0.001403    Objective Loss 0.001403    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.187021    
2024-05-12 23:17:47,511 - 

2024-05-12 23:17:47,513 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:18:00,830 - Epoch: [188][   70/   70]    Overall Loss 0.001363    Objective Loss 0.001363    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.189944    
2024-05-12 23:18:01,592 - 

2024-05-12 23:18:01,595 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:18:16,541 - Epoch: [189][   70/   70]    Overall Loss 0.001394    Objective Loss 0.001394    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.213177    
2024-05-12 23:18:17,271 - 

2024-05-12 23:18:17,273 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:18:29,874 - Epoch: [190][   70/   70]    Overall Loss 0.001364    Objective Loss 0.001364    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.179738    
2024-05-12 23:18:30,548 - --- validate (epoch=190)-----------
2024-05-12 23:18:30,549 - 1736 samples (100 per mini-batch)
2024-05-12 23:18:37,825 - Epoch: [190][   18/   18]    Loss 2.371828    Top1 59.850230    Top5 74.539171    
2024-05-12 23:18:38,659 - ==> Top1: 59.850    Top5: 74.539    Loss: 2.372

2024-05-12 23:18:38,674 - ==> Best [Top1: 60.311   Top5: 74.885   Sparsity:0.00   Params: 725752 on epoch: 150]
2024-05-12 23:18:38,675 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 23:18:38,805 - 

2024-05-12 23:18:38,807 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:18:51,939 - Epoch: [191][   70/   70]    Overall Loss 0.001368    Objective Loss 0.001368    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.187280    
2024-05-12 23:18:52,758 - 

2024-05-12 23:18:52,760 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:19:06,727 - Epoch: [192][   70/   70]    Overall Loss 0.001339    Objective Loss 0.001339    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.199244    
2024-05-12 23:19:07,478 - 

2024-05-12 23:19:07,480 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:19:23,640 - Epoch: [193][   70/   70]    Overall Loss 0.001567    Objective Loss 0.001567    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.230540    
2024-05-12 23:19:24,521 - 

2024-05-12 23:19:24,523 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:19:37,316 - Epoch: [194][   70/   70]    Overall Loss 0.001309    Objective Loss 0.001309    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.182503    
2024-05-12 23:19:38,231 - 

2024-05-12 23:19:38,234 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:19:51,153 - Epoch: [195][   70/   70]    Overall Loss 0.001415    Objective Loss 0.001415    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.184235    
2024-05-12 23:19:52,055 - 

2024-05-12 23:19:52,058 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:20:06,199 - Epoch: [196][   70/   70]    Overall Loss 0.001307    Objective Loss 0.001307    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.201663    
2024-05-12 23:20:07,016 - 

2024-05-12 23:20:07,017 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:20:19,886 - Epoch: [197][   70/   70]    Overall Loss 0.001282    Objective Loss 0.001282    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.183515    
2024-05-12 23:20:20,590 - 

2024-05-12 23:20:20,592 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:20:33,853 - Epoch: [198][   70/   70]    Overall Loss 0.001346    Objective Loss 0.001346    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.189138    
2024-05-12 23:20:34,699 - 

2024-05-12 23:20:34,700 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:20:48,330 - Epoch: [199][   70/   70]    Overall Loss 0.001283    Objective Loss 0.001283    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.194402    
2024-05-12 23:20:49,249 - 

2024-05-12 23:20:49,251 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:21:02,635 - Epoch: [200][   70/   70]    Overall Loss 0.001228    Objective Loss 0.001228    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.190889    
2024-05-12 23:21:03,358 - --- validate (epoch=200)-----------
2024-05-12 23:21:03,359 - 1736 samples (100 per mini-batch)
2024-05-12 23:21:10,260 - Epoch: [200][   18/   18]    Loss 2.390252    Top1 59.677419    Top5 74.366359    
2024-05-12 23:21:11,081 - ==> Top1: 59.677    Top5: 74.366    Loss: 2.390

2024-05-12 23:21:11,090 - ==> Best [Top1: 60.311   Top5: 74.885   Sparsity:0.00   Params: 725752 on epoch: 150]
2024-05-12 23:21:11,092 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 23:21:11,190 - 

2024-05-12 23:21:11,192 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:21:25,659 - Epoch: [201][   70/   70]    Overall Loss 0.001225    Objective Loss 0.001225    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.206366    
2024-05-12 23:21:26,479 - 

2024-05-12 23:21:26,481 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:21:41,596 - Epoch: [202][   70/   70]    Overall Loss 0.001202    Objective Loss 0.001202    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.215587    
2024-05-12 23:21:42,694 - 

2024-05-12 23:21:42,696 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:21:56,508 - Epoch: [203][   70/   70]    Overall Loss 0.001240    Objective Loss 0.001240    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.196967    
2024-05-12 23:21:57,359 - 

2024-05-12 23:21:57,361 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:22:12,051 - Epoch: [204][   70/   70]    Overall Loss 0.001204    Objective Loss 0.001204    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.209562    
2024-05-12 23:22:12,907 - 

2024-05-12 23:22:12,909 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:22:27,888 - Epoch: [205][   70/   70]    Overall Loss 0.001368    Objective Loss 0.001368    Top1 98.581560    Top5 100.000000    LR 0.000016    Time 0.213658    
2024-05-12 23:22:28,651 - 

2024-05-12 23:22:28,653 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:22:42,827 - Epoch: [206][   70/   70]    Overall Loss 0.001222    Objective Loss 0.001222    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.202189    
2024-05-12 23:22:43,505 - 

2024-05-12 23:22:43,506 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:23:00,877 - Epoch: [207][   70/   70]    Overall Loss 0.001236    Objective Loss 0.001236    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.247843    
2024-05-12 23:23:01,800 - 

2024-05-12 23:23:01,801 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:23:18,691 - Epoch: [208][   70/   70]    Overall Loss 0.001203    Objective Loss 0.001203    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.240956    
2024-05-12 23:23:19,507 - 

2024-05-12 23:23:19,509 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:23:35,270 - Epoch: [209][   70/   70]    Overall Loss 0.001237    Objective Loss 0.001237    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.224789    
2024-05-12 23:23:36,166 - 

2024-05-12 23:23:36,168 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:23:54,507 - Epoch: [210][   70/   70]    Overall Loss 0.001346    Objective Loss 0.001346    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.261592    
2024-05-12 23:23:55,345 - --- validate (epoch=210)-----------
2024-05-12 23:23:55,347 - 1736 samples (100 per mini-batch)
2024-05-12 23:24:03,362 - Epoch: [210][   18/   18]    Loss 2.421397    Top1 59.389401    Top5 74.423963    
2024-05-12 23:24:04,012 - ==> Top1: 59.389    Top5: 74.424    Loss: 2.421

2024-05-12 23:24:04,024 - ==> Best [Top1: 60.311   Top5: 74.885   Sparsity:0.00   Params: 725752 on epoch: 150]
2024-05-12 23:24:04,025 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 23:24:04,139 - 

2024-05-12 23:24:04,141 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:24:20,368 - Epoch: [211][   70/   70]    Overall Loss 0.001237    Objective Loss 0.001237    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.231481    
2024-05-12 23:24:21,358 - 

2024-05-12 23:24:21,360 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:24:36,843 - Epoch: [212][   70/   70]    Overall Loss 0.001194    Objective Loss 0.001194    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.220847    
2024-05-12 23:24:37,691 - 

2024-05-12 23:24:37,694 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:24:51,778 - Epoch: [213][   70/   70]    Overall Loss 0.001252    Objective Loss 0.001252    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.200855    
2024-05-12 23:24:52,470 - 

2024-05-12 23:24:52,471 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:25:07,988 - Epoch: [214][   70/   70]    Overall Loss 0.001164    Objective Loss 0.001164    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.221372    
2024-05-12 23:25:08,789 - 

2024-05-12 23:25:08,790 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:25:24,895 - Epoch: [215][   70/   70]    Overall Loss 0.001226    Objective Loss 0.001226    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.229779    
2024-05-12 23:25:25,596 - 

2024-05-12 23:25:25,599 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:25:39,242 - Epoch: [216][   70/   70]    Overall Loss 0.001198    Objective Loss 0.001198    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.194553    
2024-05-12 23:25:40,124 - 

2024-05-12 23:25:40,127 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:25:54,390 - Epoch: [217][   70/   70]    Overall Loss 0.001201    Objective Loss 0.001201    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.203414    
2024-05-12 23:25:55,154 - 

2024-05-12 23:25:55,155 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:26:09,558 - Epoch: [218][   70/   70]    Overall Loss 0.001211    Objective Loss 0.001211    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.205432    
2024-05-12 23:26:10,385 - 

2024-05-12 23:26:10,387 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:26:25,763 - Epoch: [219][   70/   70]    Overall Loss 0.001229    Objective Loss 0.001229    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.219288    
2024-05-12 23:26:26,632 - 

2024-05-12 23:26:26,635 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:26:41,434 - Epoch: [220][   70/   70]    Overall Loss 0.001151    Objective Loss 0.001151    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.211082    
2024-05-12 23:26:42,364 - --- validate (epoch=220)-----------
2024-05-12 23:26:42,367 - 1736 samples (100 per mini-batch)
2024-05-12 23:26:49,291 - Epoch: [220][   18/   18]    Loss 2.398079    Top1 59.504608    Top5 74.423963    
2024-05-12 23:26:50,079 - ==> Top1: 59.505    Top5: 74.424    Loss: 2.398

2024-05-12 23:26:50,091 - ==> Best [Top1: 60.311   Top5: 74.885   Sparsity:0.00   Params: 725752 on epoch: 150]
2024-05-12 23:26:50,092 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 23:26:50,219 - 

2024-05-12 23:26:50,221 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:27:03,991 - Epoch: [221][   70/   70]    Overall Loss 0.001186    Objective Loss 0.001186    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.196381    
2024-05-12 23:27:04,790 - 

2024-05-12 23:27:04,791 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:27:17,772 - Epoch: [222][   70/   70]    Overall Loss 0.001178    Objective Loss 0.001178    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.185097    
2024-05-12 23:27:18,627 - 

2024-05-12 23:27:18,630 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:27:32,876 - Epoch: [223][   70/   70]    Overall Loss 0.001161    Objective Loss 0.001161    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.203203    
2024-05-12 23:27:33,714 - 

2024-05-12 23:27:33,716 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:27:48,394 - Epoch: [224][   70/   70]    Overall Loss 0.001192    Objective Loss 0.001192    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.209359    
2024-05-12 23:27:49,184 - 

2024-05-12 23:27:49,185 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:28:03,567 - Epoch: [225][   70/   70]    Overall Loss 0.001176    Objective Loss 0.001176    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.205133    
2024-05-12 23:28:04,328 - 

2024-05-12 23:28:04,329 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:28:19,402 - Epoch: [226][   70/   70]    Overall Loss 0.001158    Objective Loss 0.001158    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.215038    
2024-05-12 23:28:20,259 - 

2024-05-12 23:28:20,261 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:28:35,741 - Epoch: [227][   70/   70]    Overall Loss 0.001182    Objective Loss 0.001182    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.220797    
2024-05-12 23:28:36,487 - 

2024-05-12 23:28:36,489 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:28:49,972 - Epoch: [228][   70/   70]    Overall Loss 0.001193    Objective Loss 0.001193    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.192291    
2024-05-12 23:28:50,746 - 

2024-05-12 23:28:50,748 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:29:05,550 - Epoch: [229][   70/   70]    Overall Loss 0.001174    Objective Loss 0.001174    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.211142    
2024-05-12 23:29:06,277 - 

2024-05-12 23:29:06,278 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:29:19,425 - Epoch: [230][   70/   70]    Overall Loss 0.001167    Objective Loss 0.001167    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.187546    
2024-05-12 23:29:20,417 - --- validate (epoch=230)-----------
2024-05-12 23:29:20,420 - 1736 samples (100 per mini-batch)
2024-05-12 23:29:27,029 - Epoch: [230][   18/   18]    Loss 2.392339    Top1 59.562212    Top5 74.423963    
2024-05-12 23:29:27,721 - ==> Top1: 59.562    Top5: 74.424    Loss: 2.392

2024-05-12 23:29:27,733 - ==> Best [Top1: 60.311   Top5: 74.885   Sparsity:0.00   Params: 725752 on epoch: 150]
2024-05-12 23:29:27,734 - Saving checkpoint to: logs/2024.05.12-222626/checkpoint.pth.tar
2024-05-12 23:29:27,820 - 

2024-05-12 23:29:27,821 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:29:41,650 - Epoch: [231][   70/   70]    Overall Loss 0.001337    Objective Loss 0.001337    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.197267    
2024-05-12 23:29:42,415 - 

2024-05-12 23:29:42,417 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:29:55,744 - Epoch: [232][   70/   70]    Overall Loss 0.001169    Objective Loss 0.001169    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.190065    
2024-05-12 23:29:56,664 - 

2024-05-12 23:29:56,666 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:30:11,631 - Epoch: [233][   70/   70]    Overall Loss 0.001135    Objective Loss 0.001135    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.213482    
2024-05-12 23:30:12,343 - 

2024-05-12 23:30:12,344 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:30:27,118 - Epoch: [234][   70/   70]    Overall Loss 0.001161    Objective Loss 0.001161    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.210751    
2024-05-12 23:30:27,809 - 

2024-05-12 23:30:27,810 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:30:41,964 - Epoch: [235][   70/   70]    Overall Loss 0.001154    Objective Loss 0.001154    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.201888    
2024-05-12 23:30:42,669 - 

2024-05-12 23:30:42,670 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:30:56,726 - Epoch: [236][   70/   70]    Overall Loss 0.001163    Objective Loss 0.001163    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.200465    
2024-05-12 23:30:57,616 - 

2024-05-12 23:30:57,618 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:31:11,130 - Epoch: [237][   70/   70]    Overall Loss 0.001156    Objective Loss 0.001156    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.192680    
2024-05-12 23:31:11,989 - 

2024-05-12 23:31:11,992 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:31:27,352 - Epoch: [238][   70/   70]    Overall Loss 0.001142    Objective Loss 0.001142    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.219070    
2024-05-12 23:31:28,197 - 

2024-05-12 23:31:28,198 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:31:42,436 - Epoch: [239][   70/   70]    Overall Loss 0.001135    Objective Loss 0.001135    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.203112    
2024-05-12 23:31:43,272 - 

2024-05-12 23:31:43,273 - Initiating quantization aware training (QAT)...
2024-05-12 23:31:43,341 - 

2024-05-12 23:31:43,342 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:31:56,746 - Epoch: [240][   70/   70]    Overall Loss 1.921894    Objective Loss 1.921894    Top1 82.269504    Top5 94.326241    LR 0.000016    Time 0.191226    
2024-05-12 23:31:57,629 - --- validate (epoch=240)-----------
2024-05-12 23:31:57,631 - 1736 samples (100 per mini-batch)
2024-05-12 23:32:05,271 - Epoch: [240][   18/   18]    Loss 2.126040    Top1 53.629032    Top5 71.082949    
2024-05-12 23:32:06,008 - ==> Top1: 53.629    Top5: 71.083    Loss: 2.126

2024-05-12 23:32:06,018 - ==> Best [Top1: 53.629   Top5: 71.083   Sparsity:0.00   Params: 725752 on epoch: 240]
2024-05-12 23:32:06,018 - Saving checkpoint to: logs/2024.05.12-222626/qat_checkpoint.pth.tar
2024-05-12 23:32:06,130 - 

2024-05-12 23:32:06,132 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:32:20,214 - Epoch: [241][   70/   70]    Overall Loss 0.475371    Objective Loss 0.475371    Top1 90.780142    Top5 98.581560    LR 0.000016    Time 0.200856    
2024-05-12 23:32:20,904 - 

2024-05-12 23:32:20,906 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:32:36,449 - Epoch: [242][   70/   70]    Overall Loss 0.285131    Objective Loss 0.285131    Top1 93.617021    Top5 98.581560    LR 0.000016    Time 0.221743    
2024-05-12 23:32:37,232 - 

2024-05-12 23:32:37,233 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:32:51,916 - Epoch: [243][   70/   70]    Overall Loss 0.209078    Objective Loss 0.209078    Top1 98.581560    Top5 100.000000    LR 0.000016    Time 0.209449    
2024-05-12 23:32:52,662 - 

2024-05-12 23:32:52,664 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:33:05,164 - Epoch: [244][   70/   70]    Overall Loss 0.157215    Objective Loss 0.157215    Top1 98.581560    Top5 100.000000    LR 0.000016    Time 0.178258    
2024-05-12 23:33:06,026 - 

2024-05-12 23:33:06,028 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:33:21,625 - Epoch: [245][   70/   70]    Overall Loss 0.121309    Objective Loss 0.121309    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.222475    
2024-05-12 23:33:22,419 - 

2024-05-12 23:33:22,421 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:33:36,960 - Epoch: [246][   70/   70]    Overall Loss 0.104290    Objective Loss 0.104290    Top1 97.872340    Top5 100.000000    LR 0.000016    Time 0.207321    
2024-05-12 23:33:37,795 - 

2024-05-12 23:33:37,798 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:33:51,918 - Epoch: [247][   70/   70]    Overall Loss 0.090621    Objective Loss 0.090621    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.201380    
2024-05-12 23:33:52,669 - 

2024-05-12 23:33:52,671 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:34:07,277 - Epoch: [248][   70/   70]    Overall Loss 0.077167    Objective Loss 0.077167    Top1 96.453901    Top5 100.000000    LR 0.000016    Time 0.208376    
2024-05-12 23:34:07,988 - 

2024-05-12 23:34:07,989 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:34:25,312 - Epoch: [249][   70/   70]    Overall Loss 0.069523    Objective Loss 0.069523    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.247130    
2024-05-12 23:34:26,269 - 

2024-05-12 23:34:26,271 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:34:40,078 - Epoch: [250][   70/   70]    Overall Loss 0.058864    Objective Loss 0.058864    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.196927    
2024-05-12 23:34:40,743 - --- validate (epoch=250)-----------
2024-05-12 23:34:40,745 - 1736 samples (100 per mini-batch)
2024-05-12 23:34:48,372 - Epoch: [250][   18/   18]    Loss 2.312277    Top1 57.315668    Top5 74.308756    
2024-05-12 23:34:49,111 - ==> Top1: 57.316    Top5: 74.309    Loss: 2.312

2024-05-12 23:34:49,122 - ==> Best [Top1: 57.316   Top5: 74.309   Sparsity:0.00   Params: 725752 on epoch: 250]
2024-05-12 23:34:49,123 - Saving checkpoint to: logs/2024.05.12-222626/qat_checkpoint.pth.tar
2024-05-12 23:34:49,277 - 

2024-05-12 23:34:49,279 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:35:03,444 - Epoch: [251][   70/   70]    Overall Loss 0.053023    Objective Loss 0.053023    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.201996    
2024-05-12 23:35:04,175 - 

2024-05-12 23:35:04,177 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:35:17,458 - Epoch: [252][   70/   70]    Overall Loss 0.047852    Objective Loss 0.047852    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.189387    
2024-05-12 23:35:18,275 - 

2024-05-12 23:35:18,278 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:35:31,401 - Epoch: [253][   70/   70]    Overall Loss 0.043332    Objective Loss 0.043332    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.187136    
2024-05-12 23:35:32,185 - 

2024-05-12 23:35:32,187 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:35:45,195 - Epoch: [254][   70/   70]    Overall Loss 0.041415    Objective Loss 0.041415    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.185525    
2024-05-12 23:35:45,863 - 

2024-05-12 23:35:45,864 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:35:58,704 - Epoch: [255][   70/   70]    Overall Loss 0.038738    Objective Loss 0.038738    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.183109    
2024-05-12 23:35:59,551 - 

2024-05-12 23:35:59,553 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:36:12,844 - Epoch: [256][   70/   70]    Overall Loss 0.035785    Objective Loss 0.035785    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.189594    
2024-05-12 23:36:13,643 - 

2024-05-12 23:36:13,645 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:36:27,913 - Epoch: [257][   70/   70]    Overall Loss 0.032932    Objective Loss 0.032932    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.203515    
2024-05-12 23:36:28,720 - 

2024-05-12 23:36:28,722 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:36:42,180 - Epoch: [258][   70/   70]    Overall Loss 0.030622    Objective Loss 0.030622    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.191891    
2024-05-12 23:36:42,878 - 

2024-05-12 23:36:42,879 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:36:55,113 - Epoch: [259][   70/   70]    Overall Loss 0.028867    Objective Loss 0.028867    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.174480    
2024-05-12 23:36:55,891 - 

2024-05-12 23:36:55,892 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:37:09,765 - Epoch: [260][   70/   70]    Overall Loss 0.027015    Objective Loss 0.027015    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.197878    
2024-05-12 23:37:10,614 - --- validate (epoch=260)-----------
2024-05-12 23:37:10,618 - 1736 samples (100 per mini-batch)
2024-05-12 23:37:18,355 - Epoch: [260][   18/   18]    Loss 2.457610    Top1 56.912442    Top5 73.444700    
2024-05-12 23:37:19,077 - ==> Top1: 56.912    Top5: 73.445    Loss: 2.458

2024-05-12 23:37:19,090 - ==> Best [Top1: 57.316   Top5: 74.309   Sparsity:0.00   Params: 725752 on epoch: 250]
2024-05-12 23:37:19,091 - Saving checkpoint to: logs/2024.05.12-222626/qat_checkpoint.pth.tar
2024-05-12 23:37:19,196 - 

2024-05-12 23:37:19,197 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:37:33,374 - Epoch: [261][   70/   70]    Overall Loss 0.025785    Objective Loss 0.025785    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.202234    
2024-05-12 23:37:34,040 - 

2024-05-12 23:37:34,041 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:37:49,469 - Epoch: [262][   70/   70]    Overall Loss 0.023860    Objective Loss 0.023860    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.220111    
2024-05-12 23:37:50,179 - 

2024-05-12 23:37:50,180 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:38:03,397 - Epoch: [263][   70/   70]    Overall Loss 0.023787    Objective Loss 0.023787    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.188500    
2024-05-12 23:38:04,249 - 

2024-05-12 23:38:04,251 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:38:18,583 - Epoch: [264][   70/   70]    Overall Loss 0.022928    Objective Loss 0.022928    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.204419    
2024-05-12 23:38:19,376 - 

2024-05-12 23:38:19,379 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:38:33,488 - Epoch: [265][   70/   70]    Overall Loss 0.020719    Objective Loss 0.020719    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.201235    
2024-05-12 23:38:34,285 - 

2024-05-12 23:38:34,287 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:38:48,450 - Epoch: [266][   70/   70]    Overall Loss 0.021995    Objective Loss 0.021995    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.201981    
2024-05-12 23:38:49,319 - 

2024-05-12 23:38:49,321 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:39:04,302 - Epoch: [267][   70/   70]    Overall Loss 0.020383    Objective Loss 0.020383    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.213708    
2024-05-12 23:39:05,017 - 

2024-05-12 23:39:05,018 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:39:16,519 - Epoch: [268][   70/   70]    Overall Loss 0.018551    Objective Loss 0.018551    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.164044    
2024-05-12 23:39:17,259 - 

2024-05-12 23:39:17,260 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:39:31,053 - Epoch: [269][   70/   70]    Overall Loss 0.019492    Objective Loss 0.019492    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.196769    
2024-05-12 23:39:31,826 - 

2024-05-12 23:39:31,828 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:39:46,845 - Epoch: [270][   70/   70]    Overall Loss 0.018042    Objective Loss 0.018042    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.214236    
2024-05-12 23:39:47,552 - --- validate (epoch=270)-----------
2024-05-12 23:39:47,553 - 1736 samples (100 per mini-batch)
2024-05-12 23:39:54,659 - Epoch: [270][   18/   18]    Loss 2.512585    Top1 58.064516    Top5 74.654378    
2024-05-12 23:39:55,486 - ==> Top1: 58.065    Top5: 74.654    Loss: 2.513

2024-05-12 23:39:55,503 - ==> Best [Top1: 58.065   Top5: 74.654   Sparsity:0.00   Params: 725752 on epoch: 270]
2024-05-12 23:39:55,504 - Saving checkpoint to: logs/2024.05.12-222626/qat_checkpoint.pth.tar
2024-05-12 23:39:55,729 - 

2024-05-12 23:39:55,732 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:40:11,157 - Epoch: [271][   70/   70]    Overall Loss 0.017627    Objective Loss 0.017627    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.220015    
2024-05-12 23:40:11,924 - 

2024-05-12 23:40:11,926 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:40:26,094 - Epoch: [272][   70/   70]    Overall Loss 0.015650    Objective Loss 0.015650    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.202095    
2024-05-12 23:40:26,966 - 

2024-05-12 23:40:26,967 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:40:42,140 - Epoch: [273][   70/   70]    Overall Loss 0.016015    Objective Loss 0.016015    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.216424    
2024-05-12 23:40:42,960 - 

2024-05-12 23:40:42,962 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:40:56,718 - Epoch: [274][   70/   70]    Overall Loss 0.015956    Objective Loss 0.015956    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.196235    
2024-05-12 23:40:57,794 - 

2024-05-12 23:40:57,796 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:41:13,935 - Epoch: [275][   70/   70]    Overall Loss 0.014282    Objective Loss 0.014282    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.230232    
2024-05-12 23:41:14,676 - 

2024-05-12 23:41:14,678 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:41:29,524 - Epoch: [276][   70/   70]    Overall Loss 0.014211    Objective Loss 0.014211    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.211776    
2024-05-12 23:41:30,166 - 

2024-05-12 23:41:30,167 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:41:43,306 - Epoch: [277][   70/   70]    Overall Loss 0.013449    Objective Loss 0.013449    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.187384    
2024-05-12 23:41:44,082 - 

2024-05-12 23:41:44,083 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:41:58,474 - Epoch: [278][   70/   70]    Overall Loss 0.013406    Objective Loss 0.013406    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.205268    
2024-05-12 23:41:59,376 - 

2024-05-12 23:41:59,378 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:42:12,750 - Epoch: [279][   70/   70]    Overall Loss 0.013386    Objective Loss 0.013386    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.190724    
2024-05-12 23:42:13,625 - 

2024-05-12 23:42:13,626 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:42:26,216 - Epoch: [280][   70/   70]    Overall Loss 0.012810    Objective Loss 0.012810    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.179602    
2024-05-12 23:42:27,097 - --- validate (epoch=280)-----------
2024-05-12 23:42:27,098 - 1736 samples (100 per mini-batch)
2024-05-12 23:42:34,912 - Epoch: [280][   18/   18]    Loss 2.518657    Top1 57.661290    Top5 74.078341    
2024-05-12 23:42:35,736 - ==> Top1: 57.661    Top5: 74.078    Loss: 2.519

2024-05-12 23:42:35,751 - ==> Best [Top1: 58.065   Top5: 74.654   Sparsity:0.00   Params: 725752 on epoch: 270]
2024-05-12 23:42:35,753 - Saving checkpoint to: logs/2024.05.12-222626/qat_checkpoint.pth.tar
2024-05-12 23:42:35,879 - 

2024-05-12 23:42:35,880 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:42:52,403 - Epoch: [281][   70/   70]    Overall Loss 0.013416    Objective Loss 0.013416    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.235693    
2024-05-12 23:42:53,359 - 

2024-05-12 23:42:53,361 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:43:11,701 - Epoch: [282][   70/   70]    Overall Loss 0.015697    Objective Loss 0.015697    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.261652    
2024-05-12 23:43:12,527 - 

2024-05-12 23:43:12,530 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:43:30,230 - Epoch: [283][   70/   70]    Overall Loss 0.013769    Objective Loss 0.013769    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.252473    
2024-05-12 23:43:31,053 - 

2024-05-12 23:43:31,055 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:43:46,130 - Epoch: [284][   70/   70]    Overall Loss 0.011653    Objective Loss 0.011653    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.215013    
2024-05-12 23:43:47,177 - 

2024-05-12 23:43:47,180 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:44:05,057 - Epoch: [285][   70/   70]    Overall Loss 0.011613    Objective Loss 0.011613    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.255023    
2024-05-12 23:44:05,803 - 

2024-05-12 23:44:05,805 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:44:24,191 - Epoch: [286][   70/   70]    Overall Loss 0.011724    Objective Loss 0.011724    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.262297    
2024-05-12 23:44:25,027 - 

2024-05-12 23:44:25,029 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:44:42,131 - Epoch: [287][   70/   70]    Overall Loss 0.011127    Objective Loss 0.011127    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.243935    
2024-05-12 23:44:42,917 - 

2024-05-12 23:44:42,919 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:44:59,972 - Epoch: [288][   70/   70]    Overall Loss 0.010878    Objective Loss 0.010878    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.243255    
2024-05-12 23:45:00,803 - 

2024-05-12 23:45:00,805 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:45:19,443 - Epoch: [289][   70/   70]    Overall Loss 0.010107    Objective Loss 0.010107    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.265914    
2024-05-12 23:45:20,618 - 

2024-05-12 23:45:20,620 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:45:32,740 - Epoch: [290][   70/   70]    Overall Loss 0.010918    Objective Loss 0.010918    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.172852    
2024-05-12 23:45:33,424 - --- validate (epoch=290)-----------
2024-05-12 23:45:33,426 - 1736 samples (100 per mini-batch)
2024-05-12 23:45:43,115 - Epoch: [290][   18/   18]    Loss 2.553032    Top1 58.410138    Top5 75.288018    
2024-05-12 23:45:43,885 - ==> Top1: 58.410    Top5: 75.288    Loss: 2.553

2024-05-12 23:45:43,897 - ==> Best [Top1: 58.410   Top5: 75.288   Sparsity:0.00   Params: 725752 on epoch: 290]
2024-05-12 23:45:43,898 - Saving checkpoint to: logs/2024.05.12-222626/qat_checkpoint.pth.tar
2024-05-12 23:45:44,075 - 

2024-05-12 23:45:44,077 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:46:00,167 - Epoch: [291][   70/   70]    Overall Loss 0.011255    Objective Loss 0.011255    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.229512    
2024-05-12 23:46:01,008 - 

2024-05-12 23:46:01,010 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:46:15,802 - Epoch: [292][   70/   70]    Overall Loss 0.010697    Objective Loss 0.010697    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.210965    
2024-05-12 23:46:16,826 - 

2024-05-12 23:46:16,830 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:46:30,392 - Epoch: [293][   70/   70]    Overall Loss 0.010386    Objective Loss 0.010386    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.193420    
2024-05-12 23:46:31,120 - 

2024-05-12 23:46:31,121 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:46:45,817 - Epoch: [294][   70/   70]    Overall Loss 0.009483    Objective Loss 0.009483    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.209607    
2024-05-12 23:46:46,600 - 

2024-05-12 23:46:46,601 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:47:00,652 - Epoch: [295][   70/   70]    Overall Loss 0.009304    Objective Loss 0.009304    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.200364    
2024-05-12 23:47:01,497 - 

2024-05-12 23:47:01,500 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:47:15,688 - Epoch: [296][   70/   70]    Overall Loss 0.009374    Objective Loss 0.009374    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.202364    
2024-05-12 23:47:16,492 - 

2024-05-12 23:47:16,494 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:47:34,727 - Epoch: [297][   70/   70]    Overall Loss 0.008466    Objective Loss 0.008466    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.260103    
2024-05-12 23:47:35,530 - 

2024-05-12 23:47:35,532 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:47:52,021 - Epoch: [298][   70/   70]    Overall Loss 0.009938    Objective Loss 0.009938    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.235234    
2024-05-12 23:47:52,811 - 

2024-05-12 23:47:52,815 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-12 23:48:06,774 - Epoch: [299][   70/   70]    Overall Loss 0.009297    Objective Loss 0.009297    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.199056    
2024-05-12 23:48:07,592 - --- test ---------------------
2024-05-12 23:48:07,594 - 1736 samples (100 per mini-batch)
2024-05-12 23:48:15,261 - Test: [   18/   18]    Loss 2.584224    Top1 58.294931    Top5 75.115207    
2024-05-12 23:48:16,102 - ==> Top1: 58.295    Top5: 75.115    Loss: 2.584

2024-05-12 23:48:16,118 - 
2024-05-12 23:48:16,120 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.12-222626/2024.05.12-222626.log
