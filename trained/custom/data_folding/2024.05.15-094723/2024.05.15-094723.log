2024-05-15 09:47:23,746 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.15-094723/2024.05.15-094723.log
2024-05-15 09:47:29,940 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2024-05-15 09:47:29,941 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2024-05-15 09:47:30,083 - Dataset sizes:
	training=6941
	validation=1736
	test=1736
2024-05-15 09:47:30,084 - Reading compression schedule from: policies/schedule-cifar100.yaml
2024-05-15 09:47:30,091 - 

2024-05-15 09:47:30,091 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 09:48:26,817 - Epoch: [0][   70/   70]    Overall Loss 3.864954    Objective Loss 3.864954    Top1 31.914894    Top5 40.425532    LR 0.001000    Time 0.810257    
2024-05-15 09:48:27,300 - --- validate (epoch=0)-----------
2024-05-15 09:48:27,301 - 1736 samples (100 per mini-batch)
2024-05-15 09:48:43,929 - Epoch: [0][   18/   18]    Loss 4.584991    Top1 2.246544    Top5 12.557604    
2024-05-15 09:48:44,135 - ==> Top1: 2.247    Top5: 12.558    Loss: 4.585

2024-05-15 09:48:44,143 - ==> Best [Top1: 2.247   Top5: 12.558   Sparsity:0.00   Params: 728560 on epoch: 0]
2024-05-15 09:48:44,144 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 09:48:44,268 - 

2024-05-15 09:48:44,269 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 09:49:44,606 - Epoch: [1][   70/   70]    Overall Loss 3.295200    Objective Loss 3.295200    Top1 34.042553    Top5 47.517730    LR 0.001000    Time 0.861849    
2024-05-15 09:49:45,370 - 

2024-05-15 09:49:45,370 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 09:50:43,339 - Epoch: [2][   70/   70]    Overall Loss 3.006811    Objective Loss 3.006811    Top1 34.751773    Top5 47.517730    LR 0.001000    Time 0.828013    
2024-05-15 09:50:43,647 - 

2024-05-15 09:50:43,647 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 09:51:44,984 - Epoch: [3][   70/   70]    Overall Loss 2.750562    Objective Loss 2.750562    Top1 27.659574    Top5 42.553191    LR 0.001000    Time 0.876129    
2024-05-15 09:51:45,222 - 

2024-05-15 09:51:45,222 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 09:52:47,833 - Epoch: [4][   70/   70]    Overall Loss 2.483226    Objective Loss 2.483226    Top1 42.553191    Top5 62.411348    LR 0.001000    Time 0.894336    
2024-05-15 09:52:48,084 - 

2024-05-15 09:52:48,084 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 09:53:46,753 - Epoch: [5][   70/   70]    Overall Loss 2.186876    Objective Loss 2.186876    Top1 53.191489    Top5 70.212766    LR 0.001000    Time 0.837997    
2024-05-15 09:53:47,323 - 

2024-05-15 09:53:47,324 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 09:54:51,029 - Epoch: [6][   70/   70]    Overall Loss 1.910487    Objective Loss 1.910487    Top1 54.609929    Top5 77.304965    LR 0.001000    Time 0.909959    
2024-05-15 09:54:51,292 - 

2024-05-15 09:54:51,292 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 09:55:56,302 - Epoch: [7][   70/   70]    Overall Loss 1.642600    Objective Loss 1.642600    Top1 60.283688    Top5 80.141844    LR 0.001000    Time 0.928610    
2024-05-15 09:55:57,143 - 

2024-05-15 09:55:57,144 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 09:56:49,235 - Epoch: [8][   70/   70]    Overall Loss 1.356791    Objective Loss 1.356791    Top1 63.120567    Top5 82.269504    LR 0.001000    Time 0.744042    
2024-05-15 09:56:49,729 - 

2024-05-15 09:56:49,730 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 09:57:54,184 - Epoch: [9][   70/   70]    Overall Loss 1.121418    Objective Loss 1.121418    Top1 67.375887    Top5 88.652482    LR 0.001000    Time 0.920648    
2024-05-15 09:57:54,567 - 

2024-05-15 09:57:54,568 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 09:58:53,164 - Epoch: [10][   70/   70]    Overall Loss 0.866421    Objective Loss 0.866421    Top1 82.269504    Top5 92.198582    LR 0.001000    Time 0.836992    
2024-05-15 09:58:53,785 - --- validate (epoch=10)-----------
2024-05-15 09:58:53,786 - 1736 samples (100 per mini-batch)
2024-05-15 09:59:14,950 - Epoch: [10][   18/   18]    Loss 2.072304    Top1 52.304147    Top5 71.774194    
2024-05-15 09:59:15,471 - ==> Top1: 52.304    Top5: 71.774    Loss: 2.072

2024-05-15 09:59:15,477 - ==> Best [Top1: 52.304   Top5: 71.774   Sparsity:0.00   Params: 728560 on epoch: 10]
2024-05-15 09:59:15,477 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 09:59:15,561 - 

2024-05-15 09:59:15,561 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:00:08,740 - Epoch: [11][   70/   70]    Overall Loss 0.633968    Objective Loss 0.633968    Top1 83.687943    Top5 93.617021    LR 0.001000    Time 0.759573    
2024-05-15 10:00:08,957 - 

2024-05-15 10:00:08,958 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:01:18,098 - Epoch: [12][   70/   70]    Overall Loss 0.450454    Objective Loss 0.450454    Top1 91.489362    Top5 99.290780    LR 0.001000    Time 0.987597    
2024-05-15 10:01:18,505 - 

2024-05-15 10:01:18,506 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:02:24,378 - Epoch: [13][   70/   70]    Overall Loss 0.289477    Objective Loss 0.289477    Top1 95.744681    Top5 100.000000    LR 0.001000    Time 0.940925    
2024-05-15 10:02:24,699 - 

2024-05-15 10:02:24,700 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:03:23,353 - Epoch: [14][   70/   70]    Overall Loss 0.172933    Objective Loss 0.172933    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.837800    
2024-05-15 10:03:23,703 - 

2024-05-15 10:03:23,704 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:04:27,628 - Epoch: [15][   70/   70]    Overall Loss 0.103169    Objective Loss 0.103169    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.913082    
2024-05-15 10:04:28,354 - 

2024-05-15 10:04:28,355 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:05:32,387 - Epoch: [16][   70/   70]    Overall Loss 0.056911    Objective Loss 0.056911    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.914650    
2024-05-15 10:05:32,600 - 

2024-05-15 10:05:32,600 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:06:31,210 - Epoch: [17][   70/   70]    Overall Loss 0.039879    Objective Loss 0.039879    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.837177    
2024-05-15 10:06:31,586 - 

2024-05-15 10:06:31,587 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:07:42,866 - Epoch: [18][   70/   70]    Overall Loss 0.028685    Objective Loss 0.028685    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.018173    
2024-05-15 10:07:43,229 - 

2024-05-15 10:07:43,229 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:08:49,622 - Epoch: [19][   70/   70]    Overall Loss 0.022871    Objective Loss 0.022871    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.948363    
2024-05-15 10:08:49,885 - 

2024-05-15 10:08:49,886 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:09:48,153 - Epoch: [20][   70/   70]    Overall Loss 0.019550    Objective Loss 0.019550    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.832300    
2024-05-15 10:09:48,786 - --- validate (epoch=20)-----------
2024-05-15 10:09:48,786 - 1736 samples (100 per mini-batch)
2024-05-15 10:10:10,205 - Epoch: [20][   18/   18]    Loss 2.107853    Top1 57.200461    Top5 73.329493    
2024-05-15 10:10:10,465 - ==> Top1: 57.200    Top5: 73.329    Loss: 2.108

2024-05-15 10:10:10,472 - ==> Best [Top1: 57.200   Top5: 73.329   Sparsity:0.00   Params: 728560 on epoch: 20]
2024-05-15 10:10:10,472 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 10:10:10,582 - 

2024-05-15 10:10:10,583 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:11:13,242 - Epoch: [21][   70/   70]    Overall Loss 0.017278    Objective Loss 0.017278    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.895023    
2024-05-15 10:11:14,066 - 

2024-05-15 10:11:14,067 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:12:12,807 - Epoch: [22][   70/   70]    Overall Loss 0.015373    Objective Loss 0.015373    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.839036    
2024-05-15 10:12:13,221 - 

2024-05-15 10:12:13,222 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:13:15,246 - Epoch: [23][   70/   70]    Overall Loss 0.014721    Objective Loss 0.014721    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.885939    
2024-05-15 10:13:16,049 - 

2024-05-15 10:13:16,050 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:14:15,420 - Epoch: [24][   70/   70]    Overall Loss 0.011592    Objective Loss 0.011592    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.848037    
2024-05-15 10:14:15,683 - 

2024-05-15 10:14:15,684 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:15:17,428 - Epoch: [25][   70/   70]    Overall Loss 0.010481    Objective Loss 0.010481    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.881960    
2024-05-15 10:15:17,704 - 

2024-05-15 10:15:17,704 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:16:20,955 - Epoch: [26][   70/   70]    Overall Loss 0.009534    Objective Loss 0.009534    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.903475    
2024-05-15 10:16:21,266 - 

2024-05-15 10:16:21,267 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:17:17,967 - Epoch: [27][   70/   70]    Overall Loss 0.008728    Objective Loss 0.008728    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.809882    
2024-05-15 10:17:18,154 - 

2024-05-15 10:17:18,155 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:18:17,987 - Epoch: [28][   70/   70]    Overall Loss 0.008087    Objective Loss 0.008087    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.854647    
2024-05-15 10:18:18,282 - 

2024-05-15 10:18:18,283 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:19:21,213 - Epoch: [29][   70/   70]    Overall Loss 0.007222    Objective Loss 0.007222    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.898891    
2024-05-15 10:19:21,835 - 

2024-05-15 10:19:21,836 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:20:21,684 - Epoch: [30][   70/   70]    Overall Loss 0.006524    Objective Loss 0.006524    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.854852    
2024-05-15 10:20:22,491 - --- validate (epoch=30)-----------
2024-05-15 10:20:22,492 - 1736 samples (100 per mini-batch)
2024-05-15 10:20:42,994 - Epoch: [30][   18/   18]    Loss 2.143212    Top1 57.315668    Top5 72.811060    
2024-05-15 10:20:43,250 - ==> Top1: 57.316    Top5: 72.811    Loss: 2.143

2024-05-15 10:20:43,258 - ==> Best [Top1: 57.316   Top5: 72.811   Sparsity:0.00   Params: 728560 on epoch: 30]
2024-05-15 10:20:43,258 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 10:20:43,331 - 

2024-05-15 10:20:43,331 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:21:38,186 - Epoch: [31][   70/   70]    Overall Loss 0.007325    Objective Loss 0.007325    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.783549    
2024-05-15 10:21:38,637 - 

2024-05-15 10:21:38,638 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:22:37,573 - Epoch: [32][   70/   70]    Overall Loss 0.006372    Objective Loss 0.006372    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.841834    
2024-05-15 10:22:38,083 - 

2024-05-15 10:22:38,084 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:23:38,767 - Epoch: [33][   70/   70]    Overall Loss 0.007209    Objective Loss 0.007209    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.866788    
2024-05-15 10:23:39,100 - 

2024-05-15 10:23:39,101 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:24:42,673 - Epoch: [34][   70/   70]    Overall Loss 0.005816    Objective Loss 0.005816    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.908059    
2024-05-15 10:24:42,955 - 

2024-05-15 10:24:42,956 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:25:41,397 - Epoch: [35][   70/   70]    Overall Loss 0.004977    Objective Loss 0.004977    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.834780    
2024-05-15 10:25:42,115 - 

2024-05-15 10:25:42,115 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:26:46,860 - Epoch: [36][   70/   70]    Overall Loss 0.004476    Objective Loss 0.004476    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.924817    
2024-05-15 10:26:47,422 - 

2024-05-15 10:26:47,424 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:27:48,074 - Epoch: [37][   70/   70]    Overall Loss 0.004764    Objective Loss 0.004764    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.866291    
2024-05-15 10:27:48,483 - 

2024-05-15 10:27:48,484 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:28:47,750 - Epoch: [38][   70/   70]    Overall Loss 0.004374    Objective Loss 0.004374    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.846573    
2024-05-15 10:28:48,503 - 

2024-05-15 10:28:48,503 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:29:51,247 - Epoch: [39][   70/   70]    Overall Loss 0.006076    Objective Loss 0.006076    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.896220    
2024-05-15 10:29:51,608 - 

2024-05-15 10:29:51,609 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:30:50,933 - Epoch: [40][   70/   70]    Overall Loss 0.005612    Objective Loss 0.005612    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.847364    
2024-05-15 10:30:51,148 - --- validate (epoch=40)-----------
2024-05-15 10:30:51,149 - 1736 samples (100 per mini-batch)
2024-05-15 10:31:07,902 - Epoch: [40][   18/   18]    Loss 2.314858    Top1 54.838710    Top5 72.465438    
2024-05-15 10:31:08,194 - ==> Top1: 54.839    Top5: 72.465    Loss: 2.315

2024-05-15 10:31:08,200 - ==> Best [Top1: 57.316   Top5: 72.811   Sparsity:0.00   Params: 728560 on epoch: 30]
2024-05-15 10:31:08,200 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 10:31:08,268 - 

2024-05-15 10:31:08,269 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:32:11,222 - Epoch: [41][   70/   70]    Overall Loss 0.004047    Objective Loss 0.004047    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.899175    
2024-05-15 10:32:11,664 - 

2024-05-15 10:32:11,665 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:33:11,672 - Epoch: [42][   70/   70]    Overall Loss 0.003641    Objective Loss 0.003641    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.857127    
2024-05-15 10:33:12,326 - 

2024-05-15 10:33:12,328 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:34:11,992 - Epoch: [43][   70/   70]    Overall Loss 0.003615    Objective Loss 0.003615    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.852214    
2024-05-15 10:34:12,296 - 

2024-05-15 10:34:12,297 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:35:15,590 - Epoch: [44][   70/   70]    Overall Loss 0.002919    Objective Loss 0.002919    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.904079    
2024-05-15 10:35:15,822 - 

2024-05-15 10:35:15,822 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:36:17,842 - Epoch: [45][   70/   70]    Overall Loss 0.002839    Objective Loss 0.002839    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.885889    
2024-05-15 10:36:18,374 - 

2024-05-15 10:36:18,375 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:37:15,971 - Epoch: [46][   70/   70]    Overall Loss 0.002518    Objective Loss 0.002518    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.822681    
2024-05-15 10:37:16,501 - 

2024-05-15 10:37:16,501 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:38:23,616 - Epoch: [47][   70/   70]    Overall Loss 0.002291    Objective Loss 0.002291    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.958673    
2024-05-15 10:38:24,145 - 

2024-05-15 10:38:24,146 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:39:21,390 - Epoch: [48][   70/   70]    Overall Loss 0.002240    Objective Loss 0.002240    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.817646    
2024-05-15 10:39:22,336 - 

2024-05-15 10:39:22,336 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:40:25,492 - Epoch: [49][   70/   70]    Overall Loss 0.002267    Objective Loss 0.002267    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.902105    
2024-05-15 10:40:25,862 - 

2024-05-15 10:40:25,863 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:41:21,865 - Epoch: [50][   70/   70]    Overall Loss 0.002140    Objective Loss 0.002140    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.799913    
2024-05-15 10:41:22,295 - --- validate (epoch=50)-----------
2024-05-15 10:41:22,297 - 1736 samples (100 per mini-batch)
2024-05-15 10:41:45,419 - Epoch: [50][   18/   18]    Loss 2.281008    Top1 57.200461    Top5 73.041475    
2024-05-15 10:41:45,991 - ==> Top1: 57.200    Top5: 73.041    Loss: 2.281

2024-05-15 10:41:46,000 - ==> Best [Top1: 57.316   Top5: 72.811   Sparsity:0.00   Params: 728560 on epoch: 30]
2024-05-15 10:41:46,000 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 10:41:46,054 - 

2024-05-15 10:41:46,056 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:42:48,037 - Epoch: [51][   70/   70]    Overall Loss 0.002029    Objective Loss 0.002029    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.885324    
2024-05-15 10:42:48,801 - 

2024-05-15 10:42:48,802 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:43:52,641 - Epoch: [52][   70/   70]    Overall Loss 0.001888    Objective Loss 0.001888    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.911888    
2024-05-15 10:43:53,003 - 

2024-05-15 10:43:53,005 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:44:55,938 - Epoch: [53][   70/   70]    Overall Loss 0.001805    Objective Loss 0.001805    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.898916    
2024-05-15 10:44:56,909 - 

2024-05-15 10:44:56,910 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:45:58,082 - Epoch: [54][   70/   70]    Overall Loss 0.001658    Objective Loss 0.001658    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.873755    
2024-05-15 10:45:58,621 - 

2024-05-15 10:45:58,622 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:47:02,496 - Epoch: [55][   70/   70]    Overall Loss 0.001676    Objective Loss 0.001676    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.912379    
2024-05-15 10:47:03,012 - 

2024-05-15 10:47:03,013 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:48:01,091 - Epoch: [56][   70/   70]    Overall Loss 0.001555    Objective Loss 0.001555    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.829570    
2024-05-15 10:48:01,309 - 

2024-05-15 10:48:01,309 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:49:00,937 - Epoch: [57][   70/   70]    Overall Loss 0.001607    Objective Loss 0.001607    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.851718    
2024-05-15 10:49:01,162 - 

2024-05-15 10:49:01,162 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:50:08,968 - Epoch: [58][   70/   70]    Overall Loss 0.001539    Objective Loss 0.001539    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.968553    
2024-05-15 10:50:10,011 - 

2024-05-15 10:50:10,012 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:51:12,582 - Epoch: [59][   70/   70]    Overall Loss 0.002023    Objective Loss 0.002023    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.893749    
2024-05-15 10:51:13,146 - 

2024-05-15 10:51:13,147 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:52:21,084 - Epoch: [60][   70/   70]    Overall Loss 0.001836    Objective Loss 0.001836    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.970403    
2024-05-15 10:52:21,556 - --- validate (epoch=60)-----------
2024-05-15 10:52:21,556 - 1736 samples (100 per mini-batch)
2024-05-15 10:52:38,214 - Epoch: [60][   18/   18]    Loss 2.300598    Top1 56.970046    Top5 72.753456    
2024-05-15 10:52:38,449 - ==> Top1: 56.970    Top5: 72.753    Loss: 2.301

2024-05-15 10:52:38,454 - ==> Best [Top1: 57.316   Top5: 72.811   Sparsity:0.00   Params: 728560 on epoch: 30]
2024-05-15 10:52:38,454 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 10:52:38,515 - 

2024-05-15 10:52:38,515 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:53:44,493 - Epoch: [61][   70/   70]    Overall Loss 0.001808    Objective Loss 0.001808    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.942427    
2024-05-15 10:53:44,792 - 

2024-05-15 10:53:44,792 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:54:39,507 - Epoch: [62][   70/   70]    Overall Loss 1.604971    Objective Loss 1.604971    Top1 49.645390    Top5 66.666667    LR 0.001000    Time 0.781519    
2024-05-15 10:54:39,816 - 

2024-05-15 10:54:39,817 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:55:35,630 - Epoch: [63][   70/   70]    Overall Loss 1.458461    Objective Loss 1.458461    Top1 59.574468    Top5 80.851064    LR 0.001000    Time 0.797177    
2024-05-15 10:55:36,243 - 

2024-05-15 10:55:36,243 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:56:37,091 - Epoch: [64][   70/   70]    Overall Loss 0.932114    Objective Loss 0.932114    Top1 70.921986    Top5 87.943262    LR 0.001000    Time 0.869155    
2024-05-15 10:56:37,392 - 

2024-05-15 10:56:37,393 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:57:38,381 - Epoch: [65][   70/   70]    Overall Loss 0.592723    Objective Loss 0.592723    Top1 87.234043    Top5 97.163121    LR 0.001000    Time 0.871140    
2024-05-15 10:57:38,729 - 

2024-05-15 10:57:38,729 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:58:36,660 - Epoch: [66][   70/   70]    Overall Loss 0.312143    Objective Loss 0.312143    Top1 93.617021    Top5 97.872340    LR 0.001000    Time 0.827480    
2024-05-15 10:58:36,966 - 

2024-05-15 10:58:36,967 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 10:59:46,988 - Epoch: [67][   70/   70]    Overall Loss 0.137225    Objective Loss 0.137225    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 1.000185    
2024-05-15 10:59:47,324 - 

2024-05-15 10:59:47,324 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:00:49,490 - Epoch: [68][   70/   70]    Overall Loss 0.051217    Objective Loss 0.051217    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.887971    
2024-05-15 11:00:50,152 - 

2024-05-15 11:00:50,152 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:01:54,100 - Epoch: [69][   70/   70]    Overall Loss 0.024229    Objective Loss 0.024229    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.913423    
2024-05-15 11:01:54,644 - 

2024-05-15 11:01:54,645 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:03:01,611 - Epoch: [70][   70/   70]    Overall Loss 0.016303    Objective Loss 0.016303    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.956554    
2024-05-15 11:03:02,137 - --- validate (epoch=70)-----------
2024-05-15 11:03:02,138 - 1736 samples (100 per mini-batch)
2024-05-15 11:03:21,130 - Epoch: [70][   18/   18]    Loss 2.111198    Top1 57.430876    Top5 76.267281    
2024-05-15 11:03:21,476 - ==> Top1: 57.431    Top5: 76.267    Loss: 2.111

2024-05-15 11:03:21,480 - ==> Best [Top1: 57.431   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 70]
2024-05-15 11:03:21,480 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 11:03:21,559 - 

2024-05-15 11:03:21,560 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:04:19,966 - Epoch: [71][   70/   70]    Overall Loss 0.012918    Objective Loss 0.012918    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.834266    
2024-05-15 11:04:20,383 - 

2024-05-15 11:04:20,384 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:05:19,419 - Epoch: [72][   70/   70]    Overall Loss 0.011081    Objective Loss 0.011081    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.843232    
2024-05-15 11:05:20,126 - 

2024-05-15 11:05:20,127 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:06:25,052 - Epoch: [73][   70/   70]    Overall Loss 0.009898    Objective Loss 0.009898    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.927389    
2024-05-15 11:06:25,637 - 

2024-05-15 11:06:25,637 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:07:29,768 - Epoch: [74][   70/   70]    Overall Loss 0.008649    Objective Loss 0.008649    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.916045    
2024-05-15 11:07:30,061 - 

2024-05-15 11:07:30,061 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:08:30,957 - Epoch: [75][   70/   70]    Overall Loss 0.008073    Objective Loss 0.008073    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.869830    
2024-05-15 11:08:31,307 - 

2024-05-15 11:08:31,307 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:09:29,595 - Epoch: [76][   70/   70]    Overall Loss 0.007181    Objective Loss 0.007181    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.832566    
2024-05-15 11:09:29,992 - 

2024-05-15 11:09:29,993 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:10:35,697 - Epoch: [77][   70/   70]    Overall Loss 0.007552    Objective Loss 0.007552    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.938525    
2024-05-15 11:10:36,071 - 

2024-05-15 11:10:36,071 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:11:39,763 - Epoch: [78][   70/   70]    Overall Loss 0.006660    Objective Loss 0.006660    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.909754    
2024-05-15 11:11:40,065 - 

2024-05-15 11:11:40,066 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:12:41,946 - Epoch: [79][   70/   70]    Overall Loss 0.005378    Objective Loss 0.005378    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.883880    
2024-05-15 11:12:42,229 - 

2024-05-15 11:12:42,230 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:13:48,553 - Epoch: [80][   70/   70]    Overall Loss 0.005105    Objective Loss 0.005105    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.947351    
2024-05-15 11:13:48,876 - --- validate (epoch=80)-----------
2024-05-15 11:13:48,877 - 1736 samples (100 per mini-batch)
2024-05-15 11:14:09,236 - Epoch: [80][   18/   18]    Loss 2.205377    Top1 58.525346    Top5 76.267281    
2024-05-15 11:14:09,796 - ==> Top1: 58.525    Top5: 76.267    Loss: 2.205

2024-05-15 11:14:09,805 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 11:14:09,805 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 11:14:09,879 - 

2024-05-15 11:14:09,879 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:15:12,351 - Epoch: [81][   70/   70]    Overall Loss 0.004741    Objective Loss 0.004741    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.892349    
2024-05-15 11:15:12,813 - 

2024-05-15 11:15:12,814 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:16:21,715 - Epoch: [82][   70/   70]    Overall Loss 0.004769    Objective Loss 0.004769    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.984185    
2024-05-15 11:16:22,299 - 

2024-05-15 11:16:22,300 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:17:23,446 - Epoch: [83][   70/   70]    Overall Loss 0.004141    Objective Loss 0.004141    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.873386    
2024-05-15 11:17:23,816 - 

2024-05-15 11:17:23,816 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:18:23,740 - Epoch: [84][   70/   70]    Overall Loss 0.004336    Objective Loss 0.004336    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.855949    
2024-05-15 11:18:24,126 - 

2024-05-15 11:18:24,126 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:19:30,140 - Epoch: [85][   70/   70]    Overall Loss 0.003986    Objective Loss 0.003986    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.942953    
2024-05-15 11:19:30,537 - 

2024-05-15 11:19:30,538 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:20:34,942 - Epoch: [86][   70/   70]    Overall Loss 0.006922    Objective Loss 0.006922    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.919951    
2024-05-15 11:20:35,640 - 

2024-05-15 11:20:35,642 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:21:39,742 - Epoch: [87][   70/   70]    Overall Loss 0.005020    Objective Loss 0.005020    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.915610    
2024-05-15 11:21:40,256 - 

2024-05-15 11:21:40,256 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:22:41,395 - Epoch: [88][   70/   70]    Overall Loss 0.003505    Objective Loss 0.003505    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.873296    
2024-05-15 11:22:41,914 - 

2024-05-15 11:22:41,915 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:23:39,076 - Epoch: [89][   70/   70]    Overall Loss 0.002962    Objective Loss 0.002962    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.816463    
2024-05-15 11:23:39,567 - 

2024-05-15 11:23:39,568 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:24:43,036 - Epoch: [90][   70/   70]    Overall Loss 0.002736    Objective Loss 0.002736    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.906593    
2024-05-15 11:24:43,369 - --- validate (epoch=90)-----------
2024-05-15 11:24:43,370 - 1736 samples (100 per mini-batch)
2024-05-15 11:25:01,605 - Epoch: [90][   18/   18]    Loss 2.284198    Top1 58.237327    Top5 76.152074    
2024-05-15 11:25:02,235 - ==> Top1: 58.237    Top5: 76.152    Loss: 2.284

2024-05-15 11:25:02,240 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 11:25:02,241 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 11:25:02,303 - 

2024-05-15 11:25:02,304 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:26:00,433 - Epoch: [91][   70/   70]    Overall Loss 0.002558    Objective Loss 0.002558    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.830301    
2024-05-15 11:26:00,757 - 

2024-05-15 11:26:00,757 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:27:05,573 - Epoch: [92][   70/   70]    Overall Loss 0.002924    Objective Loss 0.002924    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.925839    
2024-05-15 11:27:05,971 - 

2024-05-15 11:27:05,972 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:28:05,268 - Epoch: [93][   70/   70]    Overall Loss 0.003188    Objective Loss 0.003188    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.846994    
2024-05-15 11:28:05,563 - 

2024-05-15 11:28:05,564 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:29:12,041 - Epoch: [94][   70/   70]    Overall Loss 0.002419    Objective Loss 0.002419    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.949562    
2024-05-15 11:29:12,536 - 

2024-05-15 11:29:12,536 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:30:12,258 - Epoch: [95][   70/   70]    Overall Loss 0.002196    Objective Loss 0.002196    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.853071    
2024-05-15 11:30:12,506 - 

2024-05-15 11:30:12,506 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:31:15,375 - Epoch: [96][   70/   70]    Overall Loss 0.002200    Objective Loss 0.002200    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.898023    
2024-05-15 11:31:15,746 - 

2024-05-15 11:31:15,747 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:32:16,955 - Epoch: [97][   70/   70]    Overall Loss 0.001904    Objective Loss 0.001904    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.874288    
2024-05-15 11:32:17,431 - 

2024-05-15 11:32:17,432 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:33:24,669 - Epoch: [98][   70/   70]    Overall Loss 0.001899    Objective Loss 0.001899    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.960419    
2024-05-15 11:33:25,052 - 

2024-05-15 11:33:25,053 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:34:28,462 - Epoch: [99][   70/   70]    Overall Loss 0.001818    Objective Loss 0.001818    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.905736    
2024-05-15 11:34:28,754 - 

2024-05-15 11:34:28,755 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:35:32,096 - Epoch: [100][   70/   70]    Overall Loss 0.001670    Objective Loss 0.001670    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.904766    
2024-05-15 11:35:32,393 - --- validate (epoch=100)-----------
2024-05-15 11:35:32,394 - 1736 samples (100 per mini-batch)
2024-05-15 11:35:52,361 - Epoch: [100][   18/   18]    Loss 2.361438    Top1 57.891705    Top5 75.576037    
2024-05-15 11:35:52,747 - ==> Top1: 57.892    Top5: 75.576    Loss: 2.361

2024-05-15 11:35:52,753 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 11:35:52,753 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 11:35:52,816 - 

2024-05-15 11:35:52,816 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:36:56,167 - Epoch: [101][   70/   70]    Overall Loss 0.001612    Objective Loss 0.001612    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.904843    
2024-05-15 11:36:56,639 - 

2024-05-15 11:36:56,639 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:37:53,681 - Epoch: [102][   70/   70]    Overall Loss 0.001587    Objective Loss 0.001587    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.814778    
2024-05-15 11:37:53,959 - 

2024-05-15 11:37:53,960 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:38:56,051 - Epoch: [103][   70/   70]    Overall Loss 0.001602    Objective Loss 0.001602    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.886923    
2024-05-15 11:38:56,496 - 

2024-05-15 11:38:56,496 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:40:05,886 - Epoch: [104][   70/   70]    Overall Loss 0.001545    Objective Loss 0.001545    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.991186    
2024-05-15 11:40:06,657 - 

2024-05-15 11:40:06,658 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:41:08,365 - Epoch: [105][   70/   70]    Overall Loss 0.001533    Objective Loss 0.001533    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.881417    
2024-05-15 11:41:08,718 - 

2024-05-15 11:41:08,718 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:42:10,791 - Epoch: [106][   70/   70]    Overall Loss 0.001528    Objective Loss 0.001528    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.886636    
2024-05-15 11:42:11,170 - 

2024-05-15 11:42:11,171 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:43:16,498 - Epoch: [107][   70/   70]    Overall Loss 0.001523    Objective Loss 0.001523    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.933123    
2024-05-15 11:43:17,077 - 

2024-05-15 11:43:17,078 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:44:20,602 - Epoch: [108][   70/   70]    Overall Loss 0.001674    Objective Loss 0.001674    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.907381    
2024-05-15 11:44:21,070 - 

2024-05-15 11:44:21,071 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:45:22,415 - Epoch: [109][   70/   70]    Overall Loss 0.001472    Objective Loss 0.001472    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.876234    
2024-05-15 11:45:22,766 - 

2024-05-15 11:45:22,766 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:46:19,692 - Epoch: [110][   70/   70]    Overall Loss 0.001476    Objective Loss 0.001476    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.813122    
2024-05-15 11:46:20,043 - --- validate (epoch=110)-----------
2024-05-15 11:46:20,044 - 1736 samples (100 per mini-batch)
2024-05-15 11:46:40,020 - Epoch: [110][   18/   18]    Loss 2.346867    Top1 57.949309    Top5 75.518433    
2024-05-15 11:46:40,334 - ==> Top1: 57.949    Top5: 75.518    Loss: 2.347

2024-05-15 11:46:40,339 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 11:46:40,340 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 11:46:40,406 - 

2024-05-15 11:46:40,407 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:47:49,284 - Epoch: [111][   70/   70]    Overall Loss 0.001429    Objective Loss 0.001429    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.983865    
2024-05-15 11:47:49,647 - 

2024-05-15 11:47:49,648 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:48:50,132 - Epoch: [112][   70/   70]    Overall Loss 0.001367    Objective Loss 0.001367    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.863950    
2024-05-15 11:48:50,527 - 

2024-05-15 11:48:50,528 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:49:54,647 - Epoch: [113][   70/   70]    Overall Loss 0.001414    Objective Loss 0.001414    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.915879    
2024-05-15 11:49:55,218 - 

2024-05-15 11:49:55,219 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:50:53,169 - Epoch: [114][   70/   70]    Overall Loss 0.001384    Objective Loss 0.001384    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.827745    
2024-05-15 11:50:53,494 - 

2024-05-15 11:50:53,494 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:51:54,652 - Epoch: [115][   70/   70]    Overall Loss 0.001393    Objective Loss 0.001393    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.873555    
2024-05-15 11:51:55,091 - 

2024-05-15 11:51:55,092 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:52:57,009 - Epoch: [116][   70/   70]    Overall Loss 0.001351    Objective Loss 0.001351    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.884396    
2024-05-15 11:52:58,053 - 

2024-05-15 11:52:58,054 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:54:13,117 - Epoch: [117][   70/   70]    Overall Loss 0.001332    Objective Loss 0.001332    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.072204    
2024-05-15 11:54:13,442 - 

2024-05-15 11:54:13,443 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:55:17,881 - Epoch: [118][   70/   70]    Overall Loss 0.001514    Objective Loss 0.001514    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.920433    
2024-05-15 11:55:18,763 - 

2024-05-15 11:55:18,763 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:56:21,045 - Epoch: [119][   70/   70]    Overall Loss 0.001330    Objective Loss 0.001330    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.889632    
2024-05-15 11:56:21,387 - 

2024-05-15 11:56:21,388 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:57:26,272 - Epoch: [120][   70/   70]    Overall Loss 0.001314    Objective Loss 0.001314    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.926810    
2024-05-15 11:57:26,997 - --- validate (epoch=120)-----------
2024-05-15 11:57:26,997 - 1736 samples (100 per mini-batch)
2024-05-15 11:57:50,267 - Epoch: [120][   18/   18]    Loss 2.391580    Top1 58.122120    Top5 75.403226    
2024-05-15 11:57:50,595 - ==> Top1: 58.122    Top5: 75.403    Loss: 2.392

2024-05-15 11:57:50,600 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 11:57:50,601 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 11:57:50,668 - 

2024-05-15 11:57:50,668 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:58:44,570 - Epoch: [121][   70/   70]    Overall Loss 0.001283    Objective Loss 0.001283    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.769909    
2024-05-15 11:58:45,033 - 

2024-05-15 11:58:45,033 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 11:59:46,233 - Epoch: [122][   70/   70]    Overall Loss 0.001276    Objective Loss 0.001276    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.874182    
2024-05-15 11:59:46,834 - 

2024-05-15 11:59:46,835 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:00:47,094 - Epoch: [123][   70/   70]    Overall Loss 0.001342    Objective Loss 0.001342    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.860735    
2024-05-15 12:00:47,585 - 

2024-05-15 12:00:47,586 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:01:48,874 - Epoch: [124][   70/   70]    Overall Loss 0.001276    Objective Loss 0.001276    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.875444    
2024-05-15 12:01:49,264 - 

2024-05-15 12:01:49,266 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:02:48,714 - Epoch: [125][   70/   70]    Overall Loss 0.001193    Objective Loss 0.001193    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.849123    
2024-05-15 12:02:49,369 - 

2024-05-15 12:02:49,369 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:03:47,745 - Epoch: [126][   70/   70]    Overall Loss 0.001188    Objective Loss 0.001188    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.833834    
2024-05-15 12:03:48,150 - 

2024-05-15 12:03:48,151 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:04:57,160 - Epoch: [127][   70/   70]    Overall Loss 0.001222    Objective Loss 0.001222    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.985709    
2024-05-15 12:04:57,488 - 

2024-05-15 12:04:57,489 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:05:55,075 - Epoch: [128][   70/   70]    Overall Loss 0.001209    Objective Loss 0.001209    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.822538    
2024-05-15 12:05:55,533 - 

2024-05-15 12:05:55,534 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:06:57,302 - Epoch: [129][   70/   70]    Overall Loss 0.001205    Objective Loss 0.001205    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.882294    
2024-05-15 12:06:57,902 - 

2024-05-15 12:06:57,902 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:07:59,293 - Epoch: [130][   70/   70]    Overall Loss 0.001168    Objective Loss 0.001168    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.876905    
2024-05-15 12:07:59,499 - --- validate (epoch=130)-----------
2024-05-15 12:07:59,499 - 1736 samples (100 per mini-batch)
2024-05-15 12:08:21,063 - Epoch: [130][   18/   18]    Loss 2.380494    Top1 57.776498    Top5 75.345622    
2024-05-15 12:08:21,241 - ==> Top1: 57.776    Top5: 75.346    Loss: 2.380

2024-05-15 12:08:21,244 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 12:08:21,244 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 12:08:21,291 - 

2024-05-15 12:08:21,291 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:09:19,403 - Epoch: [131][   70/   70]    Overall Loss 0.001142    Objective Loss 0.001142    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.830047    
2024-05-15 12:09:20,013 - 

2024-05-15 12:09:20,013 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:10:24,074 - Epoch: [132][   70/   70]    Overall Loss 0.001136    Objective Loss 0.001136    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.915037    
2024-05-15 12:10:24,482 - 

2024-05-15 12:10:24,482 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:11:22,182 - Epoch: [133][   70/   70]    Overall Loss 0.001104    Objective Loss 0.001104    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.824148    
2024-05-15 12:11:22,540 - 

2024-05-15 12:11:22,541 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:12:24,118 - Epoch: [134][   70/   70]    Overall Loss 0.001494    Objective Loss 0.001494    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.879558    
2024-05-15 12:12:24,460 - 

2024-05-15 12:12:24,461 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:13:29,870 - Epoch: [135][   70/   70]    Overall Loss 0.001268    Objective Loss 0.001268    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.934303    
2024-05-15 12:13:30,485 - 

2024-05-15 12:13:30,486 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:14:30,555 - Epoch: [136][   70/   70]    Overall Loss 0.001394    Objective Loss 0.001394    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.858015    
2024-05-15 12:14:31,218 - 

2024-05-15 12:14:31,218 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:15:37,321 - Epoch: [137][   70/   70]    Overall Loss 0.001181    Objective Loss 0.001181    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.944218    
2024-05-15 12:15:37,590 - 

2024-05-15 12:15:37,591 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:16:42,567 - Epoch: [138][   70/   70]    Overall Loss 0.001104    Objective Loss 0.001104    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.928114    
2024-05-15 12:16:42,940 - 

2024-05-15 12:16:42,940 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:17:43,638 - Epoch: [139][   70/   70]    Overall Loss 0.001066    Objective Loss 0.001066    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.866989    
2024-05-15 12:17:44,026 - 

2024-05-15 12:17:44,027 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:18:43,909 - Epoch: [140][   70/   70]    Overall Loss 0.000991    Objective Loss 0.000991    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.855313    
2024-05-15 12:18:44,135 - --- validate (epoch=140)-----------
2024-05-15 12:18:44,136 - 1736 samples (100 per mini-batch)
2024-05-15 12:19:03,320 - Epoch: [140][   18/   18]    Loss 2.403868    Top1 57.661290    Top5 75.806452    
2024-05-15 12:19:03,516 - ==> Top1: 57.661    Top5: 75.806    Loss: 2.404

2024-05-15 12:19:03,518 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 12:19:03,519 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 12:19:03,569 - 

2024-05-15 12:19:03,570 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:20:03,902 - Epoch: [141][   70/   70]    Overall Loss 0.001199    Objective Loss 0.001199    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.861748    
2024-05-15 12:20:04,640 - 

2024-05-15 12:20:04,640 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:21:06,519 - Epoch: [142][   70/   70]    Overall Loss 0.001024    Objective Loss 0.001024    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.883877    
2024-05-15 12:21:07,042 - 

2024-05-15 12:21:07,043 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:22:05,447 - Epoch: [143][   70/   70]    Overall Loss 0.001016    Objective Loss 0.001016    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.834232    
2024-05-15 12:22:05,766 - 

2024-05-15 12:22:05,767 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:23:04,990 - Epoch: [144][   70/   70]    Overall Loss 0.001002    Objective Loss 0.001002    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.845927    
2024-05-15 12:23:05,270 - 

2024-05-15 12:23:05,270 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:24:10,593 - Epoch: [145][   70/   70]    Overall Loss 0.001005    Objective Loss 0.001005    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.933081    
2024-05-15 12:24:11,061 - 

2024-05-15 12:24:11,062 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:25:09,989 - Epoch: [146][   70/   70]    Overall Loss 0.001003    Objective Loss 0.001003    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.841708    
2024-05-15 12:25:10,279 - 

2024-05-15 12:25:10,280 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:26:12,731 - Epoch: [147][   70/   70]    Overall Loss 0.000957    Objective Loss 0.000957    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.892043    
2024-05-15 12:26:13,154 - 

2024-05-15 12:26:13,155 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:27:10,543 - Epoch: [148][   70/   70]    Overall Loss 0.001010    Objective Loss 0.001010    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.819700    
2024-05-15 12:27:10,931 - 

2024-05-15 12:27:10,932 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:28:15,742 - Epoch: [149][   70/   70]    Overall Loss 0.001368    Objective Loss 0.001368    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.925753    
2024-05-15 12:28:16,146 - 

2024-05-15 12:28:16,147 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:29:16,534 - Epoch: [150][   70/   70]    Overall Loss 0.000993    Objective Loss 0.000993    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.862570    
2024-05-15 12:29:16,885 - --- validate (epoch=150)-----------
2024-05-15 12:29:16,886 - 1736 samples (100 per mini-batch)
2024-05-15 12:29:33,985 - Epoch: [150][   18/   18]    Loss 2.416820    Top1 57.661290    Top5 75.576037    
2024-05-15 12:29:34,268 - ==> Top1: 57.661    Top5: 75.576    Loss: 2.417

2024-05-15 12:29:34,275 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 12:29:34,276 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 12:29:34,331 - 

2024-05-15 12:29:34,331 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:30:36,016 - Epoch: [151][   70/   70]    Overall Loss 0.000913    Objective Loss 0.000913    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.881093    
2024-05-15 12:30:36,711 - 

2024-05-15 12:30:36,711 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:31:42,424 - Epoch: [152][   70/   70]    Overall Loss 0.000930    Objective Loss 0.000930    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.938639    
2024-05-15 12:31:42,844 - 

2024-05-15 12:31:42,844 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:32:43,766 - Epoch: [153][   70/   70]    Overall Loss 0.000881    Objective Loss 0.000881    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.870185    
2024-05-15 12:32:44,075 - 

2024-05-15 12:32:44,075 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:33:50,648 - Epoch: [154][   70/   70]    Overall Loss 0.000902    Objective Loss 0.000902    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.950918    
2024-05-15 12:33:51,466 - 

2024-05-15 12:33:51,467 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:34:53,714 - Epoch: [155][   70/   70]    Overall Loss 0.000898    Objective Loss 0.000898    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.889124    
2024-05-15 12:34:54,300 - 

2024-05-15 12:34:54,300 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:35:56,122 - Epoch: [156][   70/   70]    Overall Loss 0.001056    Objective Loss 0.001056    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.883047    
2024-05-15 12:35:56,708 - 

2024-05-15 12:35:56,709 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:36:58,551 - Epoch: [157][   70/   70]    Overall Loss 0.000853    Objective Loss 0.000853    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.883323    
2024-05-15 12:36:59,116 - 

2024-05-15 12:36:59,117 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:37:56,292 - Epoch: [158][   70/   70]    Overall Loss 0.000882    Objective Loss 0.000882    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.816642    
2024-05-15 12:37:57,056 - 

2024-05-15 12:37:57,056 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:39:06,190 - Epoch: [159][   70/   70]    Overall Loss 0.000918    Objective Loss 0.000918    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.987523    
2024-05-15 12:39:07,226 - 

2024-05-15 12:39:07,227 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:40:04,447 - Epoch: [160][   70/   70]    Overall Loss 0.000865    Objective Loss 0.000865    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.817312    
2024-05-15 12:40:05,165 - --- validate (epoch=160)-----------
2024-05-15 12:40:05,165 - 1736 samples (100 per mini-batch)
2024-05-15 12:40:22,990 - Epoch: [160][   18/   18]    Loss 2.414957    Top1 57.488479    Top5 75.576037    
2024-05-15 12:40:23,866 - ==> Top1: 57.488    Top5: 75.576    Loss: 2.415

2024-05-15 12:40:23,874 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 12:40:23,874 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 12:40:23,948 - 

2024-05-15 12:40:23,948 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:41:22,574 - Epoch: [161][   70/   70]    Overall Loss 0.000845    Objective Loss 0.000845    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.837388    
2024-05-15 12:41:23,082 - 

2024-05-15 12:41:23,083 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:42:26,762 - Epoch: [162][   70/   70]    Overall Loss 0.000835    Objective Loss 0.000835    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.909594    
2024-05-15 12:42:27,540 - 

2024-05-15 12:42:27,540 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:43:36,259 - Epoch: [163][   70/   70]    Overall Loss 0.000832    Objective Loss 0.000832    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.981578    
2024-05-15 12:43:36,600 - 

2024-05-15 12:43:36,600 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:44:34,469 - Epoch: [164][   70/   70]    Overall Loss 0.000814    Objective Loss 0.000814    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.826587    
2024-05-15 12:44:35,196 - 

2024-05-15 12:44:35,197 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:45:35,637 - Epoch: [165][   70/   70]    Overall Loss 0.000820    Objective Loss 0.000820    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.863314    
2024-05-15 12:45:35,968 - 

2024-05-15 12:45:35,969 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:46:38,608 - Epoch: [166][   70/   70]    Overall Loss 0.000799    Objective Loss 0.000799    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.894729    
2024-05-15 12:46:38,970 - 

2024-05-15 12:46:38,971 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:47:39,507 - Epoch: [167][   70/   70]    Overall Loss 0.000865    Objective Loss 0.000865    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.864676    
2024-05-15 12:47:40,260 - 

2024-05-15 12:47:40,260 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:48:47,771 - Epoch: [168][   70/   70]    Overall Loss 0.000816    Objective Loss 0.000816    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.964330    
2024-05-15 12:48:48,154 - 

2024-05-15 12:48:48,154 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:49:49,580 - Epoch: [169][   70/   70]    Overall Loss 0.000832    Objective Loss 0.000832    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.877384    
2024-05-15 12:49:50,014 - 

2024-05-15 12:49:50,014 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:50:45,864 - Epoch: [170][   70/   70]    Overall Loss 0.000804    Objective Loss 0.000804    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.797738    
2024-05-15 12:50:46,215 - --- validate (epoch=170)-----------
2024-05-15 12:50:46,216 - 1736 samples (100 per mini-batch)
2024-05-15 12:51:05,069 - Epoch: [170][   18/   18]    Loss 2.437450    Top1 57.603687    Top5 75.576037    
2024-05-15 12:51:05,419 - ==> Top1: 57.604    Top5: 75.576    Loss: 2.437

2024-05-15 12:51:05,422 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 12:51:05,422 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 12:51:05,477 - 

2024-05-15 12:51:05,478 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:52:10,489 - Epoch: [171][   70/   70]    Overall Loss 0.000810    Objective Loss 0.000810    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.928596    
2024-05-15 12:52:11,010 - 

2024-05-15 12:52:11,011 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:53:20,230 - Epoch: [172][   70/   70]    Overall Loss 0.000811    Objective Loss 0.000811    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.988719    
2024-05-15 12:53:20,563 - 

2024-05-15 12:53:20,564 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:54:12,911 - Epoch: [173][   70/   70]    Overall Loss 0.000798    Objective Loss 0.000798    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.747696    
2024-05-15 12:54:13,298 - 

2024-05-15 12:54:13,299 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:55:16,401 - Epoch: [174][   70/   70]    Overall Loss 0.000961    Objective Loss 0.000961    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.901347    
2024-05-15 12:55:16,924 - 

2024-05-15 12:55:16,925 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:56:17,807 - Epoch: [175][   70/   70]    Overall Loss 0.000789    Objective Loss 0.000789    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.869634    
2024-05-15 12:56:18,179 - 

2024-05-15 12:56:18,179 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:57:17,393 - Epoch: [176][   70/   70]    Overall Loss 0.000771    Objective Loss 0.000771    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.845798    
2024-05-15 12:57:17,649 - 

2024-05-15 12:57:17,649 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:58:16,091 - Epoch: [177][   70/   70]    Overall Loss 0.000778    Objective Loss 0.000778    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.834774    
2024-05-15 12:58:16,552 - 

2024-05-15 12:58:16,553 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 12:59:17,581 - Epoch: [178][   70/   70]    Overall Loss 0.000787    Objective Loss 0.000787    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.871735    
2024-05-15 12:59:17,940 - 

2024-05-15 12:59:17,941 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:00:16,018 - Epoch: [179][   70/   70]    Overall Loss 0.000767    Objective Loss 0.000767    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.829565    
2024-05-15 13:00:16,327 - 

2024-05-15 13:00:16,328 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:01:16,901 - Epoch: [180][   70/   70]    Overall Loss 0.000778    Objective Loss 0.000778    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.865197    
2024-05-15 13:01:17,466 - --- validate (epoch=180)-----------
2024-05-15 13:01:17,467 - 1736 samples (100 per mini-batch)
2024-05-15 13:01:35,751 - Epoch: [180][   18/   18]    Loss 2.441590    Top1 57.200461    Top5 75.691244    
2024-05-15 13:01:36,008 - ==> Top1: 57.200    Top5: 75.691    Loss: 2.442

2024-05-15 13:01:36,014 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 13:01:36,014 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 13:01:36,073 - 

2024-05-15 13:01:36,073 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:02:38,952 - Epoch: [181][   70/   70]    Overall Loss 0.000768    Objective Loss 0.000768    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.898150    
2024-05-15 13:02:39,975 - 

2024-05-15 13:02:39,975 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:03:42,637 - Epoch: [182][   70/   70]    Overall Loss 0.000762    Objective Loss 0.000762    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.895053    
2024-05-15 13:03:43,178 - 

2024-05-15 13:03:43,179 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:04:47,581 - Epoch: [183][   70/   70]    Overall Loss 0.000755    Objective Loss 0.000755    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.919928    
2024-05-15 13:04:48,034 - 

2024-05-15 13:04:48,035 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:05:52,568 - Epoch: [184][   70/   70]    Overall Loss 0.000922    Objective Loss 0.000922    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.921784    
2024-05-15 13:05:53,085 - 

2024-05-15 13:05:53,085 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:06:54,672 - Epoch: [185][   70/   70]    Overall Loss 0.000725    Objective Loss 0.000725    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.879698    
2024-05-15 13:06:55,044 - 

2024-05-15 13:06:55,044 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:07:58,387 - Epoch: [186][   70/   70]    Overall Loss 0.000724    Objective Loss 0.000724    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.904770    
2024-05-15 13:07:58,831 - 

2024-05-15 13:07:58,832 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:09:08,788 - Epoch: [187][   70/   70]    Overall Loss 0.000729    Objective Loss 0.000729    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.999246    
2024-05-15 13:09:09,313 - 

2024-05-15 13:09:09,313 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:10:09,732 - Epoch: [188][   70/   70]    Overall Loss 0.000783    Objective Loss 0.000783    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.863020    
2024-05-15 13:10:10,245 - 

2024-05-15 13:10:10,246 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:11:12,375 - Epoch: [189][   70/   70]    Overall Loss 0.000745    Objective Loss 0.000745    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.887456    
2024-05-15 13:11:12,727 - 

2024-05-15 13:11:12,728 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:12:15,514 - Epoch: [190][   70/   70]    Overall Loss 0.000701    Objective Loss 0.000701    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.896851    
2024-05-15 13:12:15,805 - --- validate (epoch=190)-----------
2024-05-15 13:12:15,806 - 1736 samples (100 per mini-batch)
2024-05-15 13:12:35,479 - Epoch: [190][   18/   18]    Loss 2.463496    Top1 57.603687    Top5 75.172811    
2024-05-15 13:12:35,812 - ==> Top1: 57.604    Top5: 75.173    Loss: 2.463

2024-05-15 13:12:35,819 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 13:12:35,819 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 13:12:35,882 - 

2024-05-15 13:12:35,882 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:13:44,144 - Epoch: [191][   70/   70]    Overall Loss 0.000711    Objective Loss 0.000711    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.975048    
2024-05-15 13:13:44,631 - 

2024-05-15 13:13:44,632 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:14:49,431 - Epoch: [192][   70/   70]    Overall Loss 0.000748    Objective Loss 0.000748    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.925568    
2024-05-15 13:14:49,748 - 

2024-05-15 13:14:49,748 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:15:49,211 - Epoch: [193][   70/   70]    Overall Loss 0.000815    Objective Loss 0.000815    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.849362    
2024-05-15 13:15:49,535 - 

2024-05-15 13:15:49,536 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:16:43,545 - Epoch: [194][   70/   70]    Overall Loss 0.000914    Objective Loss 0.000914    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.771443    
2024-05-15 13:16:43,933 - 

2024-05-15 13:16:43,935 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:17:41,854 - Epoch: [195][   70/   70]    Overall Loss 0.000844    Objective Loss 0.000844    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.827297    
2024-05-15 13:17:42,166 - 

2024-05-15 13:17:42,167 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:18:41,633 - Epoch: [196][   70/   70]    Overall Loss 0.000747    Objective Loss 0.000747    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.849410    
2024-05-15 13:18:42,050 - 

2024-05-15 13:18:42,050 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:19:42,318 - Epoch: [197][   70/   70]    Overall Loss 0.000728    Objective Loss 0.000728    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.860861    
2024-05-15 13:19:42,640 - 

2024-05-15 13:19:42,641 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:20:49,155 - Epoch: [198][   70/   70]    Overall Loss 0.000828    Objective Loss 0.000828    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.950099    
2024-05-15 13:20:49,489 - 

2024-05-15 13:20:49,490 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:21:54,169 - Epoch: [199][   70/   70]    Overall Loss 0.000736    Objective Loss 0.000736    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.923870    
2024-05-15 13:21:54,870 - 

2024-05-15 13:21:54,871 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:22:54,115 - Epoch: [200][   70/   70]    Overall Loss 0.000680    Objective Loss 0.000680    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.846233    
2024-05-15 13:22:54,562 - --- validate (epoch=200)-----------
2024-05-15 13:22:54,563 - 1736 samples (100 per mini-batch)
2024-05-15 13:23:11,045 - Epoch: [200][   18/   18]    Loss 2.487837    Top1 57.258065    Top5 75.288018    
2024-05-15 13:23:11,529 - ==> Top1: 57.258    Top5: 75.288    Loss: 2.488

2024-05-15 13:23:11,534 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 13:23:11,534 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 13:23:11,593 - 

2024-05-15 13:23:11,594 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:24:22,306 - Epoch: [201][   70/   70]    Overall Loss 0.000678    Objective Loss 0.000678    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 1.010026    
2024-05-15 13:24:22,737 - 

2024-05-15 13:24:22,739 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:25:22,730 - Epoch: [202][   70/   70]    Overall Loss 0.000666    Objective Loss 0.000666    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.856909    
2024-05-15 13:25:23,101 - 

2024-05-15 13:25:23,102 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:26:20,955 - Epoch: [203][   70/   70]    Overall Loss 0.000665    Objective Loss 0.000665    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.826344    
2024-05-15 13:26:21,355 - 

2024-05-15 13:26:21,356 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:27:22,296 - Epoch: [204][   70/   70]    Overall Loss 0.000669    Objective Loss 0.000669    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.870457    
2024-05-15 13:27:22,645 - 

2024-05-15 13:27:22,646 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:28:24,572 - Epoch: [205][   70/   70]    Overall Loss 0.000672    Objective Loss 0.000672    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.884552    
2024-05-15 13:28:24,832 - 

2024-05-15 13:28:24,832 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:29:30,126 - Epoch: [206][   70/   70]    Overall Loss 0.000652    Objective Loss 0.000652    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.932669    
2024-05-15 13:29:30,367 - 

2024-05-15 13:29:30,367 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:30:26,975 - Epoch: [207][   70/   70]    Overall Loss 0.000655    Objective Loss 0.000655    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.808572    
2024-05-15 13:30:27,352 - 

2024-05-15 13:30:27,353 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:31:24,904 - Epoch: [208][   70/   70]    Overall Loss 0.000641    Objective Loss 0.000641    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.822063    
2024-05-15 13:31:25,157 - 

2024-05-15 13:31:25,158 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:32:21,247 - Epoch: [209][   70/   70]    Overall Loss 0.000824    Objective Loss 0.000824    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.801141    
2024-05-15 13:32:21,521 - 

2024-05-15 13:32:21,522 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:33:23,723 - Epoch: [210][   70/   70]    Overall Loss 0.000638    Objective Loss 0.000638    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.888458    
2024-05-15 13:33:24,180 - --- validate (epoch=210)-----------
2024-05-15 13:33:24,182 - 1736 samples (100 per mini-batch)
2024-05-15 13:33:43,558 - Epoch: [210][   18/   18]    Loss 2.509200    Top1 57.430876    Top5 75.230415    
2024-05-15 13:33:43,966 - ==> Top1: 57.431    Top5: 75.230    Loss: 2.509

2024-05-15 13:33:43,971 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 13:33:43,971 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 13:33:44,035 - 

2024-05-15 13:33:44,036 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:34:51,178 - Epoch: [211][   70/   70]    Overall Loss 0.000628    Objective Loss 0.000628    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.959055    
2024-05-15 13:34:51,592 - 

2024-05-15 13:34:51,593 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:35:51,445 - Epoch: [212][   70/   70]    Overall Loss 0.000649    Objective Loss 0.000649    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.854915    
2024-05-15 13:35:51,773 - 

2024-05-15 13:35:51,774 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:36:53,843 - Epoch: [213][   70/   70]    Overall Loss 0.000635    Objective Loss 0.000635    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.886591    
2024-05-15 13:36:54,153 - 

2024-05-15 13:36:54,154 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:37:56,047 - Epoch: [214][   70/   70]    Overall Loss 0.000637    Objective Loss 0.000637    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.884100    
2024-05-15 13:37:56,340 - 

2024-05-15 13:37:56,340 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:38:59,201 - Epoch: [215][   70/   70]    Overall Loss 0.000619    Objective Loss 0.000619    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.897906    
2024-05-15 13:38:59,616 - 

2024-05-15 13:38:59,616 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:39:57,300 - Epoch: [216][   70/   70]    Overall Loss 0.000623    Objective Loss 0.000623    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.823957    
2024-05-15 13:39:57,656 - 

2024-05-15 13:39:57,657 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:41:01,915 - Epoch: [217][   70/   70]    Overall Loss 0.000617    Objective Loss 0.000617    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.917870    
2024-05-15 13:41:02,135 - 

2024-05-15 13:41:02,136 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:42:02,097 - Epoch: [218][   70/   70]    Overall Loss 0.000620    Objective Loss 0.000620    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.856488    
2024-05-15 13:42:02,438 - 

2024-05-15 13:42:02,438 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:43:11,637 - Epoch: [219][   70/   70]    Overall Loss 0.000624    Objective Loss 0.000624    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.988453    
2024-05-15 13:43:12,069 - 

2024-05-15 13:43:12,070 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:44:11,237 - Epoch: [220][   70/   70]    Overall Loss 0.000619    Objective Loss 0.000619    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.845123    
2024-05-15 13:44:11,471 - --- validate (epoch=220)-----------
2024-05-15 13:44:11,472 - 1736 samples (100 per mini-batch)
2024-05-15 13:44:30,366 - Epoch: [220][   18/   18]    Loss 2.545853    Top1 57.258065    Top5 75.288018    
2024-05-15 13:44:30,702 - ==> Top1: 57.258    Top5: 75.288    Loss: 2.546

2024-05-15 13:44:30,708 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 13:44:30,708 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 13:44:30,769 - 

2024-05-15 13:44:30,770 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:45:30,221 - Epoch: [221][   70/   70]    Overall Loss 0.000618    Objective Loss 0.000618    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.849190    
2024-05-15 13:45:30,551 - 

2024-05-15 13:45:30,552 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:46:28,524 - Epoch: [222][   70/   70]    Overall Loss 0.000603    Objective Loss 0.000603    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.828056    
2024-05-15 13:46:28,889 - 

2024-05-15 13:46:28,890 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:47:33,753 - Epoch: [223][   70/   70]    Overall Loss 0.000595    Objective Loss 0.000595    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.926496    
2024-05-15 13:47:34,311 - 

2024-05-15 13:47:34,312 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:48:31,655 - Epoch: [224][   70/   70]    Overall Loss 0.000602    Objective Loss 0.000602    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.819087    
2024-05-15 13:48:32,085 - 

2024-05-15 13:48:32,086 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:49:34,188 - Epoch: [225][   70/   70]    Overall Loss 0.000583    Objective Loss 0.000583    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.887056    
2024-05-15 13:49:35,027 - 

2024-05-15 13:49:35,027 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:50:36,418 - Epoch: [226][   70/   70]    Overall Loss 0.000575    Objective Loss 0.000575    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.876905    
2024-05-15 13:50:36,807 - 

2024-05-15 13:50:36,808 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:51:40,021 - Epoch: [227][   70/   70]    Overall Loss 0.000567    Objective Loss 0.000567    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.902921    
2024-05-15 13:51:40,300 - 

2024-05-15 13:51:40,300 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:52:47,550 - Epoch: [228][   70/   70]    Overall Loss 0.000553    Objective Loss 0.000553    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.960597    
2024-05-15 13:52:47,840 - 

2024-05-15 13:52:47,841 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:53:49,195 - Epoch: [229][   70/   70]    Overall Loss 0.000554    Objective Loss 0.000554    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.876382    
2024-05-15 13:53:49,465 - 

2024-05-15 13:53:49,465 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:54:55,902 - Epoch: [230][   70/   70]    Overall Loss 0.000579    Objective Loss 0.000579    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.948997    
2024-05-15 13:54:56,230 - --- validate (epoch=230)-----------
2024-05-15 13:54:56,232 - 1736 samples (100 per mini-batch)
2024-05-15 13:55:19,458 - Epoch: [230][   18/   18]    Loss 2.522216    Top1 57.315668    Top5 75.057604    
2024-05-15 13:55:19,763 - ==> Top1: 57.316    Top5: 75.058    Loss: 2.522

2024-05-15 13:55:19,767 - ==> Best [Top1: 58.525   Top5: 76.267   Sparsity:0.00   Params: 728560 on epoch: 80]
2024-05-15 13:55:19,767 - Saving checkpoint to: logs/2024.05.15-094723/checkpoint.pth.tar
2024-05-15 13:55:19,826 - 

2024-05-15 13:55:19,827 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:56:23,485 - Epoch: [231][   70/   70]    Overall Loss 0.000543    Objective Loss 0.000543    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.909304    
2024-05-15 13:56:23,836 - 

2024-05-15 13:56:23,836 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:57:26,218 - Epoch: [232][   70/   70]    Overall Loss 0.000553    Objective Loss 0.000553    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.891070    
2024-05-15 13:57:26,730 - 

2024-05-15 13:57:26,731 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:58:24,181 - Epoch: [233][   70/   70]    Overall Loss 0.000559    Objective Loss 0.000559    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.820597    
2024-05-15 13:58:24,831 - 

2024-05-15 13:58:24,832 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 13:59:26,087 - Epoch: [234][   70/   70]    Overall Loss 0.000544    Objective Loss 0.000544    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.874963    
2024-05-15 13:59:26,602 - 

2024-05-15 13:59:26,602 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:00:39,279 - Epoch: [235][   70/   70]    Overall Loss 0.000561    Objective Loss 0.000561    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 1.038125    
2024-05-15 14:00:39,770 - 

2024-05-15 14:00:39,771 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:01:38,803 - Epoch: [236][   70/   70]    Overall Loss 0.000535    Objective Loss 0.000535    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.843199    
2024-05-15 14:01:39,374 - 

2024-05-15 14:01:39,375 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:02:41,110 - Epoch: [237][   70/   70]    Overall Loss 0.000501    Objective Loss 0.000501    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.881806    
2024-05-15 14:02:41,598 - 

2024-05-15 14:02:41,598 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:03:51,557 - Epoch: [238][   70/   70]    Overall Loss 0.000534    Objective Loss 0.000534    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.999300    
2024-05-15 14:03:52,083 - 

2024-05-15 14:03:52,084 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:04:54,167 - Epoch: [239][   70/   70]    Overall Loss 0.000506    Objective Loss 0.000506    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.886790    
2024-05-15 14:04:54,928 - 

2024-05-15 14:04:54,929 - Initiating quantization aware training (QAT)...
2024-05-15 14:04:54,989 - 

2024-05-15 14:04:54,990 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:05:52,880 - Epoch: [240][   70/   70]    Overall Loss 2.196369    Objective Loss 2.196369    Top1 78.723404    Top5 94.326241    LR 0.000016    Time 0.826904    
2024-05-15 14:05:53,296 - --- validate (epoch=240)-----------
2024-05-15 14:05:53,297 - 1736 samples (100 per mini-batch)
2024-05-15 14:06:10,648 - Epoch: [240][   18/   18]    Loss 2.253553    Top1 48.214286    Top5 68.145161    
2024-05-15 14:06:10,940 - ==> Top1: 48.214    Top5: 68.145    Loss: 2.254

2024-05-15 14:06:10,943 - ==> Best [Top1: 48.214   Top5: 68.145   Sparsity:0.00   Params: 728560 on epoch: 240]
2024-05-15 14:06:10,943 - Saving checkpoint to: logs/2024.05.15-094723/qat_checkpoint.pth.tar
2024-05-15 14:06:10,992 - 

2024-05-15 14:06:10,993 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:07:11,884 - Epoch: [241][   70/   70]    Overall Loss 0.652026    Objective Loss 0.652026    Top1 93.617021    Top5 97.872340    LR 0.000016    Time 0.869771    
2024-05-15 14:07:12,502 - 

2024-05-15 14:07:12,503 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:08:15,192 - Epoch: [242][   70/   70]    Overall Loss 0.401515    Objective Loss 0.401515    Top1 92.907801    Top5 99.290780    LR 0.000016    Time 0.895441    
2024-05-15 14:08:15,556 - 

2024-05-15 14:08:15,556 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:09:10,601 - Epoch: [243][   70/   70]    Overall Loss 0.286863    Objective Loss 0.286863    Top1 94.326241    Top5 98.581560    LR 0.000016    Time 0.786253    
2024-05-15 14:09:11,080 - 

2024-05-15 14:09:11,081 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:10:08,928 - Epoch: [244][   70/   70]    Overall Loss 0.225330    Objective Loss 0.225330    Top1 95.744681    Top5 99.290780    LR 0.000016    Time 0.826275    
2024-05-15 14:10:09,196 - 

2024-05-15 14:10:09,197 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:11:07,278 - Epoch: [245][   70/   70]    Overall Loss 0.184567    Objective Loss 0.184567    Top1 96.453901    Top5 100.000000    LR 0.000016    Time 0.829628    
2024-05-15 14:11:07,715 - 

2024-05-15 14:11:07,715 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:12:09,040 - Epoch: [246][   70/   70]    Overall Loss 0.153052    Objective Loss 0.153052    Top1 98.581560    Top5 100.000000    LR 0.000016    Time 0.875965    
2024-05-15 14:12:09,345 - 

2024-05-15 14:12:09,346 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:13:09,577 - Epoch: [247][   70/   70]    Overall Loss 0.135083    Objective Loss 0.135083    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.860344    
2024-05-15 14:13:09,870 - 

2024-05-15 14:13:09,870 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:14:06,654 - Epoch: [248][   70/   70]    Overall Loss 0.114560    Objective Loss 0.114560    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.811101    
2024-05-15 14:14:06,904 - 

2024-05-15 14:14:06,904 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:15:10,635 - Epoch: [249][   70/   70]    Overall Loss 0.096777    Objective Loss 0.096777    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.910343    
2024-05-15 14:15:10,892 - 

2024-05-15 14:15:10,893 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:16:08,874 - Epoch: [250][   70/   70]    Overall Loss 0.085967    Objective Loss 0.085967    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.828188    
2024-05-15 14:16:09,221 - --- validate (epoch=250)-----------
2024-05-15 14:16:09,222 - 1736 samples (100 per mini-batch)
2024-05-15 14:16:28,169 - Epoch: [250][   18/   18]    Loss 2.339173    Top1 55.184332    Top5 72.580645    
2024-05-15 14:16:28,498 - ==> Top1: 55.184    Top5: 72.581    Loss: 2.339

2024-05-15 14:16:28,503 - ==> Best [Top1: 55.184   Top5: 72.581   Sparsity:0.00   Params: 728560 on epoch: 250]
2024-05-15 14:16:28,503 - Saving checkpoint to: logs/2024.05.15-094723/qat_checkpoint.pth.tar
2024-05-15 14:16:28,577 - 

2024-05-15 14:16:28,577 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:17:38,347 - Epoch: [251][   70/   70]    Overall Loss 0.073584    Objective Loss 0.073584    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.996609    
2024-05-15 14:17:38,919 - 

2024-05-15 14:17:38,920 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:18:41,365 - Epoch: [252][   70/   70]    Overall Loss 0.066904    Objective Loss 0.066904    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.891976    
2024-05-15 14:18:41,710 - 

2024-05-15 14:18:41,711 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:19:43,829 - Epoch: [253][   70/   70]    Overall Loss 0.059927    Objective Loss 0.059927    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.887250    
2024-05-15 14:19:44,202 - 

2024-05-15 14:19:44,203 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:20:44,784 - Epoch: [254][   70/   70]    Overall Loss 0.057344    Objective Loss 0.057344    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.865318    
2024-05-15 14:20:45,091 - 

2024-05-15 14:20:45,091 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:21:41,284 - Epoch: [255][   70/   70]    Overall Loss 0.052035    Objective Loss 0.052035    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.802657    
2024-05-15 14:21:41,648 - 

2024-05-15 14:21:41,649 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:22:39,262 - Epoch: [256][   70/   70]    Overall Loss 0.048762    Objective Loss 0.048762    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.822936    
2024-05-15 14:22:39,593 - 

2024-05-15 14:22:39,594 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:23:42,201 - Epoch: [257][   70/   70]    Overall Loss 0.045706    Objective Loss 0.045706    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.894273    
2024-05-15 14:23:42,540 - 

2024-05-15 14:23:42,540 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:24:45,138 - Epoch: [258][   70/   70]    Overall Loss 0.039804    Objective Loss 0.039804    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.894132    
2024-05-15 14:24:45,595 - 

2024-05-15 14:24:45,596 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:25:41,950 - Epoch: [259][   70/   70]    Overall Loss 0.039856    Objective Loss 0.039856    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.804954    
2024-05-15 14:25:42,268 - 

2024-05-15 14:25:42,268 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:26:48,874 - Epoch: [260][   70/   70]    Overall Loss 0.038941    Objective Loss 0.038941    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.951411    
2024-05-15 14:26:49,165 - --- validate (epoch=260)-----------
2024-05-15 14:26:49,166 - 1736 samples (100 per mini-batch)
2024-05-15 14:27:10,975 - Epoch: [260][   18/   18]    Loss 2.460221    Top1 56.163594    Top5 73.559908    
2024-05-15 14:27:11,373 - ==> Top1: 56.164    Top5: 73.560    Loss: 2.460

2024-05-15 14:27:11,377 - ==> Best [Top1: 56.164   Top5: 73.560   Sparsity:0.00   Params: 728560 on epoch: 260]
2024-05-15 14:27:11,378 - Saving checkpoint to: logs/2024.05.15-094723/qat_checkpoint.pth.tar
2024-05-15 14:27:11,454 - 

2024-05-15 14:27:11,456 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:28:13,142 - Epoch: [261][   70/   70]    Overall Loss 0.034243    Objective Loss 0.034243    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.881105    
2024-05-15 14:28:13,566 - 

2024-05-15 14:28:13,567 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:29:13,972 - Epoch: [262][   70/   70]    Overall Loss 0.033152    Objective Loss 0.033152    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.862822    
2024-05-15 14:29:14,345 - 

2024-05-15 14:29:14,346 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:30:18,610 - Epoch: [263][   70/   70]    Overall Loss 0.031593    Objective Loss 0.031593    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.917963    
2024-05-15 14:30:19,068 - 

2024-05-15 14:30:19,069 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:31:18,355 - Epoch: [264][   70/   70]    Overall Loss 0.028696    Objective Loss 0.028696    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.846817    
2024-05-15 14:31:18,667 - 

2024-05-15 14:31:18,668 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:32:23,052 - Epoch: [265][   70/   70]    Overall Loss 0.027314    Objective Loss 0.027314    Top1 98.581560    Top5 100.000000    LR 0.000016    Time 0.919651    
2024-05-15 14:32:23,419 - 

2024-05-15 14:32:23,420 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:33:26,895 - Epoch: [266][   70/   70]    Overall Loss 0.027401    Objective Loss 0.027401    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.906680    
2024-05-15 14:33:27,438 - 

2024-05-15 14:33:27,439 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:34:23,583 - Epoch: [267][   70/   70]    Overall Loss 0.026274    Objective Loss 0.026274    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.801908    
2024-05-15 14:34:23,823 - 

2024-05-15 14:34:23,823 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:35:18,843 - Epoch: [268][   70/   70]    Overall Loss 0.022974    Objective Loss 0.022974    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.785881    
2024-05-15 14:35:19,141 - 

2024-05-15 14:35:19,141 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:36:22,187 - Epoch: [269][   70/   70]    Overall Loss 0.022604    Objective Loss 0.022604    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.900552    
2024-05-15 14:36:22,545 - 

2024-05-15 14:36:22,545 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:37:22,374 - Epoch: [270][   70/   70]    Overall Loss 0.024854    Objective Loss 0.024854    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.854548    
2024-05-15 14:37:22,592 - --- validate (epoch=270)-----------
2024-05-15 14:37:22,593 - 1736 samples (100 per mini-batch)
2024-05-15 14:37:42,387 - Epoch: [270][   18/   18]    Loss 2.603664    Top1 54.896313    Top5 73.271889    
2024-05-15 14:37:42,720 - ==> Top1: 54.896    Top5: 73.272    Loss: 2.604

2024-05-15 14:37:42,724 - ==> Best [Top1: 56.164   Top5: 73.560   Sparsity:0.00   Params: 728560 on epoch: 260]
2024-05-15 14:37:42,724 - Saving checkpoint to: logs/2024.05.15-094723/qat_checkpoint.pth.tar
2024-05-15 14:37:42,782 - 

2024-05-15 14:37:42,782 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:38:49,975 - Epoch: [271][   70/   70]    Overall Loss 0.022993    Objective Loss 0.022993    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.959791    
2024-05-15 14:38:50,669 - 

2024-05-15 14:38:50,670 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:39:51,813 - Epoch: [272][   70/   70]    Overall Loss 0.019432    Objective Loss 0.019432    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.873348    
2024-05-15 14:39:52,201 - 

2024-05-15 14:39:52,202 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:40:47,213 - Epoch: [273][   70/   70]    Overall Loss 0.018726    Objective Loss 0.018726    Top1 99.290780    Top5 100.000000    LR 0.000016    Time 0.785768    
2024-05-15 14:40:47,697 - 

2024-05-15 14:40:47,698 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:41:52,763 - Epoch: [274][   70/   70]    Overall Loss 0.019859    Objective Loss 0.019859    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.929382    
2024-05-15 14:41:53,369 - 

2024-05-15 14:41:53,370 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:42:57,971 - Epoch: [275][   70/   70]    Overall Loss 0.018239    Objective Loss 0.018239    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.922769    
2024-05-15 14:42:58,356 - 

2024-05-15 14:42:58,357 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:43:55,880 - Epoch: [276][   70/   70]    Overall Loss 0.018225    Objective Loss 0.018225    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.821650    
2024-05-15 14:43:56,385 - 

2024-05-15 14:43:56,386 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:44:57,349 - Epoch: [277][   70/   70]    Overall Loss 0.017472    Objective Loss 0.017472    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.870798    
2024-05-15 14:44:57,673 - 

2024-05-15 14:44:57,673 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:46:07,790 - Epoch: [278][   70/   70]    Overall Loss 0.018080    Objective Loss 0.018080    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 1.001579    
2024-05-15 14:46:08,646 - 

2024-05-15 14:46:08,646 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:47:17,281 - Epoch: [279][   70/   70]    Overall Loss 0.019075    Objective Loss 0.019075    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.980383    
2024-05-15 14:47:17,549 - 

2024-05-15 14:47:17,549 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:48:18,276 - Epoch: [280][   70/   70]    Overall Loss 0.015607    Objective Loss 0.015607    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.867424    
2024-05-15 14:48:18,852 - --- validate (epoch=280)-----------
2024-05-15 14:48:18,853 - 1736 samples (100 per mini-batch)
2024-05-15 14:48:42,418 - Epoch: [280][   18/   18]    Loss 2.633761    Top1 55.990783    Top5 72.983871    
2024-05-15 14:48:42,762 - ==> Top1: 55.991    Top5: 72.984    Loss: 2.634

2024-05-15 14:48:42,769 - ==> Best [Top1: 56.164   Top5: 73.560   Sparsity:0.00   Params: 728560 on epoch: 260]
2024-05-15 14:48:42,769 - Saving checkpoint to: logs/2024.05.15-094723/qat_checkpoint.pth.tar
2024-05-15 14:48:42,840 - 

2024-05-15 14:48:42,840 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:49:49,427 - Epoch: [281][   70/   70]    Overall Loss 0.015807    Objective Loss 0.015807    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.951127    
2024-05-15 14:49:49,739 - 

2024-05-15 14:49:49,740 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:50:45,725 - Epoch: [282][   70/   70]    Overall Loss 0.017308    Objective Loss 0.017308    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.799705    
2024-05-15 14:50:45,972 - 

2024-05-15 14:50:45,973 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:51:43,746 - Epoch: [283][   70/   70]    Overall Loss 0.015256    Objective Loss 0.015256    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.825235    
2024-05-15 14:51:44,168 - 

2024-05-15 14:51:44,169 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:52:37,640 - Epoch: [284][   70/   70]    Overall Loss 0.013606    Objective Loss 0.013606    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.763762    
2024-05-15 14:52:38,196 - 

2024-05-15 14:52:38,196 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:53:31,670 - Epoch: [285][   70/   70]    Overall Loss 0.014557    Objective Loss 0.014557    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.763810    
2024-05-15 14:53:31,852 - 

2024-05-15 14:53:31,853 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:54:27,056 - Epoch: [286][   70/   70]    Overall Loss 0.015990    Objective Loss 0.015990    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.788516    
2024-05-15 14:54:27,420 - 

2024-05-15 14:54:27,421 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:55:23,418 - Epoch: [287][   70/   70]    Overall Loss 0.013093    Objective Loss 0.013093    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.799854    
2024-05-15 14:55:23,967 - 

2024-05-15 14:55:23,968 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:56:18,354 - Epoch: [288][   70/   70]    Overall Loss 0.014054    Objective Loss 0.014054    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.776845    
2024-05-15 14:56:18,648 - 

2024-05-15 14:56:18,649 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:57:13,803 - Epoch: [289][   70/   70]    Overall Loss 0.013064    Objective Loss 0.013064    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.787788    
2024-05-15 14:57:13,997 - 

2024-05-15 14:57:13,997 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:58:11,583 - Epoch: [290][   70/   70]    Overall Loss 0.010818    Objective Loss 0.010818    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.822566    
2024-05-15 14:58:11,792 - --- validate (epoch=290)-----------
2024-05-15 14:58:11,793 - 1736 samples (100 per mini-batch)
2024-05-15 14:58:27,637 - Epoch: [290][   18/   18]    Loss 2.624708    Top1 55.299539    Top5 73.041475    
2024-05-15 14:58:27,943 - ==> Top1: 55.300    Top5: 73.041    Loss: 2.625

2024-05-15 14:58:27,947 - ==> Best [Top1: 56.164   Top5: 73.560   Sparsity:0.00   Params: 728560 on epoch: 260]
2024-05-15 14:58:27,947 - Saving checkpoint to: logs/2024.05.15-094723/qat_checkpoint.pth.tar
2024-05-15 14:58:28,001 - 

2024-05-15 14:58:28,002 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 14:59:19,486 - Epoch: [291][   70/   70]    Overall Loss 0.013434    Objective Loss 0.013434    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.735395    
2024-05-15 14:59:20,066 - 

2024-05-15 14:59:20,067 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 15:00:15,508 - Epoch: [292][   70/   70]    Overall Loss 0.011465    Objective Loss 0.011465    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.791912    
2024-05-15 15:00:16,152 - 

2024-05-15 15:00:16,153 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 15:01:09,391 - Epoch: [293][   70/   70]    Overall Loss 0.010637    Objective Loss 0.010637    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.760433    
2024-05-15 15:01:09,686 - 

2024-05-15 15:01:09,686 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 15:02:07,508 - Epoch: [294][   70/   70]    Overall Loss 0.009813    Objective Loss 0.009813    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.825920    
2024-05-15 15:02:07,808 - 

2024-05-15 15:02:07,809 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 15:03:01,197 - Epoch: [295][   70/   70]    Overall Loss 0.011327    Objective Loss 0.011327    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.762582    
2024-05-15 15:03:01,841 - 

2024-05-15 15:03:01,842 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 15:03:49,760 - Epoch: [296][   70/   70]    Overall Loss 0.011362    Objective Loss 0.011362    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.684448    
2024-05-15 15:03:49,950 - 

2024-05-15 15:03:49,950 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 15:04:39,773 - Epoch: [297][   70/   70]    Overall Loss 0.012312    Objective Loss 0.012312    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.711649    
2024-05-15 15:04:39,960 - 

2024-05-15 15:04:39,961 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 15:05:30,269 - Epoch: [298][   70/   70]    Overall Loss 0.009335    Objective Loss 0.009335    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.718600    
2024-05-15 15:05:30,521 - 

2024-05-15 15:05:30,522 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-15 15:06:16,319 - Epoch: [299][   70/   70]    Overall Loss 0.011167    Objective Loss 0.011167    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.654154    
2024-05-15 15:06:16,613 - --- test ---------------------
2024-05-15 15:06:16,614 - 1736 samples (100 per mini-batch)
2024-05-15 15:06:31,325 - Test: [   18/   18]    Loss 2.625015    Top1 55.184332    Top5 73.214286    
2024-05-15 15:06:31,666 - ==> Top1: 55.184    Top5: 73.214    Loss: 2.625

2024-05-15 15:06:31,670 - 
2024-05-15 15:06:31,670 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.15-094723/2024.05.15-094723.log
