2024-05-03 23:09:51,036 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.03-230951/2024.05.03-230951.log
2024-05-03 23:09:51,677 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.03-230951/2024.05.03-230951.log
2024-05-03 23:09:55,782 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2024-05-03 23:09:55,783 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}
2024-05-03 23:09:55,972 - Dataset sizes:
	training=6941
	validation=1736
	test=1736
2024-05-03 23:09:55,973 - Reading compression schedule from: policies/schedule-cifar100-mobilenetv2.yaml
2024-05-03 23:09:55,986 - 

2024-05-03 23:09:55,986 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:09:56,201 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2024-05-03 23:09:56,202 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2024-05-03 23:09:56,396 - Dataset sizes:
	training=6941
	validation=1736
	test=1736
2024-05-03 23:09:56,396 - Reading compression schedule from: policies/schedule-cifar100.yaml
2024-05-03 23:09:56,402 - 

2024-05-03 23:09:56,402 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:10:18,230 - Epoch: [0][  100/  217]    Overall Loss 4.111770    Objective Loss 4.111770                                        LR 0.001000    Time 0.218204    
2024-05-03 23:10:45,901 - Epoch: [0][  200/  217]    Overall Loss 3.820673    Objective Loss 3.820673                                        LR 0.001000    Time 0.247412    
2024-05-03 23:10:46,702 - Epoch: [0][   55/   55]    Overall Loss 3.957022    Objective Loss 3.957022    Top1 25.477707    Top5 35.668790    LR 0.100000    Time 0.921954    
2024-05-03 23:10:47,757 - --- validate (epoch=0)-----------
2024-05-03 23:10:47,759 - 1736 samples (128 per mini-batch)
2024-05-03 23:10:49,836 - Epoch: [0][  217/  217]    Overall Loss 3.790609    Objective Loss 3.790609    Top1 24.590164    Top5 36.065574    LR 0.001000    Time 0.246149    
2024-05-03 23:10:50,418 - --- validate (epoch=0)-----------
2024-05-03 23:10:50,419 - 1736 samples (32 per mini-batch)
2024-05-03 23:11:06,219 - Epoch: [0][   14/   14]    Loss 4.571666    Top1 2.361751    Top5 24.251152    
2024-05-03 23:11:06,519 - ==> Top1: 2.362    Top5: 24.251    Loss: 4.572

2024-05-03 23:11:06,532 - ==> Best [Top1: 2.362   Top5: 24.251   Sparsity:0.00   Params: 1342920 on epoch: 0]
2024-05-03 23:11:06,532 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-03 23:11:06,671 - 

2024-05-03 23:11:06,673 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:11:07,014 - Epoch: [0][   55/   55]    Loss 3.352218    Top1 29.896313    Top5 38.364055    
2024-05-03 23:11:07,252 - ==> Top1: 29.896    Top5: 38.364    Loss: 3.352

2024-05-03 23:11:07,257 - ==> Best [Top1: 29.896   Top5: 38.364   Sparsity:0.00   Params: 384080 on epoch: 0]
2024-05-03 23:11:07,258 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-03 23:11:07,321 - 

2024-05-03 23:11:07,321 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:11:36,051 - Epoch: [1][  100/  217]    Overall Loss 3.335689    Objective Loss 3.335689                                        LR 0.001000    Time 0.287203    
2024-05-03 23:12:02,485 - Epoch: [1][   55/   55]    Overall Loss 3.478115    Objective Loss 3.478115    Top1 21.656051    Top5 40.764331    LR 0.100000    Time 1.014533    
2024-05-03 23:12:02,863 - 

2024-05-03 23:12:02,864 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:12:04,534 - Epoch: [1][  200/  217]    Overall Loss 3.243487    Objective Loss 3.243487                                        LR 0.001000    Time 0.285978    
2024-05-03 23:12:07,643 - Epoch: [1][  217/  217]    Overall Loss 3.224730    Objective Loss 3.224730    Top1 34.426230    Top5 39.344262    LR 0.001000    Time 0.277891    
2024-05-03 23:12:07,899 - 

2024-05-03 23:12:07,900 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:12:34,139 - Epoch: [2][  100/  217]    Overall Loss 2.953977    Objective Loss 2.953977                                        LR 0.001000    Time 0.262306    
2024-05-03 23:13:01,911 - Epoch: [2][   55/   55]    Overall Loss 3.306111    Objective Loss 3.306111    Top1 24.203822    Top5 35.668790    LR 0.100000    Time 1.073403    
2024-05-03 23:13:02,263 - 

2024-05-03 23:13:02,265 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:13:03,013 - Epoch: [2][  200/  217]    Overall Loss 2.906122    Objective Loss 2.906122                                        LR 0.001000    Time 0.275475    
2024-05-03 23:13:09,112 - Epoch: [2][  217/  217]    Overall Loss 2.896656    Objective Loss 2.896656    Top1 37.704918    Top5 50.819672    LR 0.001000    Time 0.281994    
2024-05-03 23:13:09,326 - 

2024-05-03 23:13:09,326 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:13:33,260 - Epoch: [3][  100/  217]    Overall Loss 2.619467    Objective Loss 2.619467                                        LR 0.001000    Time 0.239242    
2024-05-03 23:14:01,664 - Epoch: [3][   55/   55]    Overall Loss 3.144712    Objective Loss 3.144712    Top1 28.662420    Top5 41.401274    LR 0.100000    Time 1.079806    
2024-05-03 23:14:01,816 - Epoch: [3][  200/  217]    Overall Loss 2.583802    Objective Loss 2.583802                                        LR 0.001000    Time 0.262352    
2024-05-03 23:14:02,304 - 

2024-05-03 23:14:02,305 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:14:06,287 - Epoch: [3][  217/  217]    Overall Loss 2.582046    Objective Loss 2.582046    Top1 40.983607    Top5 57.377049    LR 0.001000    Time 0.262394    
2024-05-03 23:14:06,545 - 

2024-05-03 23:14:06,546 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:14:36,507 - Epoch: [4][  100/  217]    Overall Loss 2.317289    Objective Loss 2.317289                                        LR 0.001000    Time 0.299513    
2024-05-03 23:14:56,970 - Epoch: [4][   55/   55]    Overall Loss 2.986017    Objective Loss 2.986017    Top1 35.668790    Top5 46.496815    LR 0.100000    Time 0.993664    
2024-05-03 23:14:57,589 - 

2024-05-03 23:14:57,592 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:15:00,507 - Epoch: [4][  200/  217]    Overall Loss 2.289277    Objective Loss 2.289277                                        LR 0.001000    Time 0.269716    
2024-05-03 23:15:03,567 - Epoch: [4][  217/  217]    Overall Loss 2.293777    Objective Loss 2.293777    Top1 55.737705    Top5 75.409836    LR 0.001000    Time 0.262679    
2024-05-03 23:15:03,793 - 

2024-05-03 23:15:03,794 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:15:32,286 - Epoch: [5][  100/  217]    Overall Loss 2.032245    Objective Loss 2.032245                                        LR 0.001000    Time 0.284821    
2024-05-03 23:15:56,345 - Epoch: [5][   55/   55]    Overall Loss 2.884284    Objective Loss 2.884284    Top1 44.585987    Top5 58.598726    LR 0.100000    Time 1.067904    
2024-05-03 23:15:56,825 - 

2024-05-03 23:15:56,827 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:15:58,676 - Epoch: [5][  200/  217]    Overall Loss 2.029046    Objective Loss 2.029046                                        LR 0.001000    Time 0.274318    
2024-05-03 23:16:03,073 - Epoch: [5][  217/  217]    Overall Loss 2.033940    Objective Loss 2.033940    Top1 45.901639    Top5 67.213115    LR 0.001000    Time 0.273081    
2024-05-03 23:16:03,275 - 

2024-05-03 23:16:03,276 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:16:31,819 - Epoch: [6][  100/  217]    Overall Loss 1.832441    Objective Loss 1.832441                                        LR 0.001000    Time 0.285357    
2024-05-03 23:16:48,634 - Epoch: [6][   55/   55]    Overall Loss 2.796375    Objective Loss 2.796375    Top1 40.764331    Top5 55.414013    LR 0.100000    Time 0.941748    
2024-05-03 23:16:49,172 - 

2024-05-03 23:16:49,173 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:16:59,079 - Epoch: [6][  200/  217]    Overall Loss 1.806418    Objective Loss 1.806418                                        LR 0.001000    Time 0.278933    
2024-05-03 23:17:02,934 - Epoch: [6][  217/  217]    Overall Loss 1.811157    Objective Loss 1.811157    Top1 68.852459    Top5 78.688525    LR 0.001000    Time 0.274840    
2024-05-03 23:17:03,133 - 

2024-05-03 23:17:03,133 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:17:32,141 - Epoch: [7][  100/  217]    Overall Loss 1.580927    Objective Loss 1.580927                                        LR 0.001000    Time 0.289991    
2024-05-03 23:17:40,468 - Epoch: [7][   55/   55]    Overall Loss 2.646332    Objective Loss 2.646332    Top1 39.490446    Top5 56.050955    LR 0.100000    Time 0.932479    
2024-05-03 23:17:40,761 - 

2024-05-03 23:17:40,762 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:17:54,185 - Epoch: [7][  200/  217]    Overall Loss 1.587229    Objective Loss 1.587229                                        LR 0.001000    Time 0.255174    
2024-05-03 23:18:00,339 - Epoch: [7][  217/  217]    Overall Loss 1.578241    Objective Loss 1.578241    Top1 65.573770    Top5 81.967213    LR 0.001000    Time 0.263532    
2024-05-03 23:18:00,669 - 

2024-05-03 23:18:00,669 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:18:29,755 - Epoch: [8][  100/  217]    Overall Loss 1.373925    Objective Loss 1.373925                                        LR 0.001000    Time 0.290772    
2024-05-03 23:18:45,834 - Epoch: [8][   55/   55]    Overall Loss 2.570984    Objective Loss 2.570984    Top1 37.579618    Top5 58.598726    LR 0.100000    Time 1.182974    
2024-05-03 23:18:46,509 - 

2024-05-03 23:18:46,510 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:18:55,368 - Epoch: [8][  200/  217]    Overall Loss 1.380572    Objective Loss 1.380572                                        LR 0.001000    Time 0.273410    
2024-05-03 23:18:59,378 - Epoch: [8][  217/  217]    Overall Loss 1.381797    Objective Loss 1.381797    Top1 63.934426    Top5 77.049180    LR 0.001000    Time 0.270461    
2024-05-03 23:18:59,620 - 

2024-05-03 23:18:59,620 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:19:28,831 - Epoch: [9][  100/  217]    Overall Loss 1.157724    Objective Loss 1.157724                                        LR 0.001000    Time 0.292022    
2024-05-03 23:19:37,700 - Epoch: [9][   55/   55]    Overall Loss 2.435313    Objective Loss 2.435313    Top1 38.853503    Top5 52.866242    LR 0.100000    Time 0.930541    
2024-05-03 23:19:37,993 - 

2024-05-03 23:19:37,994 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:19:52,573 - Epoch: [9][  200/  217]    Overall Loss 1.187460    Objective Loss 1.187460                                        LR 0.001000    Time 0.264684    
2024-05-03 23:19:56,565 - Epoch: [9][  217/  217]    Overall Loss 1.186067    Objective Loss 1.186067    Top1 68.852459    Top5 81.967213    LR 0.001000    Time 0.262336    
2024-05-03 23:19:56,797 - 

2024-05-03 23:19:56,798 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:20:26,365 - Epoch: [10][  100/  217]    Overall Loss 0.980583    Objective Loss 0.980583                                        LR 0.001000    Time 0.295587    
2024-05-03 23:20:30,647 - Epoch: [10][   55/   55]    Overall Loss 2.400667    Objective Loss 2.400667    Top1 42.038217    Top5 56.050955    LR 0.100000    Time 0.957136    
2024-05-03 23:20:31,111 - --- validate (epoch=10)-----------
2024-05-03 23:20:31,112 - 1736 samples (128 per mini-batch)
2024-05-03 23:20:48,370 - Epoch: [10][   14/   14]    Loss 3.087039    Top1 36.866359    Top5 51.209677    
2024-05-03 23:20:48,674 - ==> Top1: 36.866    Top5: 51.210    Loss: 3.087

2024-05-03 23:20:48,688 - ==> Best [Top1: 36.866   Top5: 51.210   Sparsity:0.00   Params: 1342920 on epoch: 10]
2024-05-03 23:20:48,689 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-03 23:20:48,824 - 

2024-05-03 23:20:48,825 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:20:49,590 - Epoch: [10][  200/  217]    Overall Loss 0.988332    Objective Loss 0.988332                                        LR 0.001000    Time 0.263874    
2024-05-03 23:20:52,258 - Epoch: [10][  217/  217]    Overall Loss 0.986642    Objective Loss 0.986642    Top1 72.131148    Top5 91.803279    LR 0.001000    Time 0.255488    
2024-05-03 23:20:52,508 - --- validate (epoch=10)-----------
2024-05-03 23:20:52,508 - 1736 samples (32 per mini-batch)
2024-05-03 23:21:08,496 - Epoch: [10][   55/   55]    Loss 2.280395    Top1 48.963134    Top5 67.857143    
2024-05-03 23:21:08,817 - ==> Top1: 48.963    Top5: 67.857    Loss: 2.280

2024-05-03 23:21:08,822 - ==> Best [Top1: 48.963   Top5: 67.857   Sparsity:0.00   Params: 384080 on epoch: 10]
2024-05-03 23:21:08,822 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-03 23:21:08,875 - 

2024-05-03 23:21:08,876 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:21:35,686 - Epoch: [11][  100/  217]    Overall Loss 0.807144    Objective Loss 0.807144                                        LR 0.001000    Time 0.268014    
2024-05-03 23:21:43,723 - Epoch: [11][   55/   55]    Overall Loss 2.275088    Objective Loss 2.275088    Top1 35.668790    Top5 57.961783    LR 0.100000    Time 0.997986    
2024-05-03 23:21:44,080 - 

2024-05-03 23:21:44,081 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:21:57,630 - Epoch: [11][  200/  217]    Overall Loss 0.832708    Objective Loss 0.832708                                        LR 0.001000    Time 0.243688    
2024-05-03 23:22:01,140 - Epoch: [11][  217/  217]    Overall Loss 0.833140    Objective Loss 0.833140    Top1 85.245902    Top5 95.081967    LR 0.001000    Time 0.240766    
2024-05-03 23:22:01,813 - 

2024-05-03 23:22:01,814 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:22:33,910 - Epoch: [12][  100/  217]    Overall Loss 0.649377    Objective Loss 0.649377                                        LR 0.001000    Time 0.320863    
2024-05-03 23:22:44,735 - Epoch: [12][   55/   55]    Overall Loss 2.194834    Objective Loss 2.194834    Top1 45.859873    Top5 63.694268    LR 0.100000    Time 1.102645    
2024-05-03 23:22:44,968 - 

2024-05-03 23:22:44,969 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:22:56,528 - Epoch: [12][  200/  217]    Overall Loss 0.670116    Objective Loss 0.670116                                        LR 0.001000    Time 0.273478    
2024-05-03 23:23:00,610 - Epoch: [12][  217/  217]    Overall Loss 0.676493    Objective Loss 0.676493    Top1 81.967213    Top5 91.803279    LR 0.001000    Time 0.270855    
2024-05-03 23:23:00,828 - 

2024-05-03 23:23:00,829 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:23:33,209 - Epoch: [13][  100/  217]    Overall Loss 0.505046    Objective Loss 0.505046                                        LR 0.001000    Time 0.323712    
2024-05-03 23:23:41,009 - Epoch: [13][   55/   55]    Overall Loss 2.118033    Objective Loss 2.118033    Top1 49.044586    Top5 68.789809    LR 0.100000    Time 1.018722    
2024-05-03 23:23:41,398 - 

2024-05-03 23:23:41,399 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:23:57,495 - Epoch: [13][  200/  217]    Overall Loss 0.545877    Objective Loss 0.545877                                        LR 0.001000    Time 0.283244    
2024-05-03 23:24:02,102 - Epoch: [13][  217/  217]    Overall Loss 0.547258    Objective Loss 0.547258    Top1 81.967213    Top5 95.081967    LR 0.001000    Time 0.282275    
2024-05-03 23:24:02,392 - 

2024-05-03 23:24:02,393 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:24:29,874 - Epoch: [14][  100/  217]    Overall Loss 0.402379    Objective Loss 0.402379                                        LR 0.001000    Time 0.274739    
2024-05-03 23:24:36,198 - Epoch: [14][   55/   55]    Overall Loss 2.025780    Objective Loss 2.025780    Top1 36.305732    Top5 63.694268    LR 0.100000    Time 0.996187    
2024-05-03 23:24:36,664 - 

2024-05-03 23:24:36,665 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:24:57,394 - Epoch: [14][  200/  217]    Overall Loss 0.409560    Objective Loss 0.409560                                        LR 0.001000    Time 0.274923    
2024-05-03 23:25:02,733 - Epoch: [14][  217/  217]    Overall Loss 0.418540    Objective Loss 0.418540    Top1 86.885246    Top5 95.081967    LR 0.001000    Time 0.277981    
2024-05-03 23:25:03,196 - 

2024-05-03 23:25:03,196 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:25:31,523 - Epoch: [15][  100/  217]    Overall Loss 0.310997    Objective Loss 0.310997                                        LR 0.001000    Time 0.283179    
2024-05-03 23:25:36,052 - Epoch: [15][   55/   55]    Overall Loss 2.029974    Objective Loss 2.029974    Top1 47.770701    Top5 66.242038    LR 0.100000    Time 1.079554    
2024-05-03 23:25:36,920 - 

2024-05-03 23:25:36,921 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:25:56,013 - Epoch: [15][  200/  217]    Overall Loss 0.313503    Objective Loss 0.313503                                        LR 0.001000    Time 0.263999    
2024-05-03 23:26:01,402 - Epoch: [15][  217/  217]    Overall Loss 0.321124    Objective Loss 0.321124    Top1 85.245902    Top5 98.360656    LR 0.001000    Time 0.268144    
2024-05-03 23:26:01,783 - 

2024-05-03 23:26:01,784 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:26:28,460 - Epoch: [16][  100/  217]    Overall Loss 0.243167    Objective Loss 0.243167                                        LR 0.001000    Time 0.266675    
2024-05-03 23:26:32,410 - Epoch: [16][   55/   55]    Overall Loss 1.916860    Objective Loss 1.916860    Top1 56.687898    Top5 75.796178    LR 0.100000    Time 1.008723    
2024-05-03 23:26:32,843 - 

2024-05-03 23:26:32,844 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:26:58,932 - Epoch: [16][  200/  217]    Overall Loss 0.258287    Objective Loss 0.258287                                        LR 0.001000    Time 0.285656    
2024-05-03 23:27:02,720 - Epoch: [16][  217/  217]    Overall Loss 0.261680    Objective Loss 0.261680    Top1 90.163934    Top5 96.721311    LR 0.001000    Time 0.280722    
2024-05-03 23:27:03,021 - 

2024-05-03 23:27:03,022 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:27:30,109 - Epoch: [17][  100/  217]    Overall Loss 0.182069    Objective Loss 0.182069                                        LR 0.001000    Time 0.270771    
2024-05-03 23:27:34,727 - Epoch: [17][   55/   55]    Overall Loss 1.796733    Objective Loss 1.796733    Top1 52.866242    Top5 78.343949    LR 0.100000    Time 1.124973    
2024-05-03 23:27:35,166 - 

2024-05-03 23:27:35,167 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:27:57,719 - Epoch: [17][  200/  217]    Overall Loss 0.196453    Objective Loss 0.196453                                        LR 0.001000    Time 0.273397    
2024-05-03 23:28:00,473 - Epoch: [17][  217/  217]    Overall Loss 0.199114    Objective Loss 0.199114    Top1 95.081967    Top5 100.000000    LR 0.001000    Time 0.264661    
2024-05-03 23:28:00,830 - 

2024-05-03 23:28:00,831 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:28:29,138 - Epoch: [18][   55/   55]    Overall Loss 1.776503    Objective Loss 1.776503    Top1 50.318471    Top5 71.337580    LR 0.100000    Time 0.981117    
2024-05-03 23:28:29,429 - 

2024-05-03 23:28:29,430 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:28:33,135 - Epoch: [18][  100/  217]    Overall Loss 0.170972    Objective Loss 0.170972                                        LR 0.001000    Time 0.322953    
2024-05-03 23:29:00,022 - Epoch: [18][  200/  217]    Overall Loss 0.163016    Objective Loss 0.163016                                        LR 0.001000    Time 0.295873    
2024-05-03 23:29:04,845 - Epoch: [18][  217/  217]    Overall Loss 0.164581    Objective Loss 0.164581    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.294911    
2024-05-03 23:29:05,304 - 

2024-05-03 23:29:05,305 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:29:25,587 - Epoch: [19][   55/   55]    Overall Loss 1.689452    Objective Loss 1.689452    Top1 57.324841    Top5 73.885350    LR 0.100000    Time 1.020862    
2024-05-03 23:29:25,891 - 

2024-05-03 23:29:25,892 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:29:38,506 - Epoch: [19][  100/  217]    Overall Loss 0.113633    Objective Loss 0.113633                                        LR 0.001000    Time 0.331915    
2024-05-03 23:30:01,671 - Epoch: [19][  200/  217]    Overall Loss 0.113364    Objective Loss 0.113364                                        LR 0.001000    Time 0.281744    
2024-05-03 23:30:06,149 - Epoch: [19][  217/  217]    Overall Loss 0.114132    Objective Loss 0.114132    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.280298    
2024-05-03 23:30:06,992 - 

2024-05-03 23:30:06,993 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:30:22,973 - Epoch: [20][   55/   55]    Overall Loss 1.572716    Objective Loss 1.572716    Top1 46.496815    Top5 71.337580    LR 0.100000    Time 1.037654    
2024-05-03 23:30:23,245 - --- validate (epoch=20)-----------
2024-05-03 23:30:23,246 - 1736 samples (128 per mini-batch)
2024-05-03 23:30:34,742 - Epoch: [20][  100/  217]    Overall Loss 0.072630    Objective Loss 0.072630                                        LR 0.001000    Time 0.277389    
2024-05-03 23:30:40,814 - Epoch: [20][   14/   14]    Loss 2.942582    Top1 43.548387    Top5 59.389401    
2024-05-03 23:30:41,113 - ==> Top1: 43.548    Top5: 59.389    Loss: 2.943

2024-05-03 23:30:41,129 - ==> Best [Top1: 43.548   Top5: 59.389   Sparsity:0.00   Params: 1342920 on epoch: 20]
2024-05-03 23:30:41,129 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-03 23:30:41,260 - 

2024-05-03 23:30:41,261 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:31:02,273 - Epoch: [20][  200/  217]    Overall Loss 0.075666    Objective Loss 0.075666                                        LR 0.001000    Time 0.276301    
2024-05-03 23:31:07,300 - Epoch: [20][  217/  217]    Overall Loss 0.077347    Objective Loss 0.077347    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.277811    
2024-05-03 23:31:07,676 - --- validate (epoch=20)-----------
2024-05-03 23:31:07,676 - 1736 samples (32 per mini-batch)
2024-05-03 23:31:26,266 - Epoch: [20][   55/   55]    Loss 2.316773    Top1 55.069124    Top5 71.428571    
2024-05-03 23:31:26,586 - ==> Top1: 55.069    Top5: 71.429    Loss: 2.317

2024-05-03 23:31:26,594 - ==> Best [Top1: 55.069   Top5: 71.429   Sparsity:0.00   Params: 384080 on epoch: 20]
2024-05-03 23:31:26,594 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-03 23:31:26,648 - 

2024-05-03 23:31:26,649 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:31:39,686 - Epoch: [21][   55/   55]    Overall Loss 1.629305    Objective Loss 1.629305    Top1 49.044586    Top5 73.248408    LR 0.100000    Time 1.062074    
2024-05-03 23:31:40,080 - 

2024-05-03 23:31:40,082 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:31:58,067 - Epoch: [21][  100/  217]    Overall Loss 0.078025    Objective Loss 0.078025                                        LR 0.001000    Time 0.314079    
2024-05-03 23:32:21,106 - Epoch: [21][  200/  217]    Overall Loss 0.082858    Objective Loss 0.082858                                        LR 0.001000    Time 0.272184    
2024-05-03 23:32:25,363 - Epoch: [21][  217/  217]    Overall Loss 0.083435    Objective Loss 0.083435    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.270463    
2024-05-03 23:32:25,784 - 

2024-05-03 23:32:25,784 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:32:36,469 - Epoch: [22][   55/   55]    Overall Loss 1.572585    Objective Loss 1.572585    Top1 56.050955    Top5 80.254777    LR 0.100000    Time 1.025022    
2024-05-03 23:32:36,866 - 

2024-05-03 23:32:36,867 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:32:59,163 - Epoch: [22][  100/  217]    Overall Loss 0.057062    Objective Loss 0.057062                                        LR 0.001000    Time 0.333682    
2024-05-03 23:33:24,091 - Epoch: [22][  200/  217]    Overall Loss 0.056269    Objective Loss 0.056269                                        LR 0.001000    Time 0.291435    
2024-05-03 23:33:28,461 - Epoch: [22][  217/  217]    Overall Loss 0.055791    Objective Loss 0.055791    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.288734    
2024-05-03 23:33:28,894 - 

2024-05-03 23:33:28,895 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:33:31,570 - Epoch: [23][   55/   55]    Overall Loss 1.440906    Objective Loss 1.440906    Top1 60.509554    Top5 82.802548    LR 0.100000    Time 0.994424    
2024-05-03 23:33:31,992 - 

2024-05-03 23:33:31,993 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:33:59,165 - Epoch: [23][  100/  217]    Overall Loss 0.029922    Objective Loss 0.029922                                        LR 0.001000    Time 0.302615    
2024-05-03 23:34:26,777 - Epoch: [23][  200/  217]    Overall Loss 0.034560    Objective Loss 0.034560                                        LR 0.001000    Time 0.289316    
2024-05-03 23:34:30,180 - Epoch: [23][  217/  217]    Overall Loss 0.036140    Objective Loss 0.036140    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.282325    
2024-05-03 23:34:30,434 - 

2024-05-03 23:34:30,434 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:34:31,572 - Epoch: [24][   55/   55]    Overall Loss 1.356760    Objective Loss 1.356760    Top1 59.235669    Top5 85.350318    LR 0.100000    Time 1.083035    
2024-05-03 23:34:32,082 - 

2024-05-03 23:34:32,083 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:34:56,849 - Epoch: [24][  100/  217]    Overall Loss 0.034258    Objective Loss 0.034258                                        LR 0.001000    Time 0.264052    
2024-05-03 23:35:22,009 - Epoch: [24][  200/  217]    Overall Loss 0.037232    Objective Loss 0.037232                                        LR 0.001000    Time 0.257775    
2024-05-03 23:35:26,851 - Epoch: [24][  217/  217]    Overall Loss 0.041934    Objective Loss 0.041934    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.259882    
2024-05-03 23:35:27,329 - 

2024-05-03 23:35:27,330 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:35:37,424 - Epoch: [25][   55/   55]    Overall Loss 1.312327    Objective Loss 1.312327    Top1 64.968153    Top5 87.261146    LR 0.100000    Time 1.187829    
2024-05-03 23:35:37,778 - 

2024-05-03 23:35:37,779 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:35:57,804 - Epoch: [25][  100/  217]    Overall Loss 0.075227    Objective Loss 0.075227                                        LR 0.001000    Time 0.304657    
2024-05-03 23:36:19,886 - Epoch: [25][  200/  217]    Overall Loss 0.205627    Objective Loss 0.205627                                        LR 0.001000    Time 0.262685    
2024-05-03 23:36:23,809 - Epoch: [25][  217/  217]    Overall Loss 0.257159    Objective Loss 0.257159    Top1 63.934426    Top5 95.081967    LR 0.001000    Time 0.260177    
2024-05-03 23:36:24,234 - 

2024-05-03 23:36:24,234 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:36:33,750 - Epoch: [26][   55/   55]    Overall Loss 1.210175    Objective Loss 1.210175    Top1 64.331210    Top5 87.261146    LR 0.100000    Time 1.017448    
2024-05-03 23:36:34,484 - 

2024-05-03 23:36:34,485 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:36:53,737 - Epoch: [26][  100/  217]    Overall Loss 0.644207    Objective Loss 0.644207                                        LR 0.001000    Time 0.294923    
2024-05-03 23:37:21,152 - Epoch: [26][  200/  217]    Overall Loss 0.601085    Objective Loss 0.601085                                        LR 0.001000    Time 0.284489    
2024-05-03 23:37:23,922 - Epoch: [26][  217/  217]    Overall Loss 0.594498    Objective Loss 0.594498    Top1 78.688525    Top5 95.081967    LR 0.001000    Time 0.274957    
2024-05-03 23:37:24,164 - 

2024-05-03 23:37:24,164 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:37:36,883 - Epoch: [27][   55/   55]    Overall Loss 1.262958    Objective Loss 1.262958    Top1 64.968153    Top5 87.898089    LR 0.100000    Time 1.134333    
2024-05-03 23:37:37,887 - 

2024-05-03 23:37:37,888 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:37:52,572 - Epoch: [27][  100/  217]    Overall Loss 0.226988    Objective Loss 0.226988                                        LR 0.001000    Time 0.283995    
2024-05-03 23:38:19,507 - Epoch: [27][  200/  217]    Overall Loss 0.205832    Objective Loss 0.205832                                        LR 0.001000    Time 0.276629    
2024-05-03 23:38:23,263 - Epoch: [27][  217/  217]    Overall Loss 0.203803    Objective Loss 0.203803    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.272257    
2024-05-03 23:38:23,614 - 

2024-05-03 23:38:23,615 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:38:36,127 - Epoch: [28][   55/   55]    Overall Loss 1.249781    Objective Loss 1.249781    Top1 63.694268    Top5 85.987261    LR 0.100000    Time 1.058729    
2024-05-03 23:38:36,357 - 

2024-05-03 23:38:36,359 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:38:50,818 - Epoch: [28][  100/  217]    Overall Loss 0.067784    Objective Loss 0.067784                                        LR 0.001000    Time 0.271937    
2024-05-03 23:39:14,924 - Epoch: [28][  200/  217]    Overall Loss 0.068949    Objective Loss 0.068949                                        LR 0.001000    Time 0.256448    
2024-05-03 23:39:19,110 - Epoch: [28][  217/  217]    Overall Loss 0.070197    Objective Loss 0.070197    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.255644    
2024-05-03 23:39:19,371 - 

2024-05-03 23:39:19,372 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:39:30,326 - Epoch: [29][   55/   55]    Overall Loss 1.101736    Objective Loss 1.101736    Top1 71.337580    Top5 89.171975    LR 0.100000    Time 0.981040    
2024-05-03 23:39:30,558 - 

2024-05-03 23:39:30,559 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:39:47,795 - Epoch: [29][  100/  217]    Overall Loss 0.037806    Objective Loss 0.037806                                        LR 0.001000    Time 0.284135    
2024-05-03 23:40:10,372 - Epoch: [29][  200/  217]    Overall Loss 0.037362    Objective Loss 0.037362                                        LR 0.001000    Time 0.254913    
2024-05-03 23:40:15,615 - Epoch: [29][  217/  217]    Overall Loss 0.036820    Objective Loss 0.036820    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.259096    
2024-05-03 23:40:15,880 - 

2024-05-03 23:40:15,881 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:40:26,585 - Epoch: [30][   55/   55]    Overall Loss 1.033729    Objective Loss 1.033729    Top1 70.063694    Top5 91.082803    LR 0.100000    Time 1.018452    
2024-05-03 23:40:27,122 - --- validate (epoch=30)-----------
2024-05-03 23:40:27,123 - 1736 samples (128 per mini-batch)
2024-05-03 23:40:44,848 - Epoch: [30][  100/  217]    Overall Loss 0.019817    Objective Loss 0.019817                                        LR 0.001000    Time 0.289578    
2024-05-03 23:40:46,541 - Epoch: [30][   14/   14]    Loss 3.276367    Top1 44.066820    Top5 60.599078    
2024-05-03 23:40:46,922 - ==> Top1: 44.067    Top5: 60.599    Loss: 3.276

2024-05-03 23:40:46,937 - ==> Best [Top1: 44.067   Top5: 60.599   Sparsity:0.00   Params: 1342920 on epoch: 30]
2024-05-03 23:40:46,937 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-03 23:40:47,105 - 

2024-05-03 23:40:47,106 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:41:10,795 - Epoch: [30][  200/  217]    Overall Loss 0.020976    Objective Loss 0.020976                                        LR 0.001000    Time 0.274477    
2024-05-03 23:41:16,149 - Epoch: [30][  217/  217]    Overall Loss 0.020829    Objective Loss 0.020829    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.277639    
2024-05-03 23:41:16,553 - --- validate (epoch=30)-----------
2024-05-03 23:41:16,553 - 1736 samples (32 per mini-batch)
2024-05-03 23:41:34,490 - Epoch: [30][   55/   55]    Loss 2.420822    Top1 57.142857    Top5 72.235023    
2024-05-03 23:41:34,838 - ==> Top1: 57.143    Top5: 72.235    Loss: 2.421

2024-05-03 23:41:34,843 - ==> Best [Top1: 57.143   Top5: 72.235   Sparsity:0.00   Params: 384080 on epoch: 30]
2024-05-03 23:41:34,844 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-03 23:41:34,913 - 

2024-05-03 23:41:34,913 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:41:46,918 - Epoch: [31][   55/   55]    Overall Loss 1.056104    Objective Loss 1.056104    Top1 70.063694    Top5 90.445860    LR 0.100000    Time 1.087321    
2024-05-03 23:41:47,294 - 

2024-05-03 23:41:47,295 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:42:03,374 - Epoch: [31][  100/  217]    Overall Loss 0.013368    Objective Loss 0.013368                                        LR 0.001000    Time 0.284503    
2024-05-03 23:42:29,666 - Epoch: [31][  200/  217]    Overall Loss 0.013496    Objective Loss 0.013496                                        LR 0.001000    Time 0.273666    
2024-05-03 23:42:34,889 - Epoch: [31][  217/  217]    Overall Loss 0.013384    Objective Loss 0.013384    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.276283    
2024-05-03 23:42:35,297 - 

2024-05-03 23:42:35,298 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:42:42,732 - Epoch: [32][   55/   55]    Overall Loss 0.977564    Objective Loss 0.977564    Top1 77.070064    Top5 96.178344    LR 0.100000    Time 1.007762    
2024-05-03 23:42:43,105 - 

2024-05-03 23:42:43,106 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:43:02,986 - Epoch: [32][  100/  217]    Overall Loss 0.010521    Objective Loss 0.010521                                        LR 0.001000    Time 0.276784    
2024-05-03 23:43:29,365 - Epoch: [32][  200/  217]    Overall Loss 0.011013    Objective Loss 0.011013                                        LR 0.001000    Time 0.270241    
2024-05-03 23:43:34,474 - Epoch: [32][  217/  217]    Overall Loss 0.011090    Objective Loss 0.011090    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.272605    
2024-05-03 23:43:34,835 - 

2024-05-03 23:43:34,835 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:43:43,582 - Epoch: [33][   55/   55]    Overall Loss 0.849543    Objective Loss 0.849543    Top1 70.700637    Top5 89.171975    LR 0.100000    Time 1.099382    
2024-05-03 23:43:44,454 - 

2024-05-03 23:43:44,455 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:44:05,477 - Epoch: [33][  100/  217]    Overall Loss 0.010561    Objective Loss 0.010561                                        LR 0.001000    Time 0.306326    
2024-05-03 23:44:29,483 - Epoch: [33][  200/  217]    Overall Loss 0.011581    Objective Loss 0.011581                                        LR 0.001000    Time 0.273152    
2024-05-03 23:44:33,302 - Epoch: [33][  217/  217]    Overall Loss 0.011674    Objective Loss 0.011674    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.269340    
2024-05-03 23:44:33,610 - 

2024-05-03 23:44:33,610 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:44:44,170 - Epoch: [34][   55/   55]    Overall Loss 0.963616    Objective Loss 0.963616    Top1 70.063694    Top5 92.993631    LR 0.100000    Time 1.085554    
2024-05-03 23:44:44,746 - 

2024-05-03 23:44:44,746 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:45:04,547 - Epoch: [34][  100/  217]    Overall Loss 0.008341    Objective Loss 0.008341                                        LR 0.001000    Time 0.309265    
2024-05-03 23:45:30,732 - Epoch: [34][  200/  217]    Overall Loss 0.009419    Objective Loss 0.009419                                        LR 0.001000    Time 0.285513    
2024-05-03 23:45:35,044 - Epoch: [34][  217/  217]    Overall Loss 0.009506    Objective Loss 0.009506    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.283005    
2024-05-03 23:45:35,254 - 

2024-05-03 23:45:35,254 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:45:48,367 - Epoch: [35][   55/   55]    Overall Loss 0.817921    Objective Loss 0.817921    Top1 78.980892    Top5 95.541401    LR 0.100000    Time 1.156524    
2024-05-03 23:45:49,018 - 

2024-05-03 23:45:49,019 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:46:01,146 - Epoch: [35][  100/  217]    Overall Loss 0.008038    Objective Loss 0.008038                                        LR 0.001000    Time 0.258824    
2024-05-03 23:46:28,174 - Epoch: [35][  200/  217]    Overall Loss 0.007770    Objective Loss 0.007770                                        LR 0.001000    Time 0.264505    
2024-05-03 23:46:31,368 - Epoch: [35][  217/  217]    Overall Loss 0.007575    Objective Loss 0.007575    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.258491    
2024-05-03 23:46:31,639 - 

2024-05-03 23:46:31,640 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:46:46,203 - Epoch: [36][   55/   55]    Overall Loss 0.789330    Objective Loss 0.789330    Top1 74.522293    Top5 92.993631    LR 0.100000    Time 1.039553    
2024-05-03 23:46:46,586 - 

2024-05-03 23:46:46,586 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:46:59,786 - Epoch: [36][  100/  217]    Overall Loss 0.006584    Objective Loss 0.006584                                        LR 0.001000    Time 0.281354    
2024-05-03 23:47:28,176 - Epoch: [36][  200/  217]    Overall Loss 0.006001    Objective Loss 0.006001                                        LR 0.001000    Time 0.282578    
2024-05-03 23:47:33,280 - Epoch: [36][  217/  217]    Overall Loss 0.005890    Objective Loss 0.005890    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.283949    
2024-05-03 23:47:33,682 - 

2024-05-03 23:47:33,682 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:47:40,704 - Epoch: [37][   55/   55]    Overall Loss 0.805464    Objective Loss 0.805464    Top1 77.070064    Top5 96.815287    LR 0.100000    Time 0.983771    
2024-05-03 23:47:41,055 - 

2024-05-03 23:47:41,057 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:48:04,765 - Epoch: [37][  100/  217]    Overall Loss 0.004433    Objective Loss 0.004433                                        LR 0.001000    Time 0.310730    
2024-05-03 23:48:31,066 - Epoch: [37][  200/  217]    Overall Loss 0.004910    Objective Loss 0.004910                                        LR 0.001000    Time 0.286825    
2024-05-03 23:48:35,763 - Epoch: [37][  217/  217]    Overall Loss 0.005132    Objective Loss 0.005132    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.285994    
2024-05-03 23:48:35,971 - 

2024-05-03 23:48:35,971 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:48:38,432 - Epoch: [38][   55/   55]    Overall Loss 0.758955    Objective Loss 0.758955    Top1 66.242038    Top5 95.541401    LR 0.100000    Time 1.043018    
2024-05-03 23:48:38,758 - 

2024-05-03 23:48:38,759 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:49:09,469 - Epoch: [38][  100/  217]    Overall Loss 0.006402    Objective Loss 0.006402                                        LR 0.001000    Time 0.334873    
2024-05-03 23:49:33,605 - Epoch: [38][  200/  217]    Overall Loss 0.005887    Objective Loss 0.005887                                        LR 0.001000    Time 0.288067    
2024-05-03 23:49:34,135 - Epoch: [39][   55/   55]    Overall Loss 0.639650    Objective Loss 0.639650    Top1 78.980892    Top5 94.904459    LR 0.100000    Time 1.006675    
2024-05-03 23:49:34,395 - 

2024-05-03 23:49:34,396 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:49:37,037 - Epoch: [38][  217/  217]    Overall Loss 0.005757    Objective Loss 0.005757    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.281307    
2024-05-03 23:49:37,558 - 

2024-05-03 23:49:37,558 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:50:06,220 - Epoch: [39][  100/  217]    Overall Loss 0.003647    Objective Loss 0.003647                                        LR 0.001000    Time 0.286519    
2024-05-03 23:50:31,806 - Epoch: [40][   55/   55]    Overall Loss 0.644448    Objective Loss 0.644448    Top1 80.254777    Top5 96.178344    LR 0.100000    Time 1.043638    
2024-05-03 23:50:32,323 - --- validate (epoch=40)-----------
2024-05-03 23:50:32,324 - 1736 samples (128 per mini-batch)
2024-05-03 23:50:33,686 - Epoch: [39][  200/  217]    Overall Loss 0.004473    Objective Loss 0.004473                                        LR 0.001000    Time 0.280545    
2024-05-03 23:50:38,431 - Epoch: [39][  217/  217]    Overall Loss 0.004709    Objective Loss 0.004709    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.280420    
2024-05-03 23:50:38,885 - 

2024-05-03 23:50:38,885 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:50:53,597 - Epoch: [40][   14/   14]    Loss 3.493916    Top1 43.836406    Top5 62.327189    
2024-05-03 23:50:53,965 - ==> Top1: 43.836    Top5: 62.327    Loss: 3.494

2024-05-03 23:50:53,980 - ==> Best [Top1: 44.067   Top5: 60.599   Sparsity:0.00   Params: 1342920 on epoch: 30]
2024-05-03 23:50:53,981 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-03 23:50:54,088 - 

2024-05-03 23:50:54,089 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:51:07,786 - Epoch: [40][  100/  217]    Overall Loss 0.003688    Objective Loss 0.003688                                        LR 0.001000    Time 0.288906    
2024-05-03 23:51:32,342 - Epoch: [40][  200/  217]    Overall Loss 0.004522    Objective Loss 0.004522                                        LR 0.001000    Time 0.267183    
2024-05-03 23:51:35,839 - Epoch: [40][  217/  217]    Overall Loss 0.004494    Objective Loss 0.004494    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.262355    
2024-05-03 23:51:36,240 - --- validate (epoch=40)-----------
2024-05-03 23:51:36,240 - 1736 samples (32 per mini-batch)
2024-05-03 23:51:52,722 - Epoch: [41][   55/   55]    Overall Loss 0.611850    Objective Loss 0.611850    Top1 78.343949    Top5 95.541401    LR 0.100000    Time 1.065851    
2024-05-03 23:51:53,232 - 

2024-05-03 23:51:53,233 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:51:53,766 - Epoch: [40][   55/   55]    Loss 2.537270    Top1 56.854839    Top5 72.523041    
2024-05-03 23:51:54,070 - ==> Top1: 56.855    Top5: 72.523    Loss: 2.537

2024-05-03 23:51:54,076 - ==> Best [Top1: 57.143   Top5: 72.235   Sparsity:0.00   Params: 384080 on epoch: 30]
2024-05-03 23:51:54,076 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-03 23:51:54,126 - 

2024-05-03 23:51:54,127 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:52:22,127 - Epoch: [41][  100/  217]    Overall Loss 0.847100    Objective Loss 0.847100                                        LR 0.001000    Time 0.279899    
2024-05-03 23:52:45,253 - Epoch: [42][   55/   55]    Overall Loss 0.713040    Objective Loss 0.713040    Top1 71.974522    Top5 94.267516    LR 0.100000    Time 0.945625    
2024-05-03 23:52:45,917 - 

2024-05-03 23:52:45,918 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:52:48,603 - Epoch: [41][  200/  217]    Overall Loss 1.068775    Objective Loss 1.068775                                        LR 0.001000    Time 0.272278    
2024-05-03 23:52:53,084 - Epoch: [41][  217/  217]    Overall Loss 1.064366    Objective Loss 1.064366    Top1 70.491803    Top5 93.442623    LR 0.001000    Time 0.271586    
2024-05-03 23:52:53,348 - 

2024-05-03 23:52:53,349 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:53:23,172 - Epoch: [42][  100/  217]    Overall Loss 0.474183    Objective Loss 0.474183                                        LR 0.001000    Time 0.298137    
2024-05-03 23:53:44,990 - Epoch: [43][   55/   55]    Overall Loss 0.668580    Objective Loss 0.668580    Top1 80.891720    Top5 94.904459    LR 0.100000    Time 1.073810    
2024-05-03 23:53:45,588 - 

2024-05-03 23:53:45,589 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:53:49,392 - Epoch: [42][  200/  217]    Overall Loss 0.450877    Objective Loss 0.450877                                        LR 0.001000    Time 0.280112    
2024-05-03 23:53:52,861 - Epoch: [42][  217/  217]    Overall Loss 0.441228    Objective Loss 0.441228    Top1 91.803279    Top5 100.000000    LR 0.001000    Time 0.274146    
2024-05-03 23:53:53,150 - 

2024-05-03 23:53:53,151 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:54:23,180 - Epoch: [43][  100/  217]    Overall Loss 0.148971    Objective Loss 0.148971                                        LR 0.001000    Time 0.300158    
2024-05-03 23:54:41,682 - Epoch: [44][   55/   55]    Overall Loss 0.505054    Objective Loss 0.505054    Top1 81.528662    Top5 96.815287    LR 0.100000    Time 1.019668    
2024-05-03 23:54:42,253 - 

2024-05-03 23:54:42,255 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:54:50,727 - Epoch: [43][  200/  217]    Overall Loss 0.133251    Objective Loss 0.133251                                        LR 0.001000    Time 0.287761    
2024-05-03 23:54:54,183 - Epoch: [43][  217/  217]    Overall Loss 0.131532    Objective Loss 0.131532    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.281137    
2024-05-03 23:54:54,508 - 

2024-05-03 23:54:54,509 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:55:24,278 - Epoch: [44][  100/  217]    Overall Loss 0.051650    Objective Loss 0.051650                                        LR 0.001000    Time 0.297594    
2024-05-03 23:55:37,296 - Epoch: [45][   55/   55]    Overall Loss 0.496300    Objective Loss 0.496300    Top1 85.987261    Top5 98.089172    LR 0.100000    Time 1.000508    
2024-05-03 23:55:37,602 - 

2024-05-03 23:55:37,604 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:55:47,758 - Epoch: [44][  200/  217]    Overall Loss 0.050713    Objective Loss 0.050713                                        LR 0.001000    Time 0.266145    
2024-05-03 23:55:52,083 - Epoch: [44][  217/  217]    Overall Loss 0.051034    Objective Loss 0.051034    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.265220    
2024-05-03 23:55:52,426 - 

2024-05-03 23:55:52,427 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:56:18,945 - Epoch: [45][  100/  217]    Overall Loss 0.030305    Objective Loss 0.030305                                        LR 0.001000    Time 0.265080    
2024-05-03 23:56:35,240 - Epoch: [46][   55/   55]    Overall Loss 0.389326    Objective Loss 0.389326    Top1 85.987261    Top5 97.452229    LR 0.100000    Time 1.047741    
2024-05-03 23:56:35,565 - 

2024-05-03 23:56:35,566 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:56:45,604 - Epoch: [45][  200/  217]    Overall Loss 0.027951    Objective Loss 0.027951                                        LR 0.001000    Time 0.265793    
2024-05-03 23:56:49,052 - Epoch: [45][  217/  217]    Overall Loss 0.028067    Objective Loss 0.028067    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.260851    
2024-05-03 23:56:49,370 - 

2024-05-03 23:56:49,371 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:57:17,357 - Epoch: [46][  100/  217]    Overall Loss 0.017163    Objective Loss 0.017163                                        LR 0.001000    Time 0.279756    
2024-05-03 23:57:35,265 - Epoch: [47][   55/   55]    Overall Loss 0.376679    Objective Loss 0.376679    Top1 87.898089    Top5 98.089172    LR 0.100000    Time 1.085246    
2024-05-03 23:57:35,650 - 

2024-05-03 23:57:35,652 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:57:43,250 - Epoch: [46][  200/  217]    Overall Loss 0.018164    Objective Loss 0.018164                                        LR 0.001000    Time 0.269297    
2024-05-03 23:57:47,349 - Epoch: [46][  217/  217]    Overall Loss 0.018173    Objective Loss 0.018173    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.267081    
2024-05-03 23:57:47,678 - 

2024-05-03 23:57:47,679 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:58:16,783 - Epoch: [47][  100/  217]    Overall Loss 0.012514    Objective Loss 0.012514                                        LR 0.001000    Time 0.290943    
2024-05-03 23:58:33,786 - Epoch: [48][   55/   55]    Overall Loss 0.391639    Objective Loss 0.391639    Top1 87.898089    Top5 100.000000    LR 0.100000    Time 1.056799    
2024-05-03 23:58:34,676 - 

2024-05-03 23:58:34,677 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:58:42,756 - Epoch: [47][  200/  217]    Overall Loss 0.013485    Objective Loss 0.013485                                        LR 0.001000    Time 0.275292    
2024-05-03 23:58:45,860 - Epoch: [47][  217/  217]    Overall Loss 0.013545    Objective Loss 0.013545    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.268019    
2024-05-03 23:58:46,151 - 

2024-05-03 23:58:46,152 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:59:11,986 - Epoch: [48][  100/  217]    Overall Loss 0.011167    Objective Loss 0.011167                                        LR 0.001000    Time 0.258242    
2024-05-03 23:59:34,477 - Epoch: [49][   55/   55]    Overall Loss 0.411261    Objective Loss 0.411261    Top1 86.624204    Top5 98.726115    LR 0.100000    Time 1.087072    
2024-05-03 23:59:34,798 - 

2024-05-03 23:59:34,798 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-03 23:59:38,528 - Epoch: [48][  200/  217]    Overall Loss 0.011186    Objective Loss 0.011186                                        LR 0.001000    Time 0.261778    
2024-05-03 23:59:42,552 - Epoch: [48][  217/  217]    Overall Loss 0.011207    Objective Loss 0.011207    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.259805    
2024-05-03 23:59:42,823 - 

2024-05-03 23:59:42,824 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:00:12,645 - Epoch: [49][  100/  217]    Overall Loss 0.008017    Objective Loss 0.008017                                        LR 0.001000    Time 0.298106    
2024-05-04 00:00:35,136 - Epoch: [49][  200/  217]    Overall Loss 0.010282    Objective Loss 0.010282                                        LR 0.001000    Time 0.261466    
2024-05-04 00:00:37,342 - Epoch: [50][   55/   55]    Overall Loss 0.449756    Objective Loss 0.449756    Top1 83.439490    Top5 97.452229    LR 0.100000    Time 1.137008    
2024-05-04 00:00:37,767 - --- validate (epoch=50)-----------
2024-05-04 00:00:37,767 - 1736 samples (128 per mini-batch)
2024-05-04 00:00:39,254 - Epoch: [49][  217/  217]    Overall Loss 0.010237    Objective Loss 0.010237    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.259951    
2024-05-04 00:00:39,564 - 

2024-05-04 00:00:39,565 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:00:56,648 - Epoch: [50][   14/   14]    Loss 3.818315    Top1 46.370968    Top5 61.923963    
2024-05-04 00:00:56,995 - ==> Top1: 46.371    Top5: 61.924    Loss: 3.818

2024-05-04 00:00:57,013 - ==> Best [Top1: 46.371   Top5: 61.924   Sparsity:0.00   Params: 1342920 on epoch: 50]
2024-05-04 00:00:57,014 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 00:00:57,160 - 

2024-05-04 00:00:57,161 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:01:08,111 - Epoch: [50][  100/  217]    Overall Loss 0.009507    Objective Loss 0.009507                                        LR 0.001000    Time 0.285336    
2024-05-04 00:01:32,626 - Epoch: [50][  200/  217]    Overall Loss 0.009007    Objective Loss 0.009007                                        LR 0.001000    Time 0.265203    
2024-05-04 00:01:36,940 - Epoch: [50][  217/  217]    Overall Loss 0.009331    Objective Loss 0.009331    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.264300    
2024-05-04 00:01:37,156 - --- validate (epoch=50)-----------
2024-05-04 00:01:37,157 - 1736 samples (32 per mini-batch)
2024-05-04 00:01:50,902 - Epoch: [51][   55/   55]    Overall Loss 0.415031    Objective Loss 0.415031    Top1 88.535032    Top5 98.089172    LR 0.100000    Time 0.976933    
2024-05-04 00:01:51,214 - 

2024-05-04 00:01:51,216 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:01:51,463 - Epoch: [50][   55/   55]    Loss 2.424507    Top1 58.006912    Top5 74.884793    
2024-05-04 00:01:51,791 - ==> Top1: 58.007    Top5: 74.885    Loss: 2.425

2024-05-04 00:01:51,798 - ==> Best [Top1: 58.007   Top5: 74.885   Sparsity:0.00   Params: 384080 on epoch: 50]
2024-05-04 00:01:51,798 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 00:01:51,868 - 

2024-05-04 00:01:51,869 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:02:20,566 - Epoch: [51][  100/  217]    Overall Loss 0.009737    Objective Loss 0.009737                                        LR 0.001000    Time 0.286881    
2024-05-04 00:02:44,528 - Epoch: [51][  200/  217]    Overall Loss 0.010074    Objective Loss 0.010074                                        LR 0.001000    Time 0.263208    
2024-05-04 00:02:47,517 - Epoch: [51][  217/  217]    Overall Loss 0.009889    Objective Loss 0.009889    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.256354    
2024-05-04 00:02:47,821 - 

2024-05-04 00:02:47,821 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:02:51,937 - Epoch: [52][   55/   55]    Overall Loss 0.310338    Objective Loss 0.310338    Top1 91.082803    Top5 98.089172    LR 0.100000    Time 1.103780    
2024-05-04 00:02:52,423 - 

2024-05-04 00:02:52,424 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:03:15,777 - Epoch: [52][  100/  217]    Overall Loss 0.007709    Objective Loss 0.007709                                        LR 0.001000    Time 0.279465    
2024-05-04 00:03:42,082 - Epoch: [52][  200/  217]    Overall Loss 0.006604    Objective Loss 0.006604                                        LR 0.001000    Time 0.271210    
2024-05-04 00:03:46,185 - Epoch: [52][  217/  217]    Overall Loss 0.007385    Objective Loss 0.007385    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.268866    
2024-05-04 00:03:46,573 - 

2024-05-04 00:03:46,573 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:03:52,608 - Epoch: [53][   55/   55]    Overall Loss 0.263243    Objective Loss 0.263243    Top1 93.630573    Top5 99.363057    LR 0.100000    Time 1.094045    
2024-05-04 00:03:53,113 - 

2024-05-04 00:03:53,113 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:04:14,238 - Epoch: [53][  100/  217]    Overall Loss 0.006352    Objective Loss 0.006352                                        LR 0.001000    Time 0.276537    
2024-05-04 00:04:40,521 - Epoch: [53][  200/  217]    Overall Loss 0.006113    Objective Loss 0.006113                                        LR 0.001000    Time 0.269638    
2024-05-04 00:04:43,754 - Epoch: [53][  217/  217]    Overall Loss 0.006519    Objective Loss 0.006519    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.263406    
2024-05-04 00:04:44,605 - 

2024-05-04 00:04:44,606 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:04:53,135 - Epoch: [54][   55/   55]    Overall Loss 0.222200    Objective Loss 0.222200    Top1 87.898089    Top5 98.089172    LR 0.100000    Time 1.091127    
2024-05-04 00:04:53,737 - 

2024-05-04 00:04:53,738 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:05:12,438 - Epoch: [54][  100/  217]    Overall Loss 0.007171    Objective Loss 0.007171                                        LR 0.001000    Time 0.278225    
2024-05-04 00:05:38,913 - Epoch: [54][  200/  217]    Overall Loss 0.006694    Objective Loss 0.006694                                        LR 0.001000    Time 0.271446    
2024-05-04 00:05:42,774 - Epoch: [54][  217/  217]    Overall Loss 0.006521    Objective Loss 0.006521    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.267964    
2024-05-04 00:05:43,087 - 

2024-05-04 00:05:43,088 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:06:00,956 - Epoch: [55][   55/   55]    Overall Loss 0.398357    Objective Loss 0.398357    Top1 85.987261    Top5 97.452229    LR 0.100000    Time 1.222002    
2024-05-04 00:06:01,279 - 

2024-05-04 00:06:01,280 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:06:07,374 - Epoch: [55][  100/  217]    Overall Loss 0.003804    Objective Loss 0.003804                                        LR 0.001000    Time 0.242774    
2024-05-04 00:06:32,164 - Epoch: [55][  200/  217]    Overall Loss 0.007951    Objective Loss 0.007951                                        LR 0.001000    Time 0.245289    
2024-05-04 00:06:36,459 - Epoch: [55][  217/  217]    Overall Loss 0.009921    Objective Loss 0.009921    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.245855    
2024-05-04 00:06:36,740 - 

2024-05-04 00:06:36,741 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:06:57,944 - Epoch: [56][   55/   55]    Overall Loss 0.455277    Objective Loss 0.455277    Top1 87.898089    Top5 98.726115    LR 0.100000    Time 1.030080    
2024-05-04 00:06:58,387 - 

2024-05-04 00:06:58,387 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:07:06,767 - Epoch: [56][  100/  217]    Overall Loss 0.596044    Objective Loss 0.596044                                        LR 0.001000    Time 0.300143    
2024-05-04 00:07:32,976 - Epoch: [56][  200/  217]    Overall Loss 0.725622    Objective Loss 0.725622                                        LR 0.001000    Time 0.281074    
2024-05-04 00:07:35,873 - Epoch: [56][  217/  217]    Overall Loss 0.719501    Objective Loss 0.719501    Top1 77.049180    Top5 96.721311    LR 0.001000    Time 0.272391    
2024-05-04 00:07:36,176 - 

2024-05-04 00:07:36,176 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:07:56,649 - Epoch: [57][   55/   55]    Overall Loss 0.391634    Objective Loss 0.391634    Top1 82.802548    Top5 97.452229    LR 0.100000    Time 1.059147    
2024-05-04 00:07:57,132 - 

2024-05-04 00:07:57,133 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:08:04,105 - Epoch: [57][  100/  217]    Overall Loss 0.294459    Objective Loss 0.294459                                        LR 0.001000    Time 0.279200    
2024-05-04 00:08:30,182 - Epoch: [57][  200/  217]    Overall Loss 0.258388    Objective Loss 0.258388                                        LR 0.001000    Time 0.269941    
2024-05-04 00:08:32,765 - Epoch: [57][  217/  217]    Overall Loss 0.254425    Objective Loss 0.254425    Top1 93.442623    Top5 100.000000    LR 0.001000    Time 0.260689    
2024-05-04 00:08:33,059 - 

2024-05-04 00:08:33,059 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:08:58,227 - Epoch: [58][   55/   55]    Overall Loss 0.436702    Objective Loss 0.436702    Top1 85.350318    Top5 100.000000    LR 0.100000    Time 1.110626    
2024-05-04 00:08:58,879 - 

2024-05-04 00:08:58,880 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:09:00,077 - Epoch: [58][  100/  217]    Overall Loss 0.067830    Objective Loss 0.067830                                        LR 0.001000    Time 0.270081    
2024-05-04 00:09:25,075 - Epoch: [58][  200/  217]    Overall Loss 0.059878    Objective Loss 0.059878                                        LR 0.001000    Time 0.259984    
2024-05-04 00:09:29,673 - Epoch: [58][  217/  217]    Overall Loss 0.058386    Objective Loss 0.058386    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.260795    
2024-05-04 00:09:30,038 - 

2024-05-04 00:09:30,040 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:09:59,407 - Epoch: [59][  100/  217]    Overall Loss 0.022866    Objective Loss 0.022866                                        LR 0.001000    Time 0.293539    
2024-05-04 00:10:00,343 - Epoch: [59][   55/   55]    Overall Loss 0.377085    Objective Loss 0.377085    Top1 86.624204    Top5 98.089172    LR 0.100000    Time 1.117317    
2024-05-04 00:10:00,724 - 

2024-05-04 00:10:00,724 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:10:23,735 - Epoch: [59][  200/  217]    Overall Loss 0.022483    Objective Loss 0.022483                                        LR 0.001000    Time 0.268361    
2024-05-04 00:10:27,124 - Epoch: [59][  217/  217]    Overall Loss 0.022265    Objective Loss 0.022265    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.262946    
2024-05-04 00:10:27,548 - 

2024-05-04 00:10:27,548 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:10:56,693 - Epoch: [60][  100/  217]    Overall Loss 0.014795    Objective Loss 0.014795                                        LR 0.001000    Time 0.291346    
2024-05-04 00:10:57,173 - Epoch: [60][   55/   55]    Overall Loss 0.307608    Objective Loss 0.307608    Top1 88.535032    Top5 98.089172    LR 0.100000    Time 1.026159    
2024-05-04 00:10:57,623 - --- validate (epoch=60)-----------
2024-05-04 00:10:57,624 - 1736 samples (128 per mini-batch)
2024-05-04 00:11:13,251 - Epoch: [60][   14/   14]    Loss 3.887219    Top1 47.926267    Top5 63.133641    
2024-05-04 00:11:13,526 - ==> Top1: 47.926    Top5: 63.134    Loss: 3.887

2024-05-04 00:11:13,542 - ==> Best [Top1: 47.926   Top5: 63.134   Sparsity:0.00   Params: 1342920 on epoch: 60]
2024-05-04 00:11:13,542 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 00:11:13,680 - 

2024-05-04 00:11:13,680 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:11:21,703 - Epoch: [60][  200/  217]    Overall Loss 0.013839    Objective Loss 0.013839                                        LR 0.001000    Time 0.270673    
2024-05-04 00:11:25,816 - Epoch: [60][  217/  217]    Overall Loss 0.013714    Objective Loss 0.013714    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.268413    
2024-05-04 00:11:26,091 - --- validate (epoch=60)-----------
2024-05-04 00:11:26,092 - 1736 samples (32 per mini-batch)
2024-05-04 00:11:42,516 - Epoch: [60][   55/   55]    Loss 2.474238    Top1 58.755760    Top5 73.559908    
2024-05-04 00:11:42,820 - ==> Top1: 58.756    Top5: 73.560    Loss: 2.474

2024-05-04 00:11:42,835 - ==> Best [Top1: 58.756   Top5: 73.560   Sparsity:0.00   Params: 384080 on epoch: 60]
2024-05-04 00:11:42,837 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 00:11:42,967 - 

2024-05-04 00:11:42,969 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:12:10,871 - Epoch: [61][  100/  217]    Overall Loss 0.010465    Objective Loss 0.010465                                        LR 0.001000    Time 0.278914    
2024-05-04 00:12:12,613 - Epoch: [61][   55/   55]    Overall Loss 0.271224    Objective Loss 0.271224    Top1 92.356688    Top5 100.000000    LR 0.100000    Time 1.071332    
2024-05-04 00:12:13,033 - 

2024-05-04 00:12:13,035 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:12:38,871 - Epoch: [61][  200/  217]    Overall Loss 0.010546    Objective Loss 0.010546                                        LR 0.001000    Time 0.279409    
2024-05-04 00:12:43,509 - Epoch: [61][  217/  217]    Overall Loss 0.010730    Objective Loss 0.010730    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.278886    
2024-05-04 00:12:43,756 - 

2024-05-04 00:12:43,757 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:13:12,457 - Epoch: [62][  100/  217]    Overall Loss 0.007904    Objective Loss 0.007904                                        LR 0.001000    Time 0.286909    
2024-05-04 00:13:15,200 - Epoch: [62][   55/   55]    Overall Loss 0.192491    Objective Loss 0.192491    Top1 94.904459    Top5 100.000000    LR 0.100000    Time 1.130057    
2024-05-04 00:13:15,540 - 

2024-05-04 00:13:15,541 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:13:36,491 - Epoch: [62][  200/  217]    Overall Loss 0.007633    Objective Loss 0.007633                                        LR 0.001000    Time 0.263577    
2024-05-04 00:13:40,850 - Epoch: [62][  217/  217]    Overall Loss 0.007820    Objective Loss 0.007820    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.263005    
2024-05-04 00:13:41,216 - 

2024-05-04 00:13:41,216 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:14:11,257 - Epoch: [63][  100/  217]    Overall Loss 0.009945    Objective Loss 0.009945                                        LR 0.001000    Time 0.300311    
2024-05-04 00:14:15,466 - Epoch: [63][   55/   55]    Overall Loss 0.166598    Objective Loss 0.166598    Top1 94.904459    Top5 99.363057    LR 0.100000    Time 1.089356    
2024-05-04 00:14:15,830 - 

2024-05-04 00:14:15,832 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:14:35,897 - Epoch: [63][  200/  217]    Overall Loss 0.010192    Objective Loss 0.010192                                        LR 0.001000    Time 0.273305    
2024-05-04 00:14:41,185 - Epoch: [63][  217/  217]    Overall Loss 0.010910    Objective Loss 0.010910    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.276254    
2024-05-04 00:14:41,483 - 

2024-05-04 00:14:41,484 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:15:11,132 - Epoch: [64][  100/  217]    Overall Loss 0.011024    Objective Loss 0.011024                                        LR 0.001000    Time 0.296383    
2024-05-04 00:15:17,580 - Epoch: [64][   55/   55]    Overall Loss 0.114076    Objective Loss 0.114076    Top1 95.541401    Top5 100.000000    LR 0.100000    Time 1.122506    
2024-05-04 00:15:17,912 - 

2024-05-04 00:15:17,912 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:15:34,675 - Epoch: [64][  200/  217]    Overall Loss 0.011161    Objective Loss 0.011161                                        LR 0.001000    Time 0.265849    
2024-05-04 00:15:40,041 - Epoch: [64][  217/  217]    Overall Loss 0.011066    Objective Loss 0.011066    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.269741    
2024-05-04 00:15:40,337 - 

2024-05-04 00:15:40,338 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:16:11,250 - Epoch: [65][  100/  217]    Overall Loss 0.009261    Objective Loss 0.009261                                        LR 0.001000    Time 0.309035    
2024-05-04 00:16:18,276 - Epoch: [65][   55/   55]    Overall Loss 0.111170    Objective Loss 0.111170    Top1 96.815287    Top5 100.000000    LR 0.100000    Time 1.097342    
2024-05-04 00:16:18,604 - 

2024-05-04 00:16:18,605 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:16:35,401 - Epoch: [65][  200/  217]    Overall Loss 0.008606    Objective Loss 0.008606                                        LR 0.001000    Time 0.275228    
2024-05-04 00:16:38,638 - Epoch: [65][  217/  217]    Overall Loss 0.008689    Objective Loss 0.008689    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.268572    
2024-05-04 00:16:38,901 - 

2024-05-04 00:16:38,902 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:17:12,483 - Epoch: [66][  100/  217]    Overall Loss 0.006295    Objective Loss 0.006295                                        LR 0.001000    Time 0.335701    
2024-05-04 00:17:18,033 - Epoch: [66][   55/   55]    Overall Loss 0.084947    Objective Loss 0.084947    Top1 96.815287    Top5 100.000000    LR 0.100000    Time 1.080339    
2024-05-04 00:17:18,724 - 

2024-05-04 00:17:18,725 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:17:38,896 - Epoch: [66][  200/  217]    Overall Loss 0.005836    Objective Loss 0.005836                                        LR 0.001000    Time 0.299870    
2024-05-04 00:17:43,590 - Epoch: [66][  217/  217]    Overall Loss 0.005748    Objective Loss 0.005748    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.297997    
2024-05-04 00:17:44,566 - 

2024-05-04 00:17:44,566 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:18:13,401 - Epoch: [67][  100/  217]    Overall Loss 0.004625    Objective Loss 0.004625                                        LR 0.001000    Time 0.288247    
2024-05-04 00:18:21,118 - Epoch: [67][   55/   55]    Overall Loss 0.065883    Objective Loss 0.065883    Top1 94.904459    Top5 100.000000    LR 0.100000    Time 1.134231    
2024-05-04 00:18:22,011 - 

2024-05-04 00:18:22,012 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:18:41,805 - Epoch: [67][  200/  217]    Overall Loss 0.004143    Objective Loss 0.004143                                        LR 0.001000    Time 0.286102    
2024-05-04 00:18:46,740 - Epoch: [67][  217/  217]    Overall Loss 0.004026    Objective Loss 0.004026    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.286422    
2024-05-04 00:18:47,084 - 

2024-05-04 00:18:47,085 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:19:16,213 - Epoch: [68][  100/  217]    Overall Loss 0.003265    Objective Loss 0.003265                                        LR 0.001000    Time 0.291185    
2024-05-04 00:19:17,835 - Epoch: [68][   55/   55]    Overall Loss 0.152434    Objective Loss 0.152434    Top1 96.178344    Top5 100.000000    LR 0.100000    Time 1.014806    
2024-05-04 00:19:18,379 - 

2024-05-04 00:19:18,380 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:19:42,335 - Epoch: [68][  200/  217]    Overall Loss 0.003499    Objective Loss 0.003499                                        LR 0.001000    Time 0.276162    
2024-05-04 00:19:45,237 - Epoch: [68][  217/  217]    Overall Loss 0.003599    Objective Loss 0.003599    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.267889    
2024-05-04 00:19:45,735 - 

2024-05-04 00:19:45,736 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:20:15,424 - Epoch: [69][   55/   55]    Overall Loss 0.082460    Objective Loss 0.082460    Top1 94.904459    Top5 100.000000    LR 0.100000    Time 1.036957    
2024-05-04 00:20:15,610 - Epoch: [69][  100/  217]    Overall Loss 0.004127    Objective Loss 0.004127                                        LR 0.001000    Time 0.298643    
2024-05-04 00:20:15,751 - 

2024-05-04 00:20:15,752 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:20:42,481 - Epoch: [69][  200/  217]    Overall Loss 0.004081    Objective Loss 0.004081                                        LR 0.001000    Time 0.283625    
2024-05-04 00:20:47,378 - Epoch: [69][  217/  217]    Overall Loss 0.004011    Objective Loss 0.004011    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.283964    
2024-05-04 00:20:47,912 - 

2024-05-04 00:20:47,912 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:21:12,336 - Epoch: [70][   55/   55]    Overall Loss 0.073672    Objective Loss 0.073672    Top1 95.541401    Top5 99.363057    LR 0.100000    Time 1.028608    
2024-05-04 00:21:12,596 - --- validate (epoch=70)-----------
2024-05-04 00:21:12,598 - 1736 samples (128 per mini-batch)
2024-05-04 00:21:17,314 - Epoch: [70][  100/  217]    Overall Loss 0.003596    Objective Loss 0.003596                                        LR 0.001000    Time 0.293921    
2024-05-04 00:21:29,881 - Epoch: [70][   14/   14]    Loss 4.223508    Top1 47.638249    Top5 63.824885    
2024-05-04 00:21:30,180 - ==> Top1: 47.638    Top5: 63.825    Loss: 4.224

2024-05-04 00:21:30,206 - ==> Best [Top1: 47.926   Top5: 63.134   Sparsity:0.00   Params: 1342920 on epoch: 60]
2024-05-04 00:21:30,207 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 00:21:30,318 - 

2024-05-04 00:21:30,319 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:21:43,121 - Epoch: [70][  200/  217]    Overall Loss 0.004622    Objective Loss 0.004622                                        LR 0.001000    Time 0.275900    
2024-05-04 00:21:47,989 - Epoch: [70][  217/  217]    Overall Loss 0.004520    Objective Loss 0.004520    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.276712    
2024-05-04 00:21:48,411 - --- validate (epoch=70)-----------
2024-05-04 00:21:48,412 - 1736 samples (32 per mini-batch)
2024-05-04 00:22:07,470 - Epoch: [70][   55/   55]    Loss 2.586354    Top1 58.179724    Top5 74.769585    
2024-05-04 00:22:07,812 - ==> Top1: 58.180    Top5: 74.770    Loss: 2.586

2024-05-04 00:22:07,818 - ==> Best [Top1: 58.756   Top5: 73.560   Sparsity:0.00   Params: 384080 on epoch: 60]
2024-05-04 00:22:07,818 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 00:22:07,875 - 

2024-05-04 00:22:07,876 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:22:26,977 - Epoch: [71][   55/   55]    Overall Loss 0.102325    Objective Loss 0.102325    Top1 94.267516    Top5 100.000000    LR 0.100000    Time 1.029950    
2024-05-04 00:22:27,377 - 

2024-05-04 00:22:27,378 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:22:37,141 - Epoch: [71][  100/  217]    Overall Loss 0.002553    Objective Loss 0.002553                                        LR 0.001000    Time 0.292548    
2024-05-04 00:23:06,016 - Epoch: [71][  200/  217]    Overall Loss 0.003101    Objective Loss 0.003101                                        LR 0.001000    Time 0.290597    
2024-05-04 00:23:09,062 - Epoch: [71][  217/  217]    Overall Loss 0.003454    Objective Loss 0.003454    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.281856    
2024-05-04 00:23:09,379 - 

2024-05-04 00:23:09,380 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:23:29,080 - Epoch: [72][   55/   55]    Overall Loss 0.069991    Objective Loss 0.069991    Top1 98.726115    Top5 100.000000    LR 0.100000    Time 1.121692    
2024-05-04 00:23:29,890 - 

2024-05-04 00:23:29,891 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:23:40,585 - Epoch: [72][  100/  217]    Overall Loss 0.004616    Objective Loss 0.004616                                        LR 0.001000    Time 0.311955    
2024-05-04 00:24:04,835 - Epoch: [72][  200/  217]    Overall Loss 0.211526    Objective Loss 0.211526                                        LR 0.001000    Time 0.277180    
2024-05-04 00:24:10,057 - Epoch: [72][  217/  217]    Overall Loss 0.265781    Objective Loss 0.265781    Top1 77.049180    Top5 90.163934    LR 0.001000    Time 0.279521    
2024-05-04 00:24:10,361 - 

2024-05-04 00:24:10,361 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:24:28,908 - Epoch: [73][   55/   55]    Overall Loss 0.042421    Objective Loss 0.042421    Top1 99.363057    Top5 100.000000    LR 0.100000    Time 1.072848    
2024-05-04 00:24:29,649 - 

2024-05-04 00:24:29,650 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:24:43,187 - Epoch: [73][  100/  217]    Overall Loss 0.709985    Objective Loss 0.709985                                        LR 0.001000    Time 0.328166    
2024-05-04 00:25:09,281 - Epoch: [73][  200/  217]    Overall Loss 0.586480    Objective Loss 0.586480                                        LR 0.001000    Time 0.294509    
2024-05-04 00:25:13,269 - Epoch: [73][  217/  217]    Overall Loss 0.575574    Objective Loss 0.575574    Top1 85.245902    Top5 96.721311    LR 0.001000    Time 0.289801    
2024-05-04 00:25:13,701 - 

2024-05-04 00:25:13,701 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:25:29,192 - Epoch: [74][   55/   55]    Overall Loss 0.038098    Objective Loss 0.038098    Top1 99.363057    Top5 100.000000    LR 0.100000    Time 1.082398    
2024-05-04 00:25:30,418 - 

2024-05-04 00:25:30,418 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:25:42,698 - Epoch: [74][  100/  217]    Overall Loss 0.124596    Objective Loss 0.124596                                        LR 0.001000    Time 0.289861    
2024-05-04 00:26:09,851 - Epoch: [74][  200/  217]    Overall Loss 0.113060    Objective Loss 0.113060                                        LR 0.001000    Time 0.280655    
2024-05-04 00:26:14,612 - Epoch: [74][  217/  217]    Overall Loss 0.109851    Objective Loss 0.109851    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.280599    
2024-05-04 00:26:14,918 - 

2024-05-04 00:26:14,920 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:26:25,676 - Epoch: [75][   55/   55]    Overall Loss 0.016453    Objective Loss 0.016453    Top1 99.363057    Top5 100.000000    LR 0.100000    Time 1.004487    
2024-05-04 00:26:25,934 - 

2024-05-04 00:26:25,936 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:26:44,359 - Epoch: [75][  100/  217]    Overall Loss 0.030290    Objective Loss 0.030290                                        LR 0.001000    Time 0.294282    
2024-05-04 00:27:11,995 - Epoch: [75][  200/  217]    Overall Loss 0.030459    Objective Loss 0.030459                                        LR 0.001000    Time 0.285278    
2024-05-04 00:27:15,621 - Epoch: [75][  217/  217]    Overall Loss 0.030032    Objective Loss 0.030032    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.279631    
2024-05-04 00:27:15,895 - 

2024-05-04 00:27:15,896 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:27:19,989 - Epoch: [76][   55/   55]    Overall Loss 0.010145    Objective Loss 0.010145    Top1 99.363057    Top5 100.000000    LR 0.100000    Time 0.982614    
2024-05-04 00:27:20,287 - 

2024-05-04 00:27:20,289 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:27:45,049 - Epoch: [76][  100/  217]    Overall Loss 0.015349    Objective Loss 0.015349                                        LR 0.001000    Time 0.291440    
2024-05-04 00:28:10,565 - Epoch: [76][  200/  217]    Overall Loss 0.015532    Objective Loss 0.015532                                        LR 0.001000    Time 0.273253    
2024-05-04 00:28:15,791 - Epoch: [76][  217/  217]    Overall Loss 0.015368    Objective Loss 0.015368    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.275921    
2024-05-04 00:28:16,167 - 

2024-05-04 00:28:16,167 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:28:24,011 - Epoch: [77][   55/   55]    Overall Loss 0.004936    Objective Loss 0.004936    Top1 99.363057    Top5 100.000000    LR 0.100000    Time 1.158394    
2024-05-04 00:28:24,401 - 

2024-05-04 00:28:24,402 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:28:47,821 - Epoch: [77][  100/  217]    Overall Loss 0.010505    Objective Loss 0.010505                                        LR 0.001000    Time 0.316443    
2024-05-04 00:29:13,320 - Epoch: [77][  200/  217]    Overall Loss 0.010222    Objective Loss 0.010222                                        LR 0.001000    Time 0.285673    
2024-05-04 00:29:16,898 - Epoch: [77][  217/  217]    Overall Loss 0.010103    Objective Loss 0.010103    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.279768    
2024-05-04 00:29:17,384 - 

2024-05-04 00:29:17,385 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:29:19,766 - Epoch: [78][   55/   55]    Overall Loss 0.004693    Objective Loss 0.004693    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.006430    
2024-05-04 00:29:20,031 - 

2024-05-04 00:29:20,032 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:29:46,755 - Epoch: [78][  100/  217]    Overall Loss 0.007099    Objective Loss 0.007099                                        LR 0.001000    Time 0.293611    
2024-05-04 00:30:15,103 - Epoch: [78][  200/  217]    Overall Loss 0.008252    Objective Loss 0.008252                                        LR 0.001000    Time 0.288501    
2024-05-04 00:30:16,738 - Epoch: [79][   55/   55]    Overall Loss 0.004024    Objective Loss 0.004024    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.030804    
2024-05-04 00:30:17,115 - 

2024-05-04 00:30:17,116 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:30:20,387 - Epoch: [78][  217/  217]    Overall Loss 0.008355    Objective Loss 0.008355    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.290244    
2024-05-04 00:30:20,709 - 

2024-05-04 00:30:20,710 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:30:51,679 - Epoch: [79][  100/  217]    Overall Loss 0.008072    Objective Loss 0.008072                                        LR 0.001000    Time 0.309584    
2024-05-04 00:31:14,540 - Epoch: [80][   55/   55]    Overall Loss 0.003712    Objective Loss 0.003712    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.043903    
2024-05-04 00:31:14,868 - --- validate (epoch=80)-----------
2024-05-04 00:31:14,870 - 1736 samples (128 per mini-batch)
2024-05-04 00:31:18,100 - Epoch: [79][  200/  217]    Overall Loss 0.007645    Objective Loss 0.007645                                        LR 0.001000    Time 0.286852    
2024-05-04 00:31:23,197 - Epoch: [79][  217/  217]    Overall Loss 0.007657    Objective Loss 0.007657    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.287854    
2024-05-04 00:31:23,460 - 

2024-05-04 00:31:23,460 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:31:34,260 - Epoch: [80][   14/   14]    Loss 3.695203    Top1 50.172811    Top5 66.993088    
2024-05-04 00:31:34,495 - ==> Top1: 50.173    Top5: 66.993    Loss: 3.695

2024-05-04 00:31:34,514 - ==> Best [Top1: 50.173   Top5: 66.993   Sparsity:0.00   Params: 1342920 on epoch: 80]
2024-05-04 00:31:34,515 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 00:31:34,623 - 

2024-05-04 00:31:34,623 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:31:54,811 - Epoch: [80][  100/  217]    Overall Loss 0.005233    Objective Loss 0.005233                                        LR 0.001000    Time 0.313407    
2024-05-04 00:32:23,444 - Epoch: [80][  200/  217]    Overall Loss 0.005555    Objective Loss 0.005555                                        LR 0.001000    Time 0.299821    
2024-05-04 00:32:27,722 - Epoch: [80][  217/  217]    Overall Loss 0.006026    Objective Loss 0.006026    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.296040    
2024-05-04 00:32:28,405 - --- validate (epoch=80)-----------
2024-05-04 00:32:28,405 - 1736 samples (32 per mini-batch)
2024-05-04 00:32:31,524 - Epoch: [81][   55/   55]    Overall Loss 0.005571    Objective Loss 0.005571    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.034375    
2024-05-04 00:32:32,236 - 

2024-05-04 00:32:32,237 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:32:45,672 - Epoch: [80][   55/   55]    Loss 2.586905    Top1 58.410138    Top5 74.942396    
2024-05-04 00:32:45,855 - ==> Top1: 58.410    Top5: 74.942    Loss: 2.587

2024-05-04 00:32:45,862 - ==> Best [Top1: 58.756   Top5: 73.560   Sparsity:0.00   Params: 384080 on epoch: 60]
2024-05-04 00:32:45,863 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 00:32:45,914 - 

2024-05-04 00:32:45,914 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:33:13,415 - Epoch: [81][  100/  217]    Overall Loss 0.005614    Objective Loss 0.005614                                        LR 0.001000    Time 0.274889    
2024-05-04 00:33:29,955 - Epoch: [82][   55/   55]    Overall Loss 0.005110    Objective Loss 0.005110    Top1 99.363057    Top5 100.000000    LR 0.100000    Time 1.049247    
2024-05-04 00:33:30,189 - 

2024-05-04 00:33:30,189 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:33:38,845 - Epoch: [81][  200/  217]    Overall Loss 0.005506    Objective Loss 0.005506                                        LR 0.001000    Time 0.264547    
2024-05-04 00:33:42,087 - Epoch: [81][  217/  217]    Overall Loss 0.005659    Objective Loss 0.005659    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.258755    
2024-05-04 00:33:42,364 - 

2024-05-04 00:33:42,364 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:34:11,026 - Epoch: [82][  100/  217]    Overall Loss 0.005302    Objective Loss 0.005302                                        LR 0.001000    Time 0.286525    
2024-05-04 00:34:30,298 - Epoch: [83][   55/   55]    Overall Loss 0.004970    Objective Loss 0.004970    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.092732    
2024-05-04 00:34:30,634 - 

2024-05-04 00:34:30,635 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:34:39,569 - Epoch: [82][  200/  217]    Overall Loss 0.004468    Objective Loss 0.004468                                        LR 0.001000    Time 0.285932    
2024-05-04 00:34:43,629 - Epoch: [82][  217/  217]    Overall Loss 0.004812    Objective Loss 0.004812    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.282232    
2024-05-04 00:34:44,016 - 

2024-05-04 00:34:44,017 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:35:14,129 - Epoch: [83][  100/  217]    Overall Loss 0.004147    Objective Loss 0.004147                                        LR 0.001000    Time 0.301021    
2024-05-04 00:35:28,253 - Epoch: [84][   55/   55]    Overall Loss 0.003271    Objective Loss 0.003271    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.047442    
2024-05-04 00:35:28,731 - 

2024-05-04 00:35:28,732 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:35:39,747 - Epoch: [83][  200/  217]    Overall Loss 0.003927    Objective Loss 0.003927                                        LR 0.001000    Time 0.278558    
2024-05-04 00:35:43,130 - Epoch: [83][  217/  217]    Overall Loss 0.003991    Objective Loss 0.003991    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.272317    
2024-05-04 00:35:43,475 - 

2024-05-04 00:35:43,476 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:36:13,388 - Epoch: [84][  100/  217]    Overall Loss 0.003463    Objective Loss 0.003463                                        LR 0.001000    Time 0.299013    
2024-05-04 00:36:25,802 - Epoch: [85][   55/   55]    Overall Loss 0.004431    Objective Loss 0.004431    Top1 99.363057    Top5 100.000000    LR 0.100000    Time 1.037491    
2024-05-04 00:36:26,119 - 

2024-05-04 00:36:26,120 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:36:37,704 - Epoch: [84][  200/  217]    Overall Loss 0.004413    Objective Loss 0.004413                                        LR 0.001000    Time 0.271046    
2024-05-04 00:36:41,011 - Epoch: [84][  217/  217]    Overall Loss 0.004999    Objective Loss 0.004999    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.265039    
2024-05-04 00:36:41,240 - 

2024-05-04 00:36:41,240 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:37:10,060 - Epoch: [85][  100/  217]    Overall Loss 0.034974    Objective Loss 0.034974                                        LR 0.001000    Time 0.288091    
2024-05-04 00:37:21,521 - Epoch: [86][   55/   55]    Overall Loss 0.014099    Objective Loss 0.014099    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.007100    
2024-05-04 00:37:21,720 - 

2024-05-04 00:37:21,721 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:37:37,635 - Epoch: [85][  200/  217]    Overall Loss 0.138841    Objective Loss 0.138841                                        LR 0.001000    Time 0.281877    
2024-05-04 00:37:42,970 - Epoch: [85][  217/  217]    Overall Loss 0.169813    Objective Loss 0.169813    Top1 86.885246    Top5 98.360656    LR 0.001000    Time 0.284369    
2024-05-04 00:37:43,400 - 

2024-05-04 00:37:43,400 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:38:13,892 - Epoch: [86][  100/  217]    Overall Loss 0.293111    Objective Loss 0.293111                                        LR 0.001000    Time 0.304817    
2024-05-04 00:38:20,612 - Epoch: [87][   55/   55]    Overall Loss 0.007301    Objective Loss 0.007301    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.070558    
2024-05-04 00:38:21,063 - 

2024-05-04 00:38:21,064 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:38:40,985 - Epoch: [86][  200/  217]    Overall Loss 0.262793    Objective Loss 0.262793                                        LR 0.001000    Time 0.287828    
2024-05-04 00:38:43,802 - Epoch: [86][  217/  217]    Overall Loss 0.257863    Objective Loss 0.257863    Top1 90.163934    Top5 98.360656    LR 0.001000    Time 0.278249    
2024-05-04 00:38:44,466 - 

2024-05-04 00:38:44,467 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:39:14,246 - Epoch: [87][  100/  217]    Overall Loss 0.075833    Objective Loss 0.075833                                        LR 0.001000    Time 0.297693    
2024-05-04 00:39:22,384 - Epoch: [88][   55/   55]    Overall Loss 0.005972    Objective Loss 0.005972    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.114735    
2024-05-04 00:39:23,198 - 

2024-05-04 00:39:23,199 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:39:38,989 - Epoch: [87][  200/  217]    Overall Loss 0.061558    Objective Loss 0.061558                                        LR 0.001000    Time 0.272516    
2024-05-04 00:39:42,999 - Epoch: [87][  217/  217]    Overall Loss 0.060416    Objective Loss 0.060416    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.269636    
2024-05-04 00:39:43,342 - 

2024-05-04 00:39:43,343 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:40:09,278 - Epoch: [88][  100/  217]    Overall Loss 0.018001    Objective Loss 0.018001                                        LR 0.001000    Time 0.259253    
2024-05-04 00:40:28,001 - Epoch: [89][   55/   55]    Overall Loss 0.004248    Objective Loss 0.004248    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.178067    
2024-05-04 00:40:29,057 - 

2024-05-04 00:40:29,058 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:40:36,235 - Epoch: [88][  200/  217]    Overall Loss 0.017494    Objective Loss 0.017494                                        LR 0.001000    Time 0.264348    
2024-05-04 00:40:39,513 - Epoch: [88][  217/  217]    Overall Loss 0.017569    Objective Loss 0.017569    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.258739    
2024-05-04 00:40:39,765 - 

2024-05-04 00:40:39,766 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:41:11,024 - Epoch: [89][  100/  217]    Overall Loss 0.010231    Objective Loss 0.010231                                        LR 0.001000    Time 0.312500    
2024-05-04 00:41:25,661 - Epoch: [90][   55/   55]    Overall Loss 0.002992    Objective Loss 0.002992    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.028943    
2024-05-04 00:41:26,084 - --- validate (epoch=90)-----------
2024-05-04 00:41:26,084 - 1736 samples (128 per mini-batch)
2024-05-04 00:41:35,055 - Epoch: [89][  200/  217]    Overall Loss 0.008880    Objective Loss 0.008880                                        LR 0.001000    Time 0.276364    
2024-05-04 00:41:37,997 - Epoch: [89][  217/  217]    Overall Loss 0.009091    Objective Loss 0.009091    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.268263    
2024-05-04 00:41:38,340 - 

2024-05-04 00:41:38,341 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:41:44,562 - Epoch: [90][   14/   14]    Loss 3.544575    Top1 50.691244    Top5 66.993088    
2024-05-04 00:41:44,912 - ==> Top1: 50.691    Top5: 66.993    Loss: 3.545

2024-05-04 00:41:44,925 - ==> Best [Top1: 50.691   Top5: 66.993   Sparsity:0.00   Params: 1342920 on epoch: 90]
2024-05-04 00:41:44,925 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 00:41:45,029 - 

2024-05-04 00:41:45,030 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:42:10,912 - Epoch: [90][  100/  217]    Overall Loss 0.005831    Objective Loss 0.005831                                        LR 0.001000    Time 0.325621    
2024-05-04 00:42:35,258 - Epoch: [90][  200/  217]    Overall Loss 0.005821    Objective Loss 0.005821                                        LR 0.001000    Time 0.284498    
2024-05-04 00:42:38,941 - Epoch: [90][  217/  217]    Overall Loss 0.005761    Objective Loss 0.005761    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.279173    
2024-05-04 00:42:39,409 - --- validate (epoch=90)-----------
2024-05-04 00:42:39,410 - 1736 samples (32 per mini-batch)
2024-05-04 00:42:40,244 - Epoch: [91][   55/   55]    Overall Loss 0.003088    Objective Loss 0.003088    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.003732    
2024-05-04 00:42:40,490 - 

2024-05-04 00:42:40,490 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:42:55,794 - Epoch: [90][   55/   55]    Loss 2.635184    Top1 59.043779    Top5 75.000000    
2024-05-04 00:42:56,089 - ==> Top1: 59.044    Top5: 75.000    Loss: 2.635

2024-05-04 00:42:56,094 - ==> Best [Top1: 59.044   Top5: 75.000   Sparsity:0.00   Params: 384080 on epoch: 90]
2024-05-04 00:42:56,094 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 00:42:56,152 - 

2024-05-04 00:42:56,153 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:43:31,390 - Epoch: [91][  100/  217]    Overall Loss 0.004581    Objective Loss 0.004581                                        LR 0.001000    Time 0.352275    
2024-05-04 00:43:35,459 - Epoch: [92][   55/   55]    Overall Loss 0.002670    Objective Loss 0.002670    Top1 99.363057    Top5 100.000000    LR 0.100000    Time 0.999264    
2024-05-04 00:43:35,774 - 

2024-05-04 00:43:35,775 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:43:54,682 - Epoch: [91][  200/  217]    Overall Loss 0.004451    Objective Loss 0.004451                                        LR 0.001000    Time 0.292551    
2024-05-04 00:43:59,200 - Epoch: [91][  217/  217]    Overall Loss 0.004360    Objective Loss 0.004360    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.290445    
2024-05-04 00:43:59,441 - 

2024-05-04 00:43:59,442 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:44:35,795 - Epoch: [92][  100/  217]    Overall Loss 0.004777    Objective Loss 0.004777                                        LR 0.001000    Time 0.363444    
2024-05-04 00:44:42,296 - Epoch: [93][   55/   55]    Overall Loss 0.002955    Objective Loss 0.002955    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.209322    
2024-05-04 00:44:42,638 - 

2024-05-04 00:44:42,639 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:44:59,645 - Epoch: [92][  200/  217]    Overall Loss 0.004919    Objective Loss 0.004919                                        LR 0.001000    Time 0.300921    
2024-05-04 00:45:03,862 - Epoch: [92][  217/  217]    Overall Loss 0.004848    Objective Loss 0.004848    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.296773    
2024-05-04 00:45:04,534 - 

2024-05-04 00:45:04,535 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:45:38,459 - Epoch: [93][  100/  217]    Overall Loss 0.005524    Objective Loss 0.005524                                        LR 0.001000    Time 0.339156    
2024-05-04 00:45:41,374 - Epoch: [94][   55/   55]    Overall Loss 0.002901    Objective Loss 0.002901    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.067758    
2024-05-04 00:45:42,187 - 

2024-05-04 00:45:42,188 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:46:05,148 - Epoch: [93][  200/  217]    Overall Loss 0.005388    Objective Loss 0.005388                                        LR 0.001000    Time 0.302975    
2024-05-04 00:46:09,179 - Epoch: [93][  217/  217]    Overall Loss 0.005205    Objective Loss 0.005205    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.297805    
2024-05-04 00:46:09,751 - 

2024-05-04 00:46:09,752 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:46:39,577 - Epoch: [94][  100/  217]    Overall Loss 0.003155    Objective Loss 0.003155                                        LR 0.001000    Time 0.298162    
2024-05-04 00:46:40,954 - Epoch: [95][   55/   55]    Overall Loss 0.002611    Objective Loss 0.002611    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.068313    
2024-05-04 00:46:41,190 - 

2024-05-04 00:46:41,192 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:47:08,079 - Epoch: [94][  200/  217]    Overall Loss 0.002977    Objective Loss 0.002977                                        LR 0.001000    Time 0.291538    
2024-05-04 00:47:12,134 - Epoch: [94][  217/  217]    Overall Loss 0.003010    Objective Loss 0.003010    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.287367    
2024-05-04 00:47:12,557 - 

2024-05-04 00:47:12,557 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:47:39,134 - Epoch: [95][  100/  217]    Overall Loss 0.003026    Objective Loss 0.003026                                        LR 0.001000    Time 0.265658    
2024-05-04 00:47:42,644 - Epoch: [96][   55/   55]    Overall Loss 0.002965    Objective Loss 0.002965    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.117129    
2024-05-04 00:47:43,085 - 

2024-05-04 00:47:43,086 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:48:06,050 - Epoch: [95][  200/  217]    Overall Loss 0.002951    Objective Loss 0.002951                                        LR 0.001000    Time 0.267357    
2024-05-04 00:48:10,681 - Epoch: [95][  217/  217]    Overall Loss 0.003153    Objective Loss 0.003153    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.267746    
2024-05-04 00:48:10,949 - 

2024-05-04 00:48:10,950 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:48:40,774 - Epoch: [96][  100/  217]    Overall Loss 0.002956    Objective Loss 0.002956                                        LR 0.001000    Time 0.298158    
2024-05-04 00:48:44,976 - Epoch: [97][   55/   55]    Overall Loss 0.003393    Objective Loss 0.003393    Top1 99.363057    Top5 100.000000    LR 0.100000    Time 1.125094    
2024-05-04 00:48:45,413 - 

2024-05-04 00:48:45,413 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:49:04,167 - Epoch: [96][  200/  217]    Overall Loss 0.002889    Objective Loss 0.002889                                        LR 0.001000    Time 0.266000    
2024-05-04 00:49:09,305 - Epoch: [96][  217/  217]    Overall Loss 0.002850    Objective Loss 0.002850    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.268826    
2024-05-04 00:49:09,579 - 

2024-05-04 00:49:09,580 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:49:39,238 - Epoch: [97][  100/  217]    Overall Loss 0.002661    Objective Loss 0.002661                                        LR 0.001000    Time 0.296496    
2024-05-04 00:49:45,234 - Epoch: [98][   55/   55]    Overall Loss 0.003201    Objective Loss 0.003201    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.087484    
2024-05-04 00:49:45,455 - 

2024-05-04 00:49:45,456 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:50:05,146 - Epoch: [97][  200/  217]    Overall Loss 0.002517    Objective Loss 0.002517                                        LR 0.001000    Time 0.277744    
2024-05-04 00:50:09,652 - Epoch: [97][  217/  217]    Overall Loss 0.002430    Objective Loss 0.002430    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.276742    
2024-05-04 00:50:09,941 - 

2024-05-04 00:50:09,942 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:50:38,141 - Epoch: [98][  100/  217]    Overall Loss 0.001958    Objective Loss 0.001958                                        LR 0.001000    Time 0.281903    
2024-05-04 00:50:43,262 - Epoch: [99][   55/   55]    Overall Loss 0.002765    Objective Loss 0.002765    Top1 100.000000    Top5 100.000000    LR 0.100000    Time 1.050857    
2024-05-04 00:50:43,520 - 

2024-05-04 00:50:43,521 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:51:05,019 - Epoch: [98][  200/  217]    Overall Loss 0.002303    Objective Loss 0.002303                                        LR 0.001000    Time 0.275293    
2024-05-04 00:51:09,304 - Epoch: [98][  217/  217]    Overall Loss 0.002230    Objective Loss 0.002230    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.273466    
2024-05-04 00:51:09,857 - 

2024-05-04 00:51:09,858 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:51:39,929 - Epoch: [99][  100/  217]    Overall Loss 0.002281    Objective Loss 0.002281                                        LR 0.001000    Time 0.300602    
2024-05-04 00:51:41,631 - Epoch: [100][   55/   55]    Overall Loss 0.002716    Objective Loss 0.002716    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.056375    
2024-05-04 00:51:41,881 - --- validate (epoch=100)-----------
2024-05-04 00:51:41,881 - 1736 samples (128 per mini-batch)
2024-05-04 00:51:59,237 - Epoch: [100][   14/   14]    Loss 3.380248    Top1 51.267281    Top5 67.857143    
2024-05-04 00:51:59,880 - ==> Top1: 51.267    Top5: 67.857    Loss: 3.380

2024-05-04 00:51:59,895 - ==> Best [Top1: 51.267   Top5: 67.857   Sparsity:0.00   Params: 1342920 on epoch: 100]
2024-05-04 00:51:59,896 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 00:52:00,057 - 

2024-05-04 00:52:00,058 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:52:03,049 - Epoch: [99][  200/  217]    Overall Loss 0.002157    Objective Loss 0.002157                                        LR 0.001000    Time 0.265859    
2024-05-04 00:52:06,635 - Epoch: [99][  217/  217]    Overall Loss 0.002120    Objective Loss 0.002120    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.261546    
2024-05-04 00:52:07,741 - 

2024-05-04 00:52:07,743 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:52:37,639 - Epoch: [100][  100/  217]    Overall Loss 0.001324    Objective Loss 0.001324                                        LR 0.000250    Time 0.298850    
2024-05-04 00:52:54,526 - Epoch: [101][   55/   55]    Overall Loss 0.002962    Objective Loss 0.002962    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 0.990164    
2024-05-04 00:52:54,836 - 

2024-05-04 00:52:54,837 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:53:02,113 - Epoch: [100][  200/  217]    Overall Loss 0.001688    Objective Loss 0.001688                                        LR 0.000250    Time 0.271750    
2024-05-04 00:53:05,702 - Epoch: [100][  217/  217]    Overall Loss 0.001755    Objective Loss 0.001755    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.266995    
2024-05-04 00:53:05,936 - --- validate (epoch=100)-----------
2024-05-04 00:53:05,936 - 1736 samples (32 per mini-batch)
2024-05-04 00:53:22,806 - Epoch: [100][   55/   55]    Loss 2.692346    Top1 59.331797    Top5 74.827189    
2024-05-04 00:53:23,045 - ==> Top1: 59.332    Top5: 74.827    Loss: 2.692

2024-05-04 00:53:23,050 - ==> Best [Top1: 59.332   Top5: 74.827   Sparsity:0.00   Params: 384080 on epoch: 100]
2024-05-04 00:53:23,050 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 00:53:23,117 - 

2024-05-04 00:53:23,118 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:53:51,638 - Epoch: [102][   55/   55]    Overall Loss 0.002369    Objective Loss 0.002369    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.032571    
2024-05-04 00:53:52,071 - 

2024-05-04 00:53:52,073 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:53:52,755 - Epoch: [101][  100/  217]    Overall Loss 0.001275    Objective Loss 0.001275                                        LR 0.000250    Time 0.296283    
2024-05-04 00:54:18,683 - Epoch: [101][  200/  217]    Overall Loss 0.001309    Objective Loss 0.001309                                        LR 0.000250    Time 0.277742    
2024-05-04 00:54:23,095 - Epoch: [101][  217/  217]    Overall Loss 0.001540    Objective Loss 0.001540    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.276310    
2024-05-04 00:54:23,314 - 

2024-05-04 00:54:23,315 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:54:51,744 - Epoch: [102][  100/  217]    Overall Loss 0.001862    Objective Loss 0.001862                                        LR 0.000250    Time 0.284203    
2024-05-04 00:54:53,704 - Epoch: [103][   55/   55]    Overall Loss 0.002634    Objective Loss 0.002634    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.120347    
2024-05-04 00:54:53,989 - 

2024-05-04 00:54:53,990 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:55:14,122 - Epoch: [102][  200/  217]    Overall Loss 0.001719    Objective Loss 0.001719                                        LR 0.000250    Time 0.253953    
2024-05-04 00:55:17,238 - Epoch: [102][  217/  217]    Overall Loss 0.001665    Objective Loss 0.001665    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.248410    
2024-05-04 00:55:17,519 - 

2024-05-04 00:55:17,520 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:55:46,376 - Epoch: [103][  100/  217]    Overall Loss 0.001587    Objective Loss 0.001587                                        LR 0.000250    Time 0.288480    
2024-05-04 00:55:51,808 - Epoch: [104][   55/   55]    Overall Loss 0.002766    Objective Loss 0.002766    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.050947    
2024-05-04 00:55:52,169 - 

2024-05-04 00:55:52,170 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:56:13,783 - Epoch: [103][  200/  217]    Overall Loss 0.001611    Objective Loss 0.001611                                        LR 0.000250    Time 0.281240    
2024-05-04 00:56:18,534 - Epoch: [103][  217/  217]    Overall Loss 0.001568    Objective Loss 0.001568    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.281091    
2024-05-04 00:56:18,826 - 

2024-05-04 00:56:18,826 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:56:47,916 - Epoch: [104][  100/  217]    Overall Loss 0.001333    Objective Loss 0.001333                                        LR 0.000250    Time 0.290810    
2024-05-04 00:56:50,387 - Epoch: [105][   55/   55]    Overall Loss 0.002491    Objective Loss 0.002491    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.058321    
2024-05-04 00:56:50,696 - 

2024-05-04 00:56:50,698 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:57:15,180 - Epoch: [104][  200/  217]    Overall Loss 0.001543    Objective Loss 0.001543                                        LR 0.000250    Time 0.281683    
2024-05-04 00:57:19,192 - Epoch: [104][  217/  217]    Overall Loss 0.001646    Objective Loss 0.001646    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.278094    
2024-05-04 00:57:19,380 - 

2024-05-04 00:57:19,381 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:57:48,854 - Epoch: [105][  100/  217]    Overall Loss 0.001722    Objective Loss 0.001722                                        LR 0.000250    Time 0.294644    
2024-05-04 00:57:51,709 - Epoch: [106][   55/   55]    Overall Loss 0.002670    Objective Loss 0.002670    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.109136    
2024-05-04 00:57:51,961 - 

2024-05-04 00:57:51,961 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:58:17,295 - Epoch: [105][  200/  217]    Overall Loss 0.001495    Objective Loss 0.001495                                        LR 0.000250    Time 0.289489    
2024-05-04 00:58:22,411 - Epoch: [105][  217/  217]    Overall Loss 0.001451    Objective Loss 0.001451    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.290378    
2024-05-04 00:58:22,722 - 

2024-05-04 00:58:22,723 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:58:49,039 - Epoch: [106][  100/  217]    Overall Loss 0.000959    Objective Loss 0.000959                                        LR 0.000250    Time 0.263076    
2024-05-04 00:58:55,897 - Epoch: [107][   55/   55]    Overall Loss 0.002817    Objective Loss 0.002817    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.162312    
2024-05-04 00:58:56,483 - 

2024-05-04 00:58:56,484 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 00:59:19,165 - Epoch: [106][  200/  217]    Overall Loss 0.001448    Objective Loss 0.001448                                        LR 0.000250    Time 0.282129    
2024-05-04 00:59:25,147 - Epoch: [106][  217/  217]    Overall Loss 0.001406    Objective Loss 0.001406    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.287587    
2024-05-04 00:59:25,455 - 

2024-05-04 00:59:25,455 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:59:52,726 - Epoch: [108][   55/   55]    Overall Loss 0.002529    Objective Loss 0.002529    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.022399    
2024-05-04 00:59:53,002 - Epoch: [107][  100/  217]    Overall Loss 0.001520    Objective Loss 0.001520                                        LR 0.000250    Time 0.275372    
2024-05-04 00:59:53,106 - 

2024-05-04 00:59:53,108 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:00:19,311 - Epoch: [107][  200/  217]    Overall Loss 0.001423    Objective Loss 0.001423                                        LR 0.000250    Time 0.269179    
2024-05-04 01:00:23,128 - Epoch: [107][  217/  217]    Overall Loss 0.001519    Objective Loss 0.001519    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.265675    
2024-05-04 01:00:23,354 - 

2024-05-04 01:00:23,355 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:00:48,473 - Epoch: [109][   55/   55]    Overall Loss 0.002747    Objective Loss 0.002747    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.006470    
2024-05-04 01:00:49,152 - 

2024-05-04 01:00:49,153 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:00:50,859 - Epoch: [108][  100/  217]    Overall Loss 0.001499    Objective Loss 0.001499                                        LR 0.000250    Time 0.274972    
2024-05-04 01:01:15,832 - Epoch: [108][  200/  217]    Overall Loss 0.001553    Objective Loss 0.001553                                        LR 0.000250    Time 0.262307    
2024-05-04 01:01:19,684 - Epoch: [108][  217/  217]    Overall Loss 0.001506    Objective Loss 0.001506    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.259501    
2024-05-04 01:01:19,872 - 

2024-05-04 01:01:19,873 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:01:47,384 - Epoch: [109][  100/  217]    Overall Loss 0.001410    Objective Loss 0.001410                                        LR 0.000250    Time 0.275025    
2024-05-04 01:01:51,711 - Epoch: [110][   55/   55]    Overall Loss 0.002534    Objective Loss 0.002534    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.137249    
2024-05-04 01:01:52,423 - --- validate (epoch=110)-----------
2024-05-04 01:01:52,424 - 1736 samples (128 per mini-batch)
2024-05-04 01:02:10,412 - Epoch: [110][   14/   14]    Loss 3.343981    Top1 51.324885    Top5 68.029954    
2024-05-04 01:02:10,623 - ==> Top1: 51.325    Top5: 68.030    Loss: 3.344

2024-05-04 01:02:10,641 - ==> Best [Top1: 51.325   Top5: 68.030   Sparsity:0.00   Params: 1342920 on epoch: 110]
2024-05-04 01:02:10,642 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 01:02:10,786 - 

2024-05-04 01:02:10,786 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:02:11,550 - Epoch: [109][  200/  217]    Overall Loss 0.001276    Objective Loss 0.001276                                        LR 0.000250    Time 0.258307    
2024-05-04 01:02:14,500 - Epoch: [109][  217/  217]    Overall Loss 0.001369    Objective Loss 0.001369    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.251657    
2024-05-04 01:02:14,698 - 

2024-05-04 01:02:14,699 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:02:44,293 - Epoch: [110][  100/  217]    Overall Loss 0.000881    Objective Loss 0.000881                                        LR 0.000250    Time 0.295858    
2024-05-04 01:03:07,223 - Epoch: [110][  200/  217]    Overall Loss 0.001206    Objective Loss 0.001206                                        LR 0.000250    Time 0.262539    
2024-05-04 01:03:09,890 - Epoch: [111][   55/   55]    Overall Loss 0.002976    Objective Loss 0.002976    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.074446    
2024-05-04 01:03:10,179 - 

2024-05-04 01:03:10,180 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:03:11,127 - Epoch: [110][  217/  217]    Overall Loss 0.001331    Objective Loss 0.001331    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.259952    
2024-05-04 01:03:11,320 - --- validate (epoch=110)-----------
2024-05-04 01:03:11,321 - 1736 samples (32 per mini-batch)
2024-05-04 01:03:28,596 - Epoch: [110][   55/   55]    Loss 2.708662    Top1 59.274194    Top5 75.057604    
2024-05-04 01:03:28,857 - ==> Top1: 59.274    Top5: 75.058    Loss: 2.709

2024-05-04 01:03:28,861 - ==> Best [Top1: 59.332   Top5: 74.827   Sparsity:0.00   Params: 384080 on epoch: 100]
2024-05-04 01:03:28,862 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 01:03:28,898 - 

2024-05-04 01:03:28,898 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:03:56,909 - Epoch: [111][  100/  217]    Overall Loss 0.000928    Objective Loss 0.000928                                        LR 0.000250    Time 0.280028    
2024-05-04 01:04:17,747 - Epoch: [112][   55/   55]    Overall Loss 0.002755    Objective Loss 0.002755    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.228273    
2024-05-04 01:04:18,383 - 

2024-05-04 01:04:18,384 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:04:23,766 - Epoch: [111][  200/  217]    Overall Loss 0.001382    Objective Loss 0.001382                                        LR 0.000250    Time 0.274260    
2024-05-04 01:04:26,682 - Epoch: [111][  217/  217]    Overall Loss 0.001326    Objective Loss 0.001326    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.266204    
2024-05-04 01:04:26,856 - 

2024-05-04 01:04:26,857 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:04:55,088 - Epoch: [112][  100/  217]    Overall Loss 0.001315    Objective Loss 0.001315                                        LR 0.000250    Time 0.282227    
2024-05-04 01:05:14,588 - Epoch: [113][   55/   55]    Overall Loss 0.002670    Objective Loss 0.002670    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.021716    
2024-05-04 01:05:14,982 - 

2024-05-04 01:05:14,983 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:05:20,977 - Epoch: [112][  200/  217]    Overall Loss 0.001134    Objective Loss 0.001134                                        LR 0.000250    Time 0.270517    
2024-05-04 01:05:25,494 - Epoch: [112][  217/  217]    Overall Loss 0.001264    Objective Loss 0.001264    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.270130    
2024-05-04 01:05:25,860 - 

2024-05-04 01:05:25,860 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:05:54,167 - Epoch: [113][  100/  217]    Overall Loss 0.001458    Objective Loss 0.001458                                        LR 0.000250    Time 0.282977    
2024-05-04 01:06:13,543 - Epoch: [114][   55/   55]    Overall Loss 0.002655    Objective Loss 0.002655    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.064519    
2024-05-04 01:06:14,201 - 

2024-05-04 01:06:14,202 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:06:18,198 - Epoch: [113][  200/  217]    Overall Loss 0.001366    Objective Loss 0.001366                                        LR 0.000250    Time 0.261590    
2024-05-04 01:06:24,054 - Epoch: [113][  217/  217]    Overall Loss 0.001338    Objective Loss 0.001338    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.268076    
2024-05-04 01:06:24,339 - 

2024-05-04 01:06:24,340 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:06:51,444 - Epoch: [114][  100/  217]    Overall Loss 0.000992    Objective Loss 0.000992                                        LR 0.000250    Time 0.270952    
2024-05-04 01:07:07,781 - Epoch: [115][   55/   55]    Overall Loss 0.002571    Objective Loss 0.002571    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 0.973979    
2024-05-04 01:07:08,338 - 

2024-05-04 01:07:08,338 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:07:17,213 - Epoch: [114][  200/  217]    Overall Loss 0.001314    Objective Loss 0.001314                                        LR 0.000250    Time 0.264279    
2024-05-04 01:07:22,341 - Epoch: [114][  217/  217]    Overall Loss 0.001266    Objective Loss 0.001266    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.267197    
2024-05-04 01:07:22,618 - 

2024-05-04 01:07:22,619 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:07:52,346 - Epoch: [115][  100/  217]    Overall Loss 0.002345    Objective Loss 0.002345                                        LR 0.000250    Time 0.297176    
2024-05-04 01:08:13,073 - Epoch: [116][   55/   55]    Overall Loss 0.002502    Objective Loss 0.002502    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.176808    
2024-05-04 01:08:13,369 - 

2024-05-04 01:08:13,370 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:08:18,371 - Epoch: [115][  200/  217]    Overall Loss 0.001580    Objective Loss 0.001580                                        LR 0.000250    Time 0.278667    
2024-05-04 01:08:22,144 - Epoch: [115][  217/  217]    Overall Loss 0.001520    Objective Loss 0.001520    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.274216    
2024-05-04 01:08:22,645 - 

2024-05-04 01:08:22,647 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:08:51,944 - Epoch: [116][  100/  217]    Overall Loss 0.000897    Objective Loss 0.000897                                        LR 0.000250    Time 0.292875    
2024-05-04 01:09:05,905 - Epoch: [117][   55/   55]    Overall Loss 0.002560    Objective Loss 0.002560    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 0.955021    
2024-05-04 01:09:07,026 - 

2024-05-04 01:09:07,027 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:09:18,697 - Epoch: [116][  200/  217]    Overall Loss 0.001414    Objective Loss 0.001414                                        LR 0.000250    Time 0.280161    
2024-05-04 01:09:21,451 - Epoch: [116][  217/  217]    Overall Loss 0.001359    Objective Loss 0.001359    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.270895    
2024-05-04 01:09:21,627 - 

2024-05-04 01:09:21,628 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:09:50,410 - Epoch: [117][  100/  217]    Overall Loss 0.000906    Objective Loss 0.000906                                        LR 0.000250    Time 0.287730    
2024-05-04 01:10:06,910 - Epoch: [118][   55/   55]    Overall Loss 0.002745    Objective Loss 0.002745    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.088595    
2024-05-04 01:10:07,191 - 

2024-05-04 01:10:07,192 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:10:15,726 - Epoch: [117][  200/  217]    Overall Loss 0.000846    Objective Loss 0.000846                                        LR 0.000250    Time 0.270405    
2024-05-04 01:10:19,061 - Epoch: [117][  217/  217]    Overall Loss 0.001414    Objective Loss 0.001414    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.264582    
2024-05-04 01:10:19,250 - 

2024-05-04 01:10:19,251 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:10:49,674 - Epoch: [118][  100/  217]    Overall Loss 0.001084    Objective Loss 0.001084                                        LR 0.000250    Time 0.304140    
2024-05-04 01:11:06,754 - Epoch: [119][   55/   55]    Overall Loss 0.002554    Objective Loss 0.002554    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.082794    
2024-05-04 01:11:07,560 - 

2024-05-04 01:11:07,561 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:11:13,370 - Epoch: [118][  200/  217]    Overall Loss 0.001066    Objective Loss 0.001066                                        LR 0.000250    Time 0.270507    
2024-05-04 01:11:16,331 - Epoch: [118][  217/  217]    Overall Loss 0.001171    Objective Loss 0.001171    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.262957    
2024-05-04 01:11:16,624 - 

2024-05-04 01:11:16,624 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:11:49,594 - Epoch: [119][  100/  217]    Overall Loss 0.001083    Objective Loss 0.001083                                        LR 0.000250    Time 0.329621    
2024-05-04 01:12:06,275 - Epoch: [120][   55/   55]    Overall Loss 0.003592    Objective Loss 0.003592    Top1 99.363057    Top5 100.000000    LR 0.023500    Time 1.067355    
2024-05-04 01:12:06,938 - --- validate (epoch=120)-----------
2024-05-04 01:12:06,939 - 1736 samples (128 per mini-batch)
2024-05-04 01:12:15,189 - Epoch: [119][  200/  217]    Overall Loss 0.001085    Objective Loss 0.001085                                        LR 0.000250    Time 0.292736    
2024-05-04 01:12:18,390 - Epoch: [119][  217/  217]    Overall Loss 0.001050    Objective Loss 0.001050    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.284549    
2024-05-04 01:12:18,624 - 

2024-05-04 01:12:18,624 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:12:27,599 - Epoch: [120][   14/   14]    Loss 3.328593    Top1 51.497696    Top5 67.569124    
2024-05-04 01:12:28,649 - ==> Top1: 51.498    Top5: 67.569    Loss: 3.329

2024-05-04 01:12:28,663 - ==> Best [Top1: 51.498   Top5: 67.569   Sparsity:0.00   Params: 1342920 on epoch: 120]
2024-05-04 01:12:28,664 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 01:12:28,814 - 

2024-05-04 01:12:28,815 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:12:46,762 - Epoch: [120][  100/  217]    Overall Loss 0.000866    Objective Loss 0.000866                                        LR 0.000250    Time 0.281283    
2024-05-04 01:13:12,494 - Epoch: [120][  200/  217]    Overall Loss 0.001144    Objective Loss 0.001144                                        LR 0.000250    Time 0.269238    
2024-05-04 01:13:17,843 - Epoch: [120][  217/  217]    Overall Loss 0.001091    Objective Loss 0.001091    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.272784    
2024-05-04 01:13:18,086 - --- validate (epoch=120)-----------
2024-05-04 01:13:18,087 - 1736 samples (32 per mini-batch)
2024-05-04 01:13:21,188 - Epoch: [121][   55/   55]    Overall Loss 0.002839    Objective Loss 0.002839    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 0.952067    
2024-05-04 01:13:21,533 - 

2024-05-04 01:13:21,533 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:13:35,598 - Epoch: [120][   55/   55]    Loss 2.819229    Top1 58.986175    Top5 74.942396    
2024-05-04 01:13:35,806 - ==> Top1: 58.986    Top5: 74.942    Loss: 2.819

2024-05-04 01:13:35,809 - ==> Best [Top1: 59.332   Top5: 74.827   Sparsity:0.00   Params: 384080 on epoch: 100]
2024-05-04 01:13:35,809 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 01:13:35,844 - 

2024-05-04 01:13:35,844 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:14:03,652 - Epoch: [121][  100/  217]    Overall Loss 0.001025    Objective Loss 0.001025                                        LR 0.000250    Time 0.277989    
2024-05-04 01:14:12,137 - Epoch: [122][   55/   55]    Overall Loss 0.002991    Objective Loss 0.002991    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 0.919872    
2024-05-04 01:14:13,215 - 

2024-05-04 01:14:13,216 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:14:28,420 - Epoch: [121][  200/  217]    Overall Loss 0.001066    Objective Loss 0.001066                                        LR 0.000250    Time 0.262790    
2024-05-04 01:14:32,536 - Epoch: [121][  217/  217]    Overall Loss 0.001025    Objective Loss 0.001025    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.261164    
2024-05-04 01:14:32,833 - 

2024-05-04 01:14:32,834 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:15:02,893 - Epoch: [122][  100/  217]    Overall Loss 0.000576    Objective Loss 0.000576                                        LR 0.000250    Time 0.300491    
2024-05-04 01:15:12,046 - Epoch: [123][   55/   55]    Overall Loss 0.002731    Objective Loss 0.002731    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.069421    
2024-05-04 01:15:12,739 - 

2024-05-04 01:15:12,741 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:15:24,445 - Epoch: [122][  200/  217]    Overall Loss 0.001282    Objective Loss 0.001282                                        LR 0.000250    Time 0.257952    
2024-05-04 01:15:27,094 - Epoch: [122][  217/  217]    Overall Loss 0.001317    Objective Loss 0.001317    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.249945    
2024-05-04 01:15:27,322 - 

2024-05-04 01:15:27,322 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:15:57,020 - Epoch: [123][  100/  217]    Overall Loss 0.001571    Objective Loss 0.001571                                        LR 0.000250    Time 0.296899    
2024-05-04 01:16:11,455 - Epoch: [124][   55/   55]    Overall Loss 0.002981    Objective Loss 0.002981    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.067337    
2024-05-04 01:16:11,917 - 

2024-05-04 01:16:11,918 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:16:23,510 - Epoch: [123][  200/  217]    Overall Loss 0.001687    Objective Loss 0.001687                                        LR 0.000250    Time 0.280856    
2024-05-04 01:16:27,695 - Epoch: [123][  217/  217]    Overall Loss 0.001609    Objective Loss 0.001609    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.278131    
2024-05-04 01:16:28,036 - 

2024-05-04 01:16:28,036 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:16:54,787 - Epoch: [124][  100/  217]    Overall Loss 0.001379    Objective Loss 0.001379                                        LR 0.000250    Time 0.267420    
2024-05-04 01:17:08,598 - Epoch: [125][   55/   55]    Overall Loss 0.002831    Objective Loss 0.002831    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.030401    
2024-05-04 01:17:09,040 - 

2024-05-04 01:17:09,041 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:17:18,936 - Epoch: [124][  200/  217]    Overall Loss 0.001186    Objective Loss 0.001186                                        LR 0.000250    Time 0.254413    
2024-05-04 01:17:22,422 - Epoch: [124][  217/  217]    Overall Loss 0.001166    Objective Loss 0.001166    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.250536    
2024-05-04 01:17:22,616 - 

2024-05-04 01:17:22,617 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:17:53,187 - Epoch: [125][  100/  217]    Overall Loss 0.001021    Objective Loss 0.001021                                        LR 0.000250    Time 0.305609    
2024-05-04 01:18:02,876 - Epoch: [126][   55/   55]    Overall Loss 0.002739    Objective Loss 0.002739    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 0.978667    
2024-05-04 01:18:03,230 - 

2024-05-04 01:18:03,231 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:18:21,768 - Epoch: [125][  200/  217]    Overall Loss 0.001000    Objective Loss 0.001000                                        LR 0.000250    Time 0.295665    
2024-05-04 01:18:26,486 - Epoch: [125][  217/  217]    Overall Loss 0.001079    Objective Loss 0.001079    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.294235    
2024-05-04 01:18:26,718 - 

2024-05-04 01:18:26,719 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:18:56,087 - Epoch: [126][  100/  217]    Overall Loss 0.000877    Objective Loss 0.000877                                        LR 0.000250    Time 0.293581    
2024-05-04 01:19:00,294 - Epoch: [127][   55/   55]    Overall Loss 0.002672    Objective Loss 0.002672    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.037329    
2024-05-04 01:19:01,105 - 

2024-05-04 01:19:01,106 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:19:25,325 - Epoch: [126][  200/  217]    Overall Loss 0.003869    Objective Loss 0.003869                                        LR 0.000250    Time 0.292931    
2024-05-04 01:19:30,094 - Epoch: [126][  217/  217]    Overall Loss 0.005768    Objective Loss 0.005768    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.291953    
2024-05-04 01:19:30,304 - 

2024-05-04 01:19:30,304 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:19:58,680 - Epoch: [127][  100/  217]    Overall Loss 0.015333    Objective Loss 0.015333                                        LR 0.000250    Time 0.283659    
2024-05-04 01:20:00,069 - Epoch: [128][   55/   55]    Overall Loss 0.003116    Objective Loss 0.003116    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.071883    
2024-05-04 01:20:00,986 - 

2024-05-04 01:20:00,988 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:20:24,468 - Epoch: [127][  200/  217]    Overall Loss 0.011473    Objective Loss 0.011473                                        LR 0.000250    Time 0.270723    
2024-05-04 01:20:28,211 - Epoch: [127][  217/  217]    Overall Loss 0.010935    Objective Loss 0.010935    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.266752    
2024-05-04 01:20:28,677 - 

2024-05-04 01:20:28,678 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:20:56,719 - Epoch: [128][  100/  217]    Overall Loss 0.003820    Objective Loss 0.003820                                        LR 0.000250    Time 0.280325    
2024-05-04 01:21:01,128 - Epoch: [129][   55/   55]    Overall Loss 0.003174    Objective Loss 0.003174    Top1 99.363057    Top5 100.000000    LR 0.023500    Time 1.093288    
2024-05-04 01:21:01,658 - 

2024-05-04 01:21:01,659 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:21:22,224 - Epoch: [128][  200/  217]    Overall Loss 0.003792    Objective Loss 0.003792                                        LR 0.000250    Time 0.267639    
2024-05-04 01:21:27,389 - Epoch: [128][  217/  217]    Overall Loss 0.003768    Objective Loss 0.003768    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.270465    
2024-05-04 01:21:27,772 - 

2024-05-04 01:21:27,772 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:21:57,228 - Epoch: [129][  100/  217]    Overall Loss 0.002068    Objective Loss 0.002068                                        LR 0.000250    Time 0.294466    
2024-05-04 01:22:00,512 - Epoch: [130][   55/   55]    Overall Loss 0.002893    Objective Loss 0.002893    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.069855    
2024-05-04 01:22:00,857 - --- validate (epoch=130)-----------
2024-05-04 01:22:00,857 - 1736 samples (128 per mini-batch)
2024-05-04 01:22:17,292 - Epoch: [130][   14/   14]    Loss 3.302859    Top1 51.440092    Top5 67.914747    
2024-05-04 01:22:18,062 - ==> Top1: 51.440    Top5: 67.915    Loss: 3.303

2024-05-04 01:22:18,096 - ==> Best [Top1: 51.498   Top5: 67.569   Sparsity:0.00   Params: 1342920 on epoch: 120]
2024-05-04 01:22:18,096 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 01:22:18,208 - 

2024-05-04 01:22:18,209 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:22:19,181 - Epoch: [129][  200/  217]    Overall Loss 0.002004    Objective Loss 0.002004                                        LR 0.000250    Time 0.256962    
2024-05-04 01:22:22,720 - Epoch: [129][  217/  217]    Overall Loss 0.001950    Objective Loss 0.001950    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.253129    
2024-05-04 01:22:22,929 - 

2024-05-04 01:22:22,930 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:22:53,924 - Epoch: [130][  100/  217]    Overall Loss 0.001186    Objective Loss 0.001186                                        LR 0.000250    Time 0.309852    
2024-05-04 01:23:14,560 - Epoch: [131][   55/   55]    Overall Loss 0.002793    Objective Loss 0.002793    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.024353    
2024-05-04 01:23:14,794 - 

2024-05-04 01:23:14,795 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:23:18,792 - Epoch: [130][  200/  217]    Overall Loss 0.001434    Objective Loss 0.001434                                        LR 0.000250    Time 0.279233    
2024-05-04 01:23:24,021 - Epoch: [130][  217/  217]    Overall Loss 0.001488    Objective Loss 0.001488    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.281443    
2024-05-04 01:23:24,342 - --- validate (epoch=130)-----------
2024-05-04 01:23:24,342 - 1736 samples (32 per mini-batch)
2024-05-04 01:23:41,646 - Epoch: [130][   55/   55]    Loss 2.902475    Top1 59.216590    Top5 75.057604    
2024-05-04 01:23:42,193 - ==> Top1: 59.217    Top5: 75.058    Loss: 2.902

2024-05-04 01:23:42,201 - ==> Best [Top1: 59.332   Top5: 74.827   Sparsity:0.00   Params: 384080 on epoch: 100]
2024-05-04 01:23:42,201 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 01:23:42,246 - 

2024-05-04 01:23:42,247 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:24:09,511 - Epoch: [131][  100/  217]    Overall Loss 0.001599    Objective Loss 0.001599                                        LR 0.000250    Time 0.272548    
2024-05-04 01:24:16,785 - Epoch: [132][   55/   55]    Overall Loss 0.002730    Objective Loss 0.002730    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.126936    
2024-05-04 01:24:17,442 - 

2024-05-04 01:24:17,443 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:24:38,026 - Epoch: [131][  200/  217]    Overall Loss 0.001326    Objective Loss 0.001326                                        LR 0.000250    Time 0.278804    
2024-05-04 01:24:41,466 - Epoch: [131][  217/  217]    Overall Loss 0.001461    Objective Loss 0.001461    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.272807    
2024-05-04 01:24:41,696 - 

2024-05-04 01:24:41,697 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:25:08,191 - Epoch: [132][  100/  217]    Overall Loss 0.001175    Objective Loss 0.001175                                        LR 0.000250    Time 0.264858    
2024-05-04 01:25:11,824 - Epoch: [133][   55/   55]    Overall Loss 0.002974    Objective Loss 0.002974    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 0.988575    
2024-05-04 01:25:12,285 - 

2024-05-04 01:25:12,286 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:25:34,127 - Epoch: [132][  200/  217]    Overall Loss 0.001166    Objective Loss 0.001166                                        LR 0.000250    Time 0.262063    
2024-05-04 01:25:38,451 - Epoch: [132][  217/  217]    Overall Loss 0.001292    Objective Loss 0.001292    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.261449    
2024-05-04 01:25:38,777 - 

2024-05-04 01:25:38,778 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:26:08,469 - Epoch: [133][  100/  217]    Overall Loss 0.001446    Objective Loss 0.001446                                        LR 0.000250    Time 0.296824    
2024-05-04 01:26:18,499 - Epoch: [134][   55/   55]    Overall Loss 0.002679    Objective Loss 0.002679    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.203717    
2024-05-04 01:26:18,834 - 

2024-05-04 01:26:18,836 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:26:34,202 - Epoch: [133][  200/  217]    Overall Loss 0.001053    Objective Loss 0.001053                                        LR 0.000250    Time 0.277033    
2024-05-04 01:26:39,439 - Epoch: [133][  217/  217]    Overall Loss 0.001128    Objective Loss 0.001128    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.279454    
2024-05-04 01:26:40,529 - 

2024-05-04 01:26:40,530 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:27:09,316 - Epoch: [134][  100/  217]    Overall Loss 0.001137    Objective Loss 0.001137                                        LR 0.000250    Time 0.287770    
2024-05-04 01:27:12,489 - Epoch: [135][   55/   55]    Overall Loss 0.002866    Objective Loss 0.002866    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 0.975301    
2024-05-04 01:27:12,824 - 

2024-05-04 01:27:12,824 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:27:36,104 - Epoch: [134][  200/  217]    Overall Loss 0.001156    Objective Loss 0.001156                                        LR 0.000250    Time 0.277780    
2024-05-04 01:27:40,126 - Epoch: [134][  217/  217]    Overall Loss 0.001112    Objective Loss 0.001112    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.274548    
2024-05-04 01:27:40,586 - 

2024-05-04 01:27:40,586 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:28:09,493 - Epoch: [135][  100/  217]    Overall Loss 0.000689    Objective Loss 0.000689                                        LR 0.000250    Time 0.288975    
2024-05-04 01:28:12,939 - Epoch: [136][   55/   55]    Overall Loss 0.002781    Objective Loss 0.002781    Top1 99.363057    Top5 100.000000    LR 0.023500    Time 1.092817    
2024-05-04 01:28:13,221 - 

2024-05-04 01:28:13,222 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:28:37,289 - Epoch: [135][  200/  217]    Overall Loss 0.000940    Objective Loss 0.000940                                        LR 0.000250    Time 0.283420    
2024-05-04 01:28:40,856 - Epoch: [135][  217/  217]    Overall Loss 0.001070    Objective Loss 0.001070    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.277643    
2024-05-04 01:28:41,433 - 

2024-05-04 01:28:41,433 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:29:07,205 - Epoch: [137][   55/   55]    Overall Loss 0.002698    Objective Loss 0.002698    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 0.981312    
2024-05-04 01:29:08,577 - 

2024-05-04 01:29:08,578 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:29:11,698 - Epoch: [136][  100/  217]    Overall Loss 0.000562    Objective Loss 0.000562                                        LR 0.000250    Time 0.302549    
2024-05-04 01:29:40,391 - Epoch: [136][  200/  217]    Overall Loss 0.000762    Objective Loss 0.000762                                        LR 0.000250    Time 0.294665    
2024-05-04 01:29:44,894 - Epoch: [136][  217/  217]    Overall Loss 0.000932    Objective Loss 0.000932    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.292324    
2024-05-04 01:29:45,444 - 

2024-05-04 01:29:45,445 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:30:09,506 - Epoch: [138][   55/   55]    Overall Loss 0.002777    Objective Loss 0.002777    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.107622    
2024-05-04 01:30:09,950 - 

2024-05-04 01:30:09,951 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:30:14,872 - Epoch: [137][  100/  217]    Overall Loss 0.000666    Objective Loss 0.000666                                        LR 0.000250    Time 0.294177    
2024-05-04 01:30:41,299 - Epoch: [137][  200/  217]    Overall Loss 0.000818    Objective Loss 0.000818                                        LR 0.000250    Time 0.279148    
2024-05-04 01:30:46,909 - Epoch: [137][  217/  217]    Overall Loss 0.000903    Objective Loss 0.000903    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.283123    
2024-05-04 01:30:47,368 - 

2024-05-04 01:30:47,369 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:31:06,793 - Epoch: [139][   55/   55]    Overall Loss 0.002845    Objective Loss 0.002845    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.033307    
2024-05-04 01:31:07,330 - 

2024-05-04 01:31:07,331 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:31:16,310 - Epoch: [138][  100/  217]    Overall Loss 0.000438    Objective Loss 0.000438                                        LR 0.000250    Time 0.289300    
2024-05-04 01:31:44,498 - Epoch: [138][  200/  217]    Overall Loss 0.000795    Objective Loss 0.000795                                        LR 0.000250    Time 0.285546    
2024-05-04 01:31:47,305 - Epoch: [138][  217/  217]    Overall Loss 0.000883    Objective Loss 0.000883    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.276105    
2024-05-04 01:31:47,609 - 

2024-05-04 01:31:47,609 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:32:11,107 - Epoch: [140][   55/   55]    Overall Loss 0.003114    Objective Loss 0.003114    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.159336    
2024-05-04 01:32:11,570 - --- validate (epoch=140)-----------
2024-05-04 01:32:11,570 - 1736 samples (128 per mini-batch)
2024-05-04 01:32:13,881 - Epoch: [139][  100/  217]    Overall Loss 0.000599    Objective Loss 0.000599                                        LR 0.000250    Time 0.262628    
2024-05-04 01:32:31,264 - Epoch: [140][   14/   14]    Loss 3.253587    Top1 50.979263    Top5 67.914747    
2024-05-04 01:32:31,537 - ==> Top1: 50.979    Top5: 67.915    Loss: 3.254

2024-05-04 01:32:31,550 - ==> Best [Top1: 51.498   Top5: 67.569   Sparsity:0.00   Params: 1342920 on epoch: 120]
2024-05-04 01:32:31,551 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 01:32:31,662 - 

2024-05-04 01:32:31,663 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:32:39,830 - Epoch: [139][  200/  217]    Overall Loss 0.000860    Objective Loss 0.000860                                        LR 0.000250    Time 0.261013    
2024-05-04 01:32:44,976 - Epoch: [139][  217/  217]    Overall Loss 0.000817    Objective Loss 0.000817    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.264268    
2024-05-04 01:32:45,280 - 

2024-05-04 01:32:45,281 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:33:15,333 - Epoch: [140][  100/  217]    Overall Loss 0.000798    Objective Loss 0.000798                                        LR 0.000250    Time 0.300423    
2024-05-04 01:33:27,758 - Epoch: [141][   55/   55]    Overall Loss 0.003008    Objective Loss 0.003008    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.019703    
2024-05-04 01:33:28,061 - 

2024-05-04 01:33:28,061 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:33:42,498 - Epoch: [140][  200/  217]    Overall Loss 0.000806    Objective Loss 0.000806                                        LR 0.000250    Time 0.285995    
2024-05-04 01:33:45,502 - Epoch: [140][  217/  217]    Overall Loss 0.000778    Objective Loss 0.000778    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.277423    
2024-05-04 01:33:45,966 - --- validate (epoch=140)-----------
2024-05-04 01:33:45,967 - 1736 samples (32 per mini-batch)
2024-05-04 01:34:00,040 - Epoch: [140][   55/   55]    Loss 2.899503    Top1 59.792627    Top5 74.423963    
2024-05-04 01:34:00,268 - ==> Top1: 59.793    Top5: 74.424    Loss: 2.900

2024-05-04 01:34:00,271 - ==> Best [Top1: 59.793   Top5: 74.424   Sparsity:0.00   Params: 384080 on epoch: 140]
2024-05-04 01:34:00,271 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 01:34:00,311 - 

2024-05-04 01:34:00,311 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:34:19,676 - Epoch: [142][   55/   55]    Overall Loss 0.002739    Objective Loss 0.002739    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 0.938293    
2024-05-04 01:34:19,919 - 

2024-05-04 01:34:19,920 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:34:30,962 - Epoch: [141][  100/  217]    Overall Loss 0.000752    Objective Loss 0.000752                                        LR 0.000250    Time 0.306412    
2024-05-04 01:34:59,364 - Epoch: [141][  200/  217]    Overall Loss 0.000888    Objective Loss 0.000888                                        LR 0.000250    Time 0.295160    
2024-05-04 01:35:02,895 - Epoch: [141][  217/  217]    Overall Loss 0.000875    Objective Loss 0.000875    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.288303    
2024-05-04 01:35:03,133 - 

2024-05-04 01:35:03,134 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:35:19,786 - Epoch: [143][   55/   55]    Overall Loss 0.003020    Objective Loss 0.003020    Top1 99.363057    Top5 100.000000    LR 0.023500    Time 1.088325    
2024-05-04 01:35:20,108 - 

2024-05-04 01:35:20,108 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:35:29,570 - Epoch: [142][  100/  217]    Overall Loss 0.000669    Objective Loss 0.000669                                        LR 0.000250    Time 0.264263    
2024-05-04 01:35:57,351 - Epoch: [142][  200/  217]    Overall Loss 0.000649    Objective Loss 0.000649                                        LR 0.000250    Time 0.270997    
2024-05-04 01:36:00,811 - Epoch: [142][  217/  217]    Overall Loss 0.000894    Objective Loss 0.000894    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.265699    
2024-05-04 01:36:01,129 - 

2024-05-04 01:36:01,130 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:36:16,833 - Epoch: [144][   55/   55]    Overall Loss 0.002824    Objective Loss 0.002824    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.031189    
2024-05-04 01:36:17,183 - 

2024-05-04 01:36:17,184 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:36:29,193 - Epoch: [143][  100/  217]    Overall Loss 0.000617    Objective Loss 0.000617                                        LR 0.000250    Time 0.280530    
2024-05-04 01:36:56,410 - Epoch: [143][  200/  217]    Overall Loss 0.000846    Objective Loss 0.000846                                        LR 0.000250    Time 0.276305    
2024-05-04 01:37:01,978 - Epoch: [143][  217/  217]    Overall Loss 0.000804    Objective Loss 0.000804    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.280310    
2024-05-04 01:37:02,510 - 

2024-05-04 01:37:02,510 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:37:21,414 - Epoch: [145][   55/   55]    Overall Loss 0.002843    Objective Loss 0.002843    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.167645    
2024-05-04 01:37:21,686 - 

2024-05-04 01:37:21,687 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:37:31,721 - Epoch: [144][  100/  217]    Overall Loss 0.001208    Objective Loss 0.001208                                        LR 0.000250    Time 0.291997    
2024-05-04 01:38:01,231 - Epoch: [144][  200/  217]    Overall Loss 0.000780    Objective Loss 0.000780                                        LR 0.000250    Time 0.293503    
2024-05-04 01:38:05,459 - Epoch: [144][  217/  217]    Overall Loss 0.000876    Objective Loss 0.000876    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.289983    
2024-05-04 01:38:05,940 - 

2024-05-04 01:38:05,941 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:38:23,955 - Epoch: [146][   55/   55]    Overall Loss 0.002837    Objective Loss 0.002837    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.131971    
2024-05-04 01:38:24,246 - 

2024-05-04 01:38:24,247 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:38:37,271 - Epoch: [145][  100/  217]    Overall Loss 0.001056    Objective Loss 0.001056                                        LR 0.000250    Time 0.313206    
2024-05-04 01:39:02,686 - Epoch: [145][  200/  217]    Overall Loss 0.000850    Objective Loss 0.000850                                        LR 0.000250    Time 0.283634    
2024-05-04 01:39:07,446 - Epoch: [145][  217/  217]    Overall Loss 0.000812    Objective Loss 0.000812    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.283339    
2024-05-04 01:39:07,746 - 

2024-05-04 01:39:07,748 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:39:22,997 - Epoch: [147][   55/   55]    Overall Loss 0.002927    Objective Loss 0.002927    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.068007    
2024-05-04 01:39:23,346 - 

2024-05-04 01:39:23,348 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:39:39,794 - Epoch: [146][  100/  217]    Overall Loss 0.000788    Objective Loss 0.000788                                        LR 0.000250    Time 0.320372    
2024-05-04 01:40:04,120 - Epoch: [146][  200/  217]    Overall Loss 0.000676    Objective Loss 0.000676                                        LR 0.000250    Time 0.281768    
2024-05-04 01:40:07,779 - Epoch: [146][  217/  217]    Overall Loss 0.000815    Objective Loss 0.000815    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.276549    
2024-05-04 01:40:08,464 - 

2024-05-04 01:40:08,464 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:40:16,117 - Epoch: [148][   55/   55]    Overall Loss 0.002995    Objective Loss 0.002995    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 0.959260    
2024-05-04 01:40:16,608 - 

2024-05-04 01:40:16,609 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:40:39,993 - Epoch: [147][  100/  217]    Overall Loss 0.000910    Objective Loss 0.000910                                        LR 0.000250    Time 0.315194    
2024-05-04 01:41:05,478 - Epoch: [147][  200/  217]    Overall Loss 0.000879    Objective Loss 0.000879                                        LR 0.000250    Time 0.284977    
2024-05-04 01:41:10,638 - Epoch: [147][  217/  217]    Overall Loss 0.000856    Objective Loss 0.000856    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.286424    
2024-05-04 01:41:10,943 - 

2024-05-04 01:41:10,943 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:41:19,637 - Epoch: [149][   55/   55]    Overall Loss 0.002785    Objective Loss 0.002785    Top1 100.000000    Top5 100.000000    LR 0.023500    Time 1.145800    
2024-05-04 01:41:20,521 - 

2024-05-04 01:41:20,523 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:41:45,718 - Epoch: [148][  100/  217]    Overall Loss 0.007008    Objective Loss 0.007008                                        LR 0.000250    Time 0.347644    
2024-05-04 01:42:09,992 - Epoch: [148][  200/  217]    Overall Loss 0.013169    Objective Loss 0.013169                                        LR 0.000250    Time 0.295148    
2024-05-04 01:42:15,207 - Epoch: [148][  217/  217]    Overall Loss 0.013442    Objective Loss 0.013442    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.296052    
2024-05-04 01:42:15,554 - 

2024-05-04 01:42:15,555 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:42:16,565 - Epoch: [150][   55/   55]    Overall Loss 0.002625    Objective Loss 0.002625    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.018755    
2024-05-04 01:42:17,063 - --- validate (epoch=150)-----------
2024-05-04 01:42:17,064 - 1736 samples (128 per mini-batch)
2024-05-04 01:42:39,692 - Epoch: [150][   14/   14]    Loss 3.229235    Top1 51.152074    Top5 68.029954    
2024-05-04 01:42:40,069 - ==> Top1: 51.152    Top5: 68.030    Loss: 3.229

2024-05-04 01:42:40,089 - ==> Best [Top1: 51.498   Top5: 67.569   Sparsity:0.00   Params: 1342920 on epoch: 120]
2024-05-04 01:42:40,090 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 01:42:40,197 - 

2024-05-04 01:42:40,198 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:42:45,312 - Epoch: [149][  100/  217]    Overall Loss 0.013602    Objective Loss 0.013602                                        LR 0.000250    Time 0.297476    
2024-05-04 01:43:13,783 - Epoch: [149][  200/  217]    Overall Loss 0.009634    Objective Loss 0.009634                                        LR 0.000250    Time 0.291051    
2024-05-04 01:43:19,117 - Epoch: [149][  217/  217]    Overall Loss 0.009054    Objective Loss 0.009054    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.292818    
2024-05-04 01:43:19,429 - 

2024-05-04 01:43:19,429 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:43:33,566 - Epoch: [151][   55/   55]    Overall Loss 0.002820    Objective Loss 0.002820    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 0.970176    
2024-05-04 01:43:33,926 - 

2024-05-04 01:43:33,927 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:43:49,957 - Epoch: [150][  100/  217]    Overall Loss 0.002726    Objective Loss 0.002726                                        LR 0.000063    Time 0.305180    
2024-05-04 01:44:15,711 - Epoch: [150][  200/  217]    Overall Loss 0.002697    Objective Loss 0.002697                                        LR 0.000063    Time 0.281316    
2024-05-04 01:44:19,509 - Epoch: [150][  217/  217]    Overall Loss 0.002630    Objective Loss 0.002630    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.276769    
2024-05-04 01:44:19,878 - --- validate (epoch=150)-----------
2024-05-04 01:44:19,878 - 1736 samples (32 per mini-batch)
2024-05-04 01:44:34,716 - Epoch: [152][   55/   55]    Overall Loss 0.002780    Objective Loss 0.002780    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.105085    
2024-05-04 01:44:35,258 - 

2024-05-04 01:44:35,259 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:44:38,398 - Epoch: [150][   55/   55]    Loss 3.129711    Top1 57.200461    Top5 73.041475    
2024-05-04 01:44:38,800 - ==> Top1: 57.200    Top5: 73.041    Loss: 3.130

2024-05-04 01:44:38,805 - ==> Best [Top1: 59.793   Top5: 74.424   Sparsity:0.00   Params: 384080 on epoch: 140]
2024-05-04 01:44:38,805 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 01:44:38,851 - 

2024-05-04 01:44:38,852 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:45:07,308 - Epoch: [151][  100/  217]    Overall Loss 0.002380    Objective Loss 0.002380                                        LR 0.000063    Time 0.284468    
2024-05-04 01:45:33,936 - Epoch: [151][  200/  217]    Overall Loss 0.002046    Objective Loss 0.002046                                        LR 0.000063    Time 0.275333    
2024-05-04 01:45:36,012 - Epoch: [153][   55/   55]    Overall Loss 0.003154    Objective Loss 0.003154    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.104405    
2024-05-04 01:45:36,332 - 

2024-05-04 01:45:36,334 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:45:36,539 - Epoch: [151][  217/  217]    Overall Loss 0.002084    Objective Loss 0.002084    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.265747    
2024-05-04 01:45:36,925 - 

2024-05-04 01:45:36,926 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:46:03,741 - Epoch: [152][  100/  217]    Overall Loss 0.001346    Objective Loss 0.001346                                        LR 0.000063    Time 0.268045    
2024-05-04 01:46:30,884 - Epoch: [152][  200/  217]    Overall Loss 0.001781    Objective Loss 0.001781                                        LR 0.000063    Time 0.269694    
2024-05-04 01:46:34,881 - Epoch: [154][   55/   55]    Overall Loss 0.002651    Objective Loss 0.002651    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.064299    
2024-05-04 01:46:35,319 - 

2024-05-04 01:46:35,320 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:46:36,236 - Epoch: [152][  217/  217]    Overall Loss 0.001905    Objective Loss 0.001905    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.273223    
2024-05-04 01:46:36,850 - 

2024-05-04 01:46:36,851 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:47:06,708 - Epoch: [153][  100/  217]    Overall Loss 0.001580    Objective Loss 0.001580                                        LR 0.000063    Time 0.298475    
2024-05-04 01:47:27,811 - Epoch: [155][   55/   55]    Overall Loss 0.002725    Objective Loss 0.002725    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 0.954159    
2024-05-04 01:47:28,240 - 

2024-05-04 01:47:28,240 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:47:33,692 - Epoch: [153][  200/  217]    Overall Loss 0.001721    Objective Loss 0.001721                                        LR 0.000063    Time 0.284115    
2024-05-04 01:47:39,031 - Epoch: [153][  217/  217]    Overall Loss 0.001663    Objective Loss 0.001663    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.286450    
2024-05-04 01:47:39,330 - 

2024-05-04 01:47:39,330 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:48:10,353 - Epoch: [154][  100/  217]    Overall Loss 0.001723    Objective Loss 0.001723                                        LR 0.000063    Time 0.310131    
2024-05-04 01:48:23,956 - Epoch: [156][   55/   55]    Overall Loss 0.002695    Objective Loss 0.002695    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.012785    
2024-05-04 01:48:24,248 - 

2024-05-04 01:48:24,249 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:48:32,828 - Epoch: [154][  200/  217]    Overall Loss 0.001594    Objective Loss 0.001594                                        LR 0.000063    Time 0.267396    
2024-05-04 01:48:36,991 - Epoch: [154][  217/  217]    Overall Loss 0.001685    Objective Loss 0.001685    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.265625    
2024-05-04 01:48:37,380 - 

2024-05-04 01:48:37,381 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:49:06,795 - Epoch: [155][  100/  217]    Overall Loss 0.001317    Objective Loss 0.001317                                        LR 0.000063    Time 0.294050    
2024-05-04 01:49:30,747 - Epoch: [157][   55/   55]    Overall Loss 0.002812    Objective Loss 0.002812    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.208884    
2024-05-04 01:49:31,503 - 

2024-05-04 01:49:31,504 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:49:32,121 - Epoch: [155][  200/  217]    Overall Loss 0.001502    Objective Loss 0.001502                                        LR 0.000063    Time 0.273609    
2024-05-04 01:49:35,034 - Epoch: [155][  217/  217]    Overall Loss 0.001477    Objective Loss 0.001477    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.265573    
2024-05-04 01:49:35,417 - 

2024-05-04 01:49:35,418 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:50:03,698 - Epoch: [156][  100/  217]    Overall Loss 0.001498    Objective Loss 0.001498                                        LR 0.000063    Time 0.282708    
2024-05-04 01:50:27,982 - Epoch: [158][   55/   55]    Overall Loss 0.002725    Objective Loss 0.002725    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.026697    
2024-05-04 01:50:28,346 - 

2024-05-04 01:50:28,346 - Epoch: [156][  200/  217]    Overall Loss 0.001294    Objective Loss 0.001294                                        LR 0.000063    Time 0.264550    
2024-05-04 01:50:28,346 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:50:31,969 - Epoch: [156][  217/  217]    Overall Loss 0.001309    Objective Loss 0.001309    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.260515    
2024-05-04 01:50:32,244 - 

2024-05-04 01:50:32,245 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:51:01,814 - Epoch: [157][  100/  217]    Overall Loss 0.001292    Objective Loss 0.001292                                        LR 0.000063    Time 0.295606    
2024-05-04 01:51:24,003 - Epoch: [159][   55/   55]    Overall Loss 0.002837    Objective Loss 0.002837    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.011749    
2024-05-04 01:51:24,523 - 

2024-05-04 01:51:24,525 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:51:26,687 - Epoch: [157][  200/  217]    Overall Loss 0.001260    Objective Loss 0.001260                                        LR 0.000063    Time 0.272119    
2024-05-04 01:51:30,073 - Epoch: [157][  217/  217]    Overall Loss 0.001208    Objective Loss 0.001208    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.266392    
2024-05-04 01:51:30,546 - 

2024-05-04 01:51:30,547 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:52:01,704 - Epoch: [158][  100/  217]    Overall Loss 0.001621    Objective Loss 0.001621                                        LR 0.000063    Time 0.311460    
2024-05-04 01:52:21,834 - Epoch: [158][  200/  217]    Overall Loss 0.001141    Objective Loss 0.001141                                        LR 0.000063    Time 0.256332    
2024-05-04 01:52:22,374 - Epoch: [160][   55/   55]    Overall Loss 0.003422    Objective Loss 0.003422    Top1 99.363057    Top5 100.000000    LR 0.005522    Time 1.051623    
2024-05-04 01:52:22,825 - --- validate (epoch=160)-----------
2024-05-04 01:52:22,826 - 1736 samples (128 per mini-batch)
2024-05-04 01:52:25,589 - Epoch: [158][  217/  217]    Overall Loss 0.001244    Objective Loss 0.001244    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.253549    
2024-05-04 01:52:25,984 - 

2024-05-04 01:52:25,984 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:52:45,474 - Epoch: [160][   14/   14]    Loss 3.219824    Top1 50.921659    Top5 68.087558    
2024-05-04 01:52:46,002 - ==> Top1: 50.922    Top5: 68.088    Loss: 3.220

2024-05-04 01:52:46,015 - ==> Best [Top1: 51.498   Top5: 67.569   Sparsity:0.00   Params: 1342920 on epoch: 120]
2024-05-04 01:52:46,015 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 01:52:46,124 - 

2024-05-04 01:52:46,125 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:52:58,178 - Epoch: [159][  100/  217]    Overall Loss 0.000734    Objective Loss 0.000734                                        LR 0.000063    Time 0.321849    
2024-05-04 01:53:23,172 - Epoch: [159][  200/  217]    Overall Loss 0.001175    Objective Loss 0.001175                                        LR 0.000063    Time 0.285848    
2024-05-04 01:53:28,283 - Epoch: [159][  217/  217]    Overall Loss 0.001135    Objective Loss 0.001135    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.286998    
2024-05-04 01:53:28,870 - 

2024-05-04 01:53:28,870 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:53:42,468 - Epoch: [161][   55/   55]    Overall Loss 0.002775    Objective Loss 0.002775    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.024225    
2024-05-04 01:53:43,205 - 

2024-05-04 01:53:43,206 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:54:01,241 - Epoch: [160][  100/  217]    Overall Loss 0.001061    Objective Loss 0.001061                                        LR 0.000063    Time 0.323610    
2024-05-04 01:54:27,311 - Epoch: [160][  200/  217]    Overall Loss 0.001063    Objective Loss 0.001063                                        LR 0.000063    Time 0.292104    
2024-05-04 01:54:31,555 - Epoch: [160][  217/  217]    Overall Loss 0.001157    Objective Loss 0.001157    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.288771    
2024-05-04 01:54:31,961 - --- validate (epoch=160)-----------
2024-05-04 01:54:31,962 - 1736 samples (32 per mini-batch)
2024-05-04 01:54:42,019 - Epoch: [162][   55/   55]    Overall Loss 0.002855    Objective Loss 0.002855    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.069149    
2024-05-04 01:54:42,447 - 

2024-05-04 01:54:42,448 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:54:46,649 - Epoch: [160][   55/   55]    Loss 3.076873    Top1 58.237327    Top5 74.020737    
2024-05-04 01:54:46,873 - ==> Top1: 58.237    Top5: 74.021    Loss: 3.077

2024-05-04 01:54:46,880 - ==> Best [Top1: 59.793   Top5: 74.424   Sparsity:0.00   Params: 384080 on epoch: 140]
2024-05-04 01:54:46,880 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 01:54:46,925 - 

2024-05-04 01:54:46,925 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:55:15,009 - Epoch: [161][  100/  217]    Overall Loss 0.000534    Objective Loss 0.000534                                        LR 0.000063    Time 0.280738    
2024-05-04 01:55:42,163 - Epoch: [161][  200/  217]    Overall Loss 0.001034    Objective Loss 0.001034                                        LR 0.000063    Time 0.276097    
2024-05-04 01:55:45,271 - Epoch: [163][   55/   55]    Overall Loss 0.002836    Objective Loss 0.002836    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.142038    
2024-05-04 01:55:45,934 - 

2024-05-04 01:55:45,934 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:55:46,150 - Epoch: [161][  217/  217]    Overall Loss 0.000996    Objective Loss 0.000996    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.272827    
2024-05-04 01:55:46,399 - 

2024-05-04 01:55:46,399 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:56:14,357 - Epoch: [162][  100/  217]    Overall Loss 0.000775    Objective Loss 0.000775                                        LR 0.000063    Time 0.279482    
2024-05-04 01:56:37,723 - Epoch: [162][  200/  217]    Overall Loss 0.001112    Objective Loss 0.001112                                        LR 0.000063    Time 0.256525    
2024-05-04 01:56:40,575 - Epoch: [162][  217/  217]    Overall Loss 0.001068    Objective Loss 0.001068    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.249569    
2024-05-04 01:56:40,832 - 

2024-05-04 01:56:40,833 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:56:40,944 - Epoch: [164][   55/   55]    Overall Loss 0.002863    Objective Loss 0.002863    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.000042    
2024-05-04 01:56:41,964 - 

2024-05-04 01:56:41,965 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:57:09,757 - Epoch: [163][  100/  217]    Overall Loss 0.001353    Objective Loss 0.001353                                        LR 0.000063    Time 0.289140    
2024-05-04 01:57:35,209 - Epoch: [163][  200/  217]    Overall Loss 0.001128    Objective Loss 0.001128                                        LR 0.000063    Time 0.271779    
2024-05-04 01:57:38,892 - Epoch: [163][  217/  217]    Overall Loss 0.001079    Objective Loss 0.001079    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.267449    
2024-05-04 01:57:39,186 - 

2024-05-04 01:57:39,186 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:57:45,342 - Epoch: [165][   55/   55]    Overall Loss 0.002814    Objective Loss 0.002814    Top1 99.363057    Top5 100.000000    LR 0.005522    Time 1.152116    
2024-05-04 01:57:45,907 - 

2024-05-04 01:57:45,908 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:58:06,861 - Epoch: [164][  100/  217]    Overall Loss 0.000572    Objective Loss 0.000572                                        LR 0.000063    Time 0.276655    
2024-05-04 01:58:34,988 - Epoch: [164][  200/  217]    Overall Loss 0.000982    Objective Loss 0.000982                                        LR 0.000063    Time 0.278916    
2024-05-04 01:58:39,000 - Epoch: [164][  217/  217]    Overall Loss 0.000954    Objective Loss 0.000954    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.275543    
2024-05-04 01:58:39,219 - 

2024-05-04 01:58:39,219 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:58:54,518 - Epoch: [166][   55/   55]    Overall Loss 0.002705    Objective Loss 0.002705    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.247287    
2024-05-04 01:58:55,510 - 

2024-05-04 01:58:55,511 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 01:59:07,821 - Epoch: [165][  100/  217]    Overall Loss 0.000680    Objective Loss 0.000680                                        LR 0.000063    Time 0.285914    
2024-05-04 01:59:38,453 - Epoch: [165][  200/  217]    Overall Loss 0.000817    Objective Loss 0.000817                                        LR 0.000063    Time 0.296071    
2024-05-04 01:59:41,675 - Epoch: [165][  217/  217]    Overall Loss 0.000917    Objective Loss 0.000917    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.287718    
2024-05-04 01:59:41,916 - 

2024-05-04 01:59:41,916 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:59:52,377 - Epoch: [167][   55/   55]    Overall Loss 0.002728    Objective Loss 0.002728    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.033729    
2024-05-04 01:59:52,710 - 

2024-05-04 01:59:52,711 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:00:10,329 - Epoch: [166][  100/  217]    Overall Loss 0.001173    Objective Loss 0.001173                                        LR 0.000063    Time 0.284039    
2024-05-04 02:00:38,238 - Epoch: [166][  200/  217]    Overall Loss 0.000913    Objective Loss 0.000913                                        LR 0.000063    Time 0.281526    
2024-05-04 02:00:43,520 - Epoch: [166][  217/  217]    Overall Loss 0.000877    Objective Loss 0.000877    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.283801    
2024-05-04 02:00:43,857 - 

2024-05-04 02:00:43,857 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:00:51,081 - Epoch: [168][   55/   55]    Overall Loss 0.002978    Objective Loss 0.002978    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.061097    
2024-05-04 02:00:51,335 - 

2024-05-04 02:00:51,336 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:01:12,897 - Epoch: [167][  100/  217]    Overall Loss 0.000601    Objective Loss 0.000601                                        LR 0.000063    Time 0.290313    
2024-05-04 02:01:37,206 - Epoch: [167][  200/  217]    Overall Loss 0.000763    Objective Loss 0.000763                                        LR 0.000063    Time 0.266658    
2024-05-04 02:01:41,324 - Epoch: [167][  217/  217]    Overall Loss 0.000881    Objective Loss 0.000881    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.264740    
2024-05-04 02:01:41,660 - 

2024-05-04 02:01:41,662 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:01:50,347 - Epoch: [169][   55/   55]    Overall Loss 0.002796    Objective Loss 0.002796    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.072746    
2024-05-04 02:01:50,617 - 

2024-05-04 02:01:50,617 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:02:10,200 - Epoch: [168][  100/  217]    Overall Loss 0.001092    Objective Loss 0.001092                                        LR 0.000063    Time 0.285284    
2024-05-04 02:02:35,329 - Epoch: [168][  200/  217]    Overall Loss 0.000851    Objective Loss 0.000851                                        LR 0.000063    Time 0.268246    
2024-05-04 02:02:39,037 - Epoch: [168][  217/  217]    Overall Loss 0.000835    Objective Loss 0.000835    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.264309    
2024-05-04 02:02:39,430 - 

2024-05-04 02:02:39,430 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:02:57,604 - Epoch: [170][   55/   55]    Overall Loss 0.002645    Objective Loss 0.002645    Top1 99.363057    Top5 100.000000    LR 0.005522    Time 1.217784    
2024-05-04 02:02:57,877 - --- validate (epoch=170)-----------
2024-05-04 02:02:57,878 - 1736 samples (128 per mini-batch)
2024-05-04 02:03:09,986 - Epoch: [169][  100/  217]    Overall Loss 0.000374    Objective Loss 0.000374                                        LR 0.000063    Time 0.305455    
2024-05-04 02:03:16,558 - Epoch: [170][   14/   14]    Loss 3.241336    Top1 51.036866    Top5 68.029954    
2024-05-04 02:03:16,774 - ==> Top1: 51.037    Top5: 68.030    Loss: 3.241

2024-05-04 02:03:16,785 - ==> Best [Top1: 51.498   Top5: 67.569   Sparsity:0.00   Params: 1342920 on epoch: 120]
2024-05-04 02:03:16,785 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 02:03:16,857 - 

2024-05-04 02:03:16,857 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:03:38,039 - Epoch: [169][  200/  217]    Overall Loss 0.000748    Objective Loss 0.000748                                        LR 0.000063    Time 0.292943    
2024-05-04 02:03:43,254 - Epoch: [169][  217/  217]    Overall Loss 0.000837    Objective Loss 0.000837    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.294015    
2024-05-04 02:03:43,631 - 

2024-05-04 02:03:43,632 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:04:12,606 - Epoch: [170][  100/  217]    Overall Loss 0.000368    Objective Loss 0.000368                                        LR 0.000063    Time 0.289643    
2024-05-04 02:04:12,919 - Epoch: [171][   55/   55]    Overall Loss 0.002681    Objective Loss 0.002681    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.019157    
2024-05-04 02:04:13,238 - 

2024-05-04 02:04:13,238 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:04:37,894 - Epoch: [170][  200/  217]    Overall Loss 0.000806    Objective Loss 0.000806                                        LR 0.000063    Time 0.271218    
2024-05-04 02:04:41,754 - Epoch: [170][  217/  217]    Overall Loss 0.000772    Objective Loss 0.000772    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.267753    
2024-05-04 02:04:41,998 - --- validate (epoch=170)-----------
2024-05-04 02:04:41,998 - 1736 samples (32 per mini-batch)
2024-05-04 02:04:56,765 - Epoch: [170][   55/   55]    Loss 3.073383    Top1 58.640553    Top5 74.135945    
2024-05-04 02:04:57,143 - ==> Top1: 58.641    Top5: 74.136    Loss: 3.073

2024-05-04 02:04:57,148 - ==> Best [Top1: 59.793   Top5: 74.424   Sparsity:0.00   Params: 384080 on epoch: 140]
2024-05-04 02:04:57,148 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 02:04:57,206 - 

2024-05-04 02:04:57,207 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:05:19,639 - Epoch: [172][   55/   55]    Overall Loss 0.002870    Objective Loss 0.002870    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.207107    
2024-05-04 02:05:19,893 - 

2024-05-04 02:05:19,894 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:05:24,370 - Epoch: [171][  100/  217]    Overall Loss 0.000576    Objective Loss 0.000576                                        LR 0.000063    Time 0.271533    
2024-05-04 02:05:51,080 - Epoch: [171][  200/  217]    Overall Loss 0.000706    Objective Loss 0.000706                                        LR 0.000063    Time 0.269269    
2024-05-04 02:05:56,506 - Epoch: [171][  217/  217]    Overall Loss 0.000780    Objective Loss 0.000780    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.273168    
2024-05-04 02:05:56,963 - 

2024-05-04 02:05:56,964 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:06:22,465 - Epoch: [173][   55/   55]    Overall Loss 0.002952    Objective Loss 0.002952    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.137472    
2024-05-04 02:06:22,823 - 

2024-05-04 02:06:22,824 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:06:25,242 - Epoch: [172][  100/  217]    Overall Loss 0.000332    Objective Loss 0.000332                                        LR 0.000063    Time 0.282682    
2024-05-04 02:06:53,157 - Epoch: [172][  200/  217]    Overall Loss 0.000659    Objective Loss 0.000659                                        LR 0.000063    Time 0.280875    
2024-05-04 02:06:57,436 - Epoch: [172][  217/  217]    Overall Loss 0.000722    Objective Loss 0.000722    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.278582    
2024-05-04 02:06:57,986 - 

2024-05-04 02:06:57,987 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:07:21,318 - Epoch: [174][   55/   55]    Overall Loss 0.002911    Objective Loss 0.002911    Top1 100.000000    Top5 100.000000    LR 0.005522    Time 1.063342    
2024-05-04 02:07:21,768 - 

2024-05-04 02:07:21,768 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:07:26,376 - Epoch: [173][  100/  217]    Overall Loss 0.000985    Objective Loss 0.000985                                        LR 0.000063    Time 0.283797    
2024-05-04 02:07:49,993 - Epoch: [173][  200/  217]    Overall Loss 0.000776    Objective Loss 0.000776                                        LR 0.000063    Time 0.259931    
2024-05-04 02:07:55,193 - Epoch: [173][  217/  217]    Overall Loss 0.000744    Objective Loss 0.000744    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.263523    
2024-05-04 02:07:55,511 - 

2024-05-04 02:07:55,512 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:08:22,682 - Epoch: [175][   55/   55]    Overall Loss 0.002899    Objective Loss 0.002899    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.107365    
2024-05-04 02:08:23,324 - 

2024-05-04 02:08:23,325 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:08:26,983 - Epoch: [174][  100/  217]    Overall Loss 0.000299    Objective Loss 0.000299                                        LR 0.000063    Time 0.314604    
2024-05-04 02:08:48,349 - Epoch: [174][  200/  217]    Overall Loss 0.000668    Objective Loss 0.000668                                        LR 0.000063    Time 0.264088    
2024-05-04 02:08:52,497 - Epoch: [174][  217/  217]    Overall Loss 0.000734    Objective Loss 0.000734    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.262507    
2024-05-04 02:08:52,888 - 

2024-05-04 02:08:52,888 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:09:23,958 - Epoch: [175][  100/  217]    Overall Loss 0.000472    Objective Loss 0.000472                                        LR 0.000063    Time 0.310609    
2024-05-04 02:09:24,608 - Epoch: [176][   55/   55]    Overall Loss 0.002877    Objective Loss 0.002877    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.113986    
2024-05-04 02:09:25,181 - 

2024-05-04 02:09:25,184 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:09:46,102 - Epoch: [175][  200/  217]    Overall Loss 0.000685    Objective Loss 0.000685                                        LR 0.000063    Time 0.265985    
2024-05-04 02:09:50,783 - Epoch: [175][  217/  217]    Overall Loss 0.000744    Objective Loss 0.000744    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.266706    
2024-05-04 02:09:51,113 - 

2024-05-04 02:09:51,114 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:10:23,077 - Epoch: [176][  100/  217]    Overall Loss 0.000756    Objective Loss 0.000756                                        LR 0.000063    Time 0.319531    
2024-05-04 02:10:26,163 - Epoch: [177][   55/   55]    Overall Loss 0.002792    Objective Loss 0.002792    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.108522    
2024-05-04 02:10:26,459 - 

2024-05-04 02:10:26,460 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:10:45,580 - Epoch: [176][  200/  217]    Overall Loss 0.000723    Objective Loss 0.000723                                        LR 0.000063    Time 0.272229    
2024-05-04 02:10:50,860 - Epoch: [176][  217/  217]    Overall Loss 0.000692    Objective Loss 0.000692    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.275226    
2024-05-04 02:10:51,662 - 

2024-05-04 02:10:51,663 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:11:20,273 - Epoch: [177][  100/  217]    Overall Loss 0.000777    Objective Loss 0.000777                                        LR 0.000063    Time 0.286004    
2024-05-04 02:11:27,728 - Epoch: [178][   55/   55]    Overall Loss 0.002732    Objective Loss 0.002732    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.113762    
2024-05-04 02:11:28,191 - 

2024-05-04 02:11:28,192 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:11:44,890 - Epoch: [177][  200/  217]    Overall Loss 0.000758    Objective Loss 0.000758                                        LR 0.000063    Time 0.266040    
2024-05-04 02:11:47,719 - Epoch: [177][  217/  217]    Overall Loss 0.000721    Objective Loss 0.000721    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.258229    
2024-05-04 02:11:48,183 - 

2024-05-04 02:11:48,184 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:12:18,165 - Epoch: [179][   55/   55]    Overall Loss 0.002825    Objective Loss 0.002825    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.908429    
2024-05-04 02:12:18,461 - 

2024-05-04 02:12:18,462 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:12:19,238 - Epoch: [178][  100/  217]    Overall Loss 0.000666    Objective Loss 0.000666                                        LR 0.000063    Time 0.310441    
2024-05-04 02:12:40,253 - Epoch: [178][  200/  217]    Overall Loss 0.000699    Objective Loss 0.000699                                        LR 0.000063    Time 0.260253    
2024-05-04 02:12:45,425 - Epoch: [178][  217/  217]    Overall Loss 0.000671    Objective Loss 0.000671    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.263691    
2024-05-04 02:12:45,683 - 

2024-05-04 02:12:45,683 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:13:13,512 - Epoch: [179][  100/  217]    Overall Loss 0.000881    Objective Loss 0.000881                                        LR 0.000063    Time 0.278192    
2024-05-04 02:13:18,847 - Epoch: [180][   55/   55]    Overall Loss 0.002831    Objective Loss 0.002831    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.097748    
2024-05-04 02:13:19,394 - --- validate (epoch=180)-----------
2024-05-04 02:13:19,395 - 1736 samples (128 per mini-batch)
2024-05-04 02:13:38,329 - Epoch: [179][  200/  217]    Overall Loss 0.000712    Objective Loss 0.000712                                        LR 0.000063    Time 0.263144    
2024-05-04 02:13:39,006 - Epoch: [180][   14/   14]    Loss 3.223105    Top1 50.921659    Top5 68.029954    
2024-05-04 02:13:39,632 - ==> Top1: 50.922    Top5: 68.030    Loss: 3.223

2024-05-04 02:13:39,646 - ==> Best [Top1: 51.498   Top5: 67.569   Sparsity:0.00   Params: 1342920 on epoch: 120]
2024-05-04 02:13:39,646 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 02:13:39,770 - 

2024-05-04 02:13:39,771 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:13:40,914 - Epoch: [179][  217/  217]    Overall Loss 0.000677    Objective Loss 0.000677    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.254430    
2024-05-04 02:13:41,112 - 

2024-05-04 02:13:41,112 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:14:11,508 - Epoch: [180][  100/  217]    Overall Loss 0.000687    Objective Loss 0.000687                                        LR 0.000063    Time 0.303869    
2024-05-04 02:14:36,204 - Epoch: [181][   55/   55]    Overall Loss 0.002753    Objective Loss 0.002753    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.025882    
2024-05-04 02:14:36,527 - 

2024-05-04 02:14:36,529 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:14:37,213 - Epoch: [180][  200/  217]    Overall Loss 0.000719    Objective Loss 0.000719                                        LR 0.000063    Time 0.280414    
2024-05-04 02:14:40,401 - Epoch: [180][  217/  217]    Overall Loss 0.000680    Objective Loss 0.000680    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.273132    
2024-05-04 02:14:40,605 - --- validate (epoch=180)-----------
2024-05-04 02:14:40,605 - 1736 samples (32 per mini-batch)
2024-05-04 02:14:57,276 - Epoch: [180][   55/   55]    Loss 3.055677    Top1 58.582949    Top5 74.596774    
2024-05-04 02:14:57,477 - ==> Top1: 58.583    Top5: 74.597    Loss: 3.056

2024-05-04 02:14:57,482 - ==> Best [Top1: 59.793   Top5: 74.424   Sparsity:0.00   Params: 384080 on epoch: 140]
2024-05-04 02:14:57,482 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 02:14:57,527 - 

2024-05-04 02:14:57,527 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:15:26,100 - Epoch: [181][  100/  217]    Overall Loss 0.001300    Objective Loss 0.001300                                        LR 0.000063    Time 0.285629    
2024-05-04 02:15:37,429 - Epoch: [182][   55/   55]    Overall Loss 0.003023    Objective Loss 0.003023    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.107111    
2024-05-04 02:15:38,597 - 

2024-05-04 02:15:38,599 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:15:50,350 - Epoch: [181][  200/  217]    Overall Loss 0.000928    Objective Loss 0.000928                                        LR 0.000063    Time 0.264025    
2024-05-04 02:15:54,835 - Epoch: [181][  217/  217]    Overall Loss 0.000875    Objective Loss 0.000875    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.263995    
2024-05-04 02:15:55,081 - 

2024-05-04 02:15:55,082 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:16:25,325 - Epoch: [182][  100/  217]    Overall Loss 0.000973    Objective Loss 0.000973                                        LR 0.000063    Time 0.302336    
2024-05-04 02:16:44,723 - Epoch: [183][   55/   55]    Overall Loss 0.002648    Objective Loss 0.002648    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.202038    
2024-05-04 02:16:45,633 - 

2024-05-04 02:16:45,633 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:16:49,463 - Epoch: [182][  200/  217]    Overall Loss 0.000779    Objective Loss 0.000779                                        LR 0.000063    Time 0.271819    
2024-05-04 02:16:52,816 - Epoch: [182][  217/  217]    Overall Loss 0.000732    Objective Loss 0.000732    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.265968    
2024-05-04 02:16:53,036 - 

2024-05-04 02:16:53,037 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:17:22,694 - Epoch: [183][  100/  217]    Overall Loss 0.000933    Objective Loss 0.000933                                        LR 0.000063    Time 0.296477    
2024-05-04 02:17:47,950 - Epoch: [184][   55/   55]    Overall Loss 0.002746    Objective Loss 0.002746    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.132859    
2024-05-04 02:17:48,173 - 

2024-05-04 02:17:48,174 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:17:49,824 - Epoch: [183][  200/  217]    Overall Loss 0.000571    Objective Loss 0.000571                                        LR 0.000063    Time 0.283845    
2024-05-04 02:17:53,083 - Epoch: [183][  217/  217]    Overall Loss 0.000673    Objective Loss 0.000673    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.276616    
2024-05-04 02:17:53,387 - 

2024-05-04 02:17:53,387 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:18:21,889 - Epoch: [184][  100/  217]    Overall Loss 0.000408    Objective Loss 0.000408                                        LR 0.000063    Time 0.284904    
2024-05-04 02:18:47,875 - Epoch: [184][  200/  217]    Overall Loss 0.000712    Objective Loss 0.000712                                        LR 0.000063    Time 0.272339    
2024-05-04 02:18:48,998 - Epoch: [185][   55/   55]    Overall Loss 0.002788    Objective Loss 0.002788    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.105705    
2024-05-04 02:18:49,258 - 

2024-05-04 02:18:49,259 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:18:52,487 - Epoch: [184][  217/  217]    Overall Loss 0.000670    Objective Loss 0.000670    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.272250    
2024-05-04 02:18:52,805 - 

2024-05-04 02:18:52,805 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:19:24,513 - Epoch: [185][  100/  217]    Overall Loss 0.000779    Objective Loss 0.000779                                        LR 0.000063    Time 0.316991    
2024-05-04 02:19:46,917 - Epoch: [185][  200/  217]    Overall Loss 0.000654    Objective Loss 0.000654                                        LR 0.000063    Time 0.270475    
2024-05-04 02:19:51,865 - Epoch: [185][  217/  217]    Overall Loss 0.000617    Objective Loss 0.000617    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.272081    
2024-05-04 02:19:52,105 - 

2024-05-04 02:19:52,105 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:19:55,936 - Epoch: [186][   55/   55]    Overall Loss 0.003041    Objective Loss 0.003041    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.212132    
2024-05-04 02:19:56,154 - 

2024-05-04 02:19:56,156 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:20:21,240 - Epoch: [186][  100/  217]    Overall Loss 0.001079    Objective Loss 0.001079                                        LR 0.000063    Time 0.291258    
2024-05-04 02:20:46,496 - Epoch: [186][  200/  217]    Overall Loss 0.000798    Objective Loss 0.000798                                        LR 0.000063    Time 0.271867    
2024-05-04 02:20:50,829 - Epoch: [186][  217/  217]    Overall Loss 0.000754    Objective Loss 0.000754    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.270530    
2024-05-04 02:20:51,169 - 

2024-05-04 02:20:51,169 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:20:54,811 - Epoch: [187][   55/   55]    Overall Loss 0.002873    Objective Loss 0.002873    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.066285    
2024-05-04 02:20:55,046 - 

2024-05-04 02:20:55,048 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:21:16,925 - Epoch: [187][  100/  217]    Overall Loss 0.000498    Objective Loss 0.000498                                        LR 0.000063    Time 0.257455    
2024-05-04 02:21:42,063 - Epoch: [187][  200/  217]    Overall Loss 0.000698    Objective Loss 0.000698                                        LR 0.000063    Time 0.254376    
2024-05-04 02:21:48,453 - Epoch: [187][  217/  217]    Overall Loss 0.000660    Objective Loss 0.000660    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.263888    
2024-05-04 02:21:48,778 - 

2024-05-04 02:21:48,778 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:21:52,063 - Epoch: [188][   55/   55]    Overall Loss 0.002658    Objective Loss 0.002658    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.036472    
2024-05-04 02:21:52,406 - 

2024-05-04 02:21:52,406 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:22:15,947 - Epoch: [188][  100/  217]    Overall Loss 0.000461    Objective Loss 0.000461                                        LR 0.000063    Time 0.271604    
2024-05-04 02:22:37,742 - Epoch: [188][  200/  217]    Overall Loss 0.000747    Objective Loss 0.000747                                        LR 0.000063    Time 0.244733    
2024-05-04 02:22:41,652 - Epoch: [188][  217/  217]    Overall Loss 0.000702    Objective Loss 0.000702    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.243570    
2024-05-04 02:22:41,920 - 

2024-05-04 02:22:41,921 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:22:54,876 - Epoch: [189][   55/   55]    Overall Loss 0.003023    Objective Loss 0.003023    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.135643    
2024-05-04 02:22:55,143 - 

2024-05-04 02:22:55,144 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:23:10,316 - Epoch: [189][  100/  217]    Overall Loss 0.000600    Objective Loss 0.000600                                        LR 0.000063    Time 0.283852    
2024-05-04 02:23:36,649 - Epoch: [189][  200/  217]    Overall Loss 0.000798    Objective Loss 0.000798                                        LR 0.000063    Time 0.273552    
2024-05-04 02:23:43,947 - Epoch: [189][  217/  217]    Overall Loss 0.000749    Objective Loss 0.000749    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.285746    
2024-05-04 02:23:44,126 - 

2024-05-04 02:23:44,127 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:23:53,986 - Epoch: [190][   55/   55]    Overall Loss 0.002890    Objective Loss 0.002890    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.069681    
2024-05-04 02:23:54,370 - --- validate (epoch=190)-----------
2024-05-04 02:23:54,371 - 1736 samples (128 per mini-batch)
2024-05-04 02:24:13,614 - Epoch: [190][   14/   14]    Loss 3.225949    Top1 51.209677    Top5 68.029954    
2024-05-04 02:24:13,692 - Epoch: [190][  100/  217]    Overall Loss 0.000514    Objective Loss 0.000514                                        LR 0.000063    Time 0.295565    
2024-05-04 02:24:13,907 - ==> Top1: 51.210    Top5: 68.030    Loss: 3.226

2024-05-04 02:24:13,924 - ==> Best [Top1: 51.498   Top5: 67.569   Sparsity:0.00   Params: 1342920 on epoch: 120]
2024-05-04 02:24:13,925 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 02:24:14,041 - 

2024-05-04 02:24:14,042 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:24:41,269 - Epoch: [190][  200/  217]    Overall Loss 0.000641    Objective Loss 0.000641                                        LR 0.000063    Time 0.285625    
2024-05-04 02:24:43,872 - Epoch: [190][  217/  217]    Overall Loss 0.000602    Objective Loss 0.000602    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.275235    
2024-05-04 02:24:44,100 - --- validate (epoch=190)-----------
2024-05-04 02:24:44,100 - 1736 samples (32 per mini-batch)
2024-05-04 02:24:59,049 - Epoch: [190][   55/   55]    Loss 3.092165    Top1 59.043779    Top5 74.193548    
2024-05-04 02:24:59,203 - ==> Top1: 59.044    Top5: 74.194    Loss: 3.092

2024-05-04 02:24:59,206 - ==> Best [Top1: 59.793   Top5: 74.424   Sparsity:0.00   Params: 384080 on epoch: 140]
2024-05-04 02:24:59,206 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 02:24:59,231 - 

2024-05-04 02:24:59,232 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:25:13,620 - Epoch: [191][   55/   55]    Overall Loss 0.002732    Objective Loss 0.002732    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.083042    
2024-05-04 02:25:13,975 - 

2024-05-04 02:25:13,976 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:25:30,424 - Epoch: [191][  100/  217]    Overall Loss 0.000498    Objective Loss 0.000498                                        LR 0.000063    Time 0.311843    
2024-05-04 02:25:59,969 - Epoch: [191][  200/  217]    Overall Loss 0.000652    Objective Loss 0.000652                                        LR 0.000063    Time 0.303602    
2024-05-04 02:26:04,661 - Epoch: [191][  217/  217]    Overall Loss 0.000628    Objective Loss 0.000628    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.301433    
2024-05-04 02:26:04,936 - 

2024-05-04 02:26:04,936 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:26:09,759 - Epoch: [192][   55/   55]    Overall Loss 0.002746    Objective Loss 0.002746    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.014091    
2024-05-04 02:26:10,425 - 

2024-05-04 02:26:10,426 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:26:33,118 - Epoch: [192][  100/  217]    Overall Loss 0.000571    Objective Loss 0.000571                                        LR 0.000063    Time 0.281746    
2024-05-04 02:27:00,953 - Epoch: [192][  200/  217]    Overall Loss 0.000653    Objective Loss 0.000653                                        LR 0.000063    Time 0.280004    
2024-05-04 02:27:06,355 - Epoch: [192][  217/  217]    Overall Loss 0.000617    Objective Loss 0.000617    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.282950    
2024-05-04 02:27:06,729 - Epoch: [193][   55/   55]    Overall Loss 0.002786    Objective Loss 0.002786    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.023520    
2024-05-04 02:27:06,819 - 

2024-05-04 02:27:06,820 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:27:07,056 - 

2024-05-04 02:27:07,057 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:27:36,642 - Epoch: [193][  100/  217]    Overall Loss 0.000996    Objective Loss 0.000996                                        LR 0.000063    Time 0.298134    
2024-05-04 02:28:03,639 - Epoch: [193][  200/  217]    Overall Loss 0.000680    Objective Loss 0.000680                                        LR 0.000063    Time 0.284009    
2024-05-04 02:28:05,723 - Epoch: [194][   55/   55]    Overall Loss 0.002818    Objective Loss 0.002818    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.066505    
2024-05-04 02:28:06,028 - 

2024-05-04 02:28:06,029 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:28:07,654 - Epoch: [193][  217/  217]    Overall Loss 0.000640    Objective Loss 0.000640    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.280251    
2024-05-04 02:28:08,351 - 

2024-05-04 02:28:08,352 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:28:39,267 - Epoch: [194][  100/  217]    Overall Loss 0.000325    Objective Loss 0.000325                                        LR 0.000063    Time 0.309060    
2024-05-04 02:29:04,220 - Epoch: [195][   55/   55]    Overall Loss 0.003164    Objective Loss 0.003164    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.057854    
2024-05-04 02:29:04,546 - 

2024-05-04 02:29:04,547 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:29:07,513 - Epoch: [194][  200/  217]    Overall Loss 0.000618    Objective Loss 0.000618                                        LR 0.000063    Time 0.295718    
2024-05-04 02:29:10,726 - Epoch: [194][  217/  217]    Overall Loss 0.000581    Objective Loss 0.000581    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.287348    
2024-05-04 02:29:11,277 - 

2024-05-04 02:29:11,278 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:29:39,397 - Epoch: [195][  100/  217]    Overall Loss 0.000751    Objective Loss 0.000751                                        LR 0.000063    Time 0.281108    
2024-05-04 02:30:09,121 - Epoch: [195][  200/  217]    Overall Loss 0.000592    Objective Loss 0.000592                                        LR 0.000063    Time 0.289132    
2024-05-04 02:30:09,210 - Epoch: [196][   55/   55]    Overall Loss 0.002676    Objective Loss 0.002676    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.175550    
2024-05-04 02:30:10,534 - 

2024-05-04 02:30:10,535 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:30:12,671 - Epoch: [195][  217/  217]    Overall Loss 0.000554    Objective Loss 0.000554    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.282831    
2024-05-04 02:30:12,982 - 

2024-05-04 02:30:12,983 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:30:40,852 - Epoch: [196][  100/  217]    Overall Loss 0.000564    Objective Loss 0.000564                                        LR 0.000063    Time 0.278600    
2024-05-04 02:31:06,991 - Epoch: [196][  200/  217]    Overall Loss 0.000593    Objective Loss 0.000593                                        LR 0.000063    Time 0.269953    
2024-05-04 02:31:11,497 - Epoch: [196][  217/  217]    Overall Loss 0.000560    Objective Loss 0.000560    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.269561    
2024-05-04 02:31:11,711 - 

2024-05-04 02:31:11,712 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:31:12,481 - Epoch: [197][   55/   55]    Overall Loss 0.002848    Objective Loss 0.002848    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.126091    
2024-05-04 02:31:12,842 - 

2024-05-04 02:31:12,843 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:31:41,562 - Epoch: [197][  100/  217]    Overall Loss 0.000495    Objective Loss 0.000495                                        LR 0.000063    Time 0.298383    
2024-05-04 02:32:06,565 - Epoch: [197][  200/  217]    Overall Loss 0.000724    Objective Loss 0.000724                                        LR 0.000063    Time 0.274167    
2024-05-04 02:32:09,305 - Epoch: [198][   55/   55]    Overall Loss 0.002709    Objective Loss 0.002709    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.026411    
2024-05-04 02:32:09,746 - 

2024-05-04 02:32:09,747 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:32:09,789 - Epoch: [197][  217/  217]    Overall Loss 0.000677    Objective Loss 0.000677    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.267535    
2024-05-04 02:32:10,185 - 

2024-05-04 02:32:10,186 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:32:38,406 - Epoch: [198][  100/  217]    Overall Loss 0.000413    Objective Loss 0.000413                                        LR 0.000063    Time 0.282113    
2024-05-04 02:33:04,235 - Epoch: [198][  200/  217]    Overall Loss 0.000607    Objective Loss 0.000607                                        LR 0.000063    Time 0.270157    
2024-05-04 02:33:07,815 - Epoch: [198][  217/  217]    Overall Loss 0.000570    Objective Loss 0.000570    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.265480    
2024-05-04 02:33:08,025 - 

2024-05-04 02:33:08,026 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:33:11,399 - Epoch: [199][   55/   55]    Overall Loss 0.002781    Objective Loss 0.002781    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.120763    
2024-05-04 02:33:11,974 - 

2024-05-04 02:33:11,975 - Initiating quantization aware training (QAT)...
2024-05-04 02:33:12,086 - 

2024-05-04 02:33:12,086 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:33:36,816 - Epoch: [199][  100/  217]    Overall Loss 0.000509    Objective Loss 0.000509                                        LR 0.000063    Time 0.287802    
2024-05-04 02:34:04,183 - Epoch: [199][  200/  217]    Overall Loss 0.000466    Objective Loss 0.000466                                        LR 0.000063    Time 0.280691    
2024-05-04 02:34:08,444 - Epoch: [199][  217/  217]    Overall Loss 0.000620    Objective Loss 0.000620    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.278330    
2024-05-04 02:34:08,989 - 

2024-05-04 02:34:08,990 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:34:13,790 - Epoch: [200][   55/   55]    Overall Loss 0.000333    Objective Loss 0.000333    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.121713    
2024-05-04 02:34:14,137 - --- validate (epoch=200)-----------
2024-05-04 02:34:14,137 - 1736 samples (128 per mini-batch)
2024-05-04 02:34:30,887 - Epoch: [200][   14/   14]    Loss 5.996685    Top1 50.979263    Top5 67.338710    
2024-05-04 02:34:31,279 - ==> Top1: 50.979    Top5: 67.339    Loss: 5.997

2024-05-04 02:34:31,292 - ==> Best [Top1: 50.979   Top5: 67.339   Sparsity:0.00   Params: 1342920 on epoch: 200]
2024-05-04 02:34:31,292 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 02:34:31,388 - 

2024-05-04 02:34:31,390 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:34:33,914 - Epoch: [200][  100/  217]    Overall Loss 0.000382    Objective Loss 0.000382                                        LR 0.000016    Time 0.249142    
2024-05-04 02:35:05,275 - Epoch: [200][  200/  217]    Overall Loss 0.000549    Objective Loss 0.000549                                        LR 0.000016    Time 0.281330    
2024-05-04 02:35:07,936 - Epoch: [200][  217/  217]    Overall Loss 0.000516    Objective Loss 0.000516    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.271546    
2024-05-04 02:35:08,159 - --- validate (epoch=200)-----------
2024-05-04 02:35:08,160 - 1736 samples (32 per mini-batch)
2024-05-04 02:35:21,255 - Epoch: [201][   55/   55]    Overall Loss 0.000349    Objective Loss 0.000349    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.906389    
2024-05-04 02:35:21,520 - 

2024-05-04 02:35:21,521 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:35:22,685 - Epoch: [200][   55/   55]    Loss 3.096113    Top1 59.101382    Top5 74.539171    
2024-05-04 02:35:22,886 - ==> Top1: 59.101    Top5: 74.539    Loss: 3.096

2024-05-04 02:35:22,892 - ==> Best [Top1: 59.793   Top5: 74.424   Sparsity:0.00   Params: 384080 on epoch: 140]
2024-05-04 02:35:22,892 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 02:35:22,928 - 

2024-05-04 02:35:22,929 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:35:50,998 - Epoch: [201][  100/  217]    Overall Loss 0.000765    Objective Loss 0.000765                                        LR 0.000016    Time 0.280589    
2024-05-04 02:36:15,636 - Epoch: [202][   55/   55]    Overall Loss 0.000395    Objective Loss 0.000395    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.983728    
2024-05-04 02:36:15,925 - 

2024-05-04 02:36:15,925 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:36:18,198 - Epoch: [201][  200/  217]    Overall Loss 0.000549    Objective Loss 0.000549                                        LR 0.000016    Time 0.276252    
2024-05-04 02:36:22,454 - Epoch: [201][  217/  217]    Overall Loss 0.000515    Objective Loss 0.000515    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.274213    
2024-05-04 02:36:22,705 - 

2024-05-04 02:36:22,706 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:36:53,216 - Epoch: [202][  100/  217]    Overall Loss 0.000351    Objective Loss 0.000351                                        LR 0.000016    Time 0.305012    
2024-05-04 02:37:11,671 - Epoch: [203][   55/   55]    Overall Loss 0.000338    Objective Loss 0.000338    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.013416    
2024-05-04 02:37:12,081 - 

2024-05-04 02:37:12,082 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:37:20,522 - Epoch: [202][  200/  217]    Overall Loss 0.000451    Objective Loss 0.000451                                        LR 0.000016    Time 0.288998    
2024-05-04 02:37:23,326 - Epoch: [202][  217/  217]    Overall Loss 0.000522    Objective Loss 0.000522    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.279269    
2024-05-04 02:37:23,628 - 

2024-05-04 02:37:23,629 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:37:54,832 - Epoch: [203][  100/  217]    Overall Loss 0.000823    Objective Loss 0.000823                                        LR 0.000016    Time 0.311948    
2024-05-04 02:38:10,196 - Epoch: [204][   55/   55]    Overall Loss 0.000379    Objective Loss 0.000379    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.056441    
2024-05-04 02:38:10,456 - 

2024-05-04 02:38:10,457 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:38:18,778 - Epoch: [203][  200/  217]    Overall Loss 0.000656    Objective Loss 0.000656                                        LR 0.000016    Time 0.275649    
2024-05-04 02:38:24,393 - Epoch: [203][  217/  217]    Overall Loss 0.000612    Objective Loss 0.000612    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.279918    
2024-05-04 02:38:25,207 - 

2024-05-04 02:38:25,207 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:38:51,743 - Epoch: [204][  100/  217]    Overall Loss 0.000648    Objective Loss 0.000648                                        LR 0.000016    Time 0.265260    
2024-05-04 02:39:12,516 - Epoch: [205][   55/   55]    Overall Loss 0.000730    Objective Loss 0.000730    Top1 99.363057    Top5 100.000000    LR 0.001298    Time 1.128166    
2024-05-04 02:39:12,934 - 

2024-05-04 02:39:12,935 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:39:19,787 - Epoch: [204][  200/  217]    Overall Loss 0.000601    Objective Loss 0.000601                                        LR 0.000016    Time 0.272811    
2024-05-04 02:39:22,875 - Epoch: [204][  217/  217]    Overall Loss 0.000564    Objective Loss 0.000564    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.265663    
2024-05-04 02:39:23,202 - 

2024-05-04 02:39:23,202 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:39:52,270 - Epoch: [205][  100/  217]    Overall Loss 0.000354    Objective Loss 0.000354                                        LR 0.000016    Time 0.290586    
2024-05-04 02:40:12,149 - Epoch: [206][   55/   55]    Overall Loss 0.000333    Objective Loss 0.000333    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.076436    
2024-05-04 02:40:12,751 - 

2024-05-04 02:40:12,752 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:40:15,420 - Epoch: [205][  200/  217]    Overall Loss 0.000597    Objective Loss 0.000597                                        LR 0.000016    Time 0.260996    
2024-05-04 02:40:18,823 - Epoch: [205][  217/  217]    Overall Loss 0.000559    Objective Loss 0.000559    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.256221    
2024-05-04 02:40:19,049 - 

2024-05-04 02:40:19,050 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:40:50,503 - Epoch: [206][  100/  217]    Overall Loss 0.001146    Objective Loss 0.001146                                        LR 0.000016    Time 0.314446    
2024-05-04 02:41:08,194 - Epoch: [207][   55/   55]    Overall Loss 0.000367    Objective Loss 0.000367    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.007822    
2024-05-04 02:41:08,579 - 

2024-05-04 02:41:08,581 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:41:16,072 - Epoch: [206][  200/  217]    Overall Loss 0.000627    Objective Loss 0.000627                                        LR 0.000016    Time 0.285028    
2024-05-04 02:41:21,246 - Epoch: [206][  217/  217]    Overall Loss 0.000587    Objective Loss 0.000587    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.286533    
2024-05-04 02:41:21,537 - 

2024-05-04 02:41:21,537 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:41:51,842 - Epoch: [207][  100/  217]    Overall Loss 0.000363    Objective Loss 0.000363                                        LR 0.000016    Time 0.302952    
2024-05-04 02:42:08,083 - Epoch: [208][   55/   55]    Overall Loss 0.000386    Objective Loss 0.000386    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.081613    
2024-05-04 02:42:08,457 - 

2024-05-04 02:42:08,458 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:42:13,178 - Epoch: [207][  200/  217]    Overall Loss 0.000584    Objective Loss 0.000584                                        LR 0.000016    Time 0.258112    
2024-05-04 02:42:17,346 - Epoch: [207][  217/  217]    Overall Loss 0.000547    Objective Loss 0.000547    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.257094    
2024-05-04 02:42:17,542 - 

2024-05-04 02:42:17,543 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:42:45,365 - Epoch: [208][  100/  217]    Overall Loss 0.000856    Objective Loss 0.000856                                        LR 0.000016    Time 0.278130    
2024-05-04 02:43:06,655 - Epoch: [209][   55/   55]    Overall Loss 0.000349    Objective Loss 0.000349    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.057984    
2024-05-04 02:43:07,062 - 

2024-05-04 02:43:07,066 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:43:09,922 - Epoch: [208][  200/  217]    Overall Loss 0.000603    Objective Loss 0.000603                                        LR 0.000016    Time 0.261813    
2024-05-04 02:43:14,245 - Epoch: [208][  217/  217]    Overall Loss 0.000563    Objective Loss 0.000563    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.261216    
2024-05-04 02:43:14,461 - 

2024-05-04 02:43:14,462 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:43:40,277 - Epoch: [209][  100/  217]    Overall Loss 0.000634    Objective Loss 0.000634                                        LR 0.000016    Time 0.258054    
2024-05-04 02:44:05,124 - Epoch: [209][  200/  217]    Overall Loss 0.000469    Objective Loss 0.000469                                        LR 0.000016    Time 0.253221    
2024-05-04 02:44:05,319 - Epoch: [210][   55/   55]    Overall Loss 0.000329    Objective Loss 0.000329    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.058867    
2024-05-04 02:44:05,663 - --- validate (epoch=210)-----------
2024-05-04 02:44:05,664 - 1736 samples (128 per mini-batch)
2024-05-04 02:44:09,311 - Epoch: [209][  217/  217]    Overall Loss 0.000546    Objective Loss 0.000546    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.252671    
2024-05-04 02:44:09,528 - 

2024-05-04 02:44:09,528 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:44:21,948 - Epoch: [210][   14/   14]    Loss 5.963281    Top1 51.209677    Top5 67.453917    
2024-05-04 02:44:22,236 - ==> Top1: 51.210    Top5: 67.454    Loss: 5.963

2024-05-04 02:44:22,255 - ==> Best [Top1: 51.210   Top5: 67.454   Sparsity:0.00   Params: 1342920 on epoch: 210]
2024-05-04 02:44:22,256 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 02:44:22,346 - 

2024-05-04 02:44:22,346 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:44:37,475 - Epoch: [210][  100/  217]    Overall Loss 0.000279    Objective Loss 0.000279                                        LR 0.000016    Time 0.279388    
2024-05-04 02:45:01,138 - Epoch: [210][  200/  217]    Overall Loss 0.000564    Objective Loss 0.000564                                        LR 0.000016    Time 0.257959    
2024-05-04 02:45:06,516 - Epoch: [210][  217/  217]    Overall Loss 0.000527    Objective Loss 0.000527    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.262525    
2024-05-04 02:45:06,829 - --- validate (epoch=210)-----------
2024-05-04 02:45:06,829 - 1736 samples (32 per mini-batch)
2024-05-04 02:45:21,615 - Epoch: [210][   55/   55]    Loss 3.089514    Top1 59.792627    Top5 74.308756    
2024-05-04 02:45:21,794 - ==> Top1: 59.793    Top5: 74.309    Loss: 3.090

2024-05-04 02:45:21,804 - ==> Best [Top1: 59.793   Top5: 74.424   Sparsity:0.00   Params: 384080 on epoch: 140]
2024-05-04 02:45:21,805 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 02:45:21,848 - 

2024-05-04 02:45:21,848 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:45:26,506 - Epoch: [211][   55/   55]    Overall Loss 0.000362    Objective Loss 0.000362    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.166370    
2024-05-04 02:45:26,922 - 

2024-05-04 02:45:26,924 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:45:49,080 - Epoch: [211][  100/  217]    Overall Loss 0.000816    Objective Loss 0.000816                                        LR 0.000016    Time 0.272225    
2024-05-04 02:46:14,626 - Epoch: [211][  200/  217]    Overall Loss 0.000465    Objective Loss 0.000465                                        LR 0.000016    Time 0.263800    
2024-05-04 02:46:18,829 - Epoch: [211][  217/  217]    Overall Loss 0.000517    Objective Loss 0.000517    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.262492    
2024-05-04 02:46:19,061 - 

2024-05-04 02:46:19,062 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:46:27,729 - Epoch: [212][   55/   55]    Overall Loss 0.000329    Objective Loss 0.000329    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.105335    
2024-05-04 02:46:28,663 - 

2024-05-04 02:46:28,665 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:46:46,997 - Epoch: [212][  100/  217]    Overall Loss 0.000303    Objective Loss 0.000303                                        LR 0.000016    Time 0.279265    
2024-05-04 02:47:12,045 - Epoch: [212][  200/  217]    Overall Loss 0.000531    Objective Loss 0.000531                                        LR 0.000016    Time 0.264826    
2024-05-04 02:47:16,472 - Epoch: [212][  217/  217]    Overall Loss 0.000497    Objective Loss 0.000497    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.264474    
2024-05-04 02:47:16,986 - 

2024-05-04 02:47:16,987 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:47:27,701 - Epoch: [213][   55/   55]    Overall Loss 0.000367    Objective Loss 0.000367    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.073182    
2024-05-04 02:47:28,181 - 

2024-05-04 02:47:28,182 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:47:48,213 - Epoch: [213][  100/  217]    Overall Loss 0.000396    Objective Loss 0.000396                                        LR 0.000016    Time 0.312171    
2024-05-04 02:48:15,099 - Epoch: [213][  200/  217]    Overall Loss 0.000470    Objective Loss 0.000470                                        LR 0.000016    Time 0.290479    
2024-05-04 02:48:18,675 - Epoch: [213][  217/  217]    Overall Loss 0.000549    Objective Loss 0.000549    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.284193    
2024-05-04 02:48:19,210 - 

2024-05-04 02:48:19,211 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:48:23,693 - Epoch: [214][   55/   55]    Overall Loss 0.000315    Objective Loss 0.000315    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.009112    
2024-05-04 02:48:24,872 - 

2024-05-04 02:48:24,873 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:48:47,968 - Epoch: [214][  100/  217]    Overall Loss 0.001173    Objective Loss 0.001173                                        LR 0.000016    Time 0.287461    
2024-05-04 02:49:12,469 - Epoch: [214][  200/  217]    Overall Loss 0.000639    Objective Loss 0.000639                                        LR 0.000016    Time 0.266193    
2024-05-04 02:49:17,368 - Epoch: [214][  217/  217]    Overall Loss 0.000597    Objective Loss 0.000597    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.267908    
2024-05-04 02:49:17,575 - 

2024-05-04 02:49:17,576 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:49:22,366 - Epoch: [215][   55/   55]    Overall Loss 0.000356    Objective Loss 0.000356    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.045085    
2024-05-04 02:49:23,080 - 

2024-05-04 02:49:23,081 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:49:45,040 - Epoch: [215][  100/  217]    Overall Loss 0.000315    Objective Loss 0.000315                                        LR 0.000016    Time 0.274561    
2024-05-04 02:50:10,554 - Epoch: [215][  200/  217]    Overall Loss 0.000316    Objective Loss 0.000316                                        LR 0.000016    Time 0.264813    
2024-05-04 02:50:15,122 - Epoch: [215][  217/  217]    Overall Loss 0.000572    Objective Loss 0.000572    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.265106    
2024-05-04 02:50:15,348 - 

2024-05-04 02:50:15,349 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:50:20,450 - Epoch: [216][   55/   55]    Overall Loss 0.000406    Objective Loss 0.000406    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.042879    
2024-05-04 02:50:20,644 - 

2024-05-04 02:50:20,645 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:50:46,005 - Epoch: [216][  100/  217]    Overall Loss 0.000723    Objective Loss 0.000723                                        LR 0.000016    Time 0.306474    
2024-05-04 02:51:10,170 - Epoch: [216][  200/  217]    Overall Loss 0.000538    Objective Loss 0.000538                                        LR 0.000016    Time 0.274017    
2024-05-04 02:51:14,351 - Epoch: [216][  217/  217]    Overall Loss 0.000503    Objective Loss 0.000503    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.271809    
2024-05-04 02:51:14,617 - 

2024-05-04 02:51:14,618 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:51:19,110 - Epoch: [217][   55/   55]    Overall Loss 0.000369    Objective Loss 0.000369    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.062815    
2024-05-04 02:51:19,507 - 

2024-05-04 02:51:19,508 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:51:43,036 - Epoch: [217][  100/  217]    Overall Loss 0.000536    Objective Loss 0.000536                                        LR 0.000016    Time 0.284094    
2024-05-04 02:52:05,390 - Epoch: [217][  200/  217]    Overall Loss 0.000587    Objective Loss 0.000587                                        LR 0.000016    Time 0.253780    
2024-05-04 02:52:09,379 - Epoch: [217][  217/  217]    Overall Loss 0.000548    Objective Loss 0.000548    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.252256    
2024-05-04 02:52:09,640 - 

2024-05-04 02:52:09,641 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:52:24,871 - Epoch: [218][   55/   55]    Overall Loss 0.000364    Objective Loss 0.000364    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.188231    
2024-05-04 02:52:25,813 - 

2024-05-04 02:52:25,814 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:52:37,476 - Epoch: [218][  100/  217]    Overall Loss 0.000615    Objective Loss 0.000615                                        LR 0.000016    Time 0.278238    
2024-05-04 02:53:05,110 - Epoch: [218][  200/  217]    Overall Loss 0.000577    Objective Loss 0.000577                                        LR 0.000016    Time 0.277252    
2024-05-04 02:53:08,177 - Epoch: [218][  217/  217]    Overall Loss 0.000544    Objective Loss 0.000544    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.269656    
2024-05-04 02:53:08,536 - 

2024-05-04 02:53:08,537 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:53:19,934 - Epoch: [219][   55/   55]    Overall Loss 0.000359    Objective Loss 0.000359    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.983770    
2024-05-04 02:53:20,563 - 

2024-05-04 02:53:20,564 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:53:36,954 - Epoch: [219][  100/  217]    Overall Loss 0.000546    Objective Loss 0.000546                                        LR 0.000016    Time 0.284078    
2024-05-04 02:54:02,006 - Epoch: [219][  200/  217]    Overall Loss 0.000538    Objective Loss 0.000538                                        LR 0.000016    Time 0.267251    
2024-05-04 02:54:04,828 - Epoch: [219][  217/  217]    Overall Loss 0.000505    Objective Loss 0.000505    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.259310    
2024-05-04 02:54:05,051 - 

2024-05-04 02:54:05,052 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:54:13,597 - Epoch: [220][   55/   55]    Overall Loss 0.000337    Objective Loss 0.000337    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.964042    
2024-05-04 02:54:14,604 - --- validate (epoch=220)-----------
2024-05-04 02:54:14,606 - 1736 samples (128 per mini-batch)
2024-05-04 02:54:35,907 - Epoch: [220][   14/   14]    Loss 5.948129    Top1 50.691244    Top5 67.511521    
2024-05-04 02:54:36,467 - ==> Top1: 50.691    Top5: 67.512    Loss: 5.948

2024-05-04 02:54:36,479 - ==> Best [Top1: 51.210   Top5: 67.454   Sparsity:0.00   Params: 1342920 on epoch: 210]
2024-05-04 02:54:36,479 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 02:54:36,573 - 

2024-05-04 02:54:36,574 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:54:37,560 - Epoch: [220][  100/  217]    Overall Loss 0.000567    Objective Loss 0.000567                                        LR 0.000016    Time 0.325001    
2024-05-04 02:55:04,833 - Epoch: [220][  200/  217]    Overall Loss 0.000544    Objective Loss 0.000544                                        LR 0.000016    Time 0.298823    
2024-05-04 02:55:09,875 - Epoch: [220][  217/  217]    Overall Loss 0.000512    Objective Loss 0.000512    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.298641    
2024-05-04 02:55:10,516 - --- validate (epoch=220)-----------
2024-05-04 02:55:10,516 - 1736 samples (32 per mini-batch)
2024-05-04 02:55:30,141 - Epoch: [220][   55/   55]    Loss 3.146963    Top1 59.216590    Top5 74.884793    
2024-05-04 02:55:30,309 - ==> Top1: 59.217    Top5: 74.885    Loss: 3.147

2024-05-04 02:55:30,313 - ==> Best [Top1: 59.793   Top5: 74.424   Sparsity:0.00   Params: 384080 on epoch: 140]
2024-05-04 02:55:30,313 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 02:55:30,355 - 

2024-05-04 02:55:30,356 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:55:36,269 - Epoch: [221][   55/   55]    Overall Loss 0.000383    Objective Loss 0.000383    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.085204    
2024-05-04 02:55:37,252 - 

2024-05-04 02:55:37,253 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:56:00,673 - Epoch: [221][  100/  217]    Overall Loss 0.000311    Objective Loss 0.000311                                        LR 0.000016    Time 0.303083    
2024-05-04 02:56:25,740 - Epoch: [221][  200/  217]    Overall Loss 0.000431    Objective Loss 0.000431                                        LR 0.000016    Time 0.276835    
2024-05-04 02:56:31,090 - Epoch: [221][  217/  217]    Overall Loss 0.000496    Objective Loss 0.000496    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.279793    
2024-05-04 02:56:31,650 - 

2024-05-04 02:56:31,651 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:56:40,530 - Epoch: [222][   55/   55]    Overall Loss 0.000395    Objective Loss 0.000395    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.150307    
2024-05-04 02:56:41,143 - 

2024-05-04 02:56:41,144 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:57:00,774 - Epoch: [222][  100/  217]    Overall Loss 0.000096    Objective Loss 0.000096                                        LR 0.000016    Time 0.291141    
2024-05-04 02:57:27,783 - Epoch: [222][  200/  217]    Overall Loss 0.000554    Objective Loss 0.000554                                        LR 0.000016    Time 0.280568    
2024-05-04 02:57:32,584 - Epoch: [222][  217/  217]    Overall Loss 0.000517    Objective Loss 0.000517    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.280700    
2024-05-04 02:57:32,912 - 

2024-05-04 02:57:32,912 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:57:36,059 - Epoch: [223][   55/   55]    Overall Loss 0.000360    Objective Loss 0.000360    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.998278    
2024-05-04 02:57:36,391 - 

2024-05-04 02:57:36,392 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:58:01,131 - Epoch: [223][  100/  217]    Overall Loss 0.000545    Objective Loss 0.000545                                        LR 0.000016    Time 0.282102    
2024-05-04 02:58:27,269 - Epoch: [223][  200/  217]    Overall Loss 0.000426    Objective Loss 0.000426                                        LR 0.000016    Time 0.271699    
2024-05-04 02:58:29,849 - Epoch: [223][  217/  217]    Overall Loss 0.000546    Objective Loss 0.000546    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.262296    
2024-05-04 02:58:30,053 - 

2024-05-04 02:58:30,053 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:58:34,254 - Epoch: [224][   55/   55]    Overall Loss 0.000322    Objective Loss 0.000322    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.051845    
2024-05-04 02:58:34,533 - 

2024-05-04 02:58:34,534 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:58:58,889 - Epoch: [224][  100/  217]    Overall Loss 0.000849    Objective Loss 0.000849                                        LR 0.000016    Time 0.288270    
2024-05-04 02:59:23,328 - Epoch: [224][  200/  217]    Overall Loss 0.000573    Objective Loss 0.000573                                        LR 0.000016    Time 0.266289    
2024-05-04 02:59:29,721 - Epoch: [224][  217/  217]    Overall Loss 0.000536    Objective Loss 0.000536    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.274875    
2024-05-04 02:59:30,019 - 

2024-05-04 02:59:30,019 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:59:32,176 - Epoch: [225][   55/   55]    Overall Loss 0.000304    Objective Loss 0.000304    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.047893    
2024-05-04 02:59:33,076 - 

2024-05-04 02:59:33,077 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 02:59:59,581 - Epoch: [225][  100/  217]    Overall Loss 0.000307    Objective Loss 0.000307                                        LR 0.000016    Time 0.295529    
2024-05-04 03:00:24,215 - Epoch: [225][  200/  217]    Overall Loss 0.000552    Objective Loss 0.000552                                        LR 0.000016    Time 0.270892    
2024-05-04 03:00:27,736 - Epoch: [225][  217/  217]    Overall Loss 0.000516    Objective Loss 0.000516    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.265884    
2024-05-04 03:00:27,929 - 

2024-05-04 03:00:27,930 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:00:31,265 - Epoch: [226][   55/   55]    Overall Loss 0.000377    Objective Loss 0.000377    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.057773    
2024-05-04 03:00:32,349 - 

2024-05-04 03:00:32,351 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:00:56,901 - Epoch: [226][  100/  217]    Overall Loss 0.000760    Objective Loss 0.000760                                        LR 0.000016    Time 0.289617    
2024-05-04 03:01:24,618 - Epoch: [226][  200/  217]    Overall Loss 0.000564    Objective Loss 0.000564                                        LR 0.000016    Time 0.283354    
2024-05-04 03:01:31,352 - Epoch: [226][  217/  217]    Overall Loss 0.000525    Objective Loss 0.000525    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.292177    
2024-05-04 03:01:32,037 - 

2024-05-04 03:01:32,037 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:01:35,579 - Epoch: [227][   55/   55]    Overall Loss 0.000452    Objective Loss 0.000452    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.149421    
2024-05-04 03:01:36,348 - 

2024-05-04 03:01:36,349 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:02:00,181 - Epoch: [227][  100/  217]    Overall Loss 0.000510    Objective Loss 0.000510                                        LR 0.000016    Time 0.281350    
2024-05-04 03:02:29,669 - Epoch: [227][  200/  217]    Overall Loss 0.000438    Objective Loss 0.000438                                        LR 0.000016    Time 0.288072    
2024-05-04 03:02:34,126 - Epoch: [228][   55/   55]    Overall Loss 0.000352    Objective Loss 0.000352    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.050300    
2024-05-04 03:02:34,476 - Epoch: [227][  217/  217]    Overall Loss 0.000502    Objective Loss 0.000502    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.287644    
2024-05-04 03:02:34,590 - 

2024-05-04 03:02:34,591 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:02:34,985 - 

2024-05-04 03:02:34,986 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:03:05,179 - Epoch: [228][  100/  217]    Overall Loss 0.000713    Objective Loss 0.000713                                        LR 0.000016    Time 0.301809    
2024-05-04 03:03:28,693 - Epoch: [228][  200/  217]    Overall Loss 0.000504    Objective Loss 0.000504                                        LR 0.000016    Time 0.268433    
2024-05-04 03:03:32,335 - Epoch: [228][  217/  217]    Overall Loss 0.000472    Objective Loss 0.000472    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.264178    
2024-05-04 03:03:32,586 - 

2024-05-04 03:03:32,586 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:03:32,708 - Epoch: [229][   55/   55]    Overall Loss 0.000317    Objective Loss 0.000317    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.056492    
2024-05-04 03:03:33,120 - 

2024-05-04 03:03:33,122 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:03:58,284 - Epoch: [229][  100/  217]    Overall Loss 0.000294    Objective Loss 0.000294                                        LR 0.000016    Time 0.256881    
2024-05-04 03:04:26,657 - Epoch: [229][  200/  217]    Overall Loss 0.000395    Objective Loss 0.000395                                        LR 0.000016    Time 0.270263    
2024-05-04 03:04:29,782 - Epoch: [229][  217/  217]    Overall Loss 0.000487    Objective Loss 0.000487    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.263483    
2024-05-04 03:04:30,003 - 

2024-05-04 03:04:30,004 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:04:32,710 - Epoch: [230][   55/   55]    Overall Loss 0.000375    Objective Loss 0.000375    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.083227    
2024-05-04 03:04:32,927 - --- validate (epoch=230)-----------
2024-05-04 03:04:32,928 - 1736 samples (128 per mini-batch)
2024-05-04 03:04:51,478 - Epoch: [230][   14/   14]    Loss 5.919438    Top1 50.979263    Top5 67.453917    
2024-05-04 03:04:51,716 - ==> Top1: 50.979    Top5: 67.454    Loss: 5.919

2024-05-04 03:04:51,726 - ==> Best [Top1: 51.210   Top5: 67.454   Sparsity:0.00   Params: 1342920 on epoch: 210]
2024-05-04 03:04:51,726 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 03:04:51,810 - 

2024-05-04 03:04:51,810 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:04:58,630 - Epoch: [230][  100/  217]    Overall Loss 0.000768    Objective Loss 0.000768                                        LR 0.000016    Time 0.286185    
2024-05-04 03:05:24,388 - Epoch: [230][  200/  217]    Overall Loss 0.000548    Objective Loss 0.000548                                        LR 0.000016    Time 0.271839    
2024-05-04 03:05:28,416 - Epoch: [230][  217/  217]    Overall Loss 0.000514    Objective Loss 0.000514    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.269099    
2024-05-04 03:05:28,638 - --- validate (epoch=230)-----------
2024-05-04 03:05:28,639 - 1736 samples (32 per mini-batch)
2024-05-04 03:05:44,070 - Epoch: [230][   55/   55]    Loss 3.111953    Top1 59.101382    Top5 74.423963    
2024-05-04 03:05:44,293 - ==> Top1: 59.101    Top5: 74.424    Loss: 3.112

2024-05-04 03:05:44,299 - ==> Best [Top1: 59.793   Top5: 74.424   Sparsity:0.00   Params: 384080 on epoch: 140]
2024-05-04 03:05:44,300 - Saving checkpoint to: logs/2024.05.03-230951/checkpoint.pth.tar
2024-05-04 03:05:44,339 - 

2024-05-04 03:05:44,339 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:05:50,892 - Epoch: [231][   55/   55]    Overall Loss 0.000372    Objective Loss 0.000372    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.074054    
2024-05-04 03:05:51,451 - 

2024-05-04 03:05:51,452 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:06:13,134 - Epoch: [231][  100/  217]    Overall Loss 0.000544    Objective Loss 0.000544                                        LR 0.000016    Time 0.287863    
2024-05-04 03:06:41,404 - Epoch: [231][  200/  217]    Overall Loss 0.000531    Objective Loss 0.000531                                        LR 0.000016    Time 0.285238    
2024-05-04 03:06:44,412 - Epoch: [231][  217/  217]    Overall Loss 0.000497    Objective Loss 0.000497    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.276748    
2024-05-04 03:06:44,648 - 

2024-05-04 03:06:44,649 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:06:47,477 - Epoch: [232][   55/   55]    Overall Loss 0.000364    Objective Loss 0.000364    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.018433    
2024-05-04 03:06:48,037 - 

2024-05-04 03:06:48,039 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:07:14,093 - Epoch: [232][  100/  217]    Overall Loss 0.000510    Objective Loss 0.000510                                        LR 0.000016    Time 0.294340    
2024-05-04 03:07:40,544 - Epoch: [232][  200/  217]    Overall Loss 0.000519    Objective Loss 0.000519                                        LR 0.000016    Time 0.279382    
2024-05-04 03:07:43,358 - Epoch: [232][  217/  217]    Overall Loss 0.000483    Objective Loss 0.000483    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.270451    
2024-05-04 03:07:43,718 - 

2024-05-04 03:07:43,718 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:07:49,712 - Epoch: [233][   55/   55]    Overall Loss 0.000315    Objective Loss 0.000315    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.121094    
2024-05-04 03:07:50,420 - 

2024-05-04 03:07:50,422 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:08:12,803 - Epoch: [233][  100/  217]    Overall Loss 0.000588    Objective Loss 0.000588                                        LR 0.000016    Time 0.290753    
2024-05-04 03:08:39,461 - Epoch: [233][  200/  217]    Overall Loss 0.000562    Objective Loss 0.000562                                        LR 0.000016    Time 0.278629    
2024-05-04 03:08:42,630 - Epoch: [233][  217/  217]    Overall Loss 0.000524    Objective Loss 0.000524    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.271394    
2024-05-04 03:08:43,088 - 

2024-05-04 03:08:43,088 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:08:46,571 - Epoch: [234][   55/   55]    Overall Loss 0.000357    Objective Loss 0.000357    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.020681    
2024-05-04 03:08:47,688 - 

2024-05-04 03:08:47,689 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:09:10,497 - Epoch: [234][  100/  217]    Overall Loss 0.000282    Objective Loss 0.000282                                        LR 0.000016    Time 0.274001    
2024-05-04 03:09:34,938 - Epoch: [234][  200/  217]    Overall Loss 0.000534    Objective Loss 0.000534                                        LR 0.000016    Time 0.259161    
2024-05-04 03:09:40,068 - Epoch: [234][  217/  217]    Overall Loss 0.000500    Objective Loss 0.000500    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.262495    
2024-05-04 03:09:40,280 - 

2024-05-04 03:09:40,280 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:09:45,744 - Epoch: [235][   55/   55]    Overall Loss 0.000334    Objective Loss 0.000334    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.055346    
2024-05-04 03:09:46,637 - 

2024-05-04 03:09:46,639 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:10:07,936 - Epoch: [235][  100/  217]    Overall Loss 0.000512    Objective Loss 0.000512                                        LR 0.000016    Time 0.276469    
2024-05-04 03:10:35,857 - Epoch: [235][  200/  217]    Overall Loss 0.000566    Objective Loss 0.000566                                        LR 0.000016    Time 0.277797    
2024-05-04 03:10:39,863 - Epoch: [235][  217/  217]    Overall Loss 0.000530    Objective Loss 0.000530    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.274487    
2024-05-04 03:10:40,067 - 

2024-05-04 03:10:40,067 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:10:49,219 - Epoch: [236][   55/   55]    Overall Loss 0.000394    Objective Loss 0.000394    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.137635    
2024-05-04 03:10:50,095 - 

2024-05-04 03:10:50,096 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:11:04,491 - Epoch: [236][  100/  217]    Overall Loss 0.000089    Objective Loss 0.000089                                        LR 0.000016    Time 0.244143    
2024-05-04 03:11:31,681 - Epoch: [236][  200/  217]    Overall Loss 0.000418    Objective Loss 0.000418                                        LR 0.000016    Time 0.257981    
2024-05-04 03:11:36,461 - Epoch: [236][  217/  217]    Overall Loss 0.000505    Objective Loss 0.000505    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.259792    
2024-05-04 03:11:36,709 - 

2024-05-04 03:11:36,710 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:11:51,790 - Epoch: [237][   55/   55]    Overall Loss 0.000342    Objective Loss 0.000342    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.121517    
2024-05-04 03:11:52,998 - 

2024-05-04 03:11:52,999 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:12:01,316 - Epoch: [237][  100/  217]    Overall Loss 0.000817    Objective Loss 0.000817                                        LR 0.000016    Time 0.245972    
2024-05-04 03:12:26,958 - Epoch: [237][  200/  217]    Overall Loss 0.000559    Objective Loss 0.000559                                        LR 0.000016    Time 0.251154    
2024-05-04 03:12:30,774 - Epoch: [237][  217/  217]    Overall Loss 0.000522    Objective Loss 0.000522    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.249055    
2024-05-04 03:12:31,002 - 

2024-05-04 03:12:31,003 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:12:50,429 - Epoch: [238][   55/   55]    Overall Loss 0.000346    Objective Loss 0.000346    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.043995    
2024-05-04 03:12:51,011 - 

2024-05-04 03:12:51,011 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:12:56,635 - Epoch: [238][  100/  217]    Overall Loss 0.000732    Objective Loss 0.000732                                        LR 0.000016    Time 0.256229    
2024-05-04 03:13:21,061 - Epoch: [238][  200/  217]    Overall Loss 0.000523    Objective Loss 0.000523                                        LR 0.000016    Time 0.250206    
2024-05-04 03:13:23,875 - Epoch: [238][  217/  217]    Overall Loss 0.000488    Objective Loss 0.000488    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.243562    
2024-05-04 03:13:24,076 - 

2024-05-04 03:13:24,076 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:13:46,583 - Epoch: [239][   55/   55]    Overall Loss 0.000349    Objective Loss 0.000349    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.010207    
2024-05-04 03:13:47,069 - 

2024-05-04 03:13:47,071 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:13:52,592 - Epoch: [239][  100/  217]    Overall Loss 0.000446    Objective Loss 0.000446                                        LR 0.000016    Time 0.285066    
2024-05-04 03:14:17,176 - Epoch: [239][  200/  217]    Overall Loss 0.000443    Objective Loss 0.000443                                        LR 0.000016    Time 0.265411    
2024-05-04 03:14:21,575 - Epoch: [239][  217/  217]    Overall Loss 0.000510    Objective Loss 0.000510    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.264884    
2024-05-04 03:14:21,963 - 

2024-05-04 03:14:21,964 - Initiating quantization aware training (QAT)...
2024-05-04 03:14:22,014 - 

2024-05-04 03:14:22,015 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:14:47,532 - Epoch: [240][  100/  217]    Overall Loss 1.969469    Objective Loss 1.969469                                        LR 0.000016    Time 0.255087    
2024-05-04 03:14:49,527 - Epoch: [240][   55/   55]    Overall Loss 0.000327    Objective Loss 0.000327    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.135353    
2024-05-04 03:14:50,093 - --- validate (epoch=240)-----------
2024-05-04 03:14:50,096 - 1736 samples (128 per mini-batch)
2024-05-04 03:15:11,348 - Epoch: [240][   14/   14]    Loss 5.847933    Top1 50.691244    Top5 67.050691    
2024-05-04 03:15:12,160 - ==> Top1: 50.691    Top5: 67.051    Loss: 5.848

2024-05-04 03:15:12,174 - ==> Best [Top1: 51.210   Top5: 67.454   Sparsity:0.00   Params: 1342920 on epoch: 210]
2024-05-04 03:15:12,175 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 03:15:12,265 - 

2024-05-04 03:15:12,265 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:15:15,822 - Epoch: [240][  200/  217]    Overall Loss 1.532329    Objective Loss 1.532329                                        LR 0.000016    Time 0.268954    
2024-05-04 03:15:19,056 - Epoch: [240][  217/  217]    Overall Loss 1.480325    Objective Loss 1.480325    Top1 83.606557    Top5 96.721311    LR 0.000016    Time 0.262780    
2024-05-04 03:15:19,394 - --- validate (epoch=240)-----------
2024-05-04 03:15:19,395 - 1736 samples (32 per mini-batch)
2024-05-04 03:15:38,352 - Epoch: [240][   55/   55]    Loss 2.200537    Top1 51.036866    Top5 69.009217    
2024-05-04 03:15:38,839 - ==> Top1: 51.037    Top5: 69.009    Loss: 2.201

2024-05-04 03:15:38,843 - ==> Best [Top1: 51.037   Top5: 69.009   Sparsity:0.00   Params: 384080 on epoch: 240]
2024-05-04 03:15:38,844 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 03:15:38,898 - 

2024-05-04 03:15:38,898 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:16:06,390 - Epoch: [241][  100/  217]    Overall Loss 0.782280    Objective Loss 0.782280                                        LR 0.000016    Time 0.274811    
2024-05-04 03:16:07,061 - Epoch: [241][   55/   55]    Overall Loss 0.000321    Objective Loss 0.000321    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.996093    
2024-05-04 03:16:07,380 - 

2024-05-04 03:16:07,381 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:16:32,992 - Epoch: [241][  200/  217]    Overall Loss 0.753226    Objective Loss 0.753226                                        LR 0.000016    Time 0.270368    
2024-05-04 03:16:37,886 - Epoch: [241][  217/  217]    Overall Loss 0.745622    Objective Loss 0.745622    Top1 86.885246    Top5 100.000000    LR 0.000016    Time 0.271728    
2024-05-04 03:16:38,295 - 

2024-05-04 03:16:38,296 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:17:06,793 - Epoch: [242][  100/  217]    Overall Loss 0.607270    Objective Loss 0.607270                                        LR 0.000016    Time 0.284864    
2024-05-04 03:17:10,449 - Epoch: [242][   55/   55]    Overall Loss 0.000376    Objective Loss 0.000376    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.146507    
2024-05-04 03:17:11,228 - 

2024-05-04 03:17:11,229 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:17:34,932 - Epoch: [242][  200/  217]    Overall Loss 0.592939    Objective Loss 0.592939                                        LR 0.000016    Time 0.283086    
2024-05-04 03:17:37,715 - Epoch: [242][  217/  217]    Overall Loss 0.592042    Objective Loss 0.592042    Top1 80.327869    Top5 98.360656    LR 0.000016    Time 0.273728    
2024-05-04 03:17:38,058 - 

2024-05-04 03:17:38,059 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:18:06,524 - Epoch: [243][  100/  217]    Overall Loss 0.525362    Objective Loss 0.525362                                        LR 0.000016    Time 0.284575    
2024-05-04 03:18:11,219 - Epoch: [243][   55/   55]    Overall Loss 0.000305    Objective Loss 0.000305    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.090426    
2024-05-04 03:18:12,117 - 

2024-05-04 03:18:12,118 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:18:30,761 - Epoch: [243][  200/  217]    Overall Loss 0.504820    Objective Loss 0.504820                                        LR 0.000016    Time 0.263427    
2024-05-04 03:18:35,862 - Epoch: [243][  217/  217]    Overall Loss 0.502012    Objective Loss 0.502012    Top1 91.803279    Top5 98.360656    LR 0.000016    Time 0.266291    
2024-05-04 03:18:36,288 - 

2024-05-04 03:18:36,289 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:19:07,086 - Epoch: [244][  100/  217]    Overall Loss 0.469406    Objective Loss 0.469406                                        LR 0.000016    Time 0.307880    
2024-05-04 03:19:18,131 - Epoch: [244][   55/   55]    Overall Loss 0.000308    Objective Loss 0.000308    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.200065    
2024-05-04 03:19:18,700 - 

2024-05-04 03:19:18,701 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:19:30,326 - Epoch: [244][  200/  217]    Overall Loss 0.447247    Objective Loss 0.447247                                        LR 0.000016    Time 0.270102    
2024-05-04 03:19:37,249 - Epoch: [244][  217/  217]    Overall Loss 0.446319    Objective Loss 0.446319    Top1 90.163934    Top5 100.000000    LR 0.000016    Time 0.280835    
2024-05-04 03:19:37,614 - 

2024-05-04 03:19:37,615 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:20:09,209 - Epoch: [245][  100/  217]    Overall Loss 0.405090    Objective Loss 0.405090                                        LR 0.000016    Time 0.315856    
2024-05-04 03:20:17,387 - Epoch: [245][   55/   55]    Overall Loss 0.000368    Objective Loss 0.000368    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.066824    
2024-05-04 03:20:18,117 - 

2024-05-04 03:20:18,118 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:20:35,225 - Epoch: [245][  200/  217]    Overall Loss 0.400714    Objective Loss 0.400714                                        LR 0.000016    Time 0.287959    
2024-05-04 03:20:37,997 - Epoch: [245][  217/  217]    Overall Loss 0.401413    Objective Loss 0.401413    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.278166    
2024-05-04 03:20:38,337 - 

2024-05-04 03:20:38,338 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:21:08,922 - Epoch: [246][  100/  217]    Overall Loss 0.376669    Objective Loss 0.376669                                        LR 0.000016    Time 0.305749    
2024-05-04 03:21:17,778 - Epoch: [246][   55/   55]    Overall Loss 0.000352    Objective Loss 0.000352    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.084541    
2024-05-04 03:21:18,054 - 

2024-05-04 03:21:18,056 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:21:33,671 - Epoch: [246][  200/  217]    Overall Loss 0.371141    Objective Loss 0.371141                                        LR 0.000016    Time 0.276582    
2024-05-04 03:21:36,511 - Epoch: [246][  217/  217]    Overall Loss 0.371417    Objective Loss 0.371417    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.267994    
2024-05-04 03:21:36,721 - 

2024-05-04 03:21:36,722 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:22:03,379 - Epoch: [247][  100/  217]    Overall Loss 0.354113    Objective Loss 0.354113                                        LR 0.000016    Time 0.266479    
2024-05-04 03:22:20,202 - Epoch: [247][   55/   55]    Overall Loss 0.000370    Objective Loss 0.000370    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.129729    
2024-05-04 03:22:20,547 - 

2024-05-04 03:22:20,547 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:22:31,665 - Epoch: [247][  200/  217]    Overall Loss 0.345227    Objective Loss 0.345227                                        LR 0.000016    Time 0.274631    
2024-05-04 03:22:35,070 - Epoch: [247][  217/  217]    Overall Loss 0.345583    Objective Loss 0.345583    Top1 91.803279    Top5 98.360656    LR 0.000016    Time 0.268799    
2024-05-04 03:22:35,270 - 

2024-05-04 03:22:35,270 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:23:03,814 - Epoch: [248][  100/  217]    Overall Loss 0.304493    Objective Loss 0.304493                                        LR 0.000016    Time 0.285354    
2024-05-04 03:23:19,325 - Epoch: [248][   55/   55]    Overall Loss 0.000343    Objective Loss 0.000343    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.068502    
2024-05-04 03:23:19,635 - 

2024-05-04 03:23:19,636 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:23:32,196 - Epoch: [248][  200/  217]    Overall Loss 0.310655    Objective Loss 0.310655                                        LR 0.000016    Time 0.284550    
2024-05-04 03:23:35,025 - Epoch: [248][  217/  217]    Overall Loss 0.311058    Objective Loss 0.311058    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.275289    
2024-05-04 03:23:35,273 - 

2024-05-04 03:23:35,274 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:24:06,680 - Epoch: [249][  100/  217]    Overall Loss 0.281942    Objective Loss 0.281942                                        LR 0.000016    Time 0.313970    
2024-05-04 03:24:18,821 - Epoch: [249][   55/   55]    Overall Loss 0.000351    Objective Loss 0.000351    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.075901    
2024-05-04 03:24:19,264 - 

2024-05-04 03:24:19,265 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:24:37,114 - Epoch: [249][  200/  217]    Overall Loss 0.292491    Objective Loss 0.292491                                        LR 0.000016    Time 0.309112    
2024-05-04 03:24:42,356 - Epoch: [249][  217/  217]    Overall Loss 0.294201    Objective Loss 0.294201    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.309046    
2024-05-04 03:24:42,692 - 

2024-05-04 03:24:42,692 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:25:12,392 - Epoch: [250][   55/   55]    Overall Loss 0.000350    Objective Loss 0.000350    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.965745    
2024-05-04 03:25:12,782 - --- validate (epoch=250)-----------
2024-05-04 03:25:12,782 - 1736 samples (128 per mini-batch)
2024-05-04 03:25:15,758 - Epoch: [250][  100/  217]    Overall Loss 0.284007    Objective Loss 0.284007                                        LR 0.000016    Time 0.330577    
2024-05-04 03:25:30,893 - Epoch: [250][   14/   14]    Loss 5.855954    Top1 50.460829    Top5 67.281106    
2024-05-04 03:25:31,214 - ==> Top1: 50.461    Top5: 67.281    Loss: 5.856

2024-05-04 03:25:31,248 - ==> Best [Top1: 51.210   Top5: 67.454   Sparsity:0.00   Params: 1342920 on epoch: 210]
2024-05-04 03:25:31,248 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 03:25:31,340 - 

2024-05-04 03:25:31,340 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:25:39,219 - Epoch: [250][  200/  217]    Overall Loss 0.284564    Objective Loss 0.284564                                        LR 0.000016    Time 0.282549    
2024-05-04 03:25:43,086 - Epoch: [250][  217/  217]    Overall Loss 0.287463    Objective Loss 0.287463    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.278220    
2024-05-04 03:25:43,615 - --- validate (epoch=250)-----------
2024-05-04 03:25:43,617 - 1736 samples (32 per mini-batch)
2024-05-04 03:25:59,718 - Epoch: [250][   55/   55]    Loss 2.134001    Top1 55.184332    Top5 72.235023    
2024-05-04 03:25:59,902 - ==> Top1: 55.184    Top5: 72.235    Loss: 2.134

2024-05-04 03:25:59,907 - ==> Best [Top1: 55.184   Top5: 72.235   Sparsity:0.00   Params: 384080 on epoch: 250]
2024-05-04 03:25:59,907 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 03:25:59,955 - 

2024-05-04 03:25:59,956 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:26:26,767 - Epoch: [251][   55/   55]    Overall Loss 0.000311    Objective Loss 0.000311    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.007561    
2024-05-04 03:26:27,086 - 

2024-05-04 03:26:27,087 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:26:28,817 - Epoch: [251][  100/  217]    Overall Loss 0.260140    Objective Loss 0.260140                                        LR 0.000016    Time 0.288516    
2024-05-04 03:26:53,162 - Epoch: [251][  200/  217]    Overall Loss 0.264800    Objective Loss 0.264800                                        LR 0.000016    Time 0.265940    
2024-05-04 03:26:58,241 - Epoch: [251][  217/  217]    Overall Loss 0.266180    Objective Loss 0.266180    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.268503    
2024-05-04 03:26:58,470 - 

2024-05-04 03:26:58,471 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:27:25,875 - Epoch: [252][  100/  217]    Overall Loss 0.250655    Objective Loss 0.250655                                        LR 0.000016    Time 0.273951    
2024-05-04 03:27:27,607 - Epoch: [252][   55/   55]    Overall Loss 0.000384    Objective Loss 0.000384    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.100150    
2024-05-04 03:27:27,864 - 

2024-05-04 03:27:27,865 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:27:52,463 - Epoch: [252][  200/  217]    Overall Loss 0.253835    Objective Loss 0.253835                                        LR 0.000016    Time 0.269879    
2024-05-04 03:27:57,182 - Epoch: [252][  217/  217]    Overall Loss 0.254689    Objective Loss 0.254689    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.270473    
2024-05-04 03:27:57,525 - 

2024-05-04 03:27:57,525 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:28:27,888 - Epoch: [253][   55/   55]    Overall Loss 0.000372    Objective Loss 0.000372    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.091149    
2024-05-04 03:28:28,216 - 

2024-05-04 03:28:28,217 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:28:29,298 - Epoch: [253][  100/  217]    Overall Loss 0.239986    Objective Loss 0.239986                                        LR 0.000016    Time 0.317636    
2024-05-04 03:28:57,516 - Epoch: [253][  200/  217]    Overall Loss 0.243780    Objective Loss 0.243780                                        LR 0.000016    Time 0.299872    
2024-05-04 03:29:00,191 - Epoch: [253][  217/  217]    Overall Loss 0.242349    Objective Loss 0.242349    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.288694    
2024-05-04 03:29:00,457 - 

2024-05-04 03:29:00,458 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:29:22,338 - Epoch: [254][   55/   55]    Overall Loss 0.000382    Objective Loss 0.000382    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.983832    
2024-05-04 03:29:22,698 - 

2024-05-04 03:29:22,699 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:29:25,075 - Epoch: [254][  100/  217]    Overall Loss 0.224802    Objective Loss 0.224802                                        LR 0.000016    Time 0.246080    
2024-05-04 03:29:54,285 - Epoch: [254][  200/  217]    Overall Loss 0.225459    Objective Loss 0.225459                                        LR 0.000016    Time 0.269050    
2024-05-04 03:29:57,443 - Epoch: [254][  217/  217]    Overall Loss 0.226735    Objective Loss 0.226735    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.262515    
2024-05-04 03:29:57,636 - 

2024-05-04 03:29:57,636 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:30:22,248 - Epoch: [255][   55/   55]    Overall Loss 0.000316    Objective Loss 0.000316    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.082513    
2024-05-04 03:30:22,596 - 

2024-05-04 03:30:22,597 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:30:26,525 - Epoch: [255][  100/  217]    Overall Loss 0.223895    Objective Loss 0.223895                                        LR 0.000016    Time 0.288794    
2024-05-04 03:30:49,198 - Epoch: [255][  200/  217]    Overall Loss 0.222362    Objective Loss 0.222362                                        LR 0.000016    Time 0.257721    
2024-05-04 03:30:53,638 - Epoch: [255][  217/  217]    Overall Loss 0.221621    Objective Loss 0.221621    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.257986    
2024-05-04 03:30:53,842 - 

2024-05-04 03:30:53,843 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:31:20,973 - Epoch: [256][  100/  217]    Overall Loss 0.211204    Objective Loss 0.211204                                        LR 0.000016    Time 0.271183    
2024-05-04 03:31:21,884 - Epoch: [256][   55/   55]    Overall Loss 0.000382    Objective Loss 0.000382    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.077622    
2024-05-04 03:31:22,886 - 

2024-05-04 03:31:22,887 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:31:48,698 - Epoch: [256][  200/  217]    Overall Loss 0.209583    Objective Loss 0.209583                                        LR 0.000016    Time 0.274154    
2024-05-04 03:31:52,132 - Epoch: [256][  217/  217]    Overall Loss 0.212014    Objective Loss 0.212014    Top1 91.803279    Top5 100.000000    LR 0.000016    Time 0.268493    
2024-05-04 03:31:52,318 - 

2024-05-04 03:31:52,318 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:32:19,443 - Epoch: [257][   55/   55]    Overall Loss 0.000400    Objective Loss 0.000400    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.028102    
2024-05-04 03:32:19,452 - Epoch: [257][  100/  217]    Overall Loss 0.196452    Objective Loss 0.196452                                        LR 0.000016    Time 0.271253    
2024-05-04 03:32:19,949 - 

2024-05-04 03:32:19,950 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:32:39,163 - Epoch: [257][  200/  217]    Overall Loss 0.202798    Objective Loss 0.202798                                        LR 0.000016    Time 0.234144    
2024-05-04 03:32:43,062 - Epoch: [257][  217/  217]    Overall Loss 0.203249    Objective Loss 0.203249    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.233759    
2024-05-04 03:32:43,316 - 

2024-05-04 03:32:43,316 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:33:13,697 - Epoch: [258][  100/  217]    Overall Loss 0.192398    Objective Loss 0.192398                                        LR 0.000016    Time 0.303728    
2024-05-04 03:33:23,772 - Epoch: [258][   55/   55]    Overall Loss 0.000323    Objective Loss 0.000323    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.160210    
2024-05-04 03:33:24,740 - 

2024-05-04 03:33:24,740 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:33:36,230 - Epoch: [258][  200/  217]    Overall Loss 0.194761    Objective Loss 0.194761                                        LR 0.000016    Time 0.264492    
2024-05-04 03:33:39,886 - Epoch: [258][  217/  217]    Overall Loss 0.194330    Objective Loss 0.194330    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.260611    
2024-05-04 03:33:40,078 - 

2024-05-04 03:33:40,079 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:34:07,374 - Epoch: [259][  100/  217]    Overall Loss 0.182223    Objective Loss 0.182223                                        LR 0.000016    Time 0.272873    
2024-05-04 03:34:27,804 - Epoch: [259][   55/   55]    Overall Loss 0.000330    Objective Loss 0.000330    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.146452    
2024-05-04 03:34:28,353 - 

2024-05-04 03:34:28,354 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:34:31,417 - Epoch: [259][  200/  217]    Overall Loss 0.185833    Objective Loss 0.185833                                        LR 0.000016    Time 0.256608    
2024-05-04 03:34:37,306 - Epoch: [259][  217/  217]    Overall Loss 0.185796    Objective Loss 0.185796    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.263636    
2024-05-04 03:34:37,557 - 

2024-05-04 03:34:37,558 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:35:04,998 - Epoch: [260][  100/  217]    Overall Loss 0.172862    Objective Loss 0.172862                                        LR 0.000016    Time 0.274307    
2024-05-04 03:35:23,874 - Epoch: [260][   55/   55]    Overall Loss 0.000359    Objective Loss 0.000359    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.009276    
2024-05-04 03:35:24,175 - --- validate (epoch=260)-----------
2024-05-04 03:35:24,176 - 1736 samples (128 per mini-batch)
2024-05-04 03:35:26,570 - Epoch: [260][  200/  217]    Overall Loss 0.179601    Objective Loss 0.179601                                        LR 0.000016    Time 0.244970    
2024-05-04 03:35:30,691 - Epoch: [260][  217/  217]    Overall Loss 0.180970    Objective Loss 0.180970    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.244754    
2024-05-04 03:35:30,935 - --- validate (epoch=260)-----------
2024-05-04 03:35:30,936 - 1736 samples (32 per mini-batch)
2024-05-04 03:35:43,682 - Epoch: [260][   14/   14]    Loss 5.786643    Top1 50.000000    Top5 66.993088    
2024-05-04 03:35:44,183 - ==> Top1: 50.000    Top5: 66.993    Loss: 5.787

2024-05-04 03:35:44,208 - ==> Best [Top1: 51.210   Top5: 67.454   Sparsity:0.00   Params: 1342920 on epoch: 210]
2024-05-04 03:35:44,209 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 03:35:44,297 - 

2024-05-04 03:35:44,297 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:35:47,551 - Epoch: [260][   55/   55]    Loss 2.149485    Top1 55.414747    Top5 72.177419    
2024-05-04 03:35:47,765 - ==> Top1: 55.415    Top5: 72.177    Loss: 2.149

2024-05-04 03:35:47,769 - ==> Best [Top1: 55.415   Top5: 72.177   Sparsity:0.00   Params: 384080 on epoch: 260]
2024-05-04 03:35:47,769 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 03:35:47,803 - 

2024-05-04 03:35:47,804 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:36:18,438 - Epoch: [261][  100/  217]    Overall Loss 0.176911    Objective Loss 0.176911                                        LR 0.000016    Time 0.306265    
2024-05-04 03:36:36,797 - Epoch: [261][  200/  217]    Overall Loss 0.183131    Objective Loss 0.183131                                        LR 0.000016    Time 0.244881    
2024-05-04 03:36:39,923 - Epoch: [261][   55/   55]    Overall Loss 0.000327    Objective Loss 0.000327    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.011141    
2024-05-04 03:36:40,978 - 

2024-05-04 03:36:40,979 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:36:41,923 - Epoch: [261][  217/  217]    Overall Loss 0.182539    Objective Loss 0.182539    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.249310    
2024-05-04 03:36:42,195 - 

2024-05-04 03:36:42,196 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:37:12,678 - Epoch: [262][  100/  217]    Overall Loss 0.168055    Objective Loss 0.168055                                        LR 0.000016    Time 0.304730    
2024-05-04 03:37:34,025 - Epoch: [262][  200/  217]    Overall Loss 0.177925    Objective Loss 0.177925                                        LR 0.000016    Time 0.259064    
2024-05-04 03:37:38,757 - Epoch: [262][  217/  217]    Overall Loss 0.177641    Objective Loss 0.177641    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.260564    
2024-05-04 03:37:38,972 - 

2024-05-04 03:37:38,973 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:37:39,105 - Epoch: [262][   55/   55]    Overall Loss 0.000353    Objective Loss 0.000353    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.056648    
2024-05-04 03:37:39,426 - 

2024-05-04 03:37:39,427 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:38:10,230 - Epoch: [263][  100/  217]    Overall Loss 0.175790    Objective Loss 0.175790                                        LR 0.000016    Time 0.312490    
2024-05-04 03:38:33,300 - Epoch: [263][  200/  217]    Overall Loss 0.169536    Objective Loss 0.169536                                        LR 0.000016    Time 0.271553    
2024-05-04 03:38:38,486 - Epoch: [263][  217/  217]    Overall Loss 0.168109    Objective Loss 0.168109    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.274173    
2024-05-04 03:38:39,093 - 

2024-05-04 03:38:39,094 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:38:41,799 - Epoch: [263][   55/   55]    Overall Loss 0.000344    Objective Loss 0.000344    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.133888    
2024-05-04 03:38:42,125 - 

2024-05-04 03:38:42,126 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:39:09,000 - Epoch: [264][  100/  217]    Overall Loss 0.150111    Objective Loss 0.150111                                        LR 0.000016    Time 0.298977    
2024-05-04 03:39:31,018 - Epoch: [264][  200/  217]    Overall Loss 0.159871    Objective Loss 0.159871                                        LR 0.000016    Time 0.259540    
2024-05-04 03:39:34,375 - Epoch: [264][  217/  217]    Overall Loss 0.159627    Objective Loss 0.159627    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.254669    
2024-05-04 03:39:34,562 - 

2024-05-04 03:39:34,563 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:39:49,647 - Epoch: [264][   55/   55]    Overall Loss 0.000313    Objective Loss 0.000313    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.227500    
2024-05-04 03:39:50,344 - 

2024-05-04 03:39:50,346 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:40:04,621 - Epoch: [265][  100/  217]    Overall Loss 0.160903    Objective Loss 0.160903                                        LR 0.000016    Time 0.300494    
2024-05-04 03:40:28,171 - Epoch: [265][  200/  217]    Overall Loss 0.164634    Objective Loss 0.164634                                        LR 0.000016    Time 0.267963    
2024-05-04 03:40:31,950 - Epoch: [265][  217/  217]    Overall Loss 0.164372    Objective Loss 0.164372    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.264375    
2024-05-04 03:40:32,170 - 

2024-05-04 03:40:32,170 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:40:52,933 - Epoch: [265][   55/   55]    Overall Loss 0.000331    Objective Loss 0.000331    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.137751    
2024-05-04 03:40:53,941 - 

2024-05-04 03:40:53,942 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:40:58,515 - Epoch: [266][  100/  217]    Overall Loss 0.153717    Objective Loss 0.153717                                        LR 0.000016    Time 0.263351    
2024-05-04 03:41:26,469 - Epoch: [266][  200/  217]    Overall Loss 0.156991    Objective Loss 0.156991                                        LR 0.000016    Time 0.271411    
2024-05-04 03:41:30,652 - Epoch: [266][  217/  217]    Overall Loss 0.156620    Objective Loss 0.156620    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.269414    
2024-05-04 03:41:30,864 - 

2024-05-04 03:41:30,865 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:41:51,602 - Epoch: [266][   55/   55]    Overall Loss 0.000322    Objective Loss 0.000322    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.048126    
2024-05-04 03:41:51,954 - 

2024-05-04 03:41:51,954 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:42:00,775 - Epoch: [267][  100/  217]    Overall Loss 0.148948    Objective Loss 0.148948                                        LR 0.000016    Time 0.299008    
2024-05-04 03:42:25,295 - Epoch: [267][  200/  217]    Overall Loss 0.150897    Objective Loss 0.150897                                        LR 0.000016    Time 0.272060    
2024-05-04 03:42:29,103 - Epoch: [267][  217/  217]    Overall Loss 0.154244    Objective Loss 0.154244    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.268286    
2024-05-04 03:42:29,425 - 

2024-05-04 03:42:29,426 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:42:51,451 - Epoch: [267][   55/   55]    Overall Loss 0.000393    Objective Loss 0.000393    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.081588    
2024-05-04 03:42:52,424 - 

2024-05-04 03:42:52,426 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:42:59,207 - Epoch: [268][  100/  217]    Overall Loss 0.142911    Objective Loss 0.142911                                        LR 0.000016    Time 0.297721    
2024-05-04 03:43:24,720 - Epoch: [268][  200/  217]    Overall Loss 0.143287    Objective Loss 0.143287                                        LR 0.000016    Time 0.276390    
2024-05-04 03:43:28,820 - Epoch: [268][  217/  217]    Overall Loss 0.143121    Objective Loss 0.143121    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.273620    
2024-05-04 03:43:29,035 - 

2024-05-04 03:43:29,036 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:43:51,696 - Epoch: [268][   55/   55]    Overall Loss 0.000357    Objective Loss 0.000357    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.077368    
2024-05-04 03:43:52,001 - 

2024-05-04 03:43:52,002 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:43:56,815 - Epoch: [269][  100/  217]    Overall Loss 0.125212    Objective Loss 0.125212                                        LR 0.000016    Time 0.277702    
2024-05-04 03:44:25,434 - Epoch: [269][  200/  217]    Overall Loss 0.136341    Objective Loss 0.136341                                        LR 0.000016    Time 0.281909    
2024-05-04 03:44:29,156 - Epoch: [269][  217/  217]    Overall Loss 0.140913    Objective Loss 0.140913    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.276965    
2024-05-04 03:44:29,384 - 

2024-05-04 03:44:29,385 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:44:52,974 - Epoch: [269][   55/   55]    Overall Loss 0.000337    Objective Loss 0.000337    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.108388    
2024-05-04 03:44:53,260 - 

2024-05-04 03:44:53,263 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:44:57,058 - Epoch: [270][  100/  217]    Overall Loss 0.140640    Objective Loss 0.140640                                        LR 0.000016    Time 0.276650    
2024-05-04 03:45:24,555 - Epoch: [270][  200/  217]    Overall Loss 0.140893    Objective Loss 0.140893                                        LR 0.000016    Time 0.275766    
2024-05-04 03:45:28,947 - Epoch: [270][  217/  217]    Overall Loss 0.140714    Objective Loss 0.140714    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.274395    
2024-05-04 03:45:29,183 - --- validate (epoch=270)-----------
2024-05-04 03:45:29,184 - 1736 samples (32 per mini-batch)
2024-05-04 03:45:45,172 - Epoch: [270][   55/   55]    Loss 2.156800    Top1 55.069124    Top5 72.407834    
2024-05-04 03:45:45,381 - ==> Top1: 55.069    Top5: 72.408    Loss: 2.157

2024-05-04 03:45:45,384 - ==> Best [Top1: 55.415   Top5: 72.177   Sparsity:0.00   Params: 384080 on epoch: 260]
2024-05-04 03:45:45,385 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 03:45:45,410 - 

2024-05-04 03:45:45,410 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:45:52,172 - Epoch: [270][   55/   55]    Overall Loss 0.000279    Objective Loss 0.000279    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.070862    
2024-05-04 03:45:52,960 - --- validate (epoch=270)-----------
2024-05-04 03:45:52,964 - 1736 samples (128 per mini-batch)
2024-05-04 03:46:14,675 - Epoch: [271][  100/  217]    Overall Loss 0.134135    Objective Loss 0.134135                                        LR 0.000016    Time 0.292565    
2024-05-04 03:46:15,318 - Epoch: [270][   14/   14]    Loss 5.788996    Top1 49.884793    Top5 67.396313    
2024-05-04 03:46:16,234 - ==> Top1: 49.885    Top5: 67.396    Loss: 5.789

2024-05-04 03:46:16,247 - ==> Best [Top1: 51.210   Top5: 67.454   Sparsity:0.00   Params: 1342920 on epoch: 210]
2024-05-04 03:46:16,247 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 03:46:16,341 - 

2024-05-04 03:46:16,342 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:46:37,390 - Epoch: [271][  200/  217]    Overall Loss 0.143931    Objective Loss 0.143931                                        LR 0.000016    Time 0.259815    
2024-05-04 03:46:41,957 - Epoch: [271][  217/  217]    Overall Loss 0.145719    Objective Loss 0.145719    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.260497    
2024-05-04 03:46:42,480 - 

2024-05-04 03:46:42,481 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:47:10,670 - Epoch: [272][  100/  217]    Overall Loss 0.142847    Objective Loss 0.142847                                        LR 0.000016    Time 0.281797    
2024-05-04 03:47:22,524 - Epoch: [271][   55/   55]    Overall Loss 0.000296    Objective Loss 0.000296    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.203125    
2024-05-04 03:47:23,343 - 

2024-05-04 03:47:23,344 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:47:35,552 - Epoch: [272][  200/  217]    Overall Loss 0.138953    Objective Loss 0.138953                                        LR 0.000016    Time 0.265268    
2024-05-04 03:47:38,656 - Epoch: [272][  217/  217]    Overall Loss 0.138192    Objective Loss 0.138192    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.258780    
2024-05-04 03:47:39,171 - 

2024-05-04 03:47:39,171 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:48:06,458 - Epoch: [273][  100/  217]    Overall Loss 0.118226    Objective Loss 0.118226                                        LR 0.000016    Time 0.272785    
2024-05-04 03:48:27,428 - Epoch: [272][   55/   55]    Overall Loss 0.000349    Objective Loss 0.000349    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.164994    
2024-05-04 03:48:27,732 - 

2024-05-04 03:48:27,733 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:48:36,600 - Epoch: [273][  200/  217]    Overall Loss 0.132216    Objective Loss 0.132216                                        LR 0.000016    Time 0.287067    
2024-05-04 03:48:40,115 - Epoch: [273][  217/  217]    Overall Loss 0.134655    Objective Loss 0.134655    Top1 96.721311    Top5 98.360656    LR 0.000016    Time 0.280765    
2024-05-04 03:48:40,594 - 

2024-05-04 03:48:40,595 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:49:08,727 - Epoch: [274][  100/  217]    Overall Loss 0.131568    Objective Loss 0.131568                                        LR 0.000016    Time 0.281235    
2024-05-04 03:49:21,460 - Epoch: [273][   55/   55]    Overall Loss 0.000333    Objective Loss 0.000333    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.976663    
2024-05-04 03:49:21,707 - 

2024-05-04 03:49:21,707 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:49:34,035 - Epoch: [274][  200/  217]    Overall Loss 0.130765    Objective Loss 0.130765                                        LR 0.000016    Time 0.267119    
2024-05-04 03:49:39,226 - Epoch: [274][  217/  217]    Overall Loss 0.130484    Objective Loss 0.130484    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.270107    
2024-05-04 03:49:39,481 - 

2024-05-04 03:49:39,482 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:50:07,417 - Epoch: [275][  100/  217]    Overall Loss 0.123674    Objective Loss 0.123674                                        LR 0.000016    Time 0.279242    
2024-05-04 03:50:17,028 - Epoch: [274][   55/   55]    Overall Loss 0.000338    Objective Loss 0.000338    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.005657    
2024-05-04 03:50:17,595 - 

2024-05-04 03:50:17,596 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:50:33,624 - Epoch: [275][  200/  217]    Overall Loss 0.126116    Objective Loss 0.126116                                        LR 0.000016    Time 0.270615    
2024-05-04 03:50:37,662 - Epoch: [275][  217/  217]    Overall Loss 0.127689    Objective Loss 0.127689    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.268014    
2024-05-04 03:50:37,892 - 

2024-05-04 03:50:37,892 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:51:05,371 - Epoch: [276][  100/  217]    Overall Loss 0.115784    Objective Loss 0.115784                                        LR 0.000016    Time 0.274705    
2024-05-04 03:51:14,586 - Epoch: [275][   55/   55]    Overall Loss 0.000433    Objective Loss 0.000433    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.035957    
2024-05-04 03:51:15,399 - 

2024-05-04 03:51:15,401 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:51:32,619 - Epoch: [276][  200/  217]    Overall Loss 0.127268    Objective Loss 0.127268                                        LR 0.000016    Time 0.273548    
2024-05-04 03:51:36,001 - Epoch: [276][  217/  217]    Overall Loss 0.128141    Objective Loss 0.128141    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.267699    
2024-05-04 03:51:36,287 - 

2024-05-04 03:51:36,288 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:52:07,422 - Epoch: [277][  100/  217]    Overall Loss 0.123857    Objective Loss 0.123857                                        LR 0.000016    Time 0.311246    
2024-05-04 03:52:13,115 - Epoch: [276][   55/   55]    Overall Loss 0.000351    Objective Loss 0.000351    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.049167    
2024-05-04 03:52:13,425 - 

2024-05-04 03:52:13,428 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:52:33,897 - Epoch: [277][  200/  217]    Overall Loss 0.128402    Objective Loss 0.128402                                        LR 0.000016    Time 0.287961    
2024-05-04 03:52:37,536 - Epoch: [277][  217/  217]    Overall Loss 0.127379    Objective Loss 0.127379    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.282162    
2024-05-04 03:52:38,005 - 

2024-05-04 03:52:38,005 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:53:09,548 - Epoch: [278][  100/  217]    Overall Loss 0.120496    Objective Loss 0.120496                                        LR 0.000016    Time 0.315344    
2024-05-04 03:53:11,344 - Epoch: [277][   55/   55]    Overall Loss 0.000397    Objective Loss 0.000397    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.052764    
2024-05-04 03:53:11,944 - 

2024-05-04 03:53:11,946 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:53:35,465 - Epoch: [278][  200/  217]    Overall Loss 0.125271    Objective Loss 0.125271                                        LR 0.000016    Time 0.287212    
2024-05-04 03:53:39,590 - Epoch: [278][  217/  217]    Overall Loss 0.126643    Objective Loss 0.126643    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.283712    
2024-05-04 03:53:39,977 - 

2024-05-04 03:53:39,978 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:54:08,556 - Epoch: [279][  100/  217]    Overall Loss 0.108014    Objective Loss 0.108014                                        LR 0.000016    Time 0.285679    
2024-05-04 03:54:13,167 - Epoch: [278][   55/   55]    Overall Loss 0.000320    Objective Loss 0.000320    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.112886    
2024-05-04 03:54:13,871 - 

2024-05-04 03:54:13,872 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:54:36,431 - Epoch: [279][  200/  217]    Overall Loss 0.121729    Objective Loss 0.121729                                        LR 0.000016    Time 0.282168    
2024-05-04 03:54:40,006 - Epoch: [279][  217/  217]    Overall Loss 0.123842    Objective Loss 0.123842    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.276531    
2024-05-04 03:54:40,274 - 

2024-05-04 03:54:40,274 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:55:06,064 - Epoch: [280][  100/  217]    Overall Loss 0.108618    Objective Loss 0.108618                                        LR 0.000016    Time 0.257810    
2024-05-04 03:55:11,411 - Epoch: [279][   55/   55]    Overall Loss 0.000286    Objective Loss 0.000286    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.045968    
2024-05-04 03:55:11,749 - 

2024-05-04 03:55:11,750 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:55:33,479 - Epoch: [280][  200/  217]    Overall Loss 0.126454    Objective Loss 0.126454                                        LR 0.000016    Time 0.265936    
2024-05-04 03:55:38,687 - Epoch: [280][  217/  217]    Overall Loss 0.125563    Objective Loss 0.125563    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.269090    
2024-05-04 03:55:38,971 - --- validate (epoch=280)-----------
2024-05-04 03:55:38,972 - 1736 samples (32 per mini-batch)
2024-05-04 03:55:56,090 - Epoch: [280][   55/   55]    Loss 2.106671    Top1 55.587558    Top5 72.811060    
2024-05-04 03:55:56,366 - ==> Top1: 55.588    Top5: 72.811    Loss: 2.107

2024-05-04 03:55:56,370 - ==> Best [Top1: 55.588   Top5: 72.811   Sparsity:0.00   Params: 384080 on epoch: 280]
2024-05-04 03:55:56,370 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 03:55:56,405 - 

2024-05-04 03:55:56,406 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:56:11,951 - Epoch: [280][   55/   55]    Overall Loss 0.000408    Objective Loss 0.000408    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.094369    
2024-05-04 03:56:13,091 - --- validate (epoch=280)-----------
2024-05-04 03:56:13,093 - 1736 samples (128 per mini-batch)
2024-05-04 03:56:23,791 - Epoch: [281][  100/  217]    Overall Loss 0.113393    Objective Loss 0.113393                                        LR 0.000016    Time 0.273765    
2024-05-04 03:56:30,872 - Epoch: [280][   14/   14]    Loss 5.773572    Top1 50.288018    Top5 67.338710    
2024-05-04 03:56:31,475 - ==> Top1: 50.288    Top5: 67.339    Loss: 5.774

2024-05-04 03:56:31,492 - ==> Best [Top1: 51.210   Top5: 67.454   Sparsity:0.00   Params: 1342920 on epoch: 210]
2024-05-04 03:56:31,493 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 03:56:31,598 - 

2024-05-04 03:56:31,599 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:56:48,931 - Epoch: [281][  200/  217]    Overall Loss 0.118401    Objective Loss 0.118401                                        LR 0.000016    Time 0.262537    
2024-05-04 03:56:52,694 - Epoch: [281][  217/  217]    Overall Loss 0.119840    Objective Loss 0.119840    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.259304    
2024-05-04 03:56:52,978 - 

2024-05-04 03:56:52,979 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:57:21,487 - Epoch: [282][  100/  217]    Overall Loss 0.108598    Objective Loss 0.108598                                        LR 0.000016    Time 0.284984    
2024-05-04 03:57:29,996 - Epoch: [281][   55/   55]    Overall Loss 0.000299    Objective Loss 0.000299    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.061570    
2024-05-04 03:57:30,432 - 

2024-05-04 03:57:30,433 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:57:44,946 - Epoch: [282][  200/  217]    Overall Loss 0.119085    Objective Loss 0.119085                                        LR 0.000016    Time 0.259746    
2024-05-04 03:57:50,182 - Epoch: [282][  217/  217]    Overall Loss 0.119530    Objective Loss 0.119530    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.263520    
2024-05-04 03:57:50,428 - 

2024-05-04 03:57:50,428 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:58:16,867 - Epoch: [283][  100/  217]    Overall Loss 0.112859    Objective Loss 0.112859                                        LR 0.000016    Time 0.264293    
2024-05-04 03:58:28,351 - Epoch: [282][   55/   55]    Overall Loss 0.000351    Objective Loss 0.000351    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.052858    
2024-05-04 03:58:28,728 - 

2024-05-04 03:58:28,728 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:58:42,555 - Epoch: [283][  200/  217]    Overall Loss 0.119108    Objective Loss 0.119108                                        LR 0.000016    Time 0.260548    
2024-05-04 03:58:46,947 - Epoch: [283][  217/  217]    Overall Loss 0.120800    Objective Loss 0.120800    Top1 95.081967    Top5 98.360656    LR 0.000016    Time 0.260370    
2024-05-04 03:58:47,177 - 

2024-05-04 03:58:47,178 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:59:14,456 - Epoch: [284][  100/  217]    Overall Loss 0.110970    Objective Loss 0.110970                                        LR 0.000016    Time 0.272689    
2024-05-04 03:59:25,924 - Epoch: [283][   55/   55]    Overall Loss 0.000390    Objective Loss 0.000390    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 1.039750    
2024-05-04 03:59:26,250 - 

2024-05-04 03:59:26,250 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 03:59:40,246 - Epoch: [284][  200/  217]    Overall Loss 0.115402    Objective Loss 0.115402                                        LR 0.000016    Time 0.265256    
2024-05-04 03:59:44,121 - Epoch: [284][  217/  217]    Overall Loss 0.118390    Objective Loss 0.118390    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.262322    
2024-05-04 03:59:44,377 - 

2024-05-04 03:59:44,377 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:00:13,663 - Epoch: [285][  100/  217]    Overall Loss 0.109295    Objective Loss 0.109295                                        LR 0.000016    Time 0.292770    
2024-05-04 04:00:32,442 - Epoch: [284][   55/   55]    Overall Loss 0.000288    Objective Loss 0.000288    Top1 99.363057    Top5 100.000000    LR 0.001298    Time 1.203352    
2024-05-04 04:00:32,790 - 

2024-05-04 04:00:32,791 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:00:37,133 - Epoch: [285][  200/  217]    Overall Loss 0.116411    Objective Loss 0.116411                                        LR 0.000016    Time 0.263698    
2024-05-04 04:00:41,056 - Epoch: [285][  217/  217]    Overall Loss 0.117051    Objective Loss 0.117051    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.261111    
2024-05-04 04:00:41,299 - 

2024-05-04 04:00:41,299 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:01:08,940 - Epoch: [286][  100/  217]    Overall Loss 0.105383    Objective Loss 0.105383                                        LR 0.000016    Time 0.276316    
2024-05-04 04:01:26,180 - Epoch: [285][   55/   55]    Overall Loss 0.000313    Objective Loss 0.000313    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.970546    
2024-05-04 04:01:26,551 - 

2024-05-04 04:01:26,552 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:01:33,498 - Epoch: [286][  200/  217]    Overall Loss 0.106870    Objective Loss 0.106870                                        LR 0.000016    Time 0.260907    
2024-05-04 04:01:38,625 - Epoch: [286][  217/  217]    Overall Loss 0.108052    Objective Loss 0.108052    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.264088    
2024-05-04 04:01:39,020 - 

2024-05-04 04:01:39,020 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:02:02,719 - Epoch: [287][  100/  217]    Overall Loss 0.099461    Objective Loss 0.099461                                        LR 0.000016    Time 0.236913    
2024-05-04 04:02:16,904 - Epoch: [286][   55/   55]    Overall Loss 0.000383    Objective Loss 0.000383    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.915307    
2024-05-04 04:02:17,674 - 

2024-05-04 04:02:17,676 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:02:24,341 - Epoch: [287][  200/  217]    Overall Loss 0.104985    Objective Loss 0.104985                                        LR 0.000016    Time 0.226524    
2024-05-04 04:02:29,472 - Epoch: [287][  217/  217]    Overall Loss 0.106418    Objective Loss 0.106418    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.232414    
2024-05-04 04:02:29,993 - 

2024-05-04 04:02:29,993 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:02:55,111 - Epoch: [288][  100/  217]    Overall Loss 0.108446    Objective Loss 0.108446                                        LR 0.000016    Time 0.251094    
2024-05-04 04:03:07,211 - Epoch: [287][   55/   55]    Overall Loss 0.000349    Objective Loss 0.000349    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.900430    
2024-05-04 04:03:07,993 - 

2024-05-04 04:03:07,996 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:03:15,405 - Epoch: [288][  200/  217]    Overall Loss 0.114683    Objective Loss 0.114683                                        LR 0.000016    Time 0.226982    
2024-05-04 04:03:19,092 - Epoch: [288][  217/  217]    Overall Loss 0.116405    Objective Loss 0.116405    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.226180    
2024-05-04 04:03:19,298 - 

2024-05-04 04:03:19,298 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:03:47,046 - Epoch: [289][  100/  217]    Overall Loss 0.110315    Objective Loss 0.110315                                        LR 0.000016    Time 0.277395    
2024-05-04 04:03:59,639 - Epoch: [288][   55/   55]    Overall Loss 0.000351    Objective Loss 0.000351    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.938710    
2024-05-04 04:03:59,938 - 

2024-05-04 04:03:59,939 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:04:07,292 - Epoch: [289][  200/  217]    Overall Loss 0.121184    Objective Loss 0.121184                                        LR 0.000016    Time 0.239886    
2024-05-04 04:04:10,577 - Epoch: [289][  217/  217]    Overall Loss 0.121141    Objective Loss 0.121141    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.236223    
2024-05-04 04:04:10,807 - 

2024-05-04 04:04:10,808 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:04:34,351 - Epoch: [290][  100/  217]    Overall Loss 0.102303    Objective Loss 0.102303                                        LR 0.000016    Time 0.235337    
2024-05-04 04:04:46,778 - Epoch: [289][   55/   55]    Overall Loss 0.000329    Objective Loss 0.000329    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.851437    
2024-05-04 04:04:47,056 - 

2024-05-04 04:04:47,057 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:04:51,668 - Epoch: [290][  200/  217]    Overall Loss 0.101757    Objective Loss 0.101757                                        LR 0.000016    Time 0.204217    
2024-05-04 04:04:54,817 - Epoch: [290][  217/  217]    Overall Loss 0.101899    Objective Loss 0.101899    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.202723    
2024-05-04 04:04:55,010 - --- validate (epoch=290)-----------
2024-05-04 04:04:55,011 - 1736 samples (32 per mini-batch)
2024-05-04 04:05:10,960 - Epoch: [290][   55/   55]    Loss 2.151508    Top1 56.278802    Top5 72.926267    
2024-05-04 04:05:11,220 - ==> Top1: 56.279    Top5: 72.926    Loss: 2.152

2024-05-04 04:05:11,226 - ==> Best [Top1: 56.279   Top5: 72.926   Sparsity:0.00   Params: 384080 on epoch: 290]
2024-05-04 04:05:11,227 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 04:05:11,280 - 

2024-05-04 04:05:11,281 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:05:36,542 - Epoch: [291][  100/  217]    Overall Loss 0.105871    Objective Loss 0.105871                                        LR 0.000016    Time 0.252532    
2024-05-04 04:05:41,447 - Epoch: [290][   55/   55]    Overall Loss 0.000309    Objective Loss 0.000309    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.988727    
2024-05-04 04:05:41,806 - --- validate (epoch=290)-----------
2024-05-04 04:05:41,807 - 1736 samples (128 per mini-batch)
2024-05-04 04:05:56,595 - Epoch: [291][  200/  217]    Overall Loss 0.109369    Objective Loss 0.109369                                        LR 0.000016    Time 0.226494    
2024-05-04 04:05:57,737 - Epoch: [290][   14/   14]    Loss 5.745904    Top1 49.827189    Top5 67.050691    
2024-05-04 04:05:58,136 - ==> Top1: 49.827    Top5: 67.051    Loss: 5.746

2024-05-04 04:05:58,152 - ==> Best [Top1: 51.210   Top5: 67.454   Sparsity:0.00   Params: 1342920 on epoch: 210]
2024-05-04 04:05:58,152 - Saving checkpoint to: logs/2024.05.03-230951/qat_checkpoint.pth.tar
2024-05-04 04:05:58,216 - 

2024-05-04 04:05:58,217 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:05:59,958 - Epoch: [291][  217/  217]    Overall Loss 0.111369    Objective Loss 0.111369    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.224237    
2024-05-04 04:06:00,161 - 

2024-05-04 04:06:00,162 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:06:22,940 - Epoch: [292][  100/  217]    Overall Loss 0.091667    Objective Loss 0.091667                                        LR 0.000016    Time 0.227697    
2024-05-04 04:06:43,172 - Epoch: [291][   55/   55]    Overall Loss 0.000426    Objective Loss 0.000426    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.817214    
2024-05-04 04:06:43,556 - 

2024-05-04 04:06:43,557 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:06:44,857 - Epoch: [292][  200/  217]    Overall Loss 0.102114    Objective Loss 0.102114                                        LR 0.000016    Time 0.223401    
2024-05-04 04:06:47,941 - Epoch: [292][  217/  217]    Overall Loss 0.102085    Objective Loss 0.102085    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.220101    
2024-05-04 04:06:48,236 - 

2024-05-04 04:06:48,237 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:07:12,354 - Epoch: [293][  100/  217]    Overall Loss 0.092841    Objective Loss 0.092841                                        LR 0.000016    Time 0.241091    
2024-05-04 04:07:32,004 - Epoch: [292][   55/   55]    Overall Loss 0.000333    Objective Loss 0.000333    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.880681    
2024-05-04 04:07:32,933 - 

2024-05-04 04:07:32,933 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:07:36,073 - Epoch: [293][  200/  217]    Overall Loss 0.101729    Objective Loss 0.101729                                        LR 0.000016    Time 0.239097    
2024-05-04 04:07:41,254 - Epoch: [293][  217/  217]    Overall Loss 0.102442    Objective Loss 0.102442    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.244237    
2024-05-04 04:07:41,484 - 

2024-05-04 04:07:41,484 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:08:08,842 - Epoch: [294][  100/  217]    Overall Loss 0.098948    Objective Loss 0.098948                                        LR 0.000016    Time 0.273489    
2024-05-04 04:08:24,660 - Epoch: [293][   55/   55]    Overall Loss 0.000470    Objective Loss 0.000470    Top1 99.363057    Top5 100.000000    LR 0.001298    Time 0.940279    
2024-05-04 04:08:25,324 - 

2024-05-04 04:08:25,324 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:08:30,096 - Epoch: [294][  200/  217]    Overall Loss 0.099468    Objective Loss 0.099468                                        LR 0.000016    Time 0.242981    
2024-05-04 04:08:32,962 - Epoch: [294][  217/  217]    Overall Loss 0.101500    Objective Loss 0.101500    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.237143    
2024-05-04 04:08:33,172 - 

2024-05-04 04:08:33,173 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:08:57,411 - Epoch: [295][  100/  217]    Overall Loss 0.101129    Objective Loss 0.101129                                        LR 0.000016    Time 0.242289    
2024-05-04 04:09:18,499 - Epoch: [295][  200/  217]    Overall Loss 0.107117    Objective Loss 0.107117                                        LR 0.000016    Time 0.226545    
2024-05-04 04:09:18,848 - Epoch: [294][   55/   55]    Overall Loss 0.000392    Objective Loss 0.000392    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.972978    
2024-05-04 04:09:19,185 - 

2024-05-04 04:09:19,186 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:09:22,800 - Epoch: [295][  217/  217]    Overall Loss 0.109876    Objective Loss 0.109876    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.228607    
2024-05-04 04:09:23,100 - 

2024-05-04 04:09:23,101 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:09:47,726 - Epoch: [296][  100/  217]    Overall Loss 0.097338    Objective Loss 0.097338                                        LR 0.000016    Time 0.246164    
2024-05-04 04:10:07,469 - Epoch: [296][  200/  217]    Overall Loss 0.103280    Objective Loss 0.103280                                        LR 0.000016    Time 0.221750    
2024-05-04 04:10:10,172 - Epoch: [296][  217/  217]    Overall Loss 0.106310    Objective Loss 0.106310    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.216828    
2024-05-04 04:10:10,383 - 

2024-05-04 04:10:10,383 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:10:10,538 - Epoch: [295][   55/   55]    Overall Loss 0.000366    Objective Loss 0.000366    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.933506    
2024-05-04 04:10:10,909 - 

2024-05-04 04:10:10,912 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:10:31,720 - Epoch: [297][  100/  217]    Overall Loss 0.096939    Objective Loss 0.096939                                        LR 0.000016    Time 0.213287    
2024-05-04 04:10:50,701 - Epoch: [297][  200/  217]    Overall Loss 0.104374    Objective Loss 0.104374                                        LR 0.000016    Time 0.201509    
2024-05-04 04:10:54,232 - Epoch: [297][  217/  217]    Overall Loss 0.104624    Objective Loss 0.104624    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.201989    
2024-05-04 04:10:54,432 - 

2024-05-04 04:10:54,432 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:10:57,936 - Epoch: [296][   55/   55]    Overall Loss 0.000318    Objective Loss 0.000318    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.854744    
2024-05-04 04:10:58,865 - 

2024-05-04 04:10:58,870 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:11:18,695 - Epoch: [298][  100/  217]    Overall Loss 0.086007    Objective Loss 0.086007                                        LR 0.000016    Time 0.242538    
2024-05-04 04:11:37,747 - Epoch: [298][  200/  217]    Overall Loss 0.099317    Objective Loss 0.099317                                        LR 0.000016    Time 0.216492    
2024-05-04 04:11:40,734 - Epoch: [298][  217/  217]    Overall Loss 0.102649    Objective Loss 0.102649    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.213288    
2024-05-04 04:11:40,992 - 

2024-05-04 04:11:40,993 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:11:46,740 - Epoch: [297][   55/   55]    Overall Loss 0.000367    Objective Loss 0.000367    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.870198    
2024-05-04 04:11:47,367 - 

2024-05-04 04:11:47,368 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:12:02,888 - Epoch: [299][  100/  217]    Overall Loss 0.090589    Objective Loss 0.090589                                        LR 0.000016    Time 0.218861    
2024-05-04 04:12:20,440 - Epoch: [299][  200/  217]    Overall Loss 0.093750    Objective Loss 0.093750                                        LR 0.000016    Time 0.197153    
2024-05-04 04:12:23,028 - Epoch: [299][  217/  217]    Overall Loss 0.092956    Objective Loss 0.092956    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.193629    
2024-05-04 04:12:23,250 - --- test ---------------------
2024-05-04 04:12:23,250 - 1736 samples (32 per mini-batch)
2024-05-04 04:12:31,858 - Epoch: [298][   55/   55]    Overall Loss 0.000327    Objective Loss 0.000327    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.808747    
2024-05-04 04:12:32,157 - 

2024-05-04 04:12:32,158 - Training epoch: 6941 samples (128 per mini-batch)
2024-05-04 04:12:35,520 - Test: [   55/   55]    Loss 2.196992    Top1 57.085253    Top5 72.983871    
2024-05-04 04:12:35,692 - ==> Top1: 57.085    Top5: 72.984    Loss: 2.197

2024-05-04 04:12:35,694 - 
2024-05-04 04:12:35,694 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.03-230951/2024.05.03-230951.log
2024-05-04 04:13:15,145 - Epoch: [299][   55/   55]    Overall Loss 0.000410    Objective Loss 0.000410    Top1 100.000000    Top5 100.000000    LR 0.001298    Time 0.781421    
2024-05-04 04:13:15,486 - --- test ---------------------
2024-05-04 04:13:15,488 - 1736 samples (128 per mini-batch)
2024-05-04 04:13:32,744 - Test: [   14/   14]    Loss 5.731654    Top1 50.172811    Top5 67.396313    
2024-05-04 04:13:33,654 - ==> Top1: 50.173    Top5: 67.396    Loss: 5.732

2024-05-04 04:13:33,666 - 
2024-05-04 04:13:33,667 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.03-230951/2024.05.03-230951.log
