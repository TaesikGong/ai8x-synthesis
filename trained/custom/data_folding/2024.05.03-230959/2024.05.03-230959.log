2024-05-03 23:09:59,088 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.03-230959/2024.05.03-230959.log
2024-05-03 23:10:03,513 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2024-05-03 23:10:03,514 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2024-05-03 23:10:03,769 - Dataset sizes:
	training=6941
	validation=1736
	test=1736
2024-05-03 23:10:03,769 - Reading compression schedule from: policies/schedule-cifar100.yaml
2024-05-03 23:10:03,776 - 

2024-05-03 23:10:03,777 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:10:32,638 - Epoch: [0][  100/  217]    Overall Loss 4.045361    Objective Loss 4.045361                                        LR 0.001000    Time 0.288494    
2024-05-03 23:10:58,890 - Epoch: [0][  200/  217]    Overall Loss 3.793805    Objective Loss 3.793805                                        LR 0.001000    Time 0.275465    
2024-05-03 23:11:02,501 - Epoch: [0][  217/  217]    Overall Loss 3.780141    Objective Loss 3.780141    Top1 18.032787    Top5 26.229508    LR 0.001000    Time 0.270518    
2024-05-03 23:11:02,787 - --- validate (epoch=0)-----------
2024-05-03 23:11:02,788 - 1736 samples (32 per mini-batch)
2024-05-03 23:11:19,456 - Epoch: [0][   55/   55]    Loss 3.562328    Top1 26.382488    Top5 38.076037    
2024-05-03 23:11:19,692 - ==> Top1: 26.382    Top5: 38.076    Loss: 3.562

2024-05-03 23:11:19,698 - ==> Best [Top1: 26.382   Top5: 38.076   Sparsity:0.00   Params: 390704 on epoch: 0]
2024-05-03 23:11:19,699 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-03 23:11:19,747 - 

2024-05-03 23:11:19,748 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:11:52,196 - Epoch: [1][  100/  217]    Overall Loss 3.284584    Objective Loss 3.284584                                        LR 0.001000    Time 0.324371    
2024-05-03 23:12:18,271 - Epoch: [1][  200/  217]    Overall Loss 3.252635    Objective Loss 3.252635                                        LR 0.001000    Time 0.292482    
2024-05-03 23:12:22,134 - Epoch: [1][  217/  217]    Overall Loss 3.238313    Objective Loss 3.238313    Top1 37.704918    Top5 45.901639    LR 0.001000    Time 0.287361    
2024-05-03 23:12:22,858 - 

2024-05-03 23:12:22,859 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:12:57,134 - Epoch: [2][  100/  217]    Overall Loss 3.009559    Objective Loss 3.009559                                        LR 0.001000    Time 0.342642    
2024-05-03 23:13:23,433 - Epoch: [2][  200/  217]    Overall Loss 2.931826    Objective Loss 2.931826                                        LR 0.001000    Time 0.302771    
2024-05-03 23:13:27,168 - Epoch: [2][  217/  217]    Overall Loss 2.916136    Objective Loss 2.916136    Top1 42.622951    Top5 59.016393    LR 0.001000    Time 0.296258    
2024-05-03 23:13:27,556 - 

2024-05-03 23:13:27,556 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:14:01,346 - Epoch: [3][  100/  217]    Overall Loss 2.689806    Objective Loss 2.689806                                        LR 0.001000    Time 0.337785    
2024-05-03 23:14:29,001 - Epoch: [3][  200/  217]    Overall Loss 2.605391    Objective Loss 2.605391                                        LR 0.001000    Time 0.307123    
2024-05-03 23:14:34,542 - Epoch: [3][  217/  217]    Overall Loss 2.592000    Objective Loss 2.592000    Top1 45.901639    Top5 62.295082    LR 0.001000    Time 0.308579    
2024-05-03 23:14:35,984 - 

2024-05-03 23:14:35,985 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:15:07,189 - Epoch: [4][  100/  217]    Overall Loss 2.311648    Objective Loss 2.311648                                        LR 0.001000    Time 0.311935    
2024-05-03 23:15:39,280 - Epoch: [4][  200/  217]    Overall Loss 2.302949    Objective Loss 2.302949                                        LR 0.001000    Time 0.316377    
2024-05-03 23:15:42,706 - Epoch: [4][  217/  217]    Overall Loss 2.289155    Objective Loss 2.289155    Top1 59.016393    Top5 67.213115    LR 0.001000    Time 0.307367    
2024-05-03 23:15:44,072 - 

2024-05-03 23:15:44,073 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:16:16,811 - Epoch: [5][  100/  217]    Overall Loss 2.043734    Objective Loss 2.043734                                        LR 0.001000    Time 0.327264    
2024-05-03 23:16:44,255 - Epoch: [5][  200/  217]    Overall Loss 1.999981    Objective Loss 1.999981                                        LR 0.001000    Time 0.300804    
2024-05-03 23:16:47,675 - Epoch: [5][  217/  217]    Overall Loss 2.000689    Objective Loss 2.000689    Top1 59.016393    Top5 75.409836    LR 0.001000    Time 0.292991    
2024-05-03 23:16:48,116 - 

2024-05-03 23:16:48,116 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:17:23,448 - Epoch: [6][  100/  217]    Overall Loss 1.789130    Objective Loss 1.789130                                        LR 0.001000    Time 0.353214    
2024-05-03 23:17:47,477 - Epoch: [6][  200/  217]    Overall Loss 1.770306    Objective Loss 1.770306                                        LR 0.001000    Time 0.296712    
2024-05-03 23:17:53,186 - Epoch: [6][  217/  217]    Overall Loss 1.766479    Objective Loss 1.766479    Top1 59.016393    Top5 72.131148    LR 0.001000    Time 0.299767    
2024-05-03 23:17:53,641 - 

2024-05-03 23:17:53,642 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:18:26,322 - Epoch: [7][  100/  217]    Overall Loss 1.567617    Objective Loss 1.567617                                        LR 0.001000    Time 0.326702    
2024-05-03 23:18:53,899 - Epoch: [7][  200/  217]    Overall Loss 1.555934    Objective Loss 1.555934                                        LR 0.001000    Time 0.301188    
2024-05-03 23:18:56,769 - Epoch: [7][  217/  217]    Overall Loss 1.547315    Objective Loss 1.547315    Top1 72.131148    Top5 86.885246    LR 0.001000    Time 0.290806    
2024-05-03 23:18:57,290 - 

2024-05-03 23:18:57,290 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:19:25,252 - Epoch: [8][  100/  217]    Overall Loss 1.293159    Objective Loss 1.293159                                        LR 0.001000    Time 0.279516    
2024-05-03 23:19:48,630 - Epoch: [8][  200/  217]    Overall Loss 1.327513    Objective Loss 1.327513                                        LR 0.001000    Time 0.256604    
2024-05-03 23:19:54,893 - Epoch: [8][  217/  217]    Overall Loss 1.336570    Objective Loss 1.336570    Top1 72.131148    Top5 86.885246    LR 0.001000    Time 0.265357    
2024-05-03 23:19:55,134 - 

2024-05-03 23:19:55,134 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:20:28,022 - Epoch: [9][  100/  217]    Overall Loss 1.124251    Objective Loss 1.124251                                        LR 0.001000    Time 0.328777    
2024-05-03 23:20:52,605 - Epoch: [9][  200/  217]    Overall Loss 1.131109    Objective Loss 1.131109                                        LR 0.001000    Time 0.287251    
2024-05-03 23:20:58,633 - Epoch: [9][  217/  217]    Overall Loss 1.141972    Objective Loss 1.141972    Top1 63.934426    Top5 88.524590    LR 0.001000    Time 0.292514    
2024-05-03 23:20:58,949 - 

2024-05-03 23:20:58,949 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:21:28,875 - Epoch: [10][  100/  217]    Overall Loss 0.975349    Objective Loss 0.975349                                        LR 0.001000    Time 0.299154    
2024-05-03 23:21:50,897 - Epoch: [10][  200/  217]    Overall Loss 0.989993    Objective Loss 0.989993                                        LR 0.001000    Time 0.259639    
2024-05-03 23:21:54,837 - Epoch: [10][  217/  217]    Overall Loss 0.989427    Objective Loss 0.989427    Top1 83.606557    Top5 91.803279    LR 0.001000    Time 0.257448    
2024-05-03 23:21:55,052 - --- validate (epoch=10)-----------
2024-05-03 23:21:55,052 - 1736 samples (32 per mini-batch)
2024-05-03 23:22:14,535 - Epoch: [10][   55/   55]    Loss 2.136563    Top1 52.822581    Top5 69.700461    
2024-05-03 23:22:14,909 - ==> Top1: 52.823    Top5: 69.700    Loss: 2.137

2024-05-03 23:22:14,914 - ==> Best [Top1: 52.823   Top5: 69.700   Sparsity:0.00   Params: 390704 on epoch: 10]
2024-05-03 23:22:14,914 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-03 23:22:14,970 - 

2024-05-03 23:22:14,971 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:22:45,801 - Epoch: [11][  100/  217]    Overall Loss 0.755168    Objective Loss 0.755168                                        LR 0.001000    Time 0.308210    
2024-05-03 23:23:12,970 - Epoch: [11][  200/  217]    Overall Loss 0.778311    Objective Loss 0.778311                                        LR 0.001000    Time 0.289900    
2024-05-03 23:23:17,643 - Epoch: [11][  217/  217]    Overall Loss 0.781175    Objective Loss 0.781175    Top1 86.885246    Top5 98.360656    LR 0.001000    Time 0.288716    
2024-05-03 23:23:17,879 - 

2024-05-03 23:23:17,880 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:23:47,833 - Epoch: [12][  100/  217]    Overall Loss 0.596180    Objective Loss 0.596180                                        LR 0.001000    Time 0.299428    
2024-05-03 23:24:18,076 - Epoch: [12][  200/  217]    Overall Loss 0.623124    Objective Loss 0.623124                                        LR 0.001000    Time 0.300878    
2024-05-03 23:24:23,016 - Epoch: [12][  217/  217]    Overall Loss 0.626092    Objective Loss 0.626092    Top1 83.606557    Top5 96.721311    LR 0.001000    Time 0.300066    
2024-05-03 23:24:23,219 - 

2024-05-03 23:24:23,220 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:24:54,909 - Epoch: [13][  100/  217]    Overall Loss 0.455411    Objective Loss 0.455411                                        LR 0.001000    Time 0.316782    
2024-05-03 23:25:22,910 - Epoch: [13][  200/  217]    Overall Loss 0.486765    Objective Loss 0.486765                                        LR 0.001000    Time 0.298349    
2024-05-03 23:25:27,850 - Epoch: [13][  217/  217]    Overall Loss 0.486506    Objective Loss 0.486506    Top1 88.524590    Top5 96.721311    LR 0.001000    Time 0.297734    
2024-05-03 23:25:28,109 - 

2024-05-03 23:25:28,110 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:25:58,960 - Epoch: [14][  100/  217]    Overall Loss 0.346997    Objective Loss 0.346997                                        LR 0.001000    Time 0.308397    
2024-05-03 23:26:27,686 - Epoch: [14][  200/  217]    Overall Loss 0.377853    Objective Loss 0.377853                                        LR 0.001000    Time 0.297785    
2024-05-03 23:26:31,545 - Epoch: [14][  217/  217]    Overall Loss 0.386721    Objective Loss 0.386721    Top1 90.163934    Top5 98.360656    LR 0.001000    Time 0.292225    
2024-05-03 23:26:31,852 - 

2024-05-03 23:26:31,853 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:27:04,990 - Epoch: [15][  100/  217]    Overall Loss 0.281622    Objective Loss 0.281622                                        LR 0.001000    Time 0.331255    
2024-05-03 23:27:32,858 - Epoch: [15][  200/  217]    Overall Loss 0.286094    Objective Loss 0.286094                                        LR 0.001000    Time 0.304926    
2024-05-03 23:27:36,570 - Epoch: [15][  217/  217]    Overall Loss 0.293093    Objective Loss 0.293093    Top1 95.081967    Top5 100.000000    LR 0.001000    Time 0.298131    
2024-05-03 23:27:36,921 - 

2024-05-03 23:27:36,922 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:28:07,655 - Epoch: [16][  100/  217]    Overall Loss 0.242381    Objective Loss 0.242381                                        LR 0.001000    Time 0.307220    
2024-05-03 23:28:35,919 - Epoch: [16][  200/  217]    Overall Loss 0.233187    Objective Loss 0.233187                                        LR 0.001000    Time 0.294882    
2024-05-03 23:28:39,360 - Epoch: [16][  217/  217]    Overall Loss 0.231530    Objective Loss 0.231530    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.287631    
2024-05-03 23:28:39,606 - 

2024-05-03 23:28:39,606 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:29:07,835 - Epoch: [17][  100/  217]    Overall Loss 0.143840    Objective Loss 0.143840                                        LR 0.001000    Time 0.282183    
2024-05-03 23:29:36,796 - Epoch: [17][  200/  217]    Overall Loss 0.140637    Objective Loss 0.140637                                        LR 0.001000    Time 0.285848    
2024-05-03 23:29:42,074 - Epoch: [17][  217/  217]    Overall Loss 0.142350    Objective Loss 0.142350    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.287771    
2024-05-03 23:29:42,616 - 

2024-05-03 23:29:42,617 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:30:13,510 - Epoch: [18][  100/  217]    Overall Loss 0.101249    Objective Loss 0.101249                                        LR 0.001000    Time 0.308816    
2024-05-03 23:30:42,986 - Epoch: [18][  200/  217]    Overall Loss 0.106282    Objective Loss 0.106282                                        LR 0.001000    Time 0.301743    
2024-05-03 23:30:45,724 - Epoch: [18][  217/  217]    Overall Loss 0.106871    Objective Loss 0.106871    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.290714    
2024-05-03 23:30:46,459 - 

2024-05-03 23:30:46,460 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:31:17,375 - Epoch: [19][  100/  217]    Overall Loss 0.086509    Objective Loss 0.086509                                        LR 0.001000    Time 0.309050    
2024-05-03 23:31:42,104 - Epoch: [19][  200/  217]    Overall Loss 0.083575    Objective Loss 0.083575                                        LR 0.001000    Time 0.278122    
2024-05-03 23:31:47,317 - Epoch: [19][  217/  217]    Overall Loss 0.087215    Objective Loss 0.087215    Top1 98.360656    Top5 98.360656    LR 0.001000    Time 0.280345    
2024-05-03 23:31:47,640 - 

2024-05-03 23:31:47,641 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:32:17,343 - Epoch: [20][  100/  217]    Overall Loss 0.060139    Objective Loss 0.060139                                        LR 0.001000    Time 0.296921    
2024-05-03 23:32:42,697 - Epoch: [20][  200/  217]    Overall Loss 0.058192    Objective Loss 0.058192                                        LR 0.001000    Time 0.275183    
2024-05-03 23:32:48,415 - Epoch: [20][  217/  217]    Overall Loss 0.058495    Objective Loss 0.058495    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.279968    
2024-05-03 23:32:48,635 - --- validate (epoch=20)-----------
2024-05-03 23:32:48,636 - 1736 samples (32 per mini-batch)
2024-05-03 23:33:07,225 - Epoch: [20][   55/   55]    Loss 2.140016    Top1 58.755760    Top5 74.827189    
2024-05-03 23:33:07,669 - ==> Top1: 58.756    Top5: 74.827    Loss: 2.140

2024-05-03 23:33:07,676 - ==> Best [Top1: 58.756   Top5: 74.827   Sparsity:0.00   Params: 390704 on epoch: 20]
2024-05-03 23:33:07,677 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-03 23:33:07,736 - 

2024-05-03 23:33:07,737 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:33:42,783 - Epoch: [21][  100/  217]    Overall Loss 0.049722    Objective Loss 0.049722                                        LR 0.001000    Time 0.350365    
2024-05-03 23:34:09,830 - Epoch: [21][  200/  217]    Overall Loss 0.050906    Objective Loss 0.050906                                        LR 0.001000    Time 0.310372    
2024-05-03 23:34:14,284 - Epoch: [21][  217/  217]    Overall Loss 0.052963    Objective Loss 0.052963    Top1 98.360656    Top5 98.360656    LR 0.001000    Time 0.306571    
2024-05-03 23:34:14,860 - 

2024-05-03 23:34:14,861 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:34:44,404 - Epoch: [22][  100/  217]    Overall Loss 0.050442    Objective Loss 0.050442                                        LR 0.001000    Time 0.295326    
2024-05-03 23:35:08,899 - Epoch: [22][  200/  217]    Overall Loss 0.056872    Objective Loss 0.056872                                        LR 0.001000    Time 0.270088    
2024-05-03 23:35:12,864 - Epoch: [22][  217/  217]    Overall Loss 0.060216    Objective Loss 0.060216    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.267193    
2024-05-03 23:35:13,233 - 

2024-05-03 23:35:13,234 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:35:46,942 - Epoch: [23][  100/  217]    Overall Loss 0.248240    Objective Loss 0.248240                                        LR 0.001000    Time 0.336984    
2024-05-03 23:36:10,240 - Epoch: [23][  200/  217]    Overall Loss 0.469913    Objective Loss 0.469913                                        LR 0.001000    Time 0.284935    
2024-05-03 23:36:14,580 - Epoch: [23][  217/  217]    Overall Loss 0.490357    Objective Loss 0.490357    Top1 77.049180    Top5 91.803279    LR 0.001000    Time 0.282602    
2024-05-03 23:36:14,840 - 

2024-05-03 23:36:14,841 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:36:46,400 - Epoch: [24][  100/  217]    Overall Loss 0.428007    Objective Loss 0.428007                                        LR 0.001000    Time 0.315491    
2024-05-03 23:37:12,539 - Epoch: [24][  200/  217]    Overall Loss 0.378054    Objective Loss 0.378054                                        LR 0.001000    Time 0.288393    
2024-05-03 23:37:17,875 - Epoch: [24][  217/  217]    Overall Loss 0.373765    Objective Loss 0.373765    Top1 85.245902    Top5 100.000000    LR 0.001000    Time 0.290381    
2024-05-03 23:37:18,215 - 

2024-05-03 23:37:18,216 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:37:49,733 - Epoch: [25][  100/  217]    Overall Loss 0.137516    Objective Loss 0.137516                                        LR 0.001000    Time 0.315067    
2024-05-03 23:38:17,780 - Epoch: [25][  200/  217]    Overall Loss 0.126053    Objective Loss 0.126053                                        LR 0.001000    Time 0.297722    
2024-05-03 23:38:22,490 - Epoch: [25][  217/  217]    Overall Loss 0.123726    Objective Loss 0.123726    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.296096    
2024-05-03 23:38:22,796 - 

2024-05-03 23:38:22,796 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:38:53,746 - Epoch: [26][  100/  217]    Overall Loss 0.055321    Objective Loss 0.055321                                        LR 0.001000    Time 0.309405    
2024-05-03 23:39:20,638 - Epoch: [26][  200/  217]    Overall Loss 0.052706    Objective Loss 0.052706                                        LR 0.001000    Time 0.289117    
2024-05-03 23:39:26,017 - Epoch: [26][  217/  217]    Overall Loss 0.052430    Objective Loss 0.052430    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.291246    
2024-05-03 23:39:26,490 - 

2024-05-03 23:39:26,491 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:39:58,680 - Epoch: [27][  100/  217]    Overall Loss 0.026465    Objective Loss 0.026465                                        LR 0.001000    Time 0.321789    
2024-05-03 23:40:21,649 - Epoch: [27][  200/  217]    Overall Loss 0.026651    Objective Loss 0.026651                                        LR 0.001000    Time 0.275695    
2024-05-03 23:40:27,330 - Epoch: [27][  217/  217]    Overall Loss 0.026582    Objective Loss 0.026582    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.280266    
2024-05-03 23:40:27,633 - 

2024-05-03 23:40:27,634 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:40:55,832 - Epoch: [28][  100/  217]    Overall Loss 0.019121    Objective Loss 0.019121                                        LR 0.001000    Time 0.281884    
2024-05-03 23:41:23,049 - Epoch: [28][  200/  217]    Overall Loss 0.018360    Objective Loss 0.018360                                        LR 0.001000    Time 0.276985    
2024-05-03 23:41:27,221 - Epoch: [28][  217/  217]    Overall Loss 0.018975    Objective Loss 0.018975    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.274504    
2024-05-03 23:41:27,442 - 

2024-05-03 23:41:27,443 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:41:58,931 - Epoch: [29][  100/  217]    Overall Loss 0.012219    Objective Loss 0.012219                                        LR 0.001000    Time 0.314786    
2024-05-03 23:42:25,648 - Epoch: [29][  200/  217]    Overall Loss 0.015158    Objective Loss 0.015158                                        LR 0.001000    Time 0.290924    
2024-05-03 23:42:29,386 - Epoch: [29][  217/  217]    Overall Loss 0.015155    Objective Loss 0.015155    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.285348    
2024-05-03 23:42:29,685 - 

2024-05-03 23:42:29,685 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:43:01,487 - Epoch: [30][  100/  217]    Overall Loss 0.011927    Objective Loss 0.011927                                        LR 0.001000    Time 0.317926    
2024-05-03 23:43:28,939 - Epoch: [30][  200/  217]    Overall Loss 0.012728    Objective Loss 0.012728                                        LR 0.001000    Time 0.296181    
2024-05-03 23:43:33,461 - Epoch: [30][  217/  217]    Overall Loss 0.012528    Objective Loss 0.012528    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.293807    
2024-05-03 23:43:33,773 - --- validate (epoch=30)-----------
2024-05-03 23:43:33,773 - 1736 samples (32 per mini-batch)
2024-05-03 23:43:53,831 - Epoch: [30][   55/   55]    Loss 2.238846    Top1 59.850230    Top5 75.633641    
2024-05-03 23:43:54,060 - ==> Top1: 59.850    Top5: 75.634    Loss: 2.239

2024-05-03 23:43:54,066 - ==> Best [Top1: 59.850   Top5: 75.634   Sparsity:0.00   Params: 390704 on epoch: 30]
2024-05-03 23:43:54,066 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-03 23:43:54,121 - 

2024-05-03 23:43:54,122 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:44:23,256 - Epoch: [31][  100/  217]    Overall Loss 0.012543    Objective Loss 0.012543                                        LR 0.001000    Time 0.291238    
2024-05-03 23:44:52,327 - Epoch: [31][  200/  217]    Overall Loss 0.013187    Objective Loss 0.013187                                        LR 0.001000    Time 0.290929    
2024-05-03 23:44:55,904 - Epoch: [31][  217/  217]    Overall Loss 0.013032    Objective Loss 0.013032    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.284617    
2024-05-03 23:44:56,147 - 

2024-05-03 23:44:56,148 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:45:26,247 - Epoch: [32][  100/  217]    Overall Loss 0.010026    Objective Loss 0.010026                                        LR 0.001000    Time 0.300873    
2024-05-03 23:45:52,945 - Epoch: [32][  200/  217]    Overall Loss 0.011736    Objective Loss 0.011736                                        LR 0.001000    Time 0.283879    
2024-05-03 23:45:56,428 - Epoch: [32][  217/  217]    Overall Loss 0.011473    Objective Loss 0.011473    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.277680    
2024-05-03 23:45:56,760 - 

2024-05-03 23:45:56,761 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:46:26,085 - Epoch: [33][  100/  217]    Overall Loss 0.008271    Objective Loss 0.008271                                        LR 0.001000    Time 0.293090    
2024-05-03 23:46:56,755 - Epoch: [33][  200/  217]    Overall Loss 0.008691    Objective Loss 0.008691                                        LR 0.001000    Time 0.299849    
2024-05-03 23:47:01,117 - Epoch: [33][  217/  217]    Overall Loss 0.009005    Objective Loss 0.009005    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.296450    
2024-05-03 23:47:01,387 - 

2024-05-03 23:47:01,388 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:47:31,913 - Epoch: [34][  100/  217]    Overall Loss 0.010851    Objective Loss 0.010851                                        LR 0.001000    Time 0.305131    
2024-05-03 23:47:57,902 - Epoch: [34][  200/  217]    Overall Loss 0.010372    Objective Loss 0.010372                                        LR 0.001000    Time 0.282456    
2024-05-03 23:48:01,700 - Epoch: [34][  217/  217]    Overall Loss 0.010343    Objective Loss 0.010343    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.277822    
2024-05-03 23:48:01,916 - 

2024-05-03 23:48:01,917 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:48:31,597 - Epoch: [35][  100/  217]    Overall Loss 0.011483    Objective Loss 0.011483                                        LR 0.001000    Time 0.296702    
2024-05-03 23:49:01,260 - Epoch: [35][  200/  217]    Overall Loss 0.011477    Objective Loss 0.011477                                        LR 0.001000    Time 0.296616    
2024-05-03 23:49:05,752 - Epoch: [35][  217/  217]    Overall Loss 0.011422    Objective Loss 0.011422    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.294069    
2024-05-03 23:49:05,966 - 

2024-05-03 23:49:05,967 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:49:37,135 - Epoch: [36][  100/  217]    Overall Loss 0.008186    Objective Loss 0.008186                                        LR 0.001000    Time 0.311552    
2024-05-03 23:50:02,411 - Epoch: [36][  200/  217]    Overall Loss 0.009693    Objective Loss 0.009693                                        LR 0.001000    Time 0.282103    
2024-05-03 23:50:07,480 - Epoch: [36][  217/  217]    Overall Loss 0.010246    Objective Loss 0.010246    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.283350    
2024-05-03 23:50:07,772 - 

2024-05-03 23:50:07,772 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:50:39,852 - Epoch: [37][  100/  217]    Overall Loss 0.014228    Objective Loss 0.014228                                        LR 0.001000    Time 0.320690    
2024-05-03 23:51:05,272 - Epoch: [37][  200/  217]    Overall Loss 0.030654    Objective Loss 0.030654                                        LR 0.001000    Time 0.287396    
2024-05-03 23:51:10,897 - Epoch: [37][  217/  217]    Overall Loss 0.074598    Objective Loss 0.074598    Top1 78.688525    Top5 91.803279    LR 0.001000    Time 0.290789    
2024-05-03 23:51:11,298 - 

2024-05-03 23:51:11,299 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:51:41,557 - Epoch: [38][  100/  217]    Overall Loss 1.293299    Objective Loss 1.293299                                        LR 0.001000    Time 0.302487    
2024-05-03 23:52:08,437 - Epoch: [38][  200/  217]    Overall Loss 1.095727    Objective Loss 1.095727                                        LR 0.001000    Time 0.285593    
2024-05-03 23:52:11,320 - Epoch: [38][  217/  217]    Overall Loss 1.068160    Objective Loss 1.068160    Top1 78.688525    Top5 98.360656    LR 0.001000    Time 0.276496    
2024-05-03 23:52:11,612 - 

2024-05-03 23:52:11,613 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:52:42,670 - Epoch: [39][  100/  217]    Overall Loss 0.281532    Objective Loss 0.281532                                        LR 0.001000    Time 0.310476    
2024-05-03 23:53:10,574 - Epoch: [39][  200/  217]    Overall Loss 0.269284    Objective Loss 0.269284                                        LR 0.001000    Time 0.294708    
2024-05-03 23:53:14,513 - Epoch: [39][  217/  217]    Overall Loss 0.267415    Objective Loss 0.267415    Top1 91.803279    Top5 100.000000    LR 0.001000    Time 0.289761    
2024-05-03 23:53:15,146 - 

2024-05-03 23:53:15,147 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:53:51,397 - Epoch: [40][  100/  217]    Overall Loss 0.110753    Objective Loss 0.110753                                        LR 0.001000    Time 0.362406    
2024-05-03 23:54:16,861 - Epoch: [40][  200/  217]    Overall Loss 0.097088    Objective Loss 0.097088                                        LR 0.001000    Time 0.308479    
2024-05-03 23:54:20,410 - Epoch: [40][  217/  217]    Overall Loss 0.094439    Objective Loss 0.094439    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.300653    
2024-05-03 23:54:20,791 - --- validate (epoch=40)-----------
2024-05-03 23:54:20,791 - 1736 samples (32 per mini-batch)
2024-05-03 23:54:40,849 - Epoch: [40][   55/   55]    Loss 2.352693    Top1 56.739631    Top5 74.251152    
2024-05-03 23:54:41,136 - ==> Top1: 56.740    Top5: 74.251    Loss: 2.353

2024-05-03 23:54:41,140 - ==> Best [Top1: 59.850   Top5: 75.634   Sparsity:0.00   Params: 390704 on epoch: 30]
2024-05-03 23:54:41,141 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-03 23:54:41,186 - 

2024-05-03 23:54:41,187 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:55:12,539 - Epoch: [41][  100/  217]    Overall Loss 0.040669    Objective Loss 0.040669                                        LR 0.001000    Time 0.313411    
2024-05-03 23:55:36,089 - Epoch: [41][  200/  217]    Overall Loss 0.037509    Objective Loss 0.037509                                        LR 0.001000    Time 0.274411    
2024-05-03 23:55:39,739 - Epoch: [41][  217/  217]    Overall Loss 0.037060    Objective Loss 0.037060    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.269717    
2024-05-03 23:55:40,172 - 

2024-05-03 23:55:40,173 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:56:10,927 - Epoch: [42][  100/  217]    Overall Loss 0.019762    Objective Loss 0.019762                                        LR 0.001000    Time 0.307426    
2024-05-03 23:56:37,017 - Epoch: [42][  200/  217]    Overall Loss 0.020728    Objective Loss 0.020728                                        LR 0.001000    Time 0.284120    
2024-05-03 23:56:43,270 - Epoch: [42][  217/  217]    Overall Loss 0.020777    Objective Loss 0.020777    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.290669    
2024-05-03 23:56:43,442 - 

2024-05-03 23:56:43,443 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:57:12,022 - Epoch: [43][  100/  217]    Overall Loss 0.016549    Objective Loss 0.016549                                        LR 0.001000    Time 0.285701    
2024-05-03 23:57:36,450 - Epoch: [43][  200/  217]    Overall Loss 0.016850    Objective Loss 0.016850                                        LR 0.001000    Time 0.264946    
2024-05-03 23:57:41,168 - Epoch: [43][  217/  217]    Overall Loss 0.017041    Objective Loss 0.017041    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.265921    
2024-05-03 23:57:41,607 - 

2024-05-03 23:57:41,608 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:58:13,185 - Epoch: [44][  100/  217]    Overall Loss 0.011116    Objective Loss 0.011116                                        LR 0.001000    Time 0.315665    
2024-05-03 23:58:40,090 - Epoch: [44][  200/  217]    Overall Loss 0.012088    Objective Loss 0.012088                                        LR 0.001000    Time 0.292312    
2024-05-03 23:58:43,117 - Epoch: [44][  217/  217]    Overall Loss 0.011862    Objective Loss 0.011862    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.283352    
2024-05-03 23:58:43,411 - 

2024-05-03 23:58:43,411 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:59:15,371 - Epoch: [45][  100/  217]    Overall Loss 0.009381    Objective Loss 0.009381                                        LR 0.001000    Time 0.319508    
2024-05-03 23:59:39,560 - Epoch: [45][  200/  217]    Overall Loss 0.009658    Objective Loss 0.009658                                        LR 0.001000    Time 0.280582    
2024-05-03 23:59:45,176 - Epoch: [45][  217/  217]    Overall Loss 0.009738    Objective Loss 0.009738    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.284472    
2024-05-03 23:59:45,424 - 

2024-05-03 23:59:45,425 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:00:19,897 - Epoch: [46][  100/  217]    Overall Loss 0.008860    Objective Loss 0.008860                                        LR 0.001000    Time 0.344621    
2024-05-04 00:00:45,906 - Epoch: [46][  200/  217]    Overall Loss 0.009537    Objective Loss 0.009537                                        LR 0.001000    Time 0.302311    
2024-05-04 00:00:48,955 - Epoch: [46][  217/  217]    Overall Loss 0.009981    Objective Loss 0.009981    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.292672    
2024-05-04 00:00:49,276 - 

2024-05-04 00:00:49,276 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:01:22,215 - Epoch: [47][  100/  217]    Overall Loss 0.006446    Objective Loss 0.006446                                        LR 0.001000    Time 0.329293    
2024-05-04 00:01:49,578 - Epoch: [47][  200/  217]    Overall Loss 0.008455    Objective Loss 0.008455                                        LR 0.001000    Time 0.301415    
2024-05-04 00:01:54,511 - Epoch: [47][  217/  217]    Overall Loss 0.008444    Objective Loss 0.008444    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.300507    
2024-05-04 00:01:54,727 - 

2024-05-04 00:01:54,728 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:02:26,609 - Epoch: [48][  100/  217]    Overall Loss 0.007120    Objective Loss 0.007120                                        LR 0.001000    Time 0.318721    
2024-05-04 00:02:51,492 - Epoch: [48][  200/  217]    Overall Loss 0.009500    Objective Loss 0.009500                                        LR 0.001000    Time 0.283733    
2024-05-04 00:02:54,837 - Epoch: [48][  217/  217]    Overall Loss 0.009701    Objective Loss 0.009701    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.276909    
2024-05-04 00:02:55,379 - 

2024-05-04 00:02:55,379 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:03:27,929 - Epoch: [49][  100/  217]    Overall Loss 0.010564    Objective Loss 0.010564                                        LR 0.001000    Time 0.325404    
2024-05-04 00:03:53,020 - Epoch: [49][  200/  217]    Overall Loss 0.009484    Objective Loss 0.009484                                        LR 0.001000    Time 0.288108    
2024-05-04 00:03:56,967 - Epoch: [49][  217/  217]    Overall Loss 0.009148    Objective Loss 0.009148    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.283720    
2024-05-04 00:03:57,282 - 

2024-05-04 00:03:57,283 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:04:26,134 - Epoch: [50][  100/  217]    Overall Loss 0.005145    Objective Loss 0.005145                                        LR 0.001000    Time 0.288405    
2024-05-04 00:04:55,079 - Epoch: [50][  200/  217]    Overall Loss 0.007211    Objective Loss 0.007211                                        LR 0.001000    Time 0.288880    
2024-05-04 00:05:00,874 - Epoch: [50][  217/  217]    Overall Loss 0.007576    Objective Loss 0.007576    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.292944    
2024-05-04 00:05:01,139 - --- validate (epoch=50)-----------
2024-05-04 00:05:01,139 - 1736 samples (32 per mini-batch)
2024-05-04 00:05:19,491 - Epoch: [50][   55/   55]    Loss 2.656634    Top1 55.299539    Top5 72.580645    
2024-05-04 00:05:19,686 - ==> Top1: 55.300    Top5: 72.581    Loss: 2.657

2024-05-04 00:05:19,690 - ==> Best [Top1: 59.850   Top5: 75.634   Sparsity:0.00   Params: 390704 on epoch: 30]
2024-05-04 00:05:19,691 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 00:05:19,736 - 

2024-05-04 00:05:19,736 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:05:51,400 - Epoch: [51][  100/  217]    Overall Loss 0.006718    Objective Loss 0.006718                                        LR 0.001000    Time 0.316539    
2024-05-04 00:06:16,614 - Epoch: [51][  200/  217]    Overall Loss 0.007639    Objective Loss 0.007639                                        LR 0.001000    Time 0.284294    
2024-05-04 00:06:19,637 - Epoch: [51][  217/  217]    Overall Loss 0.007364    Objective Loss 0.007364    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.275946    
2024-05-04 00:06:19,960 - 

2024-05-04 00:06:19,960 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:06:49,278 - Epoch: [52][  100/  217]    Overall Loss 0.004279    Objective Loss 0.004279                                        LR 0.001000    Time 0.293090    
2024-05-04 00:07:14,824 - Epoch: [52][  200/  217]    Overall Loss 0.005387    Objective Loss 0.005387                                        LR 0.001000    Time 0.274231    
2024-05-04 00:07:18,970 - Epoch: [52][  217/  217]    Overall Loss 0.005533    Objective Loss 0.005533    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.271842    
2024-05-04 00:07:19,217 - 

2024-05-04 00:07:19,218 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:07:50,587 - Epoch: [53][  100/  217]    Overall Loss 0.005677    Objective Loss 0.005677                                        LR 0.001000    Time 0.313603    
2024-05-04 00:08:22,634 - Epoch: [53][  200/  217]    Overall Loss 0.006190    Objective Loss 0.006190                                        LR 0.001000    Time 0.316996    
2024-05-04 00:08:26,932 - Epoch: [53][  217/  217]    Overall Loss 0.006031    Objective Loss 0.006031    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.311960    
2024-05-04 00:08:27,141 - 

2024-05-04 00:08:27,141 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:08:57,112 - Epoch: [54][  100/  217]    Overall Loss 0.242882    Objective Loss 0.242882                                        LR 0.001000    Time 0.299613    
2024-05-04 00:09:26,556 - Epoch: [54][  200/  217]    Overall Loss 0.702430    Objective Loss 0.702430                                        LR 0.001000    Time 0.296980    
2024-05-04 00:09:32,324 - Epoch: [54][  217/  217]    Overall Loss 0.717671    Objective Loss 0.717671    Top1 86.885246    Top5 95.081967    LR 0.001000    Time 0.300283    
2024-05-04 00:09:32,688 - 

2024-05-04 00:09:32,689 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:10:02,732 - Epoch: [55][  100/  217]    Overall Loss 0.408200    Objective Loss 0.408200                                        LR 0.001000    Time 0.300330    
2024-05-04 00:10:30,309 - Epoch: [55][  200/  217]    Overall Loss 0.371697    Objective Loss 0.371697                                        LR 0.001000    Time 0.288005    
2024-05-04 00:10:35,945 - Epoch: [55][  217/  217]    Overall Loss 0.364916    Objective Loss 0.364916    Top1 91.803279    Top5 100.000000    LR 0.001000    Time 0.291402    
2024-05-04 00:10:36,461 - 

2024-05-04 00:10:36,461 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:11:08,738 - Epoch: [56][  100/  217]    Overall Loss 0.101837    Objective Loss 0.101837                                        LR 0.001000    Time 0.322666    
2024-05-04 00:11:37,156 - Epoch: [56][  200/  217]    Overall Loss 0.092611    Objective Loss 0.092611                                        LR 0.001000    Time 0.303377    
2024-05-04 00:11:42,698 - Epoch: [56][  217/  217]    Overall Loss 0.092116    Objective Loss 0.092116    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.305145    
2024-05-04 00:11:43,369 - 

2024-05-04 00:11:43,370 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:12:18,354 - Epoch: [57][  100/  217]    Overall Loss 0.037052    Objective Loss 0.037052                                        LR 0.001000    Time 0.349746    
2024-05-04 00:12:44,796 - Epoch: [57][  200/  217]    Overall Loss 0.033621    Objective Loss 0.033621                                        LR 0.001000    Time 0.307033    
2024-05-04 00:12:48,770 - Epoch: [57][  217/  217]    Overall Loss 0.033525    Objective Loss 0.033525    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.301282    
2024-05-04 00:12:49,169 - 

2024-05-04 00:12:49,170 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:13:19,317 - Epoch: [58][  100/  217]    Overall Loss 0.019080    Objective Loss 0.019080                                        LR 0.001000    Time 0.301378    
2024-05-04 00:13:47,088 - Epoch: [58][  200/  217]    Overall Loss 0.019007    Objective Loss 0.019007                                        LR 0.001000    Time 0.289490    
2024-05-04 00:13:52,599 - Epoch: [58][  217/  217]    Overall Loss 0.018594    Objective Loss 0.018594    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.292198    
2024-05-04 00:13:52,902 - 

2024-05-04 00:13:52,903 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:14:24,807 - Epoch: [59][  100/  217]    Overall Loss 0.011613    Objective Loss 0.011613                                        LR 0.001000    Time 0.318937    
2024-05-04 00:14:51,678 - Epoch: [59][  200/  217]    Overall Loss 0.012405    Objective Loss 0.012405                                        LR 0.001000    Time 0.293781    
2024-05-04 00:14:56,794 - Epoch: [59][  217/  217]    Overall Loss 0.012331    Objective Loss 0.012331    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.294331    
2024-05-04 00:14:57,142 - 

2024-05-04 00:14:57,143 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:15:26,625 - Epoch: [60][  100/  217]    Overall Loss 0.009821    Objective Loss 0.009821                                        LR 0.001000    Time 0.294713    
2024-05-04 00:15:51,492 - Epoch: [60][  200/  217]    Overall Loss 0.009758    Objective Loss 0.009758                                        LR 0.001000    Time 0.271647    
2024-05-04 00:15:55,727 - Epoch: [60][  217/  217]    Overall Loss 0.009817    Objective Loss 0.009817    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.269873    
2024-05-04 00:15:55,996 - --- validate (epoch=60)-----------
2024-05-04 00:15:55,997 - 1736 samples (32 per mini-batch)
2024-05-04 00:16:15,515 - Epoch: [60][   55/   55]    Loss 2.416086    Top1 57.949309    Top5 75.633641    
2024-05-04 00:16:15,865 - ==> Top1: 57.949    Top5: 75.634    Loss: 2.416

2024-05-04 00:16:15,874 - ==> Best [Top1: 59.850   Top5: 75.634   Sparsity:0.00   Params: 390704 on epoch: 30]
2024-05-04 00:16:15,874 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 00:16:15,923 - 

2024-05-04 00:16:15,923 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:16:48,651 - Epoch: [61][  100/  217]    Overall Loss 0.008404    Objective Loss 0.008404                                        LR 0.001000    Time 0.327185    
2024-05-04 00:17:15,826 - Epoch: [61][  200/  217]    Overall Loss 0.008263    Objective Loss 0.008263                                        LR 0.001000    Time 0.299425    
2024-05-04 00:17:21,629 - Epoch: [61][  217/  217]    Overall Loss 0.007993    Objective Loss 0.007993    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.302697    
2024-05-04 00:17:21,825 - 

2024-05-04 00:17:21,826 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:17:55,366 - Epoch: [62][  100/  217]    Overall Loss 0.006521    Objective Loss 0.006521                                        LR 0.001000    Time 0.335289    
2024-05-04 00:18:24,258 - Epoch: [62][  200/  217]    Overall Loss 0.007404    Objective Loss 0.007404                                        LR 0.001000    Time 0.312056    
2024-05-04 00:18:29,047 - Epoch: [62][  217/  217]    Overall Loss 0.007385    Objective Loss 0.007385    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.309671    
2024-05-04 00:18:29,374 - 

2024-05-04 00:18:29,375 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:19:01,906 - Epoch: [63][  100/  217]    Overall Loss 0.006274    Objective Loss 0.006274                                        LR 0.001000    Time 0.325220    
2024-05-04 00:19:29,206 - Epoch: [63][  200/  217]    Overall Loss 0.007271    Objective Loss 0.007271                                        LR 0.001000    Time 0.299071    
2024-05-04 00:19:33,750 - Epoch: [63][  217/  217]    Overall Loss 0.007318    Objective Loss 0.007318    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.296574    
2024-05-04 00:19:33,967 - 

2024-05-04 00:19:33,967 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:20:04,524 - Epoch: [64][  100/  217]    Overall Loss 0.007491    Objective Loss 0.007491                                        LR 0.001000    Time 0.305482    
2024-05-04 00:20:31,769 - Epoch: [64][  200/  217]    Overall Loss 0.007101    Objective Loss 0.007101                                        LR 0.001000    Time 0.288923    
2024-05-04 00:20:35,585 - Epoch: [64][  217/  217]    Overall Loss 0.006888    Objective Loss 0.006888    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.283864    
2024-05-04 00:20:35,994 - 

2024-05-04 00:20:35,995 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:21:11,117 - Epoch: [65][  100/  217]    Overall Loss 0.005963    Objective Loss 0.005963                                        LR 0.001000    Time 0.351128    
2024-05-04 00:21:38,960 - Epoch: [65][  200/  217]    Overall Loss 0.007467    Objective Loss 0.007467                                        LR 0.001000    Time 0.314727    
2024-05-04 00:21:44,597 - Epoch: [65][  217/  217]    Overall Loss 0.007940    Objective Loss 0.007940    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.316043    
2024-05-04 00:21:45,005 - 

2024-05-04 00:21:45,007 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:22:15,967 - Epoch: [66][  100/  217]    Overall Loss 0.007516    Objective Loss 0.007516                                        LR 0.001000    Time 0.309484    
2024-05-04 00:22:45,422 - Epoch: [66][  200/  217]    Overall Loss 0.006540    Objective Loss 0.006540                                        LR 0.001000    Time 0.301972    
2024-05-04 00:22:49,478 - Epoch: [66][  217/  217]    Overall Loss 0.006453    Objective Loss 0.006453    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.296994    
2024-05-04 00:22:49,769 - 

2024-05-04 00:22:49,769 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:23:22,517 - Epoch: [67][  100/  217]    Overall Loss 0.004145    Objective Loss 0.004145                                        LR 0.001000    Time 0.327322    
2024-05-04 00:23:50,551 - Epoch: [67][  200/  217]    Overall Loss 0.004311    Objective Loss 0.004311                                        LR 0.001000    Time 0.303791    
2024-05-04 00:23:53,510 - Epoch: [67][  217/  217]    Overall Loss 0.004429    Objective Loss 0.004429    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.293618    
2024-05-04 00:23:53,708 - 

2024-05-04 00:23:53,709 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:24:25,778 - Epoch: [68][  100/  217]    Overall Loss 0.003510    Objective Loss 0.003510                                        LR 0.001000    Time 0.320591    
2024-05-04 00:24:53,863 - Epoch: [68][  200/  217]    Overall Loss 0.004832    Objective Loss 0.004832                                        LR 0.001000    Time 0.300685    
2024-05-04 00:24:59,405 - Epoch: [68][  217/  217]    Overall Loss 0.004738    Objective Loss 0.004738    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.302659    
2024-05-04 00:25:00,110 - 

2024-05-04 00:25:00,110 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:25:29,632 - Epoch: [69][  100/  217]    Overall Loss 0.002960    Objective Loss 0.002960                                        LR 0.001000    Time 0.295121    
2024-05-04 00:25:56,625 - Epoch: [69][  200/  217]    Overall Loss 0.005001    Objective Loss 0.005001                                        LR 0.001000    Time 0.282475    
2024-05-04 00:26:02,276 - Epoch: [69][  217/  217]    Overall Loss 0.005723    Objective Loss 0.005723    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.286381    
2024-05-04 00:26:02,511 - 

2024-05-04 00:26:02,511 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:26:33,930 - Epoch: [70][  100/  217]    Overall Loss 0.073045    Objective Loss 0.073045                                        LR 0.001000    Time 0.314082    
2024-05-04 00:27:01,644 - Epoch: [70][  200/  217]    Overall Loss 0.512125    Objective Loss 0.512125                                        LR 0.001000    Time 0.295561    
2024-05-04 00:27:07,033 - Epoch: [70][  217/  217]    Overall Loss 0.529900    Objective Loss 0.529900    Top1 73.770492    Top5 98.360656    LR 0.001000    Time 0.297234    
2024-05-04 00:27:07,338 - --- validate (epoch=70)-----------
2024-05-04 00:27:07,339 - 1736 samples (32 per mini-batch)
2024-05-04 00:27:25,770 - Epoch: [70][   55/   55]    Loss 2.972928    Top1 52.764977    Top5 69.527650    
2024-05-04 00:27:26,337 - ==> Top1: 52.765    Top5: 69.528    Loss: 2.973

2024-05-04 00:27:26,343 - ==> Best [Top1: 59.850   Top5: 75.634   Sparsity:0.00   Params: 390704 on epoch: 30]
2024-05-04 00:27:26,343 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 00:27:26,388 - 

2024-05-04 00:27:26,388 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:27:54,598 - Epoch: [71][  100/  217]    Overall Loss 0.322876    Objective Loss 0.322876                                        LR 0.001000    Time 0.282001    
2024-05-04 00:28:23,124 - Epoch: [71][  200/  217]    Overall Loss 0.292237    Objective Loss 0.292237                                        LR 0.001000    Time 0.283579    
2024-05-04 00:28:28,747 - Epoch: [71][  217/  217]    Overall Loss 0.288648    Objective Loss 0.288648    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.287263    
2024-05-04 00:28:29,134 - 

2024-05-04 00:28:29,135 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:29:00,533 - Epoch: [72][  100/  217]    Overall Loss 0.085740    Objective Loss 0.085740                                        LR 0.001000    Time 0.313874    
2024-05-04 00:29:28,904 - Epoch: [72][  200/  217]    Overall Loss 0.072227    Objective Loss 0.072227                                        LR 0.001000    Time 0.298745    
2024-05-04 00:29:34,282 - Epoch: [72][  217/  217]    Overall Loss 0.071413    Objective Loss 0.071413    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.300112    
2024-05-04 00:29:34,748 - 

2024-05-04 00:29:34,749 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:30:11,398 - Epoch: [73][  100/  217]    Overall Loss 0.027316    Objective Loss 0.027316                                        LR 0.001000    Time 0.366392    
2024-05-04 00:30:38,487 - Epoch: [73][  200/  217]    Overall Loss 0.025931    Objective Loss 0.025931                                        LR 0.001000    Time 0.318590    
2024-05-04 00:30:42,900 - Epoch: [73][  217/  217]    Overall Loss 0.025897    Objective Loss 0.025897    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.313956    
2024-05-04 00:30:43,172 - 

2024-05-04 00:30:43,172 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:31:16,543 - Epoch: [74][  100/  217]    Overall Loss 0.014087    Objective Loss 0.014087                                        LR 0.001000    Time 0.333606    
2024-05-04 00:31:42,465 - Epoch: [74][  200/  217]    Overall Loss 0.013695    Objective Loss 0.013695                                        LR 0.001000    Time 0.296362    
2024-05-04 00:31:46,199 - Epoch: [74][  217/  217]    Overall Loss 0.013899    Objective Loss 0.013899    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.290341    
2024-05-04 00:31:46,622 - 

2024-05-04 00:31:46,623 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:32:20,526 - Epoch: [75][  100/  217]    Overall Loss 0.008351    Objective Loss 0.008351                                        LR 0.001000    Time 0.338914    
2024-05-04 00:32:49,304 - Epoch: [75][  200/  217]    Overall Loss 0.009892    Objective Loss 0.009892                                        LR 0.001000    Time 0.313298    
2024-05-04 00:32:52,608 - Epoch: [75][  217/  217]    Overall Loss 0.009661    Objective Loss 0.009661    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.303972    
2024-05-04 00:32:53,004 - 

2024-05-04 00:32:53,005 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:33:25,567 - Epoch: [76][  100/  217]    Overall Loss 0.007052    Objective Loss 0.007052                                        LR 0.001000    Time 0.325527    
2024-05-04 00:33:54,904 - Epoch: [76][  200/  217]    Overall Loss 0.007455    Objective Loss 0.007455                                        LR 0.001000    Time 0.309402    
2024-05-04 00:33:59,097 - Epoch: [76][  217/  217]    Overall Loss 0.007576    Objective Loss 0.007576    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.304476    
2024-05-04 00:33:59,375 - 

2024-05-04 00:33:59,376 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:34:32,216 - Epoch: [77][  100/  217]    Overall Loss 0.005635    Objective Loss 0.005635                                        LR 0.001000    Time 0.328299    
2024-05-04 00:34:59,469 - Epoch: [77][  200/  217]    Overall Loss 0.006028    Objective Loss 0.006028                                        LR 0.001000    Time 0.300370    
2024-05-04 00:35:05,026 - Epoch: [77][  217/  217]    Overall Loss 0.006272    Objective Loss 0.006272    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.302437    
2024-05-04 00:35:05,316 - 

2024-05-04 00:35:05,317 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:35:38,486 - Epoch: [78][  100/  217]    Overall Loss 0.007488    Objective Loss 0.007488                                        LR 0.001000    Time 0.331589    
2024-05-04 00:36:05,091 - Epoch: [78][  200/  217]    Overall Loss 0.006474    Objective Loss 0.006474                                        LR 0.001000    Time 0.298771    
2024-05-04 00:36:09,915 - Epoch: [78][  217/  217]    Overall Loss 0.006463    Objective Loss 0.006463    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.297591    
2024-05-04 00:36:10,140 - 

2024-05-04 00:36:10,140 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:36:39,919 - Epoch: [79][  100/  217]    Overall Loss 0.005553    Objective Loss 0.005553                                        LR 0.001000    Time 0.297687    
2024-05-04 00:37:11,248 - Epoch: [79][  200/  217]    Overall Loss 0.005332    Objective Loss 0.005332                                        LR 0.001000    Time 0.305442    
2024-05-04 00:37:16,676 - Epoch: [79][  217/  217]    Overall Loss 0.005378    Objective Loss 0.005378    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.306519    
2024-05-04 00:37:16,883 - 

2024-05-04 00:37:16,883 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:37:49,204 - Epoch: [80][  100/  217]    Overall Loss 0.003368    Objective Loss 0.003368                                        LR 0.001000    Time 0.323100    
2024-05-04 00:38:17,512 - Epoch: [80][  200/  217]    Overall Loss 0.003970    Objective Loss 0.003970                                        LR 0.001000    Time 0.303049    
2024-05-04 00:38:22,117 - Epoch: [80][  217/  217]    Overall Loss 0.004277    Objective Loss 0.004277    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.300518    
2024-05-04 00:38:22,333 - --- validate (epoch=80)-----------
2024-05-04 00:38:22,334 - 1736 samples (32 per mini-batch)
2024-05-04 00:38:41,306 - Epoch: [80][   55/   55]    Loss 2.473492    Top1 59.101382    Top5 76.324885    
2024-05-04 00:38:41,594 - ==> Top1: 59.101    Top5: 76.325    Loss: 2.473

2024-05-04 00:38:41,601 - ==> Best [Top1: 59.850   Top5: 75.634   Sparsity:0.00   Params: 390704 on epoch: 30]
2024-05-04 00:38:41,601 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 00:38:41,647 - 

2024-05-04 00:38:41,647 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:39:14,817 - Epoch: [81][  100/  217]    Overall Loss 0.003488    Objective Loss 0.003488                                        LR 0.001000    Time 0.331589    
2024-05-04 00:39:41,638 - Epoch: [81][  200/  217]    Overall Loss 0.004175    Objective Loss 0.004175                                        LR 0.001000    Time 0.299853    
2024-05-04 00:39:45,421 - Epoch: [81][  217/  217]    Overall Loss 0.004085    Objective Loss 0.004085    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.293781    
2024-05-04 00:39:45,659 - 

2024-05-04 00:39:45,659 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:40:16,304 - Epoch: [82][  100/  217]    Overall Loss 0.003832    Objective Loss 0.003832                                        LR 0.001000    Time 0.306359    
2024-05-04 00:40:41,210 - Epoch: [82][  200/  217]    Overall Loss 0.004543    Objective Loss 0.004543                                        LR 0.001000    Time 0.277653    
2024-05-04 00:40:44,927 - Epoch: [82][  217/  217]    Overall Loss 0.004455    Objective Loss 0.004455    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.273023    
2024-05-04 00:40:45,284 - 

2024-05-04 00:40:45,285 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:41:18,276 - Epoch: [83][  100/  217]    Overall Loss 0.004228    Objective Loss 0.004228                                        LR 0.001000    Time 0.329815    
2024-05-04 00:41:43,266 - Epoch: [83][  200/  217]    Overall Loss 0.003643    Objective Loss 0.003643                                        LR 0.001000    Time 0.289809    
2024-05-04 00:41:47,173 - Epoch: [83][  217/  217]    Overall Loss 0.003739    Objective Loss 0.003739    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.285104    
2024-05-04 00:41:47,395 - 

2024-05-04 00:41:47,396 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:42:25,014 - Epoch: [84][  100/  217]    Overall Loss 0.002686    Objective Loss 0.002686                                        LR 0.001000    Time 0.376081    
2024-05-04 00:42:46,314 - Epoch: [84][  200/  217]    Overall Loss 0.003467    Objective Loss 0.003467                                        LR 0.001000    Time 0.294501    
2024-05-04 00:42:49,850 - Epoch: [84][  217/  217]    Overall Loss 0.003393    Objective Loss 0.003393    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.287718    
2024-05-04 00:42:50,043 - 

2024-05-04 00:42:50,044 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:43:20,665 - Epoch: [85][  100/  217]    Overall Loss 0.003042    Objective Loss 0.003042                                        LR 0.001000    Time 0.306123    
2024-05-04 00:43:46,182 - Epoch: [85][  200/  217]    Overall Loss 0.003251    Objective Loss 0.003251                                        LR 0.001000    Time 0.280596    
2024-05-04 00:43:50,878 - Epoch: [85][  217/  217]    Overall Loss 0.003381    Objective Loss 0.003381    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.280249    
2024-05-04 00:43:51,113 - 

2024-05-04 00:43:51,114 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:44:20,504 - Epoch: [86][  100/  217]    Overall Loss 0.002795    Objective Loss 0.002795                                        LR 0.001000    Time 0.293797    
2024-05-04 00:44:44,934 - Epoch: [86][  200/  217]    Overall Loss 0.002959    Objective Loss 0.002959                                        LR 0.001000    Time 0.269004    
2024-05-04 00:44:49,819 - Epoch: [86][  217/  217]    Overall Loss 0.003130    Objective Loss 0.003130    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.270435    
2024-05-04 00:44:50,134 - 

2024-05-04 00:44:50,134 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:45:22,829 - Epoch: [87][  100/  217]    Overall Loss 0.002313    Objective Loss 0.002313                                        LR 0.001000    Time 0.326841    
2024-05-04 00:45:49,475 - Epoch: [87][  200/  217]    Overall Loss 0.003031    Objective Loss 0.003031                                        LR 0.001000    Time 0.296592    
2024-05-04 00:45:53,064 - Epoch: [87][  217/  217]    Overall Loss 0.003309    Objective Loss 0.003309    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.289884    
2024-05-04 00:45:53,860 - 

2024-05-04 00:45:53,861 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:46:25,708 - Epoch: [88][  100/  217]    Overall Loss 0.002907    Objective Loss 0.002907                                        LR 0.001000    Time 0.318369    
2024-05-04 00:46:50,433 - Epoch: [88][  200/  217]    Overall Loss 0.003632    Objective Loss 0.003632                                        LR 0.001000    Time 0.282762    
2024-05-04 00:46:56,064 - Epoch: [88][  217/  217]    Overall Loss 0.003793    Objective Loss 0.003793    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.286549    
2024-05-04 00:46:56,330 - 

2024-05-04 00:46:56,331 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:47:29,214 - Epoch: [89][  100/  217]    Overall Loss 0.019142    Objective Loss 0.019142                                        LR 0.001000    Time 0.328737    
2024-05-04 00:47:56,189 - Epoch: [89][  200/  217]    Overall Loss 0.381656    Objective Loss 0.381656                                        LR 0.001000    Time 0.299193    
2024-05-04 00:48:00,293 - Epoch: [89][  217/  217]    Overall Loss 0.408815    Objective Loss 0.408815    Top1 81.967213    Top5 96.721311    LR 0.001000    Time 0.294656    
2024-05-04 00:48:00,558 - 

2024-05-04 00:48:00,559 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:48:39,743 - Epoch: [90][  100/  217]    Overall Loss 0.399130    Objective Loss 0.399130                                        LR 0.001000    Time 0.391746    
2024-05-04 00:49:04,351 - Epoch: [90][  200/  217]    Overall Loss 0.335037    Objective Loss 0.335037                                        LR 0.001000    Time 0.318873    
2024-05-04 00:49:08,527 - Epoch: [90][  217/  217]    Overall Loss 0.326419    Objective Loss 0.326419    Top1 95.081967    Top5 100.000000    LR 0.001000    Time 0.313129    
2024-05-04 00:49:08,892 - --- validate (epoch=90)-----------
2024-05-04 00:49:08,893 - 1736 samples (32 per mini-batch)
2024-05-04 00:49:27,313 - Epoch: [90][   55/   55]    Loss 2.494910    Top1 57.488479    Top5 75.172811    
2024-05-04 00:49:27,706 - ==> Top1: 57.488    Top5: 75.173    Loss: 2.495

2024-05-04 00:49:27,713 - ==> Best [Top1: 59.850   Top5: 75.634   Sparsity:0.00   Params: 390704 on epoch: 30]
2024-05-04 00:49:27,713 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 00:49:27,769 - 

2024-05-04 00:49:27,770 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:49:57,395 - Epoch: [91][  100/  217]    Overall Loss 0.061645    Objective Loss 0.061645                                        LR 0.001000    Time 0.296154    
2024-05-04 00:50:30,145 - Epoch: [91][  200/  217]    Overall Loss 0.060309    Objective Loss 0.060309                                        LR 0.001000    Time 0.311784    
2024-05-04 00:50:32,869 - Epoch: [91][  217/  217]    Overall Loss 0.060432    Objective Loss 0.060432    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.299905    
2024-05-04 00:50:33,281 - 

2024-05-04 00:50:33,282 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:51:02,093 - Epoch: [92][  100/  217]    Overall Loss 0.023139    Objective Loss 0.023139                                        LR 0.001000    Time 0.287999    
2024-05-04 00:51:30,305 - Epoch: [92][  200/  217]    Overall Loss 0.022995    Objective Loss 0.022995                                        LR 0.001000    Time 0.285015    
2024-05-04 00:51:33,935 - Epoch: [92][  217/  217]    Overall Loss 0.023483    Objective Loss 0.023483    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.279409    
2024-05-04 00:51:34,195 - 

2024-05-04 00:51:34,196 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:52:04,714 - Epoch: [93][  100/  217]    Overall Loss 0.014468    Objective Loss 0.014468                                        LR 0.001000    Time 0.305086    
2024-05-04 00:52:31,606 - Epoch: [93][  200/  217]    Overall Loss 0.013878    Objective Loss 0.013878                                        LR 0.001000    Time 0.286954    
2024-05-04 00:52:36,223 - Epoch: [93][  217/  217]    Overall Loss 0.013974    Objective Loss 0.013974    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.285739    
2024-05-04 00:52:36,524 - 

2024-05-04 00:52:36,524 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:53:06,479 - Epoch: [94][  100/  217]    Overall Loss 0.009165    Objective Loss 0.009165                                        LR 0.001000    Time 0.299442    
2024-05-04 00:53:32,171 - Epoch: [94][  200/  217]    Overall Loss 0.009281    Objective Loss 0.009281                                        LR 0.001000    Time 0.278098    
2024-05-04 00:53:35,508 - Epoch: [94][  217/  217]    Overall Loss 0.009014    Objective Loss 0.009014    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.271677    
2024-05-04 00:53:35,716 - 

2024-05-04 00:53:35,717 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:54:04,014 - Epoch: [95][  100/  217]    Overall Loss 0.007623    Objective Loss 0.007623                                        LR 0.001000    Time 0.282874    
2024-05-04 00:54:32,393 - Epoch: [95][  200/  217]    Overall Loss 0.007488    Objective Loss 0.007488                                        LR 0.001000    Time 0.283287    
2024-05-04 00:54:36,194 - Epoch: [95][  217/  217]    Overall Loss 0.007322    Objective Loss 0.007322    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.278601    
2024-05-04 00:54:36,458 - 

2024-05-04 00:54:36,459 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:55:07,590 - Epoch: [96][  100/  217]    Overall Loss 0.005848    Objective Loss 0.005848                                        LR 0.001000    Time 0.311224    
2024-05-04 00:55:35,515 - Epoch: [96][  200/  217]    Overall Loss 0.006880    Objective Loss 0.006880                                        LR 0.001000    Time 0.295195    
2024-05-04 00:55:40,583 - Epoch: [96][  217/  217]    Overall Loss 0.006785    Objective Loss 0.006785    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.295419    
2024-05-04 00:55:40,876 - 

2024-05-04 00:55:40,877 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:56:12,960 - Epoch: [97][  100/  217]    Overall Loss 0.006577    Objective Loss 0.006577                                        LR 0.001000    Time 0.320735    
2024-05-04 00:56:40,421 - Epoch: [97][  200/  217]    Overall Loss 0.007208    Objective Loss 0.007208                                        LR 0.001000    Time 0.297633    
2024-05-04 00:56:43,012 - Epoch: [97][  217/  217]    Overall Loss 0.006990    Objective Loss 0.006990    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.286247    
2024-05-04 00:56:43,283 - 

2024-05-04 00:56:43,283 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:57:13,047 - Epoch: [98][  100/  217]    Overall Loss 0.005675    Objective Loss 0.005675                                        LR 0.001000    Time 0.297526    
2024-05-04 00:57:37,381 - Epoch: [98][  200/  217]    Overall Loss 0.005094    Objective Loss 0.005094                                        LR 0.001000    Time 0.270390    
2024-05-04 00:57:41,827 - Epoch: [98][  217/  217]    Overall Loss 0.005645    Objective Loss 0.005645    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.269689    
2024-05-04 00:57:42,085 - 

2024-05-04 00:57:42,086 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:58:12,550 - Epoch: [99][  100/  217]    Overall Loss 0.003852    Objective Loss 0.003852                                        LR 0.001000    Time 0.304548    
2024-05-04 00:58:40,722 - Epoch: [99][  200/  217]    Overall Loss 0.004572    Objective Loss 0.004572                                        LR 0.001000    Time 0.293087    
2024-05-04 00:58:45,454 - Epoch: [99][  217/  217]    Overall Loss 0.004664    Objective Loss 0.004664    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.291926    
2024-05-04 00:58:45,713 - 

2024-05-04 00:58:45,713 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:59:20,673 - Epoch: [100][  100/  217]    Overall Loss 0.003451    Objective Loss 0.003451                                        LR 0.000250    Time 0.349511    
2024-05-04 00:59:46,547 - Epoch: [100][  200/  217]    Overall Loss 0.003863    Objective Loss 0.003863                                        LR 0.000250    Time 0.304084    
2024-05-04 00:59:50,462 - Epoch: [100][  217/  217]    Overall Loss 0.003958    Objective Loss 0.003958    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.298294    
2024-05-04 00:59:50,741 - --- validate (epoch=100)-----------
2024-05-04 00:59:50,742 - 1736 samples (32 per mini-batch)
2024-05-04 01:00:09,434 - Epoch: [100][   55/   55]    Loss 2.566444    Top1 59.965438    Top5 76.152074    
2024-05-04 01:00:09,639 - ==> Top1: 59.965    Top5: 76.152    Loss: 2.566

2024-05-04 01:00:09,644 - ==> Best [Top1: 59.965   Top5: 76.152   Sparsity:0.00   Params: 390704 on epoch: 100]
2024-05-04 01:00:09,644 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 01:00:09,698 - 

2024-05-04 01:00:09,699 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:00:39,018 - Epoch: [101][  100/  217]    Overall Loss 0.002210    Objective Loss 0.002210                                        LR 0.000250    Time 0.293098    
2024-05-04 01:01:05,219 - Epoch: [101][  200/  217]    Overall Loss 0.002999    Objective Loss 0.002999                                        LR 0.000250    Time 0.277515    
2024-05-04 01:01:08,117 - Epoch: [101][  217/  217]    Overall Loss 0.003325    Objective Loss 0.003325    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.269123    
2024-05-04 01:01:08,382 - 

2024-05-04 01:01:08,383 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:01:42,039 - Epoch: [102][  100/  217]    Overall Loss 0.002852    Objective Loss 0.002852                                        LR 0.000250    Time 0.336451    
2024-05-04 01:02:08,549 - Epoch: [102][  200/  217]    Overall Loss 0.003033    Objective Loss 0.003033                                        LR 0.000250    Time 0.300733    
2024-05-04 01:02:12,764 - Epoch: [102][  217/  217]    Overall Loss 0.003233    Objective Loss 0.003233    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.296583    
2024-05-04 01:02:13,042 - 

2024-05-04 01:02:13,043 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:02:45,373 - Epoch: [103][  100/  217]    Overall Loss 0.003466    Objective Loss 0.003466                                        LR 0.000250    Time 0.323201    
2024-05-04 01:03:10,318 - Epoch: [103][  200/  217]    Overall Loss 0.003012    Objective Loss 0.003012                                        LR 0.000250    Time 0.286276    
2024-05-04 01:03:16,030 - Epoch: [103][  217/  217]    Overall Loss 0.003205    Objective Loss 0.003205    Top1 96.721311    Top5 100.000000    LR 0.000250    Time 0.290161    
2024-05-04 01:03:16,308 - 

2024-05-04 01:03:16,309 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:03:46,517 - Epoch: [104][  100/  217]    Overall Loss 0.003642    Objective Loss 0.003642                                        LR 0.000250    Time 0.301991    
2024-05-04 01:04:11,961 - Epoch: [104][  200/  217]    Overall Loss 0.003014    Objective Loss 0.003014                                        LR 0.000250    Time 0.278168    
2024-05-04 01:04:17,513 - Epoch: [104][  217/  217]    Overall Loss 0.003033    Objective Loss 0.003033    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.281954    
2024-05-04 01:04:17,767 - 

2024-05-04 01:04:17,768 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:04:50,395 - Epoch: [105][  100/  217]    Overall Loss 0.002817    Objective Loss 0.002817                                        LR 0.000250    Time 0.326177    
2024-05-04 01:05:18,012 - Epoch: [105][  200/  217]    Overall Loss 0.002902    Objective Loss 0.002902                                        LR 0.000250    Time 0.301124    
2024-05-04 01:05:22,023 - Epoch: [105][  217/  217]    Overall Loss 0.003040    Objective Loss 0.003040    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.296008    
2024-05-04 01:05:22,263 - 

2024-05-04 01:05:22,264 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:05:54,043 - Epoch: [106][  100/  217]    Overall Loss 0.002342    Objective Loss 0.002342                                        LR 0.000250    Time 0.317679    
2024-05-04 01:06:22,364 - Epoch: [106][  200/  217]    Overall Loss 0.002774    Objective Loss 0.002774                                        LR 0.000250    Time 0.300394    
2024-05-04 01:06:26,721 - Epoch: [106][  217/  217]    Overall Loss 0.002984    Objective Loss 0.002984    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.296927    
2024-05-04 01:06:27,068 - 

2024-05-04 01:06:27,069 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:06:59,660 - Epoch: [107][  100/  217]    Overall Loss 0.003411    Objective Loss 0.003411                                        LR 0.000250    Time 0.325798    
2024-05-04 01:07:27,234 - Epoch: [107][  200/  217]    Overall Loss 0.003236    Objective Loss 0.003236                                        LR 0.000250    Time 0.300729    
2024-05-04 01:07:32,528 - Epoch: [107][  217/  217]    Overall Loss 0.003120    Objective Loss 0.003120    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.301557    
2024-05-04 01:07:32,833 - 

2024-05-04 01:07:32,834 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:08:03,187 - Epoch: [108][  100/  217]    Overall Loss 0.003295    Objective Loss 0.003295                                        LR 0.000250    Time 0.303443    
2024-05-04 01:08:32,719 - Epoch: [108][  200/  217]    Overall Loss 0.002745    Objective Loss 0.002745                                        LR 0.000250    Time 0.299335    
2024-05-04 01:08:38,286 - Epoch: [108][  217/  217]    Overall Loss 0.002637    Objective Loss 0.002637    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.301527    
2024-05-04 01:08:38,610 - 

2024-05-04 01:08:38,611 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:09:08,055 - Epoch: [109][  100/  217]    Overall Loss 0.001903    Objective Loss 0.001903                                        LR 0.000250    Time 0.294330    
2024-05-04 01:09:38,989 - Epoch: [109][  200/  217]    Overall Loss 0.002411    Objective Loss 0.002411                                        LR 0.000250    Time 0.301794    
2024-05-04 01:09:43,998 - Epoch: [109][  217/  217]    Overall Loss 0.002533    Objective Loss 0.002533    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.301226    
2024-05-04 01:09:44,196 - 

2024-05-04 01:09:44,197 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:10:11,914 - Epoch: [110][  100/  217]    Overall Loss 0.001777    Objective Loss 0.001777                                        LR 0.000250    Time 0.277069    
2024-05-04 01:10:39,440 - Epoch: [110][  200/  217]    Overall Loss 0.002555    Objective Loss 0.002555                                        LR 0.000250    Time 0.276118    
2024-05-04 01:10:42,826 - Epoch: [110][  217/  217]    Overall Loss 0.002493    Objective Loss 0.002493    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.270083    
2024-05-04 01:10:43,170 - --- validate (epoch=110)-----------
2024-05-04 01:10:43,171 - 1736 samples (32 per mini-batch)
2024-05-04 01:10:59,604 - Epoch: [110][   55/   55]    Loss 2.599063    Top1 59.907834    Top5 76.612903    
2024-05-04 01:10:59,819 - ==> Top1: 59.908    Top5: 76.613    Loss: 2.599

2024-05-04 01:10:59,823 - ==> Best [Top1: 59.965   Top5: 76.152   Sparsity:0.00   Params: 390704 on epoch: 100]
2024-05-04 01:10:59,823 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 01:10:59,854 - 

2024-05-04 01:10:59,855 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:11:30,319 - Epoch: [111][  100/  217]    Overall Loss 0.003056    Objective Loss 0.003056                                        LR 0.000250    Time 0.304540    
2024-05-04 01:11:55,370 - Epoch: [111][  200/  217]    Overall Loss 0.002479    Objective Loss 0.002479                                        LR 0.000250    Time 0.277488    
2024-05-04 01:12:01,088 - Epoch: [111][  217/  217]    Overall Loss 0.002382    Objective Loss 0.002382    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.282087    
2024-05-04 01:12:01,298 - 

2024-05-04 01:12:01,298 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:12:31,457 - Epoch: [112][  100/  217]    Overall Loss 0.002129    Objective Loss 0.002129                                        LR 0.000250    Time 0.301491    
2024-05-04 01:12:54,834 - Epoch: [112][  200/  217]    Overall Loss 0.002258    Objective Loss 0.002258                                        LR 0.000250    Time 0.267589    
2024-05-04 01:12:59,901 - Epoch: [112][  217/  217]    Overall Loss 0.002314    Objective Loss 0.002314    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.269968    
2024-05-04 01:13:00,129 - 

2024-05-04 01:13:00,129 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:13:34,293 - Epoch: [113][  100/  217]    Overall Loss 0.001634    Objective Loss 0.001634                                        LR 0.000250    Time 0.341545    
2024-05-04 01:14:01,014 - Epoch: [113][  200/  217]    Overall Loss 0.002238    Objective Loss 0.002238                                        LR 0.000250    Time 0.304329    
2024-05-04 01:14:03,899 - Epoch: [113][  217/  217]    Overall Loss 0.002160    Objective Loss 0.002160    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.293775    
2024-05-04 01:14:04,133 - 

2024-05-04 01:14:04,134 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:14:36,078 - Epoch: [114][  100/  217]    Overall Loss 0.002430    Objective Loss 0.002430                                        LR 0.000250    Time 0.319339    
2024-05-04 01:15:05,661 - Epoch: [114][  200/  217]    Overall Loss 0.001923    Objective Loss 0.001923                                        LR 0.000250    Time 0.307533    
2024-05-04 01:15:11,745 - Epoch: [114][  217/  217]    Overall Loss 0.002147    Objective Loss 0.002147    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.311468    
2024-05-04 01:15:12,049 - 

2024-05-04 01:15:12,050 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:15:46,742 - Epoch: [115][  100/  217]    Overall Loss 0.002714    Objective Loss 0.002714                                        LR 0.000250    Time 0.346818    
2024-05-04 01:16:15,349 - Epoch: [115][  200/  217]    Overall Loss 0.002273    Objective Loss 0.002273                                        LR 0.000250    Time 0.316401    
2024-05-04 01:16:22,812 - Epoch: [115][  217/  217]    Overall Loss 0.002181    Objective Loss 0.002181    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.325993    
2024-05-04 01:16:23,656 - 

2024-05-04 01:16:23,656 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:16:54,139 - Epoch: [116][  100/  217]    Overall Loss 0.001466    Objective Loss 0.001466                                        LR 0.000250    Time 0.304730    
2024-05-04 01:17:21,092 - Epoch: [116][  200/  217]    Overall Loss 0.002197    Objective Loss 0.002197                                        LR 0.000250    Time 0.287089    
2024-05-04 01:17:26,635 - Epoch: [116][  217/  217]    Overall Loss 0.002105    Objective Loss 0.002105    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.290129    
2024-05-04 01:17:27,143 - 

2024-05-04 01:17:27,144 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:17:56,518 - Epoch: [117][  100/  217]    Overall Loss 0.001450    Objective Loss 0.001450                                        LR 0.000250    Time 0.293635    
2024-05-04 01:18:23,224 - Epoch: [117][  200/  217]    Overall Loss 0.002236    Objective Loss 0.002236                                        LR 0.000250    Time 0.280297    
2024-05-04 01:18:27,263 - Epoch: [117][  217/  217]    Overall Loss 0.002132    Objective Loss 0.002132    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.276934    
2024-05-04 01:18:27,556 - 

2024-05-04 01:18:27,557 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:18:56,266 - Epoch: [118][  100/  217]    Overall Loss 0.001780    Objective Loss 0.001780                                        LR 0.000250    Time 0.286997    
2024-05-04 01:19:22,329 - Epoch: [118][  200/  217]    Overall Loss 0.001925    Objective Loss 0.001925                                        LR 0.000250    Time 0.273759    
2024-05-04 01:19:27,979 - Epoch: [118][  217/  217]    Overall Loss 0.002256    Objective Loss 0.002256    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.278338    
2024-05-04 01:19:28,259 - 

2024-05-04 01:19:28,259 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:19:56,480 - Epoch: [119][  100/  217]    Overall Loss 0.001844    Objective Loss 0.001844                                        LR 0.000250    Time 0.282109    
2024-05-04 01:20:28,122 - Epoch: [119][  200/  217]    Overall Loss 0.002498    Objective Loss 0.002498                                        LR 0.000250    Time 0.299226    
2024-05-04 01:20:33,894 - Epoch: [119][  217/  217]    Overall Loss 0.002417    Objective Loss 0.002417    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.302375    
2024-05-04 01:20:34,204 - 

2024-05-04 01:20:34,205 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:21:04,973 - Epoch: [120][  100/  217]    Overall Loss 0.001477    Objective Loss 0.001477                                        LR 0.000250    Time 0.307585    
2024-05-04 01:21:35,341 - Epoch: [120][  200/  217]    Overall Loss 0.002031    Objective Loss 0.002031                                        LR 0.000250    Time 0.305582    
2024-05-04 01:21:40,111 - Epoch: [120][  217/  217]    Overall Loss 0.002181    Objective Loss 0.002181    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.303614    
2024-05-04 01:21:40,544 - --- validate (epoch=120)-----------
2024-05-04 01:21:40,545 - 1736 samples (32 per mini-batch)
2024-05-04 01:21:55,205 - Epoch: [120][   55/   55]    Loss 2.631042    Top1 59.216590    Top5 76.670507    
2024-05-04 01:21:55,472 - ==> Top1: 59.217    Top5: 76.671    Loss: 2.631

2024-05-04 01:21:55,475 - ==> Best [Top1: 59.965   Top5: 76.152   Sparsity:0.00   Params: 390704 on epoch: 100]
2024-05-04 01:21:55,476 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 01:21:55,508 - 

2024-05-04 01:21:55,509 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:22:26,995 - Epoch: [121][  100/  217]    Overall Loss 0.001963    Objective Loss 0.001963                                        LR 0.000250    Time 0.314765    
2024-05-04 01:22:54,028 - Epoch: [121][  200/  217]    Overall Loss 0.002144    Objective Loss 0.002144                                        LR 0.000250    Time 0.292500    
2024-05-04 01:22:59,562 - Epoch: [121][  217/  217]    Overall Loss 0.002065    Objective Loss 0.002065    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.295081    
2024-05-04 01:23:00,061 - 

2024-05-04 01:23:00,061 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:23:33,688 - Epoch: [122][  100/  217]    Overall Loss 0.008985    Objective Loss 0.008985                                        LR 0.000250    Time 0.336106    
2024-05-04 01:24:02,229 - Epoch: [122][  200/  217]    Overall Loss 0.007952    Objective Loss 0.007952                                        LR 0.000250    Time 0.310710    
2024-05-04 01:24:07,081 - Epoch: [122][  217/  217]    Overall Loss 0.007849    Objective Loss 0.007849    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.308716    
2024-05-04 01:24:07,357 - 

2024-05-04 01:24:07,358 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:24:40,648 - Epoch: [123][  100/  217]    Overall Loss 0.004378    Objective Loss 0.004378                                        LR 0.000250    Time 0.332812    
2024-05-04 01:25:09,284 - Epoch: [123][  200/  217]    Overall Loss 0.003557    Objective Loss 0.003557                                        LR 0.000250    Time 0.309536    
2024-05-04 01:25:13,014 - Epoch: [123][  217/  217]    Overall Loss 0.003477    Objective Loss 0.003477    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.302467    
2024-05-04 01:25:13,361 - 

2024-05-04 01:25:13,362 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:25:47,839 - Epoch: [124][  100/  217]    Overall Loss 0.002810    Objective Loss 0.002810                                        LR 0.000250    Time 0.344664    
2024-05-04 01:26:11,113 - Epoch: [124][  200/  217]    Overall Loss 0.002433    Objective Loss 0.002433                                        LR 0.000250    Time 0.288653    
2024-05-04 01:26:15,631 - Epoch: [124][  217/  217]    Overall Loss 0.002355    Objective Loss 0.002355    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.286851    
2024-05-04 01:26:16,668 - 

2024-05-04 01:26:16,670 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:26:48,186 - Epoch: [125][  100/  217]    Overall Loss 0.002355    Objective Loss 0.002355                                        LR 0.000250    Time 0.315067    
2024-05-04 01:27:15,284 - Epoch: [125][  200/  217]    Overall Loss 0.002093    Objective Loss 0.002093                                        LR 0.000250    Time 0.292978    
2024-05-04 01:27:22,628 - Epoch: [125][  217/  217]    Overall Loss 0.002024    Objective Loss 0.002024    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.303862    
2024-05-04 01:27:22,921 - 

2024-05-04 01:27:22,921 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:27:55,007 - Epoch: [126][  100/  217]    Overall Loss 0.001734    Objective Loss 0.001734                                        LR 0.000250    Time 0.320747    
2024-05-04 01:28:21,401 - Epoch: [126][  200/  217]    Overall Loss 0.001973    Objective Loss 0.001973                                        LR 0.000250    Time 0.292298    
2024-05-04 01:28:26,326 - Epoch: [126][  217/  217]    Overall Loss 0.001886    Objective Loss 0.001886    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.292084    
2024-05-04 01:28:26,671 - 

2024-05-04 01:28:26,672 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:28:55,043 - Epoch: [127][  100/  217]    Overall Loss 0.001529    Objective Loss 0.001529                                        LR 0.000250    Time 0.283599    
2024-05-04 01:29:24,615 - Epoch: [127][  200/  217]    Overall Loss 0.001957    Objective Loss 0.001957                                        LR 0.000250    Time 0.289609    
2024-05-04 01:29:29,626 - Epoch: [127][  217/  217]    Overall Loss 0.001852    Objective Loss 0.001852    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.290003    
2024-05-04 01:29:29,845 - 

2024-05-04 01:29:29,846 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:30:01,594 - Epoch: [128][  100/  217]    Overall Loss 0.001516    Objective Loss 0.001516                                        LR 0.000250    Time 0.317381    
2024-05-04 01:30:32,910 - Epoch: [128][  200/  217]    Overall Loss 0.001397    Objective Loss 0.001397                                        LR 0.000250    Time 0.315223    
2024-05-04 01:30:36,646 - Epoch: [128][  217/  217]    Overall Loss 0.001715    Objective Loss 0.001715    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.307733    
2024-05-04 01:30:36,956 - 

2024-05-04 01:30:36,957 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:31:07,108 - Epoch: [129][  100/  217]    Overall Loss 0.001788    Objective Loss 0.001788                                        LR 0.000250    Time 0.301406    
2024-05-04 01:31:39,453 - Epoch: [129][  200/  217]    Overall Loss 0.001810    Objective Loss 0.001810                                        LR 0.000250    Time 0.312381    
2024-05-04 01:31:43,291 - Epoch: [129][  217/  217]    Overall Loss 0.001722    Objective Loss 0.001722    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.305587    
2024-05-04 01:31:43,758 - 

2024-05-04 01:31:43,759 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:32:14,214 - Epoch: [130][  100/  217]    Overall Loss 0.001723    Objective Loss 0.001723                                        LR 0.000250    Time 0.304437    
2024-05-04 01:32:43,619 - Epoch: [130][  200/  217]    Overall Loss 0.001727    Objective Loss 0.001727                                        LR 0.000250    Time 0.299200    
2024-05-04 01:32:49,310 - Epoch: [130][  217/  217]    Overall Loss 0.001627    Objective Loss 0.001627    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.301976    
2024-05-04 01:32:49,597 - --- validate (epoch=130)-----------
2024-05-04 01:32:49,597 - 1736 samples (32 per mini-batch)
2024-05-04 01:33:06,975 - Epoch: [130][   55/   55]    Loss 2.722030    Top1 59.447005    Top5 76.440092    
2024-05-04 01:33:07,241 - ==> Top1: 59.447    Top5: 76.440    Loss: 2.722

2024-05-04 01:33:07,248 - ==> Best [Top1: 59.965   Top5: 76.152   Sparsity:0.00   Params: 390704 on epoch: 100]
2024-05-04 01:33:07,249 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 01:33:07,292 - 

2024-05-04 01:33:07,292 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:33:39,947 - Epoch: [131][  100/  217]    Overall Loss 0.001251    Objective Loss 0.001251                                        LR 0.000250    Time 0.326445    
2024-05-04 01:34:06,448 - Epoch: [131][  200/  217]    Overall Loss 0.001747    Objective Loss 0.001747                                        LR 0.000250    Time 0.295688    
2024-05-04 01:34:10,940 - Epoch: [131][  217/  217]    Overall Loss 0.001659    Objective Loss 0.001659    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.293215    
2024-05-04 01:34:11,293 - 

2024-05-04 01:34:11,293 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:34:46,289 - Epoch: [132][  100/  217]    Overall Loss 0.001531    Objective Loss 0.001531                                        LR 0.000250    Time 0.349861    
2024-05-04 01:35:12,701 - Epoch: [132][  200/  217]    Overall Loss 0.001601    Objective Loss 0.001601                                        LR 0.000250    Time 0.306945    
2024-05-04 01:35:17,026 - Epoch: [132][  217/  217]    Overall Loss 0.001522    Objective Loss 0.001522    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.302820    
2024-05-04 01:35:17,294 - 

2024-05-04 01:35:17,295 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:35:49,566 - Epoch: [133][  100/  217]    Overall Loss 0.001031    Objective Loss 0.001031                                        LR 0.000250    Time 0.322607    
2024-05-04 01:36:14,146 - Epoch: [133][  200/  217]    Overall Loss 0.001342    Objective Loss 0.001342                                        LR 0.000250    Time 0.284156    
2024-05-04 01:36:19,766 - Epoch: [133][  217/  217]    Overall Loss 0.001497    Objective Loss 0.001497    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.287780    
2024-05-04 01:36:20,161 - 

2024-05-04 01:36:20,162 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:36:49,271 - Epoch: [134][  100/  217]    Overall Loss 0.001446    Objective Loss 0.001446                                        LR 0.000250    Time 0.290992    
2024-05-04 01:37:17,135 - Epoch: [134][  200/  217]    Overall Loss 0.001840    Objective Loss 0.001840                                        LR 0.000250    Time 0.284765    
2024-05-04 01:37:20,926 - Epoch: [134][  217/  217]    Overall Loss 0.001772    Objective Loss 0.001772    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.279916    
2024-05-04 01:37:21,712 - 

2024-05-04 01:37:21,712 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:37:52,686 - Epoch: [135][  100/  217]    Overall Loss 0.002657    Objective Loss 0.002657                                        LR 0.000250    Time 0.309641    
2024-05-04 01:38:21,887 - Epoch: [135][  200/  217]    Overall Loss 0.004803    Objective Loss 0.004803                                        LR 0.000250    Time 0.300784    
2024-05-04 01:38:26,626 - Epoch: [135][  217/  217]    Overall Loss 0.004868    Objective Loss 0.004868    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.299047    
2024-05-04 01:38:27,241 - 

2024-05-04 01:38:27,242 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:38:59,460 - Epoch: [136][  100/  217]    Overall Loss 0.004995    Objective Loss 0.004995                                        LR 0.000250    Time 0.322079    
2024-05-04 01:39:26,754 - Epoch: [136][  200/  217]    Overall Loss 0.007678    Objective Loss 0.007678                                        LR 0.000250    Time 0.297461    
2024-05-04 01:39:33,247 - Epoch: [136][  217/  217]    Overall Loss 0.007782    Objective Loss 0.007782    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.304070    
2024-05-04 01:39:34,123 - 

2024-05-04 01:39:34,124 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:40:03,582 - Epoch: [137][  100/  217]    Overall Loss 0.004837    Objective Loss 0.004837                                        LR 0.000250    Time 0.294486    
2024-05-04 01:40:33,554 - Epoch: [137][  200/  217]    Overall Loss 0.004752    Objective Loss 0.004752                                        LR 0.000250    Time 0.297057    
2024-05-04 01:40:39,121 - Epoch: [137][  217/  217]    Overall Loss 0.005094    Objective Loss 0.005094    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.299432    
2024-05-04 01:40:39,394 - 

2024-05-04 01:40:39,394 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:41:09,824 - Epoch: [138][  100/  217]    Overall Loss 0.004135    Objective Loss 0.004135                                        LR 0.000250    Time 0.304195    
2024-05-04 01:41:34,753 - Epoch: [138][  200/  217]    Overall Loss 0.003700    Objective Loss 0.003700                                        LR 0.000250    Time 0.276696    
2024-05-04 01:41:39,695 - Epoch: [138][  217/  217]    Overall Loss 0.003936    Objective Loss 0.003936    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.277783    
2024-05-04 01:41:39,939 - 

2024-05-04 01:41:39,939 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:42:10,953 - Epoch: [139][  100/  217]    Overall Loss 0.001851    Objective Loss 0.001851                                        LR 0.000250    Time 0.310026    
2024-05-04 01:42:40,757 - Epoch: [139][  200/  217]    Overall Loss 0.002014    Objective Loss 0.002014                                        LR 0.000250    Time 0.303961    
2024-05-04 01:42:46,189 - Epoch: [139][  217/  217]    Overall Loss 0.002389    Objective Loss 0.002389    Top1 98.360656    Top5 100.000000    LR 0.000250    Time 0.305171    
2024-05-04 01:42:46,461 - 

2024-05-04 01:42:46,462 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:43:13,994 - Epoch: [140][  100/  217]    Overall Loss 0.001664    Objective Loss 0.001664                                        LR 0.000250    Time 0.275211    
2024-05-04 01:43:44,171 - Epoch: [140][  200/  217]    Overall Loss 0.001667    Objective Loss 0.001667                                        LR 0.000250    Time 0.288392    
2024-05-04 01:43:47,355 - Epoch: [140][  217/  217]    Overall Loss 0.001847    Objective Loss 0.001847    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.280462    
2024-05-04 01:43:47,616 - --- validate (epoch=140)-----------
2024-05-04 01:43:47,617 - 1736 samples (32 per mini-batch)
2024-05-04 01:44:03,858 - Epoch: [140][   55/   55]    Loss 2.762258    Top1 60.253456    Top5 75.691244    
2024-05-04 01:44:04,125 - ==> Top1: 60.253    Top5: 75.691    Loss: 2.762

2024-05-04 01:44:04,134 - ==> Best [Top1: 60.253   Top5: 75.691   Sparsity:0.00   Params: 390704 on epoch: 140]
2024-05-04 01:44:04,134 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 01:44:04,189 - 

2024-05-04 01:44:04,189 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:44:34,188 - Epoch: [141][  100/  217]    Overall Loss 0.002044    Objective Loss 0.002044                                        LR 0.000250    Time 0.299887    
2024-05-04 01:45:02,694 - Epoch: [141][  200/  217]    Overall Loss 0.001740    Objective Loss 0.001740                                        LR 0.000250    Time 0.292423    
2024-05-04 01:45:06,872 - Epoch: [141][  217/  217]    Overall Loss 0.001710    Objective Loss 0.001710    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.288749    
2024-05-04 01:45:07,154 - 

2024-05-04 01:45:07,154 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:45:37,747 - Epoch: [142][  100/  217]    Overall Loss 0.001378    Objective Loss 0.001378                                        LR 0.000250    Time 0.305712    
2024-05-04 01:46:03,934 - Epoch: [142][  200/  217]    Overall Loss 0.001463    Objective Loss 0.001463                                        LR 0.000250    Time 0.283746    
2024-05-04 01:46:07,260 - Epoch: [142][  217/  217]    Overall Loss 0.001634    Objective Loss 0.001634    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.276830    
2024-05-04 01:46:07,583 - 

2024-05-04 01:46:07,583 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:46:41,696 - Epoch: [143][  100/  217]    Overall Loss 0.001845    Objective Loss 0.001845                                        LR 0.000250    Time 0.341024    
2024-05-04 01:47:10,797 - Epoch: [143][  200/  217]    Overall Loss 0.001711    Objective Loss 0.001711                                        LR 0.000250    Time 0.315973    
2024-05-04 01:47:14,619 - Epoch: [143][  217/  217]    Overall Loss 0.001623    Objective Loss 0.001623    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.308821    
2024-05-04 01:47:14,938 - 

2024-05-04 01:47:14,939 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:47:45,364 - Epoch: [144][  100/  217]    Overall Loss 0.001192    Objective Loss 0.001192                                        LR 0.000250    Time 0.304144    
2024-05-04 01:48:11,646 - Epoch: [144][  200/  217]    Overall Loss 0.001383    Objective Loss 0.001383                                        LR 0.000250    Time 0.283436    
2024-05-04 01:48:14,399 - Epoch: [144][  217/  217]    Overall Loss 0.001407    Objective Loss 0.001407    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.273912    
2024-05-04 01:48:14,753 - 

2024-05-04 01:48:14,754 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:48:43,585 - Epoch: [145][  100/  217]    Overall Loss 0.001601    Objective Loss 0.001601                                        LR 0.000250    Time 0.288202    
2024-05-04 01:49:12,759 - Epoch: [145][  200/  217]    Overall Loss 0.001407    Objective Loss 0.001407                                        LR 0.000250    Time 0.289925    
2024-05-04 01:49:18,475 - Epoch: [145][  217/  217]    Overall Loss 0.001330    Objective Loss 0.001330    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.293543    
2024-05-04 01:49:18,762 - 

2024-05-04 01:49:18,763 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:49:48,475 - Epoch: [146][  100/  217]    Overall Loss 0.001372    Objective Loss 0.001372                                        LR 0.000250    Time 0.297021    
2024-05-04 01:50:11,337 - Epoch: [146][  200/  217]    Overall Loss 0.001369    Objective Loss 0.001369                                        LR 0.000250    Time 0.262775    
2024-05-04 01:50:14,889 - Epoch: [146][  217/  217]    Overall Loss 0.001296    Objective Loss 0.001296    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.258547    
2024-05-04 01:50:15,181 - 

2024-05-04 01:50:15,182 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:50:48,236 - Epoch: [147][  100/  217]    Overall Loss 0.001346    Objective Loss 0.001346                                        LR 0.000250    Time 0.330440    
2024-05-04 01:51:12,723 - Epoch: [147][  200/  217]    Overall Loss 0.001214    Objective Loss 0.001214                                        LR 0.000250    Time 0.287612    
2024-05-04 01:51:16,422 - Epoch: [147][  217/  217]    Overall Loss 0.001285    Objective Loss 0.001285    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.282114    
2024-05-04 01:51:16,650 - 

2024-05-04 01:51:16,651 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:51:47,288 - Epoch: [148][  100/  217]    Overall Loss 0.001686    Objective Loss 0.001686                                        LR 0.000250    Time 0.306271    
2024-05-04 01:52:13,962 - Epoch: [148][  200/  217]    Overall Loss 0.001240    Objective Loss 0.001240                                        LR 0.000250    Time 0.286463    
2024-05-04 01:52:19,143 - Epoch: [148][  217/  217]    Overall Loss 0.001275    Objective Loss 0.001275    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.287884    
2024-05-04 01:52:19,431 - 

2024-05-04 01:52:19,431 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:52:49,366 - Epoch: [149][  100/  217]    Overall Loss 0.001625    Objective Loss 0.001625                                        LR 0.000250    Time 0.299251    
2024-05-04 01:53:13,758 - Epoch: [149][  200/  217]    Overall Loss 0.001339    Objective Loss 0.001339                                        LR 0.000250    Time 0.271537    
2024-05-04 01:53:16,929 - Epoch: [149][  217/  217]    Overall Loss 0.001263    Objective Loss 0.001263    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.264870    
2024-05-04 01:53:17,214 - 

2024-05-04 01:53:17,215 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:53:48,245 - Epoch: [150][  100/  217]    Overall Loss 0.000918    Objective Loss 0.000918                                        LR 0.000063    Time 0.310203    
2024-05-04 01:54:15,582 - Epoch: [150][  200/  217]    Overall Loss 0.001246    Objective Loss 0.001246                                        LR 0.000063    Time 0.291745    
2024-05-04 01:54:19,093 - Epoch: [150][  217/  217]    Overall Loss 0.001173    Objective Loss 0.001173    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.285063    
2024-05-04 01:54:19,358 - --- validate (epoch=150)-----------
2024-05-04 01:54:19,358 - 1736 samples (32 per mini-batch)
2024-05-04 01:54:35,029 - Epoch: [150][   55/   55]    Loss 2.842889    Top1 59.792627    Top5 75.691244    
2024-05-04 01:54:35,257 - ==> Top1: 59.793    Top5: 75.691    Loss: 2.843

2024-05-04 01:54:35,262 - ==> Best [Top1: 60.253   Top5: 75.691   Sparsity:0.00   Params: 390704 on epoch: 140]
2024-05-04 01:54:35,262 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 01:54:35,290 - 

2024-05-04 01:54:35,291 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:55:05,820 - Epoch: [151][  100/  217]    Overall Loss 0.000962    Objective Loss 0.000962                                        LR 0.000063    Time 0.305188    
2024-05-04 01:55:29,868 - Epoch: [151][  200/  217]    Overall Loss 0.001198    Objective Loss 0.001198                                        LR 0.000063    Time 0.272791    
2024-05-04 01:55:34,731 - Epoch: [151][  217/  217]    Overall Loss 0.001135    Objective Loss 0.001135    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.273817    
2024-05-04 01:55:34,984 - 

2024-05-04 01:55:34,985 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:56:04,171 - Epoch: [152][  100/  217]    Overall Loss 0.000513    Objective Loss 0.000513                                        LR 0.000063    Time 0.291768    
2024-05-04 01:56:27,092 - Epoch: [152][  200/  217]    Overall Loss 0.001027    Objective Loss 0.001027                                        LR 0.000063    Time 0.260438    
2024-05-04 01:56:30,683 - Epoch: [152][  217/  217]    Overall Loss 0.001184    Objective Loss 0.001184    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.256576    
2024-05-04 01:56:30,908 - 

2024-05-04 01:56:30,909 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:57:04,748 - Epoch: [153][  100/  217]    Overall Loss 0.001416    Objective Loss 0.001416                                        LR 0.000063    Time 0.338288    
2024-05-04 01:57:31,404 - Epoch: [153][  200/  217]    Overall Loss 0.001495    Objective Loss 0.001495                                        LR 0.000063    Time 0.302383    
2024-05-04 01:57:34,866 - Epoch: [153][  217/  217]    Overall Loss 0.001406    Objective Loss 0.001406    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.294636    
2024-05-04 01:57:35,362 - 

2024-05-04 01:57:35,363 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:58:05,940 - Epoch: [154][  100/  217]    Overall Loss 0.000807    Objective Loss 0.000807                                        LR 0.000063    Time 0.305677    
2024-05-04 01:58:34,711 - Epoch: [154][  200/  217]    Overall Loss 0.001113    Objective Loss 0.001113                                        LR 0.000063    Time 0.296649    
2024-05-04 01:58:40,156 - Epoch: [154][  217/  217]    Overall Loss 0.001141    Objective Loss 0.001141    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.298492    
2024-05-04 01:58:40,513 - 

2024-05-04 01:58:40,514 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:59:14,685 - Epoch: [155][  100/  217]    Overall Loss 0.001005    Objective Loss 0.001005                                        LR 0.000063    Time 0.341618    
2024-05-04 01:59:42,720 - Epoch: [155][  200/  217]    Overall Loss 0.001100    Objective Loss 0.001100                                        LR 0.000063    Time 0.310941    
2024-05-04 01:59:46,702 - Epoch: [155][  217/  217]    Overall Loss 0.001148    Objective Loss 0.001148    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.304924    
2024-05-04 01:59:47,109 - 

2024-05-04 01:59:47,110 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:00:18,528 - Epoch: [156][  100/  217]    Overall Loss 0.000926    Objective Loss 0.000926                                        LR 0.000063    Time 0.314092    
2024-05-04 02:00:46,854 - Epoch: [156][  200/  217]    Overall Loss 0.001221    Objective Loss 0.001221                                        LR 0.000063    Time 0.298630    
2024-05-04 02:00:51,394 - Epoch: [156][  217/  217]    Overall Loss 0.001151    Objective Loss 0.001151    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.296148    
2024-05-04 02:00:51,668 - 

2024-05-04 02:00:51,668 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:01:21,363 - Epoch: [157][  100/  217]    Overall Loss 0.001355    Objective Loss 0.001355                                        LR 0.000063    Time 0.296863    
2024-05-04 02:01:49,454 - Epoch: [157][  200/  217]    Overall Loss 0.001233    Objective Loss 0.001233                                        LR 0.000063    Time 0.288809    
2024-05-04 02:01:54,466 - Epoch: [157][  217/  217]    Overall Loss 0.001157    Objective Loss 0.001157    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.289274    
2024-05-04 02:01:54,809 - 

2024-05-04 02:01:54,810 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:02:27,722 - Epoch: [158][  100/  217]    Overall Loss 0.001317    Objective Loss 0.001317                                        LR 0.000063    Time 0.329010    
2024-05-04 02:02:53,225 - Epoch: [158][  200/  217]    Overall Loss 0.001166    Objective Loss 0.001166                                        LR 0.000063    Time 0.291980    
2024-05-04 02:02:57,420 - Epoch: [158][  217/  217]    Overall Loss 0.001099    Objective Loss 0.001099    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.288427    
2024-05-04 02:02:57,648 - 

2024-05-04 02:02:57,649 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:03:29,097 - Epoch: [159][  100/  217]    Overall Loss 0.001472    Objective Loss 0.001472                                        LR 0.000063    Time 0.314372    
2024-05-04 02:03:58,532 - Epoch: [159][  200/  217]    Overall Loss 0.001237    Objective Loss 0.001237                                        LR 0.000063    Time 0.304315    
2024-05-04 02:04:01,291 - Epoch: [159][  217/  217]    Overall Loss 0.001161    Objective Loss 0.001161    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.293182    
2024-05-04 02:04:01,584 - 

2024-05-04 02:04:01,584 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:04:31,714 - Epoch: [160][  100/  217]    Overall Loss 0.001404    Objective Loss 0.001404                                        LR 0.000063    Time 0.301202    
2024-05-04 02:04:57,564 - Epoch: [160][  200/  217]    Overall Loss 0.001090    Objective Loss 0.001090                                        LR 0.000063    Time 0.279808    
2024-05-04 02:05:05,350 - Epoch: [160][  217/  217]    Overall Loss 0.001127    Objective Loss 0.001127    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.293759    
2024-05-04 02:05:05,621 - --- validate (epoch=160)-----------
2024-05-04 02:05:05,621 - 1736 samples (32 per mini-batch)
2024-05-04 02:05:22,889 - Epoch: [160][   55/   55]    Loss 2.869210    Top1 60.195853    Top5 75.806452    
2024-05-04 02:05:23,091 - ==> Top1: 60.196    Top5: 75.806    Loss: 2.869

2024-05-04 02:05:23,097 - ==> Best [Top1: 60.253   Top5: 75.691   Sparsity:0.00   Params: 390704 on epoch: 140]
2024-05-04 02:05:23,097 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 02:05:23,141 - 

2024-05-04 02:05:23,142 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:05:55,102 - Epoch: [161][  100/  217]    Overall Loss 0.001379    Objective Loss 0.001379                                        LR 0.000063    Time 0.319502    
2024-05-04 02:06:20,991 - Epoch: [161][  200/  217]    Overall Loss 0.001148    Objective Loss 0.001148                                        LR 0.000063    Time 0.289152    
2024-05-04 02:06:25,369 - Epoch: [161][  217/  217]    Overall Loss 0.001077    Objective Loss 0.001077    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.286667    
2024-05-04 02:06:25,627 - 

2024-05-04 02:06:25,627 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:07:00,473 - Epoch: [162][  100/  217]    Overall Loss 0.000945    Objective Loss 0.000945                                        LR 0.000063    Time 0.348359    
2024-05-04 02:07:30,535 - Epoch: [162][  200/  217]    Overall Loss 0.001057    Objective Loss 0.001057                                        LR 0.000063    Time 0.324445    
2024-05-04 02:07:34,644 - Epoch: [162][  217/  217]    Overall Loss 0.001101    Objective Loss 0.001101    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.317954    
2024-05-04 02:07:34,950 - 

2024-05-04 02:07:34,950 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:08:03,905 - Epoch: [163][  100/  217]    Overall Loss 0.000972    Objective Loss 0.000972                                        LR 0.000063    Time 0.289442    
2024-05-04 02:08:29,342 - Epoch: [163][  200/  217]    Overall Loss 0.001159    Objective Loss 0.001159                                        LR 0.000063    Time 0.271864    
2024-05-04 02:08:33,338 - Epoch: [163][  217/  217]    Overall Loss 0.001089    Objective Loss 0.001089    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.268970    
2024-05-04 02:08:33,703 - 

2024-05-04 02:08:33,704 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:09:03,895 - Epoch: [164][  100/  217]    Overall Loss 0.001389    Objective Loss 0.001389                                        LR 0.000063    Time 0.301807    
2024-05-04 02:09:30,137 - Epoch: [164][  200/  217]    Overall Loss 0.001152    Objective Loss 0.001152                                        LR 0.000063    Time 0.282074    
2024-05-04 02:09:33,496 - Epoch: [164][  217/  217]    Overall Loss 0.001077    Objective Loss 0.001077    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.275446    
2024-05-04 02:09:33,759 - 

2024-05-04 02:09:33,760 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:10:04,663 - Epoch: [165][  100/  217]    Overall Loss 0.001330    Objective Loss 0.001330                                        LR 0.000063    Time 0.308930    
2024-05-04 02:10:30,837 - Epoch: [165][  200/  217]    Overall Loss 0.001117    Objective Loss 0.001117                                        LR 0.000063    Time 0.285288    
2024-05-04 02:10:35,110 - Epoch: [165][  217/  217]    Overall Loss 0.001047    Objective Loss 0.001047    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.282622    
2024-05-04 02:10:35,353 - 

2024-05-04 02:10:35,354 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:11:07,742 - Epoch: [166][  100/  217]    Overall Loss 0.001054    Objective Loss 0.001054                                        LR 0.000063    Time 0.323771    
2024-05-04 02:11:35,970 - Epoch: [166][  200/  217]    Overall Loss 0.001156    Objective Loss 0.001156                                        LR 0.000063    Time 0.302979    
2024-05-04 02:11:39,554 - Epoch: [166][  217/  217]    Overall Loss 0.001077    Objective Loss 0.001077    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.295751    
2024-05-04 02:11:39,788 - 

2024-05-04 02:11:39,789 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:12:10,473 - Epoch: [167][  100/  217]    Overall Loss 0.000849    Objective Loss 0.000849                                        LR 0.000063    Time 0.306728    
2024-05-04 02:12:35,483 - Epoch: [167][  200/  217]    Overall Loss 0.000864    Objective Loss 0.000864                                        LR 0.000063    Time 0.278371    
2024-05-04 02:12:38,688 - Epoch: [167][  217/  217]    Overall Loss 0.001041    Objective Loss 0.001041    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.271327    
2024-05-04 02:12:38,995 - 

2024-05-04 02:12:38,996 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:13:07,522 - Epoch: [168][  100/  217]    Overall Loss 0.001127    Objective Loss 0.001127                                        LR 0.000063    Time 0.285169    
2024-05-04 02:13:35,964 - Epoch: [168][  200/  217]    Overall Loss 0.000781    Objective Loss 0.000781                                        LR 0.000063    Time 0.284750    
2024-05-04 02:13:38,936 - Epoch: [168][  217/  217]    Overall Loss 0.001055    Objective Loss 0.001055    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.276134    
2024-05-04 02:13:39,292 - 

2024-05-04 02:13:39,294 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:14:09,250 - Epoch: [169][  100/  217]    Overall Loss 0.001079    Objective Loss 0.001079                                        LR 0.000063    Time 0.299459    
2024-05-04 02:14:35,952 - Epoch: [169][  200/  217]    Overall Loss 0.001013    Objective Loss 0.001013                                        LR 0.000063    Time 0.283194    
2024-05-04 02:14:39,588 - Epoch: [169][  217/  217]    Overall Loss 0.001060    Objective Loss 0.001060    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.277753    
2024-05-04 02:14:39,830 - 

2024-05-04 02:14:39,830 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:15:12,836 - Epoch: [170][  100/  217]    Overall Loss 0.001555    Objective Loss 0.001555                                        LR 0.000063    Time 0.329960    
2024-05-04 02:15:37,104 - Epoch: [170][  200/  217]    Overall Loss 0.001115    Objective Loss 0.001115                                        LR 0.000063    Time 0.286273    
2024-05-04 02:15:41,497 - Epoch: [170][  217/  217]    Overall Loss 0.001048    Objective Loss 0.001048    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.284084    
2024-05-04 02:15:41,937 - --- validate (epoch=170)-----------
2024-05-04 02:15:41,937 - 1736 samples (32 per mini-batch)
2024-05-04 02:15:56,621 - Epoch: [170][   55/   55]    Loss 2.887273    Top1 60.138249    Top5 76.036866    
2024-05-04 02:15:56,912 - ==> Top1: 60.138    Top5: 76.037    Loss: 2.887

2024-05-04 02:15:56,916 - ==> Best [Top1: 60.253   Top5: 75.691   Sparsity:0.00   Params: 390704 on epoch: 140]
2024-05-04 02:15:56,916 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 02:15:56,957 - 

2024-05-04 02:15:56,958 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:16:33,941 - Epoch: [171][  100/  217]    Overall Loss 0.001653    Objective Loss 0.001653                                        LR 0.000063    Time 0.369733    
2024-05-04 02:17:00,188 - Epoch: [171][  200/  217]    Overall Loss 0.001188    Objective Loss 0.001188                                        LR 0.000063    Time 0.316059    
2024-05-04 02:17:05,803 - Epoch: [171][  217/  217]    Overall Loss 0.001113    Objective Loss 0.001113    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.317164    
2024-05-04 02:17:06,101 - 

2024-05-04 02:17:06,102 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:17:39,217 - Epoch: [172][  100/  217]    Overall Loss 0.000842    Objective Loss 0.000842                                        LR 0.000063    Time 0.331059    
2024-05-04 02:18:03,746 - Epoch: [172][  200/  217]    Overall Loss 0.000957    Objective Loss 0.000957                                        LR 0.000063    Time 0.288130    
2024-05-04 02:18:08,393 - Epoch: [172][  217/  217]    Overall Loss 0.000999    Objective Loss 0.000999    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.286963    
2024-05-04 02:18:08,624 - 

2024-05-04 02:18:08,625 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:18:38,952 - Epoch: [173][  100/  217]    Overall Loss 0.001045    Objective Loss 0.001045                                        LR 0.000063    Time 0.303171    
2024-05-04 02:19:04,256 - Epoch: [173][  200/  217]    Overall Loss 0.001080    Objective Loss 0.001080                                        LR 0.000063    Time 0.278058    
2024-05-04 02:19:10,039 - Epoch: [173][  217/  217]    Overall Loss 0.001011    Objective Loss 0.001011    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.282918    
2024-05-04 02:19:10,446 - 

2024-05-04 02:19:10,447 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:19:43,108 - Epoch: [174][  100/  217]    Overall Loss 0.001296    Objective Loss 0.001296                                        LR 0.000063    Time 0.326519    
2024-05-04 02:20:07,415 - Epoch: [174][  200/  217]    Overall Loss 0.001088    Objective Loss 0.001088                                        LR 0.000063    Time 0.284753    
2024-05-04 02:20:12,940 - Epoch: [174][  217/  217]    Overall Loss 0.001017    Objective Loss 0.001017    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.287899    
2024-05-04 02:20:13,265 - 

2024-05-04 02:20:13,265 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:20:44,465 - Epoch: [175][  100/  217]    Overall Loss 0.001303    Objective Loss 0.001303                                        LR 0.000063    Time 0.311889    
2024-05-04 02:21:10,832 - Epoch: [175][  200/  217]    Overall Loss 0.001088    Objective Loss 0.001088                                        LR 0.000063    Time 0.287739    
2024-05-04 02:21:16,411 - Epoch: [175][  217/  217]    Overall Loss 0.001019    Objective Loss 0.001019    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.290900    
2024-05-04 02:21:16,799 - 

2024-05-04 02:21:16,800 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:21:48,772 - Epoch: [176][  100/  217]    Overall Loss 0.001833    Objective Loss 0.001833                                        LR 0.000063    Time 0.319622    
2024-05-04 02:22:16,919 - Epoch: [176][  200/  217]    Overall Loss 0.001156    Objective Loss 0.001156                                        LR 0.000063    Time 0.300499    
2024-05-04 02:22:21,751 - Epoch: [176][  217/  217]    Overall Loss 0.001080    Objective Loss 0.001080    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.299213    
2024-05-04 02:22:22,366 - 

2024-05-04 02:22:22,367 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:22:53,221 - Epoch: [177][  100/  217]    Overall Loss 0.000163    Objective Loss 0.000163                                        LR 0.000063    Time 0.308442    
2024-05-04 02:23:22,293 - Epoch: [177][  200/  217]    Overall Loss 0.000863    Objective Loss 0.000863                                        LR 0.000063    Time 0.299531    
2024-05-04 02:23:26,108 - Epoch: [177][  217/  217]    Overall Loss 0.001035    Objective Loss 0.001035    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.293635    
2024-05-04 02:23:26,335 - 

2024-05-04 02:23:26,336 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:23:57,189 - Epoch: [178][  100/  217]    Overall Loss 0.000835    Objective Loss 0.000835                                        LR 0.000063    Time 0.308433    
2024-05-04 02:24:21,394 - Epoch: [178][  200/  217]    Overall Loss 0.000992    Objective Loss 0.000992                                        LR 0.000063    Time 0.275200    
2024-05-04 02:24:28,770 - Epoch: [178][  217/  217]    Overall Loss 0.001048    Objective Loss 0.001048    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.287624    
2024-05-04 02:24:29,633 - 

2024-05-04 02:24:29,634 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:24:59,666 - Epoch: [179][  100/  217]    Overall Loss 0.001173    Objective Loss 0.001173                                        LR 0.000063    Time 0.300229    
2024-05-04 02:25:27,877 - Epoch: [179][  200/  217]    Overall Loss 0.001031    Objective Loss 0.001031                                        LR 0.000063    Time 0.291126    
2024-05-04 02:25:32,095 - Epoch: [179][  217/  217]    Overall Loss 0.001088    Objective Loss 0.001088    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.287744    
2024-05-04 02:25:32,366 - 

2024-05-04 02:25:32,367 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:26:07,343 - Epoch: [180][  100/  217]    Overall Loss 0.001704    Objective Loss 0.001704                                        LR 0.000063    Time 0.349612    
2024-05-04 02:26:31,359 - Epoch: [180][  200/  217]    Overall Loss 0.001287    Objective Loss 0.001287                                        LR 0.000063    Time 0.294850    
2024-05-04 02:26:34,180 - Epoch: [180][  217/  217]    Overall Loss 0.001198    Objective Loss 0.001198    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.284744    
2024-05-04 02:26:34,932 - --- validate (epoch=180)-----------
2024-05-04 02:26:34,933 - 1736 samples (32 per mini-batch)
2024-05-04 02:26:56,263 - Epoch: [180][   55/   55]    Loss 2.938950    Top1 60.426267    Top5 76.209677    
2024-05-04 02:26:56,509 - ==> Top1: 60.426    Top5: 76.210    Loss: 2.939

2024-05-04 02:26:56,513 - ==> Best [Top1: 60.426   Top5: 76.210   Sparsity:0.00   Params: 390704 on epoch: 180]
2024-05-04 02:26:56,513 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 02:26:56,568 - 

2024-05-04 02:26:56,568 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:27:30,086 - Epoch: [181][  100/  217]    Overall Loss 0.000910    Objective Loss 0.000910                                        LR 0.000063    Time 0.335062    
2024-05-04 02:28:00,633 - Epoch: [181][  200/  217]    Overall Loss 0.001198    Objective Loss 0.001198                                        LR 0.000063    Time 0.320223    
2024-05-04 02:28:05,342 - Epoch: [181][  217/  217]    Overall Loss 0.001113    Objective Loss 0.001113    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.316825    
2024-05-04 02:28:05,716 - 

2024-05-04 02:28:05,717 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:28:34,917 - Epoch: [182][  100/  217]    Overall Loss 0.001283    Objective Loss 0.001283                                        LR 0.000063    Time 0.291897    
2024-05-04 02:29:04,220 - Epoch: [182][  200/  217]    Overall Loss 0.001071    Objective Loss 0.001071                                        LR 0.000063    Time 0.292416    
2024-05-04 02:29:09,038 - Epoch: [182][  217/  217]    Overall Loss 0.001002    Objective Loss 0.001002    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.291706    
2024-05-04 02:29:09,344 - 

2024-05-04 02:29:09,345 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:29:43,913 - Epoch: [183][  100/  217]    Overall Loss 0.001260    Objective Loss 0.001260                                        LR 0.000063    Time 0.345589    
2024-05-04 02:30:11,953 - Epoch: [183][  200/  217]    Overall Loss 0.001050    Objective Loss 0.001050                                        LR 0.000063    Time 0.312945    
2024-05-04 02:30:16,191 - Epoch: [183][  217/  217]    Overall Loss 0.000979    Objective Loss 0.000979    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.307951    
2024-05-04 02:30:16,990 - 

2024-05-04 02:30:16,991 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:30:44,876 - Epoch: [184][  100/  217]    Overall Loss 0.000964    Objective Loss 0.000964                                        LR 0.000063    Time 0.278758    
2024-05-04 02:31:13,766 - Epoch: [184][  200/  217]    Overall Loss 0.001009    Objective Loss 0.001009                                        LR 0.000063    Time 0.283778    
2024-05-04 02:31:17,218 - Epoch: [184][  217/  217]    Overall Loss 0.000941    Objective Loss 0.000941    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.277440    
2024-05-04 02:31:17,402 - 

2024-05-04 02:31:17,402 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:31:48,618 - Epoch: [185][  100/  217]    Overall Loss 0.000596    Objective Loss 0.000596                                        LR 0.000063    Time 0.312064    
2024-05-04 02:32:14,487 - Epoch: [185][  200/  217]    Overall Loss 0.000927    Objective Loss 0.000927                                        LR 0.000063    Time 0.285326    
2024-05-04 02:32:19,356 - Epoch: [185][  217/  217]    Overall Loss 0.000973    Objective Loss 0.000973    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.285398    
2024-05-04 02:32:20,114 - 

2024-05-04 02:32:20,115 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:32:55,587 - Epoch: [186][  100/  217]    Overall Loss 0.000866    Objective Loss 0.000866                                        LR 0.000063    Time 0.354615    
2024-05-04 02:33:19,889 - Epoch: [186][  200/  217]    Overall Loss 0.001000    Objective Loss 0.001000                                        LR 0.000063    Time 0.298766    
2024-05-04 02:33:24,652 - Epoch: [186][  217/  217]    Overall Loss 0.001045    Objective Loss 0.001045    Top1 98.360656    Top5 100.000000    LR 0.000063    Time 0.297297    
2024-05-04 02:33:25,570 - 

2024-05-04 02:33:25,571 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:34:01,068 - Epoch: [187][  100/  217]    Overall Loss 0.000544    Objective Loss 0.000544                                        LR 0.000063    Time 0.354872    
2024-05-04 02:34:26,329 - Epoch: [187][  200/  217]    Overall Loss 0.001044    Objective Loss 0.001044                                        LR 0.000063    Time 0.303692    
2024-05-04 02:34:30,685 - Epoch: [187][  217/  217]    Overall Loss 0.000972    Objective Loss 0.000972    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.299964    
2024-05-04 02:34:31,191 - 

2024-05-04 02:34:31,192 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:35:06,126 - Epoch: [188][  100/  217]    Overall Loss 0.001060    Objective Loss 0.001060                                        LR 0.000063    Time 0.349170    
2024-05-04 02:35:35,711 - Epoch: [188][  200/  217]    Overall Loss 0.001086    Objective Loss 0.001086                                        LR 0.000063    Time 0.322464    
2024-05-04 02:35:38,978 - Epoch: [188][  217/  217]    Overall Loss 0.001016    Objective Loss 0.001016    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.312250    
2024-05-04 02:35:39,156 - 

2024-05-04 02:35:39,156 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:36:12,679 - Epoch: [189][  100/  217]    Overall Loss 0.000114    Objective Loss 0.000114                                        LR 0.000063    Time 0.335134    
2024-05-04 02:36:40,263 - Epoch: [189][  200/  217]    Overall Loss 0.001004    Objective Loss 0.001004                                        LR 0.000063    Time 0.305442    
2024-05-04 02:36:44,059 - Epoch: [189][  217/  217]    Overall Loss 0.000934    Objective Loss 0.000934    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.298999    
2024-05-04 02:36:44,233 - 

2024-05-04 02:36:44,234 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:37:15,246 - Epoch: [190][  100/  217]    Overall Loss 0.000638    Objective Loss 0.000638                                        LR 0.000063    Time 0.310029    
2024-05-04 02:37:45,096 - Epoch: [190][  200/  217]    Overall Loss 0.001082    Objective Loss 0.001082                                        LR 0.000063    Time 0.304223    
2024-05-04 02:37:49,288 - Epoch: [190][  217/  217]    Overall Loss 0.001007    Objective Loss 0.001007    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.299699    
2024-05-04 02:37:49,467 - --- validate (epoch=190)-----------
2024-05-04 02:37:49,468 - 1736 samples (32 per mini-batch)
2024-05-04 02:38:07,949 - Epoch: [190][   55/   55]    Loss 2.968583    Top1 59.504608    Top5 76.036866    
2024-05-04 02:38:08,233 - ==> Top1: 59.505    Top5: 76.037    Loss: 2.969

2024-05-04 02:38:08,241 - ==> Best [Top1: 60.426   Top5: 76.210   Sparsity:0.00   Params: 390704 on epoch: 180]
2024-05-04 02:38:08,242 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 02:38:08,289 - 

2024-05-04 02:38:08,289 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:38:38,012 - Epoch: [191][  100/  217]    Overall Loss 0.000997    Objective Loss 0.000997                                        LR 0.000063    Time 0.297124    
2024-05-04 02:39:06,326 - Epoch: [191][  200/  217]    Overall Loss 0.001043    Objective Loss 0.001043                                        LR 0.000063    Time 0.290087    
2024-05-04 02:39:12,000 - Epoch: [191][  217/  217]    Overall Loss 0.000970    Objective Loss 0.000970    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.293496    
2024-05-04 02:39:12,647 - 

2024-05-04 02:39:12,648 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:39:46,621 - Epoch: [192][  100/  217]    Overall Loss 0.000977    Objective Loss 0.000977                                        LR 0.000063    Time 0.339620    
2024-05-04 02:40:14,498 - Epoch: [192][  200/  217]    Overall Loss 0.000784    Objective Loss 0.000784                                        LR 0.000063    Time 0.309148    
2024-05-04 02:40:19,996 - Epoch: [192][  217/  217]    Overall Loss 0.000946    Objective Loss 0.000946    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.310256    
2024-05-04 02:40:20,364 - 

2024-05-04 02:40:20,365 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:40:52,056 - Epoch: [193][  100/  217]    Overall Loss 0.000533    Objective Loss 0.000533                                        LR 0.000063    Time 0.316812    
2024-05-04 02:41:20,951 - Epoch: [193][  200/  217]    Overall Loss 0.000913    Objective Loss 0.000913                                        LR 0.000063    Time 0.302833    
2024-05-04 02:41:28,781 - Epoch: [193][  217/  217]    Overall Loss 0.000969    Objective Loss 0.000969    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.315182    
2024-05-04 02:41:29,058 - 

2024-05-04 02:41:29,059 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:42:00,148 - Epoch: [194][  100/  217]    Overall Loss 0.000811    Objective Loss 0.000811                                        LR 0.000063    Time 0.310786    
2024-05-04 02:42:23,989 - Epoch: [194][  200/  217]    Overall Loss 0.001033    Objective Loss 0.001033                                        LR 0.000063    Time 0.274549    
2024-05-04 02:42:31,591 - Epoch: [194][  217/  217]    Overall Loss 0.000963    Objective Loss 0.000963    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.288065    
2024-05-04 02:42:31,946 - 

2024-05-04 02:42:31,947 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:43:00,981 - Epoch: [195][  100/  217]    Overall Loss 0.001003    Objective Loss 0.001003                                        LR 0.000063    Time 0.290234    
2024-05-04 02:43:25,382 - Epoch: [195][  200/  217]    Overall Loss 0.001026    Objective Loss 0.001026                                        LR 0.000063    Time 0.267079    
2024-05-04 02:43:30,153 - Epoch: [195][  217/  217]    Overall Loss 0.000953    Objective Loss 0.000953    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.268134    
2024-05-04 02:43:30,414 - 

2024-05-04 02:43:30,414 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:44:05,667 - Epoch: [196][  100/  217]    Overall Loss 0.000906    Objective Loss 0.000906                                        LR 0.000063    Time 0.352423    
2024-05-04 02:44:33,314 - Epoch: [196][  200/  217]    Overall Loss 0.001063    Objective Loss 0.001063                                        LR 0.000063    Time 0.314400    
2024-05-04 02:44:37,259 - Epoch: [196][  217/  217]    Overall Loss 0.000987    Objective Loss 0.000987    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.307939    
2024-05-04 02:44:37,721 - 

2024-05-04 02:44:37,722 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:45:08,330 - Epoch: [197][  100/  217]    Overall Loss 0.001108    Objective Loss 0.001108                                        LR 0.000063    Time 0.305956    
2024-05-04 02:45:35,296 - Epoch: [197][  200/  217]    Overall Loss 0.001226    Objective Loss 0.001226                                        LR 0.000063    Time 0.287762    
2024-05-04 02:45:39,860 - Epoch: [197][  217/  217]    Overall Loss 0.001249    Objective Loss 0.001249    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.286243    
2024-05-04 02:45:40,206 - 

2024-05-04 02:45:40,206 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:46:13,698 - Epoch: [198][  100/  217]    Overall Loss 0.001089    Objective Loss 0.001089                                        LR 0.000063    Time 0.334787    
2024-05-04 02:46:40,051 - Epoch: [198][  200/  217]    Overall Loss 0.001018    Objective Loss 0.001018                                        LR 0.000063    Time 0.299112    
2024-05-04 02:46:45,838 - Epoch: [198][  217/  217]    Overall Loss 0.001045    Objective Loss 0.001045    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.302333    
2024-05-04 02:46:46,747 - 

2024-05-04 02:46:46,747 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:47:17,965 - Epoch: [199][  100/  217]    Overall Loss 0.001443    Objective Loss 0.001443                                        LR 0.000063    Time 0.312060    
2024-05-04 02:47:43,582 - Epoch: [199][  200/  217]    Overall Loss 0.001142    Objective Loss 0.001142                                        LR 0.000063    Time 0.284035    
2024-05-04 02:47:49,157 - Epoch: [199][  217/  217]    Overall Loss 0.001061    Objective Loss 0.001061    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.287463    
2024-05-04 02:47:50,242 - 

2024-05-04 02:47:50,243 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:48:24,641 - Epoch: [200][  100/  217]    Overall Loss 0.000402    Objective Loss 0.000402                                        LR 0.000016    Time 0.343862    
2024-05-04 02:48:53,154 - Epoch: [200][  200/  217]    Overall Loss 0.000937    Objective Loss 0.000937                                        LR 0.000016    Time 0.314438    
2024-05-04 02:48:56,167 - Epoch: [200][  217/  217]    Overall Loss 0.000987    Objective Loss 0.000987    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.303664    
2024-05-04 02:48:56,646 - --- validate (epoch=200)-----------
2024-05-04 02:48:56,647 - 1736 samples (32 per mini-batch)
2024-05-04 02:49:14,897 - Epoch: [200][   55/   55]    Loss 3.016147    Top1 59.965438    Top5 76.267281    
2024-05-04 02:49:15,092 - ==> Top1: 59.965    Top5: 76.267    Loss: 3.016

2024-05-04 02:49:15,096 - ==> Best [Top1: 60.426   Top5: 76.210   Sparsity:0.00   Params: 390704 on epoch: 180]
2024-05-04 02:49:15,096 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 02:49:15,141 - 

2024-05-04 02:49:15,141 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:49:47,872 - Epoch: [201][  100/  217]    Overall Loss 0.001054    Objective Loss 0.001054                                        LR 0.000016    Time 0.327194    
2024-05-04 02:50:18,006 - Epoch: [201][  200/  217]    Overall Loss 0.001029    Objective Loss 0.001029                                        LR 0.000016    Time 0.314218    
2024-05-04 02:50:22,308 - Epoch: [201][  217/  217]    Overall Loss 0.000955    Objective Loss 0.000955    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.309419    
2024-05-04 02:50:22,483 - 

2024-05-04 02:50:22,484 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:50:51,992 - Epoch: [202][  100/  217]    Overall Loss 0.001067    Objective Loss 0.001067                                        LR 0.000016    Time 0.294977    
2024-05-04 02:51:23,696 - Epoch: [202][  200/  217]    Overall Loss 0.000995    Objective Loss 0.000995                                        LR 0.000016    Time 0.305970    
2024-05-04 02:51:28,778 - Epoch: [202][  217/  217]    Overall Loss 0.000925    Objective Loss 0.000925    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.305412    
2024-05-04 02:51:30,024 - 

2024-05-04 02:51:30,024 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:52:02,380 - Epoch: [203][  100/  217]    Overall Loss 0.000725    Objective Loss 0.000725                                        LR 0.000016    Time 0.323456    
2024-05-04 02:52:29,792 - Epoch: [203][  200/  217]    Overall Loss 0.000695    Objective Loss 0.000695                                        LR 0.000016    Time 0.298743    
2024-05-04 02:52:34,984 - Epoch: [203][  217/  217]    Overall Loss 0.000984    Objective Loss 0.000984    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.299254    
2024-05-04 02:52:35,821 - 

2024-05-04 02:52:35,821 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:53:07,164 - Epoch: [204][  100/  217]    Overall Loss 0.000668    Objective Loss 0.000668                                        LR 0.000016    Time 0.313313    
2024-05-04 02:53:35,522 - Epoch: [204][  200/  217]    Overall Loss 0.001002    Objective Loss 0.001002                                        LR 0.000016    Time 0.298396    
2024-05-04 02:53:39,358 - Epoch: [204][  217/  217]    Overall Loss 0.000930    Objective Loss 0.000930    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.292684    
2024-05-04 02:53:39,836 - 

2024-05-04 02:53:39,838 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:54:09,842 - Epoch: [205][  100/  217]    Overall Loss 0.000393    Objective Loss 0.000393                                        LR 0.000016    Time 0.299930    
2024-05-04 02:54:35,927 - Epoch: [205][  200/  217]    Overall Loss 0.000824    Objective Loss 0.000824                                        LR 0.000016    Time 0.280339    
2024-05-04 02:54:41,053 - Epoch: [205][  217/  217]    Overall Loss 0.000873    Objective Loss 0.000873    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.281985    
2024-05-04 02:54:41,758 - 

2024-05-04 02:54:41,759 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:55:14,191 - Epoch: [206][  100/  217]    Overall Loss 0.000992    Objective Loss 0.000992                                        LR 0.000016    Time 0.324230    
2024-05-04 02:55:43,217 - Epoch: [206][  200/  217]    Overall Loss 0.000767    Objective Loss 0.000767                                        LR 0.000016    Time 0.307200    
2024-05-04 02:55:47,699 - Epoch: [206][  217/  217]    Overall Loss 0.001017    Objective Loss 0.001017    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.303779    
2024-05-04 02:55:48,118 - 

2024-05-04 02:55:48,119 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:56:18,872 - Epoch: [207][  100/  217]    Overall Loss 0.000535    Objective Loss 0.000535                                        LR 0.000016    Time 0.307420    
2024-05-04 02:56:48,279 - Epoch: [207][  200/  217]    Overall Loss 0.000899    Objective Loss 0.000899                                        LR 0.000016    Time 0.300700    
2024-05-04 02:56:54,039 - Epoch: [207][  217/  217]    Overall Loss 0.000959    Objective Loss 0.000959    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.303683    
2024-05-04 02:56:54,346 - 

2024-05-04 02:56:54,346 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:57:27,503 - Epoch: [208][  100/  217]    Overall Loss 0.000875    Objective Loss 0.000875                                        LR 0.000016    Time 0.331458    
2024-05-04 02:57:55,638 - Epoch: [208][  200/  217]    Overall Loss 0.000869    Objective Loss 0.000869                                        LR 0.000016    Time 0.306363    
2024-05-04 02:58:00,168 - Epoch: [208][  217/  217]    Overall Loss 0.000922    Objective Loss 0.000922    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.303231    
2024-05-04 02:58:00,841 - 

2024-05-04 02:58:00,841 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:58:33,050 - Epoch: [209][  100/  217]    Overall Loss 0.001298    Objective Loss 0.001298                                        LR 0.000016    Time 0.321992    
2024-05-04 02:59:03,782 - Epoch: [209][  200/  217]    Overall Loss 0.000972    Objective Loss 0.000972                                        LR 0.000016    Time 0.314600    
2024-05-04 02:59:08,120 - Epoch: [209][  217/  217]    Overall Loss 0.000906    Objective Loss 0.000906    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.309916    
2024-05-04 02:59:09,138 - 

2024-05-04 02:59:09,139 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:59:39,372 - Epoch: [210][  100/  217]    Overall Loss 0.000806    Objective Loss 0.000806                                        LR 0.000016    Time 0.302219    
2024-05-04 03:00:10,690 - Epoch: [210][  200/  217]    Overall Loss 0.001020    Objective Loss 0.001020                                        LR 0.000016    Time 0.307649    
2024-05-04 03:00:16,054 - Epoch: [210][  217/  217]    Overall Loss 0.000946    Objective Loss 0.000946    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.308256    
2024-05-04 03:00:17,043 - --- validate (epoch=210)-----------
2024-05-04 03:00:17,044 - 1736 samples (32 per mini-batch)
2024-05-04 03:00:35,627 - Epoch: [210][   55/   55]    Loss 2.982400    Top1 60.023041    Top5 76.324885    
2024-05-04 03:00:36,222 - ==> Top1: 60.023    Top5: 76.325    Loss: 2.982

2024-05-04 03:00:36,228 - ==> Best [Top1: 60.426   Top5: 76.210   Sparsity:0.00   Params: 390704 on epoch: 180]
2024-05-04 03:00:36,228 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 03:00:36,274 - 

2024-05-04 03:00:36,275 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:01:08,267 - Epoch: [211][  100/  217]    Overall Loss 0.000674    Objective Loss 0.000674                                        LR 0.000016    Time 0.319822    
2024-05-04 03:01:37,160 - Epoch: [211][  200/  217]    Overall Loss 0.000850    Objective Loss 0.000850                                        LR 0.000016    Time 0.304333    
2024-05-04 03:01:42,233 - Epoch: [211][  217/  217]    Overall Loss 0.000897    Objective Loss 0.000897    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.303857    
2024-05-04 03:01:43,191 - 

2024-05-04 03:01:43,192 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:02:16,929 - Epoch: [212][  100/  217]    Overall Loss 0.000762    Objective Loss 0.000762                                        LR 0.000016    Time 0.337270    
2024-05-04 03:02:49,576 - Epoch: [212][  200/  217]    Overall Loss 0.001015    Objective Loss 0.001015                                        LR 0.000016    Time 0.331826    
2024-05-04 03:02:57,310 - Epoch: [212][  217/  217]    Overall Loss 0.000943    Objective Loss 0.000943    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.341462    
2024-05-04 03:02:58,019 - 

2024-05-04 03:02:58,020 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:03:28,098 - Epoch: [213][  100/  217]    Overall Loss 0.000498    Objective Loss 0.000498                                        LR 0.000016    Time 0.300681    
2024-05-04 03:03:56,908 - Epoch: [213][  200/  217]    Overall Loss 0.001012    Objective Loss 0.001012                                        LR 0.000016    Time 0.294346    
2024-05-04 03:04:01,146 - Epoch: [213][  217/  217]    Overall Loss 0.000939    Objective Loss 0.000939    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.290811    
2024-05-04 03:04:01,470 - 

2024-05-04 03:04:01,471 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:04:33,672 - Epoch: [214][  100/  217]    Overall Loss 0.000485    Objective Loss 0.000485                                        LR 0.000016    Time 0.321892    
2024-05-04 03:05:01,548 - Epoch: [214][  200/  217]    Overall Loss 0.001003    Objective Loss 0.001003                                        LR 0.000016    Time 0.300273    
2024-05-04 03:05:06,923 - Epoch: [214][  217/  217]    Overall Loss 0.000932    Objective Loss 0.000932    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.301509    
2024-05-04 03:05:07,160 - 

2024-05-04 03:05:07,161 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:05:41,138 - Epoch: [215][  100/  217]    Overall Loss 0.000812    Objective Loss 0.000812                                        LR 0.000016    Time 0.339672    
2024-05-04 03:06:09,013 - Epoch: [215][  200/  217]    Overall Loss 0.000881    Objective Loss 0.000881                                        LR 0.000016    Time 0.309162    
2024-05-04 03:06:15,850 - Epoch: [215][  217/  217]    Overall Loss 0.000935    Objective Loss 0.000935    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.316444    
2024-05-04 03:06:16,744 - 

2024-05-04 03:06:16,744 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:06:46,362 - Epoch: [216][  100/  217]    Overall Loss 0.000614    Objective Loss 0.000614                                        LR 0.000016    Time 0.296083    
2024-05-04 03:07:18,881 - Epoch: [216][  200/  217]    Overall Loss 0.000977    Objective Loss 0.000977                                        LR 0.000016    Time 0.310581    
2024-05-04 03:07:25,774 - Epoch: [216][  217/  217]    Overall Loss 0.000908    Objective Loss 0.000908    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.318009    
2024-05-04 03:07:26,587 - 

2024-05-04 03:07:26,588 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:07:56,582 - Epoch: [217][  100/  217]    Overall Loss 0.000757    Objective Loss 0.000757                                        LR 0.000016    Time 0.299844    
2024-05-04 03:08:23,150 - Epoch: [217][  200/  217]    Overall Loss 0.000884    Objective Loss 0.000884                                        LR 0.000016    Time 0.282711    
2024-05-04 03:08:28,139 - Epoch: [217][  217/  217]    Overall Loss 0.000948    Objective Loss 0.000948    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.283542    
2024-05-04 03:08:28,631 - 

2024-05-04 03:08:28,632 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:08:57,197 - Epoch: [218][  100/  217]    Overall Loss 0.000727    Objective Loss 0.000727                                        LR 0.000016    Time 0.285551    
2024-05-04 03:09:23,201 - Epoch: [218][  200/  217]    Overall Loss 0.000708    Objective Loss 0.000708                                        LR 0.000016    Time 0.272753    
2024-05-04 03:09:27,158 - Epoch: [218][  217/  217]    Overall Loss 0.000907    Objective Loss 0.000907    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.269613    
2024-05-04 03:09:27,470 - 

2024-05-04 03:09:27,472 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:09:57,643 - Epoch: [219][  100/  217]    Overall Loss 0.001061    Objective Loss 0.001061                                        LR 0.000016    Time 0.301589    
2024-05-04 03:10:24,250 - Epoch: [219][  200/  217]    Overall Loss 0.000797    Objective Loss 0.000797                                        LR 0.000016    Time 0.283792    
2024-05-04 03:10:29,717 - Epoch: [219][  217/  217]    Overall Loss 0.000938    Objective Loss 0.000938    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.286741    
2024-05-04 03:10:30,178 - 

2024-05-04 03:10:30,179 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:11:01,265 - Epoch: [220][  100/  217]    Overall Loss 0.001031    Objective Loss 0.001031                                        LR 0.000016    Time 0.310763    
2024-05-04 03:11:32,835 - Epoch: [220][  200/  217]    Overall Loss 0.000978    Objective Loss 0.000978                                        LR 0.000016    Time 0.313183    
2024-05-04 03:11:38,506 - Epoch: [220][  217/  217]    Overall Loss 0.000908    Objective Loss 0.000908    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.314774    
2024-05-04 03:11:38,715 - --- validate (epoch=220)-----------
2024-05-04 03:11:38,716 - 1736 samples (32 per mini-batch)
2024-05-04 03:11:55,743 - Epoch: [220][   55/   55]    Loss 3.014756    Top1 60.656682    Top5 76.152074    
2024-05-04 03:11:56,205 - ==> Top1: 60.657    Top5: 76.152    Loss: 3.015

2024-05-04 03:11:56,210 - ==> Best [Top1: 60.657   Top5: 76.152   Sparsity:0.00   Params: 390704 on epoch: 220]
2024-05-04 03:11:56,210 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 03:11:56,265 - 

2024-05-04 03:11:56,266 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:12:32,327 - Epoch: [221][  100/  217]    Overall Loss 0.000679    Objective Loss 0.000679                                        LR 0.000016    Time 0.360502    
2024-05-04 03:12:56,106 - Epoch: [221][  200/  217]    Overall Loss 0.000960    Objective Loss 0.000960                                        LR 0.000016    Time 0.299101    
2024-05-04 03:13:03,417 - Epoch: [221][  217/  217]    Overall Loss 0.000891    Objective Loss 0.000891    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.309350    
2024-05-04 03:13:03,878 - 

2024-05-04 03:13:03,879 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:13:40,010 - Epoch: [222][  100/  217]    Overall Loss 0.001163    Objective Loss 0.001163                                        LR 0.000016    Time 0.361210    
2024-05-04 03:14:05,310 - Epoch: [222][  200/  217]    Overall Loss 0.000981    Objective Loss 0.000981                                        LR 0.000016    Time 0.307051    
2024-05-04 03:14:09,312 - Epoch: [222][  217/  217]    Overall Loss 0.000910    Objective Loss 0.000910    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.301428    
2024-05-04 03:14:10,188 - 

2024-05-04 03:14:10,189 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:14:42,677 - Epoch: [223][  100/  217]    Overall Loss 0.000878    Objective Loss 0.000878                                        LR 0.000016    Time 0.324768    
2024-05-04 03:15:11,000 - Epoch: [223][  200/  217]    Overall Loss 0.000938    Objective Loss 0.000938                                        LR 0.000016    Time 0.303952    
2024-05-04 03:15:15,243 - Epoch: [223][  217/  217]    Overall Loss 0.000868    Objective Loss 0.000868    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.299684    
2024-05-04 03:15:15,911 - 

2024-05-04 03:15:15,911 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:15:45,441 - Epoch: [224][  100/  217]    Overall Loss 0.000991    Objective Loss 0.000991                                        LR 0.000016    Time 0.295194    
2024-05-04 03:16:16,616 - Epoch: [224][  200/  217]    Overall Loss 0.000837    Objective Loss 0.000837                                        LR 0.000016    Time 0.303402    
2024-05-04 03:16:20,179 - Epoch: [224][  217/  217]    Overall Loss 0.000883    Objective Loss 0.000883    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.296045    
2024-05-04 03:16:20,396 - 

2024-05-04 03:16:20,396 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:16:50,173 - Epoch: [225][  100/  217]    Overall Loss 0.001025    Objective Loss 0.001025                                        LR 0.000016    Time 0.297672    
2024-05-04 03:17:15,072 - Epoch: [225][  200/  217]    Overall Loss 0.000790    Objective Loss 0.000790                                        LR 0.000016    Time 0.273288    
2024-05-04 03:17:19,375 - Epoch: [225][  217/  217]    Overall Loss 0.000928    Objective Loss 0.000928    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.271700    
2024-05-04 03:17:19,628 - 

2024-05-04 03:17:19,629 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:17:50,328 - Epoch: [226][  100/  217]    Overall Loss 0.000442    Objective Loss 0.000442                                        LR 0.000016    Time 0.306883    
2024-05-04 03:18:18,624 - Epoch: [226][  200/  217]    Overall Loss 0.000922    Objective Loss 0.000922                                        LR 0.000016    Time 0.294875    
2024-05-04 03:18:22,109 - Epoch: [226][  217/  217]    Overall Loss 0.000855    Objective Loss 0.000855    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.287823    
2024-05-04 03:18:22,342 - 

2024-05-04 03:18:22,343 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:18:50,833 - Epoch: [227][  100/  217]    Overall Loss 0.001441    Objective Loss 0.001441                                        LR 0.000016    Time 0.284808    
2024-05-04 03:19:18,215 - Epoch: [227][  200/  217]    Overall Loss 0.000858    Objective Loss 0.000858                                        LR 0.000016    Time 0.279268    
2024-05-04 03:19:21,361 - Epoch: [227][  217/  217]    Overall Loss 0.000905    Objective Loss 0.000905    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.271874    
2024-05-04 03:19:21,601 - 

2024-05-04 03:19:21,601 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:19:57,212 - Epoch: [228][  100/  217]    Overall Loss 0.000788    Objective Loss 0.000788                                        LR 0.000016    Time 0.356005    
2024-05-04 03:20:22,667 - Epoch: [228][  200/  217]    Overall Loss 0.000941    Objective Loss 0.000941                                        LR 0.000016    Time 0.305237    
2024-05-04 03:20:26,850 - Epoch: [228][  217/  217]    Overall Loss 0.000873    Objective Loss 0.000873    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.300592    
2024-05-04 03:20:27,103 - 

2024-05-04 03:20:27,104 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:21:01,708 - Epoch: [229][  100/  217]    Overall Loss 0.000900    Objective Loss 0.000900                                        LR 0.000016    Time 0.345957    
2024-05-04 03:21:27,890 - Epoch: [229][  200/  217]    Overall Loss 0.000812    Objective Loss 0.000812                                        LR 0.000016    Time 0.303847    
2024-05-04 03:21:33,410 - Epoch: [229][  217/  217]    Overall Loss 0.000866    Objective Loss 0.000866    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.305473    
2024-05-04 03:21:33,626 - 

2024-05-04 03:21:33,627 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:22:05,956 - Epoch: [230][  100/  217]    Overall Loss 0.000741    Objective Loss 0.000741                                        LR 0.000016    Time 0.323201    
2024-05-04 03:22:33,656 - Epoch: [230][  200/  217]    Overall Loss 0.000864    Objective Loss 0.000864                                        LR 0.000016    Time 0.300053    
2024-05-04 03:22:38,690 - Epoch: [230][  217/  217]    Overall Loss 0.000902    Objective Loss 0.000902    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.299736    
2024-05-04 03:22:39,183 - --- validate (epoch=230)-----------
2024-05-04 03:22:39,183 - 1736 samples (32 per mini-batch)
2024-05-04 03:22:57,680 - Epoch: [230][   55/   55]    Loss 2.990407    Top1 60.426267    Top5 76.324885    
2024-05-04 03:22:58,045 - ==> Top1: 60.426    Top5: 76.325    Loss: 2.990

2024-05-04 03:22:58,051 - ==> Best [Top1: 60.657   Top5: 76.152   Sparsity:0.00   Params: 390704 on epoch: 220]
2024-05-04 03:22:58,052 - Saving checkpoint to: logs/2024.05.03-230959/checkpoint.pth.tar
2024-05-04 03:22:58,088 - 

2024-05-04 03:22:58,089 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:23:29,575 - Epoch: [231][  100/  217]    Overall Loss 0.000962    Objective Loss 0.000962                                        LR 0.000016    Time 0.314757    
2024-05-04 03:23:59,003 - Epoch: [231][  200/  217]    Overall Loss 0.000752    Objective Loss 0.000752                                        LR 0.000016    Time 0.304476    
2024-05-04 03:24:03,834 - Epoch: [231][  217/  217]    Overall Loss 0.000918    Objective Loss 0.000918    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.302878    
2024-05-04 03:24:04,686 - 

2024-05-04 03:24:04,687 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:24:33,187 - Epoch: [232][  100/  217]    Overall Loss 0.000933    Objective Loss 0.000933                                        LR 0.000016    Time 0.284899    
2024-05-04 03:25:00,499 - Epoch: [232][  200/  217]    Overall Loss 0.000973    Objective Loss 0.000973                                        LR 0.000016    Time 0.278964    
2024-05-04 03:25:04,080 - Epoch: [232][  217/  217]    Overall Loss 0.000902    Objective Loss 0.000902    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.273604    
2024-05-04 03:25:04,327 - 

2024-05-04 03:25:04,328 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:25:32,508 - Epoch: [233][  100/  217]    Overall Loss 0.000522    Objective Loss 0.000522                                        LR 0.000016    Time 0.281702    
2024-05-04 03:26:00,417 - Epoch: [233][  200/  217]    Overall Loss 0.000872    Objective Loss 0.000872                                        LR 0.000016    Time 0.280351    
2024-05-04 03:26:04,431 - Epoch: [233][  217/  217]    Overall Loss 0.000917    Objective Loss 0.000917    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.276879    
2024-05-04 03:26:04,727 - 

2024-05-04 03:26:04,728 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:26:33,639 - Epoch: [234][  100/  217]    Overall Loss 0.000692    Objective Loss 0.000692                                        LR 0.000016    Time 0.289014    
2024-05-04 03:27:01,289 - Epoch: [234][  200/  217]    Overall Loss 0.000846    Objective Loss 0.000846                                        LR 0.000016    Time 0.282713    
2024-05-04 03:27:05,055 - Epoch: [234][  217/  217]    Overall Loss 0.000908    Objective Loss 0.000908    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.277916    
2024-05-04 03:27:05,385 - 

2024-05-04 03:27:05,385 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:27:37,853 - Epoch: [235][  100/  217]    Overall Loss 0.001139    Objective Loss 0.001139                                        LR 0.000016    Time 0.324582    
2024-05-04 03:28:03,178 - Epoch: [235][  200/  217]    Overall Loss 0.000970    Objective Loss 0.000970                                        LR 0.000016    Time 0.288867    
2024-05-04 03:28:09,881 - Epoch: [235][  217/  217]    Overall Loss 0.000899    Objective Loss 0.000899    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.297117    
2024-05-04 03:28:10,254 - 

2024-05-04 03:28:10,255 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:28:45,266 - Epoch: [236][  100/  217]    Overall Loss 0.000719    Objective Loss 0.000719                                        LR 0.000016    Time 0.350015    
2024-05-04 03:29:10,167 - Epoch: [236][  200/  217]    Overall Loss 0.000849    Objective Loss 0.000849                                        LR 0.000016    Time 0.299472    
2024-05-04 03:29:13,964 - Epoch: [236][  217/  217]    Overall Loss 0.000893    Objective Loss 0.000893    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.293497    
2024-05-04 03:29:15,069 - 

2024-05-04 03:29:15,070 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:29:46,444 - Epoch: [237][  100/  217]    Overall Loss 0.001446    Objective Loss 0.001446                                        LR 0.000016    Time 0.313640    
2024-05-04 03:30:12,449 - Epoch: [237][  200/  217]    Overall Loss 0.000978    Objective Loss 0.000978                                        LR 0.000016    Time 0.286802    
2024-05-04 03:30:18,101 - Epoch: [237][  217/  217]    Overall Loss 0.000908    Objective Loss 0.000908    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.290370    
2024-05-04 03:30:18,403 - 

2024-05-04 03:30:18,403 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:30:49,158 - Epoch: [238][  100/  217]    Overall Loss 0.001220    Objective Loss 0.001220                                        LR 0.000016    Time 0.307455    
2024-05-04 03:31:14,957 - Epoch: [238][  200/  217]    Overall Loss 0.001002    Objective Loss 0.001002                                        LR 0.000016    Time 0.282663    
2024-05-04 03:31:17,922 - Epoch: [238][  217/  217]    Overall Loss 0.000930    Objective Loss 0.000930    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.274174    
2024-05-04 03:31:18,102 - 

2024-05-04 03:31:18,102 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:31:50,549 - Epoch: [239][  100/  217]    Overall Loss 0.000437    Objective Loss 0.000437                                        LR 0.000016    Time 0.324364    
2024-05-04 03:32:14,420 - Epoch: [239][  200/  217]    Overall Loss 0.000921    Objective Loss 0.000921                                        LR 0.000016    Time 0.281497    
2024-05-04 03:32:19,659 - Epoch: [239][  217/  217]    Overall Loss 0.000855    Objective Loss 0.000855    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.283574    
2024-05-04 03:32:19,957 - 

2024-05-04 03:32:19,958 - Initiating quantization aware training (QAT)...
2024-05-04 03:32:20,018 - 

2024-05-04 03:32:20,018 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:32:49,963 - Epoch: [240][  100/  217]    Overall Loss 1.779494    Objective Loss 1.779494                                        LR 0.000016    Time 0.299342    
2024-05-04 03:33:18,265 - Epoch: [240][  200/  217]    Overall Loss 1.394520    Objective Loss 1.394520                                        LR 0.000016    Time 0.291136    
2024-05-04 03:33:21,340 - Epoch: [240][  217/  217]    Overall Loss 1.363689    Objective Loss 1.363689    Top1 67.213115    Top5 86.885246    LR 0.000016    Time 0.282486    
2024-05-04 03:33:21,797 - --- validate (epoch=240)-----------
2024-05-04 03:33:21,797 - 1736 samples (32 per mini-batch)
2024-05-04 03:33:40,815 - Epoch: [240][   55/   55]    Loss 2.087284    Top1 54.262673    Top5 71.947005    
2024-05-04 03:33:41,225 - ==> Top1: 54.263    Top5: 71.947    Loss: 2.087

2024-05-04 03:33:41,229 - ==> Best [Top1: 54.263   Top5: 71.947   Sparsity:0.00   Params: 390704 on epoch: 240]
2024-05-04 03:33:41,230 - Saving checkpoint to: logs/2024.05.03-230959/qat_checkpoint.pth.tar
2024-05-04 03:33:41,273 - 

2024-05-04 03:33:41,274 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:34:14,008 - Epoch: [241][  100/  217]    Overall Loss 0.804602    Objective Loss 0.804602                                        LR 0.000016    Time 0.327230    
2024-05-04 03:34:40,545 - Epoch: [241][  200/  217]    Overall Loss 0.761860    Objective Loss 0.761860                                        LR 0.000016    Time 0.296255    
2024-05-04 03:34:45,454 - Epoch: [241][  217/  217]    Overall Loss 0.759622    Objective Loss 0.759622    Top1 88.524590    Top5 100.000000    LR 0.000016    Time 0.295661    
2024-05-04 03:34:46,137 - 

2024-05-04 03:34:46,138 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:35:16,397 - Epoch: [242][  100/  217]    Overall Loss 0.636641    Objective Loss 0.636641                                        LR 0.000016    Time 0.302489    
2024-05-04 03:35:43,798 - Epoch: [242][  200/  217]    Overall Loss 0.629944    Objective Loss 0.629944                                        LR 0.000016    Time 0.288205    
2024-05-04 03:35:49,935 - Epoch: [242][  217/  217]    Overall Loss 0.623759    Objective Loss 0.623759    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.293899    
2024-05-04 03:35:50,504 - 

2024-05-04 03:35:50,504 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:36:25,226 - Epoch: [243][  100/  217]    Overall Loss 0.542734    Objective Loss 0.542734                                        LR 0.000016    Time 0.347122    
2024-05-04 03:36:50,715 - Epoch: [243][  200/  217]    Overall Loss 0.544306    Objective Loss 0.544306                                        LR 0.000016    Time 0.300954    
2024-05-04 03:36:55,102 - Epoch: [243][  217/  217]    Overall Loss 0.538149    Objective Loss 0.538149    Top1 91.803279    Top5 98.360656    LR 0.000016    Time 0.297583    
2024-05-04 03:36:56,181 - 

2024-05-04 03:36:56,182 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:37:28,468 - Epoch: [244][  100/  217]    Overall Loss 0.458009    Objective Loss 0.458009                                        LR 0.000016    Time 0.322758    
2024-05-04 03:37:53,685 - Epoch: [244][  200/  217]    Overall Loss 0.459347    Objective Loss 0.459347                                        LR 0.000016    Time 0.287417    
2024-05-04 03:37:58,481 - Epoch: [244][  217/  217]    Overall Loss 0.459573    Objective Loss 0.459573    Top1 86.885246    Top5 98.360656    LR 0.000016    Time 0.286992    
2024-05-04 03:37:58,716 - 

2024-05-04 03:37:58,717 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:38:30,883 - Epoch: [245][  100/  217]    Overall Loss 0.413142    Objective Loss 0.413142                                        LR 0.000016    Time 0.321547    
2024-05-04 03:39:00,290 - Epoch: [245][  200/  217]    Overall Loss 0.421926    Objective Loss 0.421926                                        LR 0.000016    Time 0.307765    
2024-05-04 03:39:04,232 - Epoch: [245][  217/  217]    Overall Loss 0.421645    Objective Loss 0.421645    Top1 85.245902    Top5 100.000000    LR 0.000016    Time 0.301806    
2024-05-04 03:39:04,444 - 

2024-05-04 03:39:04,445 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:39:35,690 - Epoch: [246][  100/  217]    Overall Loss 0.376553    Objective Loss 0.376553                                        LR 0.000016    Time 0.312349    
2024-05-04 03:40:05,475 - Epoch: [246][  200/  217]    Overall Loss 0.374957    Objective Loss 0.374957                                        LR 0.000016    Time 0.305059    
2024-05-04 03:40:08,682 - Epoch: [246][  217/  217]    Overall Loss 0.376008    Objective Loss 0.376008    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.295930    
2024-05-04 03:40:08,985 - 

2024-05-04 03:40:08,986 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:40:35,642 - Epoch: [247][  100/  217]    Overall Loss 0.356676    Objective Loss 0.356676                                        LR 0.000016    Time 0.266458    
2024-05-04 03:41:03,433 - Epoch: [247][  200/  217]    Overall Loss 0.348449    Objective Loss 0.348449                                        LR 0.000016    Time 0.272133    
2024-05-04 03:41:08,791 - Epoch: [247][  217/  217]    Overall Loss 0.348523    Objective Loss 0.348523    Top1 90.163934    Top5 96.721311    LR 0.000016    Time 0.275500    
2024-05-04 03:41:08,954 - 

2024-05-04 03:41:08,955 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:41:39,610 - Epoch: [248][  100/  217]    Overall Loss 0.323589    Objective Loss 0.323589                                        LR 0.000016    Time 0.306450    
2024-05-04 03:42:05,787 - Epoch: [248][  200/  217]    Overall Loss 0.321089    Objective Loss 0.321089                                        LR 0.000016    Time 0.284063    
2024-05-04 03:42:11,849 - Epoch: [248][  217/  217]    Overall Loss 0.324054    Objective Loss 0.324054    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.289735    
2024-05-04 03:42:12,061 - 

2024-05-04 03:42:12,061 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:42:41,647 - Epoch: [249][  100/  217]    Overall Loss 0.316975    Objective Loss 0.316975                                        LR 0.000016    Time 0.295759    
2024-05-04 03:43:09,459 - Epoch: [249][  200/  217]    Overall Loss 0.318579    Objective Loss 0.318579                                        LR 0.000016    Time 0.286893    
2024-05-04 03:43:15,097 - Epoch: [249][  217/  217]    Overall Loss 0.315671    Objective Loss 0.315671    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.290392    
2024-05-04 03:43:15,838 - 

2024-05-04 03:43:15,840 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:43:49,860 - Epoch: [250][  100/  217]    Overall Loss 0.293558    Objective Loss 0.293558                                        LR 0.000016    Time 0.340087    
2024-05-04 03:44:19,987 - Epoch: [250][  200/  217]    Overall Loss 0.291231    Objective Loss 0.291231                                        LR 0.000016    Time 0.320636    
2024-05-04 03:44:23,927 - Epoch: [250][  217/  217]    Overall Loss 0.291510    Objective Loss 0.291510    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.313663    
2024-05-04 03:44:24,406 - --- validate (epoch=250)-----------
2024-05-04 03:44:24,408 - 1736 samples (32 per mini-batch)
2024-05-04 03:44:42,737 - Epoch: [250][   55/   55]    Loss 2.017219    Top1 57.200461    Top5 74.481567    
2024-05-04 03:44:43,086 - ==> Top1: 57.200    Top5: 74.482    Loss: 2.017

2024-05-04 03:44:43,092 - ==> Best [Top1: 57.200   Top5: 74.482   Sparsity:0.00   Params: 390704 on epoch: 250]
2024-05-04 03:44:43,092 - Saving checkpoint to: logs/2024.05.03-230959/qat_checkpoint.pth.tar
2024-05-04 03:44:43,147 - 

2024-05-04 03:44:43,148 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:45:17,747 - Epoch: [251][  100/  217]    Overall Loss 0.264449    Objective Loss 0.264449                                        LR 0.000016    Time 0.345885    
2024-05-04 03:45:43,370 - Epoch: [251][  200/  217]    Overall Loss 0.276032    Objective Loss 0.276032                                        LR 0.000016    Time 0.301010    
2024-05-04 03:45:47,550 - Epoch: [251][  217/  217]    Overall Loss 0.273381    Objective Loss 0.273381    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.296688    
2024-05-04 03:45:48,442 - 

2024-05-04 03:45:48,442 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:46:16,005 - Epoch: [252][  100/  217]    Overall Loss 0.256040    Objective Loss 0.256040                                        LR 0.000016    Time 0.275508    
2024-05-04 03:46:43,110 - Epoch: [252][  200/  217]    Overall Loss 0.263098    Objective Loss 0.263098                                        LR 0.000016    Time 0.273222    
2024-05-04 03:46:47,539 - Epoch: [252][  217/  217]    Overall Loss 0.264433    Objective Loss 0.264433    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.272217    
2024-05-04 03:46:47,964 - 

2024-05-04 03:46:47,965 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:47:20,028 - Epoch: [253][  100/  217]    Overall Loss 0.245925    Objective Loss 0.245925                                        LR 0.000016    Time 0.320530    
2024-05-04 03:47:47,294 - Epoch: [253][  200/  217]    Overall Loss 0.242461    Objective Loss 0.242461                                        LR 0.000016    Time 0.296550    
2024-05-04 03:47:52,972 - Epoch: [253][  217/  217]    Overall Loss 0.244981    Objective Loss 0.244981    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.299473    
2024-05-04 03:47:53,296 - 

2024-05-04 03:47:53,296 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:48:27,209 - Epoch: [254][  100/  217]    Overall Loss 0.223116    Objective Loss 0.223116                                        LR 0.000016    Time 0.339021    
2024-05-04 03:48:55,192 - Epoch: [254][  200/  217]    Overall Loss 0.231067    Objective Loss 0.231067                                        LR 0.000016    Time 0.309382    
2024-05-04 03:48:58,868 - Epoch: [254][  217/  217]    Overall Loss 0.231320    Objective Loss 0.231320    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.302077    
2024-05-04 03:48:59,076 - 

2024-05-04 03:48:59,077 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:49:31,251 - Epoch: [255][  100/  217]    Overall Loss 0.221202    Objective Loss 0.221202                                        LR 0.000016    Time 0.321634    
2024-05-04 03:50:00,234 - Epoch: [255][  200/  217]    Overall Loss 0.222928    Objective Loss 0.222928                                        LR 0.000016    Time 0.305686    
2024-05-04 03:50:03,710 - Epoch: [255][  217/  217]    Overall Loss 0.226774    Objective Loss 0.226774    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.297746    
2024-05-04 03:50:04,398 - 

2024-05-04 03:50:04,399 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:50:35,837 - Epoch: [256][  100/  217]    Overall Loss 0.219249    Objective Loss 0.219249                                        LR 0.000016    Time 0.314277    
2024-05-04 03:51:06,400 - Epoch: [256][  200/  217]    Overall Loss 0.219772    Objective Loss 0.219772                                        LR 0.000016    Time 0.309909    
2024-05-04 03:51:09,971 - Epoch: [256][  217/  217]    Overall Loss 0.220669    Objective Loss 0.220669    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.302079    
2024-05-04 03:51:10,188 - 

2024-05-04 03:51:10,189 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:51:42,202 - Epoch: [257][  100/  217]    Overall Loss 0.193025    Objective Loss 0.193025                                        LR 0.000016    Time 0.320038    
2024-05-04 03:52:07,839 - Epoch: [257][  200/  217]    Overall Loss 0.208074    Objective Loss 0.208074                                        LR 0.000016    Time 0.288160    
2024-05-04 03:52:11,537 - Epoch: [257][  217/  217]    Overall Loss 0.208860    Objective Loss 0.208860    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.282618    
2024-05-04 03:52:11,928 - 

2024-05-04 03:52:11,928 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:52:47,018 - Epoch: [258][  100/  217]    Overall Loss 0.190909    Objective Loss 0.190909                                        LR 0.000016    Time 0.350802    
2024-05-04 03:53:14,926 - Epoch: [258][  200/  217]    Overall Loss 0.200266    Objective Loss 0.200266                                        LR 0.000016    Time 0.314901    
2024-05-04 03:53:20,767 - Epoch: [258][  217/  217]    Overall Loss 0.200424    Objective Loss 0.200424    Top1 91.803279    Top5 100.000000    LR 0.000016    Time 0.317143    
2024-05-04 03:53:21,205 - 

2024-05-04 03:53:21,206 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:53:52,217 - Epoch: [259][  100/  217]    Overall Loss 0.193287    Objective Loss 0.193287                                        LR 0.000016    Time 0.310009    
2024-05-04 03:54:21,700 - Epoch: [259][  200/  217]    Overall Loss 0.202817    Objective Loss 0.202817                                        LR 0.000016    Time 0.302378    
2024-05-04 03:54:25,888 - Epoch: [259][  217/  217]    Overall Loss 0.203639    Objective Loss 0.203639    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.297980    
2024-05-04 03:54:26,219 - 

2024-05-04 03:54:26,220 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:54:58,238 - Epoch: [260][  100/  217]    Overall Loss 0.184126    Objective Loss 0.184126                                        LR 0.000016    Time 0.320094    
2024-05-04 03:55:25,176 - Epoch: [260][  200/  217]    Overall Loss 0.192430    Objective Loss 0.192430                                        LR 0.000016    Time 0.294690    
2024-05-04 03:55:28,771 - Epoch: [260][  217/  217]    Overall Loss 0.193720    Objective Loss 0.193720    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.288162    
2024-05-04 03:55:29,546 - --- validate (epoch=260)-----------
2024-05-04 03:55:29,546 - 1736 samples (32 per mini-batch)
2024-05-04 03:55:48,339 - Epoch: [260][   55/   55]    Loss 2.015981    Top1 58.237327    Top5 73.905530    
2024-05-04 03:55:49,161 - ==> Top1: 58.237    Top5: 73.906    Loss: 2.016

2024-05-04 03:55:49,167 - ==> Best [Top1: 58.237   Top5: 73.906   Sparsity:0.00   Params: 390704 on epoch: 260]
2024-05-04 03:55:49,168 - Saving checkpoint to: logs/2024.05.03-230959/qat_checkpoint.pth.tar
2024-05-04 03:55:49,217 - 

2024-05-04 03:55:49,217 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:56:18,387 - Epoch: [261][  100/  217]    Overall Loss 0.180241    Objective Loss 0.180241                                        LR 0.000016    Time 0.291592    
2024-05-04 03:56:46,801 - Epoch: [261][  200/  217]    Overall Loss 0.190479    Objective Loss 0.190479                                        LR 0.000016    Time 0.287820    
2024-05-04 03:56:51,219 - Epoch: [261][  217/  217]    Overall Loss 0.189110    Objective Loss 0.189110    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.285620    
2024-05-04 03:56:51,576 - 

2024-05-04 03:56:51,577 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:57:22,717 - Epoch: [262][  100/  217]    Overall Loss 0.163455    Objective Loss 0.163455                                        LR 0.000016    Time 0.311306    
2024-05-04 03:57:50,092 - Epoch: [262][  200/  217]    Overall Loss 0.178254    Objective Loss 0.178254                                        LR 0.000016    Time 0.292486    
2024-05-04 03:57:55,065 - Epoch: [262][  217/  217]    Overall Loss 0.178713    Objective Loss 0.178713    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.292479    
2024-05-04 03:57:55,292 - 

2024-05-04 03:57:55,293 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:58:26,388 - Epoch: [263][  100/  217]    Overall Loss 0.152502    Objective Loss 0.152502                                        LR 0.000016    Time 0.310841    
2024-05-04 03:58:57,014 - Epoch: [263][  200/  217]    Overall Loss 0.170085    Objective Loss 0.170085                                        LR 0.000016    Time 0.308510    
2024-05-04 03:59:02,801 - Epoch: [263][  217/  217]    Overall Loss 0.171783    Objective Loss 0.171783    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.311003    
2024-05-04 03:59:02,987 - 

2024-05-04 03:59:02,988 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:59:30,317 - Epoch: [264][  100/  217]    Overall Loss 0.168827    Objective Loss 0.168827                                        LR 0.000016    Time 0.273197    
2024-05-04 04:00:01,878 - Epoch: [264][  200/  217]    Overall Loss 0.174399    Objective Loss 0.174399                                        LR 0.000016    Time 0.294355    
2024-05-04 04:00:06,436 - Epoch: [264][  217/  217]    Overall Loss 0.173334    Objective Loss 0.173334    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.292292    
2024-05-04 04:00:06,606 - 

2024-05-04 04:00:06,607 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:00:32,066 - Epoch: [265][  100/  217]    Overall Loss 0.167115    Objective Loss 0.167115                                        LR 0.000016    Time 0.254483    
2024-05-04 04:01:01,049 - Epoch: [265][  200/  217]    Overall Loss 0.178206    Objective Loss 0.178206                                        LR 0.000016    Time 0.272116    
2024-05-04 04:01:04,196 - Epoch: [265][  217/  217]    Overall Loss 0.178794    Objective Loss 0.178794    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.265288    
2024-05-04 04:01:04,971 - 

2024-05-04 04:01:04,972 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:01:32,734 - Epoch: [266][  100/  217]    Overall Loss 0.157678    Objective Loss 0.157678                                        LR 0.000016    Time 0.277514    
2024-05-04 04:01:59,037 - Epoch: [266][  200/  217]    Overall Loss 0.170462    Objective Loss 0.170462                                        LR 0.000016    Time 0.270230    
2024-05-04 04:02:02,327 - Epoch: [266][  217/  217]    Overall Loss 0.169636    Objective Loss 0.169636    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.264200    
2024-05-04 04:02:02,654 - 

2024-05-04 04:02:02,655 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:02:31,266 - Epoch: [267][  100/  217]    Overall Loss 0.160163    Objective Loss 0.160163                                        LR 0.000016    Time 0.286012    
2024-05-04 04:02:56,132 - Epoch: [267][  200/  217]    Overall Loss 0.156854    Objective Loss 0.156854                                        LR 0.000016    Time 0.267294    
2024-05-04 04:02:58,857 - Epoch: [267][  217/  217]    Overall Loss 0.157422    Objective Loss 0.157422    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.258900    
2024-05-04 04:02:59,082 - 

2024-05-04 04:02:59,082 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:03:23,169 - Epoch: [268][  100/  217]    Overall Loss 0.157677    Objective Loss 0.157677                                        LR 0.000016    Time 0.240764    
2024-05-04 04:03:49,854 - Epoch: [268][  200/  217]    Overall Loss 0.165131    Objective Loss 0.165131                                        LR 0.000016    Time 0.253764    
2024-05-04 04:03:53,470 - Epoch: [268][  217/  217]    Overall Loss 0.162075    Objective Loss 0.162075    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.250539    
2024-05-04 04:03:53,690 - 

2024-05-04 04:03:53,690 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:04:24,744 - Epoch: [269][  100/  217]    Overall Loss 0.138584    Objective Loss 0.138584                                        LR 0.000016    Time 0.310447    
2024-05-04 04:04:48,026 - Epoch: [269][  200/  217]    Overall Loss 0.140097    Objective Loss 0.140097                                        LR 0.000016    Time 0.271587    
2024-05-04 04:04:50,834 - Epoch: [269][  217/  217]    Overall Loss 0.140804    Objective Loss 0.140804    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.263244    
2024-05-04 04:04:51,034 - 

2024-05-04 04:04:51,034 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:05:19,352 - Epoch: [270][  100/  217]    Overall Loss 0.130584    Objective Loss 0.130584                                        LR 0.000016    Time 0.283085    
2024-05-04 04:05:41,297 - Epoch: [270][  200/  217]    Overall Loss 0.136026    Objective Loss 0.136026                                        LR 0.000016    Time 0.251229    
2024-05-04 04:05:44,302 - Epoch: [270][  217/  217]    Overall Loss 0.143355    Objective Loss 0.143355    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.245383    
2024-05-04 04:05:44,514 - --- validate (epoch=270)-----------
2024-05-04 04:05:44,514 - 1736 samples (32 per mini-batch)
2024-05-04 04:06:00,215 - Epoch: [270][   55/   55]    Loss 2.098808    Top1 57.142857    Top5 74.251152    
2024-05-04 04:06:00,426 - ==> Top1: 57.143    Top5: 74.251    Loss: 2.099

2024-05-04 04:06:00,430 - ==> Best [Top1: 58.237   Top5: 73.906   Sparsity:0.00   Params: 390704 on epoch: 260]
2024-05-04 04:06:00,431 - Saving checkpoint to: logs/2024.05.03-230959/qat_checkpoint.pth.tar
2024-05-04 04:06:00,454 - 

2024-05-04 04:06:00,455 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:06:30,128 - Epoch: [271][  100/  217]    Overall Loss 0.135428    Objective Loss 0.135428                                        LR 0.000016    Time 0.296639    
2024-05-04 04:06:51,902 - Epoch: [271][  200/  217]    Overall Loss 0.149422    Objective Loss 0.149422                                        LR 0.000016    Time 0.257149    
2024-05-04 04:06:55,900 - Epoch: [271][  217/  217]    Overall Loss 0.149404    Objective Loss 0.149404    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.255419    
2024-05-04 04:06:56,130 - 

2024-05-04 04:06:56,130 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:07:27,655 - Epoch: [272][  100/  217]    Overall Loss 0.143357    Objective Loss 0.143357                                        LR 0.000016    Time 0.315161    
2024-05-04 04:07:51,260 - Epoch: [272][  200/  217]    Overall Loss 0.146511    Objective Loss 0.146511                                        LR 0.000016    Time 0.275565    
2024-05-04 04:07:55,641 - Epoch: [272][  217/  217]    Overall Loss 0.150002    Objective Loss 0.150002    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.274154    
2024-05-04 04:07:56,401 - 

2024-05-04 04:07:56,401 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:08:24,156 - Epoch: [273][  100/  217]    Overall Loss 0.135520    Objective Loss 0.135520                                        LR 0.000016    Time 0.277460    
2024-05-04 04:08:50,803 - Epoch: [273][  200/  217]    Overall Loss 0.142020    Objective Loss 0.142020                                        LR 0.000016    Time 0.271919    
2024-05-04 04:08:54,968 - Epoch: [273][  217/  217]    Overall Loss 0.140981    Objective Loss 0.140981    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.269799    
2024-05-04 04:08:55,853 - 

2024-05-04 04:08:55,853 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:09:25,663 - Epoch: [274][  100/  217]    Overall Loss 0.130521    Objective Loss 0.130521                                        LR 0.000016    Time 0.298003    
2024-05-04 04:09:47,076 - Epoch: [274][  200/  217]    Overall Loss 0.136297    Objective Loss 0.136297                                        LR 0.000016    Time 0.256018    
2024-05-04 04:09:50,336 - Epoch: [274][  217/  217]    Overall Loss 0.136407    Objective Loss 0.136407    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.250971    
2024-05-04 04:09:51,360 - 

2024-05-04 04:09:51,361 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:10:18,121 - Epoch: [275][  100/  217]    Overall Loss 0.135745    Objective Loss 0.135745                                        LR 0.000016    Time 0.267423    
2024-05-04 04:10:40,244 - Epoch: [275][  200/  217]    Overall Loss 0.143060    Objective Loss 0.143060                                        LR 0.000016    Time 0.244276    
2024-05-04 04:10:43,869 - Epoch: [275][  217/  217]    Overall Loss 0.143076    Objective Loss 0.143076    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.241836    
2024-05-04 04:10:44,685 - 

2024-05-04 04:10:44,686 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:11:14,722 - Epoch: [276][  100/  217]    Overall Loss 0.122734    Objective Loss 0.122734                                        LR 0.000016    Time 0.300267    
2024-05-04 04:11:36,583 - Epoch: [276][  200/  217]    Overall Loss 0.128236    Objective Loss 0.128236                                        LR 0.000016    Time 0.259392    
2024-05-04 04:11:40,638 - Epoch: [276][  217/  217]    Overall Loss 0.129737    Objective Loss 0.129737    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.257747    
2024-05-04 04:11:41,205 - 

2024-05-04 04:11:41,206 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:12:11,080 - Epoch: [277][  100/  217]    Overall Loss 0.125258    Objective Loss 0.125258                                        LR 0.000016    Time 0.298647    
2024-05-04 04:12:31,874 - Epoch: [277][  200/  217]    Overall Loss 0.126942    Objective Loss 0.126942                                        LR 0.000016    Time 0.253249    
2024-05-04 04:12:35,556 - Epoch: [277][  217/  217]    Overall Loss 0.129679    Objective Loss 0.129679    Top1 98.360656    Top5 98.360656    LR 0.000016    Time 0.250366    
2024-05-04 04:12:36,058 - 

2024-05-04 04:12:36,059 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:12:56,307 - Epoch: [278][  100/  217]    Overall Loss 0.131970    Objective Loss 0.131970                                        LR 0.000016    Time 0.202389    
2024-05-04 04:13:13,332 - Epoch: [278][  200/  217]    Overall Loss 0.136370    Objective Loss 0.136370                                        LR 0.000016    Time 0.186275    
2024-05-04 04:13:16,416 - Epoch: [278][  217/  217]    Overall Loss 0.136431    Objective Loss 0.136431    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.185885    
2024-05-04 04:13:16,638 - 

2024-05-04 04:13:16,638 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:13:39,590 - Epoch: [279][  100/  217]    Overall Loss 0.120207    Objective Loss 0.120207                                        LR 0.000016    Time 0.229414    
2024-05-04 04:13:56,522 - Epoch: [279][  200/  217]    Overall Loss 0.123949    Objective Loss 0.123949                                        LR 0.000016    Time 0.199328    
2024-05-04 04:13:59,713 - Epoch: [279][  217/  217]    Overall Loss 0.127506    Objective Loss 0.127506    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.198411    
2024-05-04 04:13:59,883 - 

2024-05-04 04:13:59,884 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:14:19,214 - Epoch: [280][  100/  217]    Overall Loss 0.124227    Objective Loss 0.124227                                        LR 0.000016    Time 0.193221    
2024-05-04 04:14:35,654 - Epoch: [280][  200/  217]    Overall Loss 0.126105    Objective Loss 0.126105                                        LR 0.000016    Time 0.178772    
2024-05-04 04:14:38,799 - Epoch: [280][  217/  217]    Overall Loss 0.127452    Objective Loss 0.127452    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.179253    
2024-05-04 04:14:39,009 - --- validate (epoch=280)-----------
2024-05-04 04:14:39,010 - 1736 samples (32 per mini-batch)
2024-05-04 04:14:50,241 - Epoch: [280][   55/   55]    Loss 2.095825    Top1 58.006912    Top5 73.732719    
2024-05-04 04:14:50,559 - ==> Top1: 58.007    Top5: 73.733    Loss: 2.096

2024-05-04 04:14:50,562 - ==> Best [Top1: 58.237   Top5: 73.906   Sparsity:0.00   Params: 390704 on epoch: 260]
2024-05-04 04:14:50,562 - Saving checkpoint to: logs/2024.05.03-230959/qat_checkpoint.pth.tar
2024-05-04 04:14:50,586 - 

2024-05-04 04:14:50,587 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:15:09,578 - Epoch: [281][  100/  217]    Overall Loss 0.122166    Objective Loss 0.122166                                        LR 0.000016    Time 0.189836    
2024-05-04 04:15:25,909 - Epoch: [281][  200/  217]    Overall Loss 0.127426    Objective Loss 0.127426                                        LR 0.000016    Time 0.176542    
2024-05-04 04:15:28,353 - Epoch: [281][  217/  217]    Overall Loss 0.127690    Objective Loss 0.127690    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.173966    
2024-05-04 04:15:28,521 - 

2024-05-04 04:15:28,522 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:15:47,774 - Epoch: [282][  100/  217]    Overall Loss 0.127442    Objective Loss 0.127442                                        LR 0.000016    Time 0.192441    
2024-05-04 04:16:02,243 - Epoch: [282][  200/  217]    Overall Loss 0.134136    Objective Loss 0.134136                                        LR 0.000016    Time 0.168536    
2024-05-04 04:16:05,093 - Epoch: [282][  217/  217]    Overall Loss 0.133404    Objective Loss 0.133404    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.168458    
2024-05-04 04:16:05,269 - 

2024-05-04 04:16:05,269 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:16:25,344 - Epoch: [283][  100/  217]    Overall Loss 0.114735    Objective Loss 0.114735                                        LR 0.000016    Time 0.200664    
2024-05-04 04:16:41,286 - Epoch: [283][  200/  217]    Overall Loss 0.117410    Objective Loss 0.117410                                        LR 0.000016    Time 0.180008    
2024-05-04 04:16:44,767 - Epoch: [283][  217/  217]    Overall Loss 0.118205    Objective Loss 0.118205    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.181940    
2024-05-04 04:16:44,983 - 

2024-05-04 04:16:44,984 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:17:02,954 - Epoch: [284][  100/  217]    Overall Loss 0.110850    Objective Loss 0.110850                                        LR 0.000016    Time 0.179625    
2024-05-04 04:17:18,752 - Epoch: [284][  200/  217]    Overall Loss 0.112887    Objective Loss 0.112887                                        LR 0.000016    Time 0.168774    
2024-05-04 04:17:21,386 - Epoch: [284][  217/  217]    Overall Loss 0.114067    Objective Loss 0.114067    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.167681    
2024-05-04 04:17:21,553 - 

2024-05-04 04:17:21,553 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:17:40,015 - Epoch: [285][  100/  217]    Overall Loss 0.116691    Objective Loss 0.116691                                        LR 0.000016    Time 0.184529    
2024-05-04 04:17:54,764 - Epoch: [285][  200/  217]    Overall Loss 0.120676    Objective Loss 0.120676                                        LR 0.000016    Time 0.165977    
2024-05-04 04:17:57,323 - Epoch: [285][  217/  217]    Overall Loss 0.121163    Objective Loss 0.121163    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.164760    
2024-05-04 04:17:57,522 - 

2024-05-04 04:17:57,522 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:18:15,913 - Epoch: [286][  100/  217]    Overall Loss 0.109278    Objective Loss 0.109278                                        LR 0.000016    Time 0.183819    
2024-05-04 04:18:31,117 - Epoch: [286][  200/  217]    Overall Loss 0.114167    Objective Loss 0.114167                                        LR 0.000016    Time 0.167897    
2024-05-04 04:18:33,624 - Epoch: [286][  217/  217]    Overall Loss 0.114714    Objective Loss 0.114714    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.166294    
2024-05-04 04:18:33,764 - 

2024-05-04 04:18:33,764 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:18:52,255 - Epoch: [287][  100/  217]    Overall Loss 0.102838    Objective Loss 0.102838                                        LR 0.000016    Time 0.184844    
2024-05-04 04:19:08,858 - Epoch: [287][  200/  217]    Overall Loss 0.108043    Objective Loss 0.108043                                        LR 0.000016    Time 0.175399    
2024-05-04 04:19:12,005 - Epoch: [287][  217/  217]    Overall Loss 0.110070    Objective Loss 0.110070    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.176156    
2024-05-04 04:19:12,197 - 

2024-05-04 04:19:12,197 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:19:31,658 - Epoch: [288][  100/  217]    Overall Loss 0.119974    Objective Loss 0.119974                                        LR 0.000016    Time 0.194531    
2024-05-04 04:19:47,137 - Epoch: [288][  200/  217]    Overall Loss 0.118525    Objective Loss 0.118525                                        LR 0.000016    Time 0.174633    
2024-05-04 04:19:49,593 - Epoch: [288][  217/  217]    Overall Loss 0.118667    Objective Loss 0.118667    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.172263    
2024-05-04 04:19:49,754 - 

2024-05-04 04:19:49,755 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:20:11,252 - Epoch: [289][  100/  217]    Overall Loss 0.108032    Objective Loss 0.108032                                        LR 0.000016    Time 0.214888    
2024-05-04 04:20:26,048 - Epoch: [289][  200/  217]    Overall Loss 0.112806    Objective Loss 0.112806                                        LR 0.000016    Time 0.181396    
2024-05-04 04:20:28,457 - Epoch: [289][  217/  217]    Overall Loss 0.114594    Objective Loss 0.114594    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.178278    
2024-05-04 04:20:28,611 - 

2024-05-04 04:20:28,612 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:20:48,412 - Epoch: [290][  100/  217]    Overall Loss 0.107375    Objective Loss 0.107375                                        LR 0.000016    Time 0.197935    
2024-05-04 04:21:03,844 - Epoch: [290][  200/  217]    Overall Loss 0.110137    Objective Loss 0.110137                                        LR 0.000016    Time 0.176094    
2024-05-04 04:21:06,787 - Epoch: [290][  217/  217]    Overall Loss 0.109923    Objective Loss 0.109923    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.175851    
2024-05-04 04:21:06,960 - --- validate (epoch=290)-----------
2024-05-04 04:21:06,961 - 1736 samples (32 per mini-batch)
2024-05-04 04:21:17,151 - Epoch: [290][   55/   55]    Loss 2.024730    Top1 57.200461    Top5 74.539171    
2024-05-04 04:21:17,424 - ==> Top1: 57.200    Top5: 74.539    Loss: 2.025

2024-05-04 04:21:17,431 - ==> Best [Top1: 58.237   Top5: 73.906   Sparsity:0.00   Params: 390704 on epoch: 260]
2024-05-04 04:21:17,431 - Saving checkpoint to: logs/2024.05.03-230959/qat_checkpoint.pth.tar
2024-05-04 04:21:17,466 - 

2024-05-04 04:21:17,466 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:21:35,907 - Epoch: [291][  100/  217]    Overall Loss 0.099201    Objective Loss 0.099201                                        LR 0.000016    Time 0.184340    
2024-05-04 04:21:51,015 - Epoch: [291][  200/  217]    Overall Loss 0.108991    Objective Loss 0.108991                                        LR 0.000016    Time 0.167672    
2024-05-04 04:21:54,150 - Epoch: [291][  217/  217]    Overall Loss 0.111336    Objective Loss 0.111336    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.168973    
2024-05-04 04:21:54,325 - 

2024-05-04 04:21:54,326 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:22:12,853 - Epoch: [292][  100/  217]    Overall Loss 0.110005    Objective Loss 0.110005                                        LR 0.000016    Time 0.185188    
2024-05-04 04:22:28,502 - Epoch: [292][  200/  217]    Overall Loss 0.115225    Objective Loss 0.115225                                        LR 0.000016    Time 0.170801    
2024-05-04 04:22:30,904 - Epoch: [292][  217/  217]    Overall Loss 0.114768    Objective Loss 0.114768    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.168484    
2024-05-04 04:22:31,083 - 

2024-05-04 04:22:31,083 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:22:49,189 - Epoch: [293][  100/  217]    Overall Loss 0.091168    Objective Loss 0.091168                                        LR 0.000016    Time 0.180965    
2024-05-04 04:23:05,577 - Epoch: [293][  200/  217]    Overall Loss 0.102321    Objective Loss 0.102321                                        LR 0.000016    Time 0.172390    
2024-05-04 04:23:08,574 - Epoch: [293][  217/  217]    Overall Loss 0.104158    Objective Loss 0.104158    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.172689    
2024-05-04 04:23:08,722 - 

2024-05-04 04:23:08,722 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:23:27,610 - Epoch: [294][  100/  217]    Overall Loss 0.108105    Objective Loss 0.108105                                        LR 0.000016    Time 0.188793    
2024-05-04 04:23:43,932 - Epoch: [294][  200/  217]    Overall Loss 0.107831    Objective Loss 0.107831                                        LR 0.000016    Time 0.175961    
2024-05-04 04:23:46,476 - Epoch: [294][  217/  217]    Overall Loss 0.107262    Objective Loss 0.107262    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.173895    
2024-05-04 04:23:46,666 - 

2024-05-04 04:23:46,666 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:24:05,536 - Epoch: [295][  100/  217]    Overall Loss 0.098762    Objective Loss 0.098762                                        LR 0.000016    Time 0.188621    
2024-05-04 04:24:22,629 - Epoch: [295][  200/  217]    Overall Loss 0.099832    Objective Loss 0.099832                                        LR 0.000016    Time 0.179740    
2024-05-04 04:24:25,179 - Epoch: [295][  217/  217]    Overall Loss 0.099748    Objective Loss 0.099748    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.177405    
2024-05-04 04:24:25,375 - 

2024-05-04 04:24:25,375 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:24:44,521 - Epoch: [296][  100/  217]    Overall Loss 0.095681    Objective Loss 0.095681                                        LR 0.000016    Time 0.191355    
2024-05-04 04:25:00,969 - Epoch: [296][  200/  217]    Overall Loss 0.097697    Objective Loss 0.097697                                        LR 0.000016    Time 0.177884    
2024-05-04 04:25:03,397 - Epoch: [296][  217/  217]    Overall Loss 0.099063    Objective Loss 0.099063    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.175131    
2024-05-04 04:25:03,791 - 

2024-05-04 04:25:03,792 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:25:23,207 - Epoch: [297][  100/  217]    Overall Loss 0.093056    Objective Loss 0.093056                                        LR 0.000016    Time 0.194053    
2024-05-04 04:25:39,746 - Epoch: [297][  200/  217]    Overall Loss 0.104200    Objective Loss 0.104200                                        LR 0.000016    Time 0.179681    
2024-05-04 04:25:42,325 - Epoch: [297][  217/  217]    Overall Loss 0.105428    Objective Loss 0.105428    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.177481    
2024-05-04 04:25:42,642 - 

2024-05-04 04:25:42,643 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:26:02,248 - Epoch: [298][  100/  217]    Overall Loss 0.085917    Objective Loss 0.085917                                        LR 0.000016    Time 0.195968    
2024-05-04 04:26:19,667 - Epoch: [298][  200/  217]    Overall Loss 0.092366    Objective Loss 0.092366                                        LR 0.000016    Time 0.185046    
2024-05-04 04:26:22,259 - Epoch: [298][  217/  217]    Overall Loss 0.096901    Objective Loss 0.096901    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.182487    
2024-05-04 04:26:22,542 - 

2024-05-04 04:26:22,543 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:26:42,167 - Epoch: [299][  100/  217]    Overall Loss 0.105627    Objective Loss 0.105627                                        LR 0.000016    Time 0.196145    
2024-05-04 04:26:58,685 - Epoch: [299][  200/  217]    Overall Loss 0.107086    Objective Loss 0.107086                                        LR 0.000016    Time 0.180625    
2024-05-04 04:27:01,135 - Epoch: [299][  217/  217]    Overall Loss 0.108024    Objective Loss 0.108024    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.177762    
2024-05-04 04:27:01,385 - --- test ---------------------
2024-05-04 04:27:01,385 - 1736 samples (32 per mini-batch)
2024-05-04 04:27:13,451 - Test: [   55/   55]    Loss 2.073235    Top1 58.122120    Top5 73.905530    
2024-05-04 04:27:13,699 - ==> Top1: 58.122    Top5: 73.906    Loss: 2.073

2024-05-04 04:27:13,703 - 
2024-05-04 04:27:13,704 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.03-230959/2024.05.03-230959.log
