2024-05-03 23:09:45,714 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.03-230945/2024.05.03-230945.log
2024-05-03 23:09:50,063 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2024-05-03 23:09:50,063 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2024-05-03 23:09:50,211 - Dataset sizes:
	training=6941
	validation=1736
	test=1736
2024-05-03 23:09:50,211 - Reading compression schedule from: policies/schedule-cifar100.yaml
2024-05-03 23:09:50,216 - 

2024-05-03 23:09:50,216 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:10:11,514 - Epoch: [0][  100/  217]    Overall Loss 4.048652    Objective Loss 4.048652                                        LR 0.001000    Time 0.212903    
2024-05-03 23:10:40,442 - Epoch: [0][  200/  217]    Overall Loss 3.774696    Objective Loss 3.774696                                        LR 0.001000    Time 0.251037    
2024-05-03 23:10:44,009 - Epoch: [0][  217/  217]    Overall Loss 3.746123    Objective Loss 3.746123    Top1 21.311475    Top5 29.508197    LR 0.001000    Time 0.247801    
2024-05-03 23:10:44,598 - --- validate (epoch=0)-----------
2024-05-03 23:10:44,598 - 1736 samples (32 per mini-batch)
2024-05-03 23:11:01,015 - Epoch: [0][   55/   55]    Loss 3.430026    Top1 27.880184    Top5 35.368664    
2024-05-03 23:11:01,419 - ==> Top1: 27.880    Top5: 35.369    Loss: 3.430

2024-05-03 23:11:01,422 - ==> Best [Top1: 27.880   Top5: 35.369   Sparsity:0.00   Params: 381920 on epoch: 0]
2024-05-03 23:11:01,422 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-03 23:11:01,452 - 

2024-05-03 23:11:01,452 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:11:27,999 - Epoch: [1][  100/  217]    Overall Loss 3.317607    Objective Loss 3.317607                                        LR 0.001000    Time 0.265164    
2024-05-03 23:11:53,440 - Epoch: [1][  200/  217]    Overall Loss 3.236201    Objective Loss 3.236201                                        LR 0.001000    Time 0.259739    
2024-05-03 23:11:57,145 - Epoch: [1][  217/  217]    Overall Loss 3.218920    Objective Loss 3.218920    Top1 22.950820    Top5 34.426230    LR 0.001000    Time 0.256458    
2024-05-03 23:11:57,417 - 

2024-05-03 23:11:57,417 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:12:23,409 - Epoch: [2][  100/  217]    Overall Loss 3.001430    Objective Loss 3.001430                                        LR 0.001000    Time 0.259805    
2024-05-03 23:12:49,092 - Epoch: [2][  200/  217]    Overall Loss 2.924809    Objective Loss 2.924809                                        LR 0.001000    Time 0.258263    
2024-05-03 23:12:52,989 - Epoch: [2][  217/  217]    Overall Loss 2.932435    Objective Loss 2.932435    Top1 44.262295    Top5 55.737705    LR 0.001000    Time 0.255984    
2024-05-03 23:12:53,267 - 

2024-05-03 23:12:53,268 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:13:24,050 - Epoch: [3][  100/  217]    Overall Loss 2.728773    Objective Loss 2.728773                                        LR 0.001000    Time 0.307721    
2024-05-03 23:13:49,044 - Epoch: [3][  200/  217]    Overall Loss 2.663206    Objective Loss 2.663206                                        LR 0.001000    Time 0.278773    
2024-05-03 23:13:53,526 - Epoch: [3][  217/  217]    Overall Loss 2.649588    Objective Loss 2.649588    Top1 39.344262    Top5 63.934426    LR 0.001000    Time 0.277571    
2024-05-03 23:13:53,991 - 

2024-05-03 23:13:53,991 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:14:23,722 - Epoch: [4][  100/  217]    Overall Loss 2.381505    Objective Loss 2.381505                                        LR 0.001000    Time 0.297211    
2024-05-03 23:14:50,428 - Epoch: [4][  200/  217]    Overall Loss 2.366154    Objective Loss 2.366154                                        LR 0.001000    Time 0.282076    
2024-05-03 23:14:54,816 - Epoch: [4][  217/  217]    Overall Loss 2.367917    Objective Loss 2.367917    Top1 47.540984    Top5 67.213115    LR 0.001000    Time 0.280194    
2024-05-03 23:14:55,620 - 

2024-05-03 23:14:55,622 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:15:23,641 - Epoch: [5][  100/  217]    Overall Loss 2.055742    Objective Loss 2.055742                                        LR 0.001000    Time 0.280082    
2024-05-03 23:15:50,190 - Epoch: [5][  200/  217]    Overall Loss 2.099858    Objective Loss 2.099858                                        LR 0.001000    Time 0.272724    
2024-05-03 23:15:54,297 - Epoch: [5][  217/  217]    Overall Loss 2.087682    Objective Loss 2.087682    Top1 50.819672    Top5 70.491803    LR 0.001000    Time 0.270277    
2024-05-03 23:15:54,646 - 

2024-05-03 23:15:54,646 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:16:20,468 - Epoch: [6][  100/  217]    Overall Loss 1.820227    Objective Loss 1.820227                                        LR 0.001000    Time 0.258124    
2024-05-03 23:16:46,775 - Epoch: [6][  200/  217]    Overall Loss 1.854185    Objective Loss 1.854185                                        LR 0.001000    Time 0.260550    
2024-05-03 23:16:50,483 - Epoch: [6][  217/  217]    Overall Loss 1.859163    Objective Loss 1.859163    Top1 63.934426    Top5 77.049180    LR 0.001000    Time 0.257214    
2024-05-03 23:16:50,822 - 

2024-05-03 23:16:50,822 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:17:23,172 - Epoch: [7][  100/  217]    Overall Loss 1.658518    Objective Loss 1.658518                                        LR 0.001000    Time 0.323392    
2024-05-03 23:17:43,922 - Epoch: [7][  200/  217]    Overall Loss 1.664210    Objective Loss 1.664210                                        LR 0.001000    Time 0.265403    
2024-05-03 23:17:49,275 - Epoch: [7][  217/  217]    Overall Loss 1.660497    Objective Loss 1.660497    Top1 73.770492    Top5 86.885246    LR 0.001000    Time 0.269262    
2024-05-03 23:17:49,917 - 

2024-05-03 23:17:49,918 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:18:19,451 - Epoch: [8][  100/  217]    Overall Loss 1.392090    Objective Loss 1.392090                                        LR 0.001000    Time 0.295226    
2024-05-03 23:18:42,722 - Epoch: [8][  200/  217]    Overall Loss 1.426478    Objective Loss 1.426478                                        LR 0.001000    Time 0.263918    
2024-05-03 23:18:46,943 - Epoch: [8][  217/  217]    Overall Loss 1.432784    Objective Loss 1.432784    Top1 67.213115    Top5 88.524590    LR 0.001000    Time 0.262685    
2024-05-03 23:18:47,353 - 

2024-05-03 23:18:47,353 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:19:14,406 - Epoch: [9][  100/  217]    Overall Loss 1.219404    Objective Loss 1.219404                                        LR 0.001000    Time 0.270425    
2024-05-03 23:19:38,648 - Epoch: [9][  200/  217]    Overall Loss 1.245147    Objective Loss 1.245147                                        LR 0.001000    Time 0.256377    
2024-05-03 23:19:42,326 - Epoch: [9][  217/  217]    Overall Loss 1.245460    Objective Loss 1.245460    Top1 72.131148    Top5 93.442623    LR 0.001000    Time 0.253231    
2024-05-03 23:19:42,748 - 

2024-05-03 23:19:42,748 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:20:14,099 - Epoch: [10][  100/  217]    Overall Loss 1.033116    Objective Loss 1.033116                                        LR 0.001000    Time 0.313411    
2024-05-03 23:20:38,113 - Epoch: [10][  200/  217]    Overall Loss 1.039440    Objective Loss 1.039440                                        LR 0.001000    Time 0.276728    
2024-05-03 23:20:43,099 - Epoch: [10][  217/  217]    Overall Loss 1.049249    Objective Loss 1.049249    Top1 70.491803    Top5 90.163934    LR 0.001000    Time 0.278018    
2024-05-03 23:20:43,464 - --- validate (epoch=10)-----------
2024-05-03 23:20:43,465 - 1736 samples (32 per mini-batch)
2024-05-03 23:20:59,580 - Epoch: [10][   55/   55]    Loss 2.377863    Top1 47.062212    Top5 64.688940    
2024-05-03 23:20:59,862 - ==> Top1: 47.062    Top5: 64.689    Loss: 2.378

2024-05-03 23:20:59,866 - ==> Best [Top1: 47.062   Top5: 64.689   Sparsity:0.00   Params: 381920 on epoch: 10]
2024-05-03 23:20:59,866 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-03 23:20:59,913 - 

2024-05-03 23:20:59,913 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:21:29,201 - Epoch: [11][  100/  217]    Overall Loss 0.832473    Objective Loss 0.832473                                        LR 0.001000    Time 0.292779    
2024-05-03 23:21:49,007 - Epoch: [11][  200/  217]    Overall Loss 0.873994    Objective Loss 0.873994                                        LR 0.001000    Time 0.245372    
2024-05-03 23:21:53,112 - Epoch: [11][  217/  217]    Overall Loss 0.880858    Objective Loss 0.880858    Top1 65.573770    Top5 85.245902    LR 0.001000    Time 0.245058    
2024-05-03 23:21:53,626 - 

2024-05-03 23:21:53,627 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:22:28,021 - Epoch: [12][  100/  217]    Overall Loss 0.693910    Objective Loss 0.693910                                        LR 0.001000    Time 0.343836    
2024-05-03 23:22:56,916 - Epoch: [12][  200/  217]    Overall Loss 0.722972    Objective Loss 0.722972                                        LR 0.001000    Time 0.316344    
2024-05-03 23:22:59,703 - Epoch: [12][  217/  217]    Overall Loss 0.726434    Objective Loss 0.726434    Top1 80.327869    Top5 93.442623    LR 0.001000    Time 0.304396    
2024-05-03 23:23:00,658 - 

2024-05-03 23:23:00,658 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:23:30,996 - Epoch: [13][  100/  217]    Overall Loss 0.543964    Objective Loss 0.543964                                        LR 0.001000    Time 0.303281    
2024-05-03 23:23:54,998 - Epoch: [13][  200/  217]    Overall Loss 0.558306    Objective Loss 0.558306                                        LR 0.001000    Time 0.271607    
2024-05-03 23:23:58,462 - Epoch: [13][  217/  217]    Overall Loss 0.559528    Objective Loss 0.559528    Top1 83.606557    Top5 96.721311    LR 0.001000    Time 0.266282    
2024-05-03 23:23:58,864 - 

2024-05-03 23:23:58,865 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:24:28,811 - Epoch: [14][  100/  217]    Overall Loss 0.400612    Objective Loss 0.400612                                        LR 0.001000    Time 0.299372    
2024-05-03 23:24:53,673 - Epoch: [14][  200/  217]    Overall Loss 0.422131    Objective Loss 0.422131                                        LR 0.001000    Time 0.273948    
2024-05-03 23:24:57,372 - Epoch: [14][  217/  217]    Overall Loss 0.433420    Objective Loss 0.433420    Top1 88.524590    Top5 96.721311    LR 0.001000    Time 0.269524    
2024-05-03 23:24:57,655 - 

2024-05-03 23:24:57,655 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:25:25,809 - Epoch: [15][  100/  217]    Overall Loss 0.319920    Objective Loss 0.319920                                        LR 0.001000    Time 0.281425    
2024-05-03 23:25:51,812 - Epoch: [15][  200/  217]    Overall Loss 0.333322    Objective Loss 0.333322                                        LR 0.001000    Time 0.270682    
2024-05-03 23:25:56,170 - Epoch: [15][  217/  217]    Overall Loss 0.335487    Objective Loss 0.335487    Top1 93.442623    Top5 100.000000    LR 0.001000    Time 0.269547    
2024-05-03 23:25:56,668 - 

2024-05-03 23:25:56,669 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:26:25,412 - Epoch: [16][  100/  217]    Overall Loss 0.242392    Objective Loss 0.242392                                        LR 0.001000    Time 0.287316    
2024-05-03 23:26:49,901 - Epoch: [16][  200/  217]    Overall Loss 0.254218    Objective Loss 0.254218                                        LR 0.001000    Time 0.266053    
2024-05-03 23:26:54,482 - Epoch: [16][  217/  217]    Overall Loss 0.259036    Objective Loss 0.259036    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.266311    
2024-05-03 23:26:55,271 - 

2024-05-03 23:26:55,272 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:27:27,494 - Epoch: [17][  100/  217]    Overall Loss 0.176065    Objective Loss 0.176065                                        LR 0.001000    Time 0.322118    
2024-05-03 23:27:52,858 - Epoch: [17][  200/  217]    Overall Loss 0.185093    Objective Loss 0.185093                                        LR 0.001000    Time 0.287840    
2024-05-03 23:27:56,790 - Epoch: [17][  217/  217]    Overall Loss 0.189947    Objective Loss 0.189947    Top1 95.081967    Top5 98.360656    LR 0.001000    Time 0.283403    
2024-05-03 23:27:57,484 - 

2024-05-03 23:27:57,485 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:28:27,083 - Epoch: [18][  100/  217]    Overall Loss 0.145936    Objective Loss 0.145936                                        LR 0.001000    Time 0.295868    
2024-05-03 23:28:53,345 - Epoch: [18][  200/  217]    Overall Loss 0.156293    Objective Loss 0.156293                                        LR 0.001000    Time 0.279194    
2024-05-03 23:28:58,759 - Epoch: [18][  217/  217]    Overall Loss 0.154536    Objective Loss 0.154536    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.282261    
2024-05-03 23:28:59,066 - 

2024-05-03 23:28:59,067 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:29:29,497 - Epoch: [19][  100/  217]    Overall Loss 0.087322    Objective Loss 0.087322                                        LR 0.001000    Time 0.304182    
2024-05-03 23:30:00,277 - Epoch: [19][  200/  217]    Overall Loss 0.096906    Objective Loss 0.096906                                        LR 0.001000    Time 0.305940    
2024-05-03 23:30:03,332 - Epoch: [19][  217/  217]    Overall Loss 0.098388    Objective Loss 0.098388    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.296042    
2024-05-03 23:30:03,726 - 

2024-05-03 23:30:03,727 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:30:32,934 - Epoch: [20][  100/  217]    Overall Loss 0.068258    Objective Loss 0.068258                                        LR 0.001000    Time 0.291962    
2024-05-03 23:30:58,670 - Epoch: [20][  200/  217]    Overall Loss 0.068382    Objective Loss 0.068382                                        LR 0.001000    Time 0.274612    
2024-05-03 23:31:03,025 - Epoch: [20][  217/  217]    Overall Loss 0.068571    Objective Loss 0.068571    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.273160    
2024-05-03 23:31:03,293 - --- validate (epoch=20)-----------
2024-05-03 23:31:03,294 - 1736 samples (32 per mini-batch)
2024-05-03 23:31:18,318 - Epoch: [20][   55/   55]    Loss 2.380200    Top1 53.168203    Top5 70.449309    
2024-05-03 23:31:18,769 - ==> Top1: 53.168    Top5: 70.449    Loss: 2.380

2024-05-03 23:31:18,775 - ==> Best [Top1: 53.168   Top5: 70.449   Sparsity:0.00   Params: 381920 on epoch: 20]
2024-05-03 23:31:18,775 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-03 23:31:18,821 - 

2024-05-03 23:31:18,822 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:31:46,125 - Epoch: [21][  100/  217]    Overall Loss 0.058429    Objective Loss 0.058429                                        LR 0.001000    Time 0.272919    
2024-05-03 23:32:16,185 - Epoch: [21][  200/  217]    Overall Loss 0.055983    Objective Loss 0.055983                                        LR 0.001000    Time 0.286713    
2024-05-03 23:32:18,699 - Epoch: [21][  217/  217]    Overall Loss 0.055799    Objective Loss 0.055799    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.275831    
2024-05-03 23:32:19,032 - 

2024-05-03 23:32:19,032 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:32:49,354 - Epoch: [22][  100/  217]    Overall Loss 0.046719    Objective Loss 0.046719                                        LR 0.001000    Time 0.303112    
2024-05-03 23:33:14,138 - Epoch: [22][  200/  217]    Overall Loss 0.049940    Objective Loss 0.049940                                        LR 0.001000    Time 0.275407    
2024-05-03 23:33:17,780 - Epoch: [22][  217/  217]    Overall Loss 0.051747    Objective Loss 0.051747    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.270600    
2024-05-03 23:33:18,138 - 

2024-05-03 23:33:18,139 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:33:49,179 - Epoch: [23][  100/  217]    Overall Loss 0.050024    Objective Loss 0.050024                                        LR 0.001000    Time 0.310288    
2024-05-03 23:34:14,826 - Epoch: [23][  200/  217]    Overall Loss 0.072601    Objective Loss 0.072601                                        LR 0.001000    Time 0.283334    
2024-05-03 23:34:19,032 - Epoch: [23][  217/  217]    Overall Loss 0.082728    Objective Loss 0.082728    Top1 91.803279    Top5 100.000000    LR 0.001000    Time 0.280510    
2024-05-03 23:34:19,411 - 

2024-05-03 23:34:19,412 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:34:51,608 - Epoch: [24][  100/  217]    Overall Loss 0.390488    Objective Loss 0.390488                                        LR 0.001000    Time 0.321865    
2024-05-03 23:35:13,659 - Epoch: [24][  200/  217]    Overall Loss 0.561969    Objective Loss 0.561969                                        LR 0.001000    Time 0.271137    
2024-05-03 23:35:18,269 - Epoch: [24][  217/  217]    Overall Loss 0.574773    Objective Loss 0.574773    Top1 78.688525    Top5 96.721311    LR 0.001000    Time 0.271131    
2024-05-03 23:35:18,982 - 

2024-05-03 23:35:18,984 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:35:53,457 - Epoch: [25][  100/  217]    Overall Loss 0.358570    Objective Loss 0.358570                                        LR 0.001000    Time 0.344629    
2024-05-03 23:36:17,121 - Epoch: [25][  200/  217]    Overall Loss 0.332045    Objective Loss 0.332045                                        LR 0.001000    Time 0.290588    
2024-05-03 23:36:21,874 - Epoch: [25][  217/  217]    Overall Loss 0.328764    Objective Loss 0.328764    Top1 95.081967    Top5 98.360656    LR 0.001000    Time 0.289717    
2024-05-03 23:36:22,655 - 

2024-05-03 23:36:22,655 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:36:53,195 - Epoch: [26][  100/  217]    Overall Loss 0.113704    Objective Loss 0.113704                                        LR 0.001000    Time 0.305283    
2024-05-03 23:37:18,067 - Epoch: [26][  200/  217]    Overall Loss 0.107590    Objective Loss 0.107590                                        LR 0.001000    Time 0.276950    
2024-05-03 23:37:21,640 - Epoch: [26][  217/  217]    Overall Loss 0.108008    Objective Loss 0.108008    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.271708    
2024-05-03 23:37:22,147 - 

2024-05-03 23:37:22,149 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:37:50,087 - Epoch: [27][  100/  217]    Overall Loss 0.053439    Objective Loss 0.053439                                        LR 0.001000    Time 0.279277    
2024-05-03 23:38:14,829 - Epoch: [27][  200/  217]    Overall Loss 0.053813    Objective Loss 0.053813                                        LR 0.001000    Time 0.263304    
2024-05-03 23:38:20,130 - Epoch: [27][  217/  217]    Overall Loss 0.053630    Objective Loss 0.053630    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.267094    
2024-05-03 23:38:20,420 - 

2024-05-03 23:38:20,420 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:38:52,074 - Epoch: [28][  100/  217]    Overall Loss 0.030280    Objective Loss 0.030280                                        LR 0.001000    Time 0.316438    
2024-05-03 23:39:15,942 - Epoch: [28][  200/  217]    Overall Loss 0.030818    Objective Loss 0.030818                                        LR 0.001000    Time 0.277517    
2024-05-03 23:39:18,617 - Epoch: [28][  217/  217]    Overall Loss 0.030613    Objective Loss 0.030613    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.268094    
2024-05-03 23:39:18,872 - 

2024-05-03 23:39:18,872 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:39:48,879 - Epoch: [29][  100/  217]    Overall Loss 0.018057    Objective Loss 0.018057                                        LR 0.001000    Time 0.299990    
2024-05-03 23:40:13,232 - Epoch: [29][  200/  217]    Overall Loss 0.019253    Objective Loss 0.019253                                        LR 0.001000    Time 0.271713    
2024-05-03 23:40:17,743 - Epoch: [29][  217/  217]    Overall Loss 0.020000    Objective Loss 0.020000    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.271204    
2024-05-03 23:40:18,157 - 

2024-05-03 23:40:18,157 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:40:48,492 - Epoch: [30][  100/  217]    Overall Loss 0.019511    Objective Loss 0.019511                                        LR 0.001000    Time 0.303249    
2024-05-03 23:41:13,197 - Epoch: [30][  200/  217]    Overall Loss 0.017965    Objective Loss 0.017965                                        LR 0.001000    Time 0.275101    
2024-05-03 23:41:18,705 - Epoch: [30][  217/  217]    Overall Loss 0.017553    Objective Loss 0.017553    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.278923    
2024-05-03 23:41:19,156 - --- validate (epoch=30)-----------
2024-05-03 23:41:19,157 - 1736 samples (32 per mini-batch)
2024-05-03 23:41:39,310 - Epoch: [30][   55/   55]    Loss 2.482995    Top1 55.587558    Top5 71.428571    
2024-05-03 23:41:40,186 - ==> Top1: 55.588    Top5: 71.429    Loss: 2.483

2024-05-03 23:41:40,192 - ==> Best [Top1: 55.588   Top5: 71.429   Sparsity:0.00   Params: 381920 on epoch: 30]
2024-05-03 23:41:40,193 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-03 23:41:40,248 - 

2024-05-03 23:41:40,249 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:42:11,364 - Epoch: [31][  100/  217]    Overall Loss 0.011711    Objective Loss 0.011711                                        LR 0.001000    Time 0.311054    
2024-05-03 23:42:39,104 - Epoch: [31][  200/  217]    Overall Loss 0.012709    Objective Loss 0.012709                                        LR 0.001000    Time 0.294186    
2024-05-03 23:42:43,848 - Epoch: [31][  217/  217]    Overall Loss 0.012873    Objective Loss 0.012873    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.292993    
2024-05-03 23:42:44,222 - 

2024-05-03 23:42:44,222 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:43:11,201 - Epoch: [32][  100/  217]    Overall Loss 0.011813    Objective Loss 0.011813                                        LR 0.001000    Time 0.269692    
2024-05-03 23:43:36,776 - Epoch: [32][  200/  217]    Overall Loss 0.010821    Objective Loss 0.010821                                        LR 0.001000    Time 0.262673    
2024-05-03 23:43:40,510 - Epoch: [32][  217/  217]    Overall Loss 0.011142    Objective Loss 0.011142    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.259296    
2024-05-03 23:43:40,749 - 

2024-05-03 23:43:40,750 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:44:12,327 - Epoch: [33][  100/  217]    Overall Loss 0.011419    Objective Loss 0.011419                                        LR 0.001000    Time 0.315682    
2024-05-03 23:44:41,796 - Epoch: [33][  200/  217]    Overall Loss 0.013415    Objective Loss 0.013415                                        LR 0.001000    Time 0.305141    
2024-05-03 23:44:46,402 - Epoch: [33][  217/  217]    Overall Loss 0.013292    Objective Loss 0.013292    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.302450    
2024-05-03 23:44:47,025 - 

2024-05-03 23:44:47,026 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:45:19,560 - Epoch: [34][  100/  217]    Overall Loss 0.015051    Objective Loss 0.015051                                        LR 0.001000    Time 0.325250    
2024-05-03 23:45:46,279 - Epoch: [34][  200/  217]    Overall Loss 0.014031    Objective Loss 0.014031                                        LR 0.001000    Time 0.296177    
2024-05-03 23:45:49,700 - Epoch: [34][  217/  217]    Overall Loss 0.013591    Objective Loss 0.013591    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.288724    
2024-05-03 23:45:50,549 - 

2024-05-03 23:45:50,549 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:46:21,231 - Epoch: [35][  100/  217]    Overall Loss 0.009094    Objective Loss 0.009094                                        LR 0.001000    Time 0.306720    
2024-05-03 23:46:50,515 - Epoch: [35][  200/  217]    Overall Loss 0.008434    Objective Loss 0.008434                                        LR 0.001000    Time 0.299739    
2024-05-03 23:46:54,556 - Epoch: [35][  217/  217]    Overall Loss 0.008284    Objective Loss 0.008284    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.294872    
2024-05-03 23:46:55,368 - 

2024-05-03 23:46:55,369 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:47:27,463 - Epoch: [36][  100/  217]    Overall Loss 0.008415    Objective Loss 0.008415                                        LR 0.001000    Time 0.320833    
2024-05-03 23:47:53,767 - Epoch: [36][  200/  217]    Overall Loss 0.008758    Objective Loss 0.008758                                        LR 0.001000    Time 0.291892    
2024-05-03 23:47:58,605 - Epoch: [36][  217/  217]    Overall Loss 0.009005    Objective Loss 0.009005    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.291309    
2024-05-03 23:47:58,896 - 

2024-05-03 23:47:58,897 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:48:28,299 - Epoch: [37][  100/  217]    Overall Loss 0.019741    Objective Loss 0.019741                                        LR 0.001000    Time 0.293927    
2024-05-03 23:48:58,034 - Epoch: [37][  200/  217]    Overall Loss 0.388307    Objective Loss 0.388307                                        LR 0.001000    Time 0.295587    
2024-05-03 23:49:03,219 - Epoch: [37][  217/  217]    Overall Loss 0.472641    Objective Loss 0.472641    Top1 72.131148    Top5 91.803279    LR 0.001000    Time 0.296312    
2024-05-03 23:49:03,652 - 

2024-05-03 23:49:03,653 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:49:35,258 - Epoch: [38][  100/  217]    Overall Loss 0.917695    Objective Loss 0.917695                                        LR 0.001000    Time 0.315964    
2024-05-03 23:49:59,820 - Epoch: [38][  200/  217]    Overall Loss 0.781471    Objective Loss 0.781471                                        LR 0.001000    Time 0.280740    
2024-05-03 23:50:05,008 - Epoch: [38][  217/  217]    Overall Loss 0.766163    Objective Loss 0.766163    Top1 80.327869    Top5 98.360656    LR 0.001000    Time 0.282643    
2024-05-03 23:50:05,383 - 

2024-05-03 23:50:05,384 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:50:36,197 - Epoch: [39][  100/  217]    Overall Loss 0.234170    Objective Loss 0.234170                                        LR 0.001000    Time 0.308022    
2024-05-03 23:51:01,784 - Epoch: [39][  200/  217]    Overall Loss 0.210195    Objective Loss 0.210195                                        LR 0.001000    Time 0.281891    
2024-05-03 23:51:04,566 - Epoch: [39][  217/  217]    Overall Loss 0.207094    Objective Loss 0.207094    Top1 95.081967    Top5 100.000000    LR 0.001000    Time 0.272623    
2024-05-03 23:51:05,048 - 

2024-05-03 23:51:05,049 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:51:38,414 - Epoch: [40][  100/  217]    Overall Loss 0.081365    Objective Loss 0.081365                                        LR 0.001000    Time 0.333515    
2024-05-03 23:52:03,306 - Epoch: [40][  200/  217]    Overall Loss 0.069505    Objective Loss 0.069505                                        LR 0.001000    Time 0.291169    
2024-05-03 23:52:08,222 - Epoch: [40][  217/  217]    Overall Loss 0.067937    Objective Loss 0.067937    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.291004    
2024-05-03 23:52:08,951 - --- validate (epoch=40)-----------
2024-05-03 23:52:08,951 - 1736 samples (32 per mini-batch)
2024-05-03 23:52:24,336 - Epoch: [40][   55/   55]    Loss 2.541128    Top1 55.069124    Top5 72.235023    
2024-05-03 23:52:24,718 - ==> Top1: 55.069    Top5: 72.235    Loss: 2.541

2024-05-03 23:52:24,724 - ==> Best [Top1: 55.588   Top5: 71.429   Sparsity:0.00   Params: 381920 on epoch: 30]
2024-05-03 23:52:24,725 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-03 23:52:24,769 - 

2024-05-03 23:52:24,770 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:52:54,317 - Epoch: [41][  100/  217]    Overall Loss 0.029684    Objective Loss 0.029684                                        LR 0.001000    Time 0.295348    
2024-05-03 23:53:20,632 - Epoch: [41][  200/  217]    Overall Loss 0.031908    Objective Loss 0.031908                                        LR 0.001000    Time 0.279198    
2024-05-03 23:53:24,702 - Epoch: [41][  217/  217]    Overall Loss 0.031595    Objective Loss 0.031595    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.276075    
2024-05-03 23:53:25,266 - 

2024-05-03 23:53:25,266 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:53:51,319 - Epoch: [42][  100/  217]    Overall Loss 0.017322    Objective Loss 0.017322                                        LR 0.001000    Time 0.260427    
2024-05-03 23:54:14,554 - Epoch: [42][  200/  217]    Overall Loss 0.018453    Objective Loss 0.018453                                        LR 0.001000    Time 0.246342    
2024-05-03 23:54:18,671 - Epoch: [42][  217/  217]    Overall Loss 0.018248    Objective Loss 0.018248    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.246011    
2024-05-03 23:54:18,985 - 

2024-05-03 23:54:18,986 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:54:47,976 - Epoch: [43][  100/  217]    Overall Loss 0.014410    Objective Loss 0.014410                                        LR 0.001000    Time 0.289793    
2024-05-03 23:55:11,897 - Epoch: [43][  200/  217]    Overall Loss 0.014655    Objective Loss 0.014655                                        LR 0.001000    Time 0.264455    
2024-05-03 23:55:16,516 - Epoch: [43][  217/  217]    Overall Loss 0.014846    Objective Loss 0.014846    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.265013    
2024-05-03 23:55:16,966 - 

2024-05-03 23:55:16,966 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:55:46,895 - Epoch: [44][  100/  217]    Overall Loss 0.012444    Objective Loss 0.012444                                        LR 0.001000    Time 0.299189    
2024-05-03 23:56:10,007 - Epoch: [44][  200/  217]    Overall Loss 0.013020    Objective Loss 0.013020                                        LR 0.001000    Time 0.265109    
2024-05-03 23:56:14,686 - Epoch: [44][  217/  217]    Overall Loss 0.013548    Objective Loss 0.013548    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.265898    
2024-05-03 23:56:14,942 - 

2024-05-03 23:56:14,943 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:56:44,606 - Epoch: [45][  100/  217]    Overall Loss 0.009149    Objective Loss 0.009149                                        LR 0.001000    Time 0.296531    
2024-05-03 23:57:09,139 - Epoch: [45][  200/  217]    Overall Loss 0.010806    Objective Loss 0.010806                                        LR 0.001000    Time 0.270882    
2024-05-03 23:57:13,182 - Epoch: [45][  217/  217]    Overall Loss 0.010974    Objective Loss 0.010974    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.268286    
2024-05-03 23:57:13,454 - 

2024-05-03 23:57:13,455 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:57:44,811 - Epoch: [46][  100/  217]    Overall Loss 0.007315    Objective Loss 0.007315                                        LR 0.001000    Time 0.313461    
2024-05-03 23:58:11,309 - Epoch: [46][  200/  217]    Overall Loss 0.008323    Objective Loss 0.008323                                        LR 0.001000    Time 0.289180    
2024-05-03 23:58:14,116 - Epoch: [46][  217/  217]    Overall Loss 0.008769    Objective Loss 0.008769    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.279458    
2024-05-03 23:58:14,384 - 

2024-05-03 23:58:14,385 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:58:42,072 - Epoch: [47][  100/  217]    Overall Loss 0.007508    Objective Loss 0.007508                                        LR 0.001000    Time 0.276767    
2024-05-03 23:59:07,476 - Epoch: [47][  200/  217]    Overall Loss 0.008025    Objective Loss 0.008025                                        LR 0.001000    Time 0.265359    
2024-05-03 23:59:10,692 - Epoch: [47][  217/  217]    Overall Loss 0.007828    Objective Loss 0.007828    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.259381    
2024-05-03 23:59:10,990 - 

2024-05-03 23:59:10,991 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-03 23:59:42,096 - Epoch: [48][  100/  217]    Overall Loss 0.008154    Objective Loss 0.008154                                        LR 0.001000    Time 0.310955    
2024-05-04 00:00:06,400 - Epoch: [48][  200/  217]    Overall Loss 0.007944    Objective Loss 0.007944                                        LR 0.001000    Time 0.276949    
2024-05-04 00:00:11,718 - Epoch: [48][  217/  217]    Overall Loss 0.008099    Objective Loss 0.008099    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.279745    
2024-05-04 00:00:11,968 - 

2024-05-04 00:00:11,969 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:00:37,358 - Epoch: [49][  100/  217]    Overall Loss 0.014953    Objective Loss 0.014953                                        LR 0.001000    Time 0.253788    
2024-05-04 00:01:01,289 - Epoch: [49][  200/  217]    Overall Loss 0.016464    Objective Loss 0.016464                                        LR 0.001000    Time 0.246505    
2024-05-04 00:01:04,657 - Epoch: [49][  217/  217]    Overall Loss 0.016154    Objective Loss 0.016154    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.242706    
2024-05-04 00:01:04,924 - 

2024-05-04 00:01:04,925 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:01:33,724 - Epoch: [50][  100/  217]    Overall Loss 0.009616    Objective Loss 0.009616                                        LR 0.001000    Time 0.287883    
2024-05-04 00:01:56,343 - Epoch: [50][  200/  217]    Overall Loss 0.012744    Objective Loss 0.012744                                        LR 0.001000    Time 0.256991    
2024-05-04 00:01:59,769 - Epoch: [50][  217/  217]    Overall Loss 0.015207    Objective Loss 0.015207    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.252636    
2024-05-04 00:01:59,997 - --- validate (epoch=50)-----------
2024-05-04 00:01:59,998 - 1736 samples (32 per mini-batch)
2024-05-04 00:02:14,950 - Epoch: [50][   55/   55]    Loss 2.889697    Top1 52.476959    Top5 69.815668    
2024-05-04 00:02:15,215 - ==> Top1: 52.477    Top5: 69.816    Loss: 2.890

2024-05-04 00:02:15,219 - ==> Best [Top1: 55.588   Top5: 71.429   Sparsity:0.00   Params: 381920 on epoch: 30]
2024-05-04 00:02:15,219 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 00:02:15,269 - 

2024-05-04 00:02:15,269 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:02:42,532 - Epoch: [51][  100/  217]    Overall Loss 0.142809    Objective Loss 0.142809                                        LR 0.001000    Time 0.272537    
2024-05-04 00:03:05,842 - Epoch: [51][  200/  217]    Overall Loss 0.547250    Objective Loss 0.547250                                        LR 0.001000    Time 0.252771    
2024-05-04 00:03:08,757 - Epoch: [51][  217/  217]    Overall Loss 0.571157    Objective Loss 0.571157    Top1 80.327869    Top5 96.721311    LR 0.001000    Time 0.246393    
2024-05-04 00:03:09,063 - 

2024-05-04 00:03:09,064 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:03:32,877 - Epoch: [52][  100/  217]    Overall Loss 0.420630    Objective Loss 0.420630                                        LR 0.001000    Time 0.238047    
2024-05-04 00:03:56,098 - Epoch: [52][  200/  217]    Overall Loss 0.363955    Objective Loss 0.363955                                        LR 0.001000    Time 0.235084    
2024-05-04 00:03:59,774 - Epoch: [52][  217/  217]    Overall Loss 0.358068    Objective Loss 0.358068    Top1 91.803279    Top5 98.360656    LR 0.001000    Time 0.233598    
2024-05-04 00:04:00,027 - 

2024-05-04 00:04:00,028 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:04:27,105 - Epoch: [53][  100/  217]    Overall Loss 0.099861    Objective Loss 0.099861                                        LR 0.001000    Time 0.270675    
2024-05-04 00:04:49,937 - Epoch: [53][  200/  217]    Overall Loss 0.085807    Objective Loss 0.085807                                        LR 0.001000    Time 0.249460    
2024-05-04 00:04:53,590 - Epoch: [53][  217/  217]    Overall Loss 0.084788    Objective Loss 0.084788    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.246740    
2024-05-04 00:04:53,881 - 

2024-05-04 00:04:53,882 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:05:21,947 - Epoch: [54][  100/  217]    Overall Loss 0.037439    Objective Loss 0.037439                                        LR 0.001000    Time 0.280549    
2024-05-04 00:05:48,329 - Epoch: [54][  200/  217]    Overall Loss 0.032567    Objective Loss 0.032567                                        LR 0.001000    Time 0.272138    
2024-05-04 00:05:53,732 - Epoch: [54][  217/  217]    Overall Loss 0.031837    Objective Loss 0.031837    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.275711    
2024-05-04 00:05:54,019 - 

2024-05-04 00:05:54,019 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:06:24,011 - Epoch: [55][  100/  217]    Overall Loss 0.019059    Objective Loss 0.019059                                        LR 0.001000    Time 0.299824    
2024-05-04 00:06:51,440 - Epoch: [55][  200/  217]    Overall Loss 0.017246    Objective Loss 0.017246                                        LR 0.001000    Time 0.287010    
2024-05-04 00:06:55,996 - Epoch: [55][  217/  217]    Overall Loss 0.016853    Objective Loss 0.016853    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.285513    
2024-05-04 00:06:56,783 - 

2024-05-04 00:06:56,783 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:07:27,909 - Epoch: [56][  100/  217]    Overall Loss 0.011852    Objective Loss 0.011852                                        LR 0.001000    Time 0.311165    
2024-05-04 00:07:51,458 - Epoch: [56][  200/  217]    Overall Loss 0.012012    Objective Loss 0.012012                                        LR 0.001000    Time 0.273280    
2024-05-04 00:07:54,874 - Epoch: [56][  217/  217]    Overall Loss 0.011721    Objective Loss 0.011721    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.267605    
2024-05-04 00:07:55,205 - 

2024-05-04 00:07:55,206 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:08:28,377 - Epoch: [57][  100/  217]    Overall Loss 0.006963    Objective Loss 0.006963                                        LR 0.001000    Time 0.331603    
2024-05-04 00:08:54,132 - Epoch: [57][  200/  217]    Overall Loss 0.007938    Objective Loss 0.007938                                        LR 0.001000    Time 0.294528    
2024-05-04 00:08:58,173 - Epoch: [57][  217/  217]    Overall Loss 0.008187    Objective Loss 0.008187    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.290071    
2024-05-04 00:08:59,230 - 

2024-05-04 00:08:59,231 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:09:32,786 - Epoch: [58][  100/  217]    Overall Loss 0.006650    Objective Loss 0.006650                                        LR 0.001000    Time 0.335446    
2024-05-04 00:10:01,908 - Epoch: [58][  200/  217]    Overall Loss 0.007965    Objective Loss 0.007965                                        LR 0.001000    Time 0.313288    
2024-05-04 00:10:04,785 - Epoch: [58][  217/  217]    Overall Loss 0.007801    Objective Loss 0.007801    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.301992    
2024-05-04 00:10:05,153 - 

2024-05-04 00:10:05,153 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:10:30,455 - Epoch: [59][  100/  217]    Overall Loss 0.005739    Objective Loss 0.005739                                        LR 0.001000    Time 0.252911    
2024-05-04 00:10:56,251 - Epoch: [59][  200/  217]    Overall Loss 0.005624    Objective Loss 0.005624                                        LR 0.001000    Time 0.255392    
2024-05-04 00:11:01,597 - Epoch: [59][  217/  217]    Overall Loss 0.005839    Objective Loss 0.005839    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.260011    
2024-05-04 00:11:02,164 - 

2024-05-04 00:11:02,165 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:11:27,577 - Epoch: [60][  100/  217]    Overall Loss 0.004980    Objective Loss 0.004980                                        LR 0.001000    Time 0.253998    
2024-05-04 00:11:56,133 - Epoch: [60][  200/  217]    Overall Loss 0.006009    Objective Loss 0.006009                                        LR 0.001000    Time 0.269730    
2024-05-04 00:12:00,606 - Epoch: [60][  217/  217]    Overall Loss 0.006367    Objective Loss 0.006367    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.269206    
2024-05-04 00:12:01,196 - --- validate (epoch=60)-----------
2024-05-04 00:12:01,196 - 1736 samples (32 per mini-batch)
2024-05-04 00:12:18,419 - Epoch: [60][   55/   55]    Loss 2.719958    Top1 55.357143    Top5 71.774194    
2024-05-04 00:12:18,916 - ==> Top1: 55.357    Top5: 71.774    Loss: 2.720

2024-05-04 00:12:18,923 - ==> Best [Top1: 55.588   Top5: 71.429   Sparsity:0.00   Params: 381920 on epoch: 30]
2024-05-04 00:12:18,923 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 00:12:18,988 - 

2024-05-04 00:12:18,989 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:12:51,477 - Epoch: [61][  100/  217]    Overall Loss 0.005511    Objective Loss 0.005511                                        LR 0.001000    Time 0.324775    
2024-05-04 00:13:16,401 - Epoch: [61][  200/  217]    Overall Loss 0.005318    Objective Loss 0.005318                                        LR 0.001000    Time 0.286957    
2024-05-04 00:13:21,444 - Epoch: [61][  217/  217]    Overall Loss 0.006021    Objective Loss 0.006021    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.287701    
2024-05-04 00:13:22,082 - 

2024-05-04 00:13:22,083 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:13:53,738 - Epoch: [62][  100/  217]    Overall Loss 0.007424    Objective Loss 0.007424                                        LR 0.001000    Time 0.316439    
2024-05-04 00:14:19,328 - Epoch: [62][  200/  217]    Overall Loss 0.026914    Objective Loss 0.026914                                        LR 0.001000    Time 0.286130    
2024-05-04 00:14:23,730 - Epoch: [62][  217/  217]    Overall Loss 0.030717    Objective Loss 0.030717    Top1 96.721311    Top5 100.000000    LR 0.001000    Time 0.283990    
2024-05-04 00:14:24,209 - 

2024-05-04 00:14:24,210 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:14:57,545 - Epoch: [63][  100/  217]    Overall Loss 0.230566    Objective Loss 0.230566                                        LR 0.001000    Time 0.333244    
2024-05-04 00:15:24,445 - Epoch: [63][  200/  217]    Overall Loss 0.377417    Objective Loss 0.377417                                        LR 0.001000    Time 0.301076    
2024-05-04 00:15:29,847 - Epoch: [63][  217/  217]    Overall Loss 0.382930    Objective Loss 0.382930    Top1 86.885246    Top5 98.360656    LR 0.001000    Time 0.302374    
2024-05-04 00:15:30,247 - 

2024-05-04 00:15:30,248 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:16:00,351 - Epoch: [64][  100/  217]    Overall Loss 0.214658    Objective Loss 0.214658                                        LR 0.001000    Time 0.300911    
2024-05-04 00:16:23,378 - Epoch: [64][  200/  217]    Overall Loss 0.185050    Objective Loss 0.185050                                        LR 0.001000    Time 0.265548    
2024-05-04 00:16:27,596 - Epoch: [64][  217/  217]    Overall Loss 0.179748    Objective Loss 0.179748    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.264173    
2024-05-04 00:16:27,896 - 

2024-05-04 00:16:27,897 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:16:58,821 - Epoch: [65][  100/  217]    Overall Loss 0.050011    Objective Loss 0.050011                                        LR 0.001000    Time 0.309118    
2024-05-04 00:17:28,674 - Epoch: [65][  200/  217]    Overall Loss 0.045772    Objective Loss 0.045772                                        LR 0.001000    Time 0.303780    
2024-05-04 00:17:32,419 - Epoch: [65][  217/  217]    Overall Loss 0.045213    Objective Loss 0.045213    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.297229    
2024-05-04 00:17:33,311 - 

2024-05-04 00:17:33,311 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:18:03,749 - Epoch: [66][  100/  217]    Overall Loss 0.020260    Objective Loss 0.020260                                        LR 0.001000    Time 0.304273    
2024-05-04 00:18:29,236 - Epoch: [66][  200/  217]    Overall Loss 0.018401    Objective Loss 0.018401                                        LR 0.001000    Time 0.279517    
2024-05-04 00:18:33,426 - Epoch: [66][  217/  217]    Overall Loss 0.017823    Objective Loss 0.017823    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.276921    
2024-05-04 00:18:33,765 - 

2024-05-04 00:18:33,766 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:19:03,607 - Epoch: [67][  100/  217]    Overall Loss 0.008873    Objective Loss 0.008873                                        LR 0.001000    Time 0.298317    
2024-05-04 00:19:32,078 - Epoch: [67][  200/  217]    Overall Loss 0.010362    Objective Loss 0.010362                                        LR 0.001000    Time 0.291466    
2024-05-04 00:19:36,161 - Epoch: [67][  217/  217]    Overall Loss 0.010140    Objective Loss 0.010140    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.287439    
2024-05-04 00:19:36,503 - 

2024-05-04 00:19:36,504 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:20:07,065 - Epoch: [68][  100/  217]    Overall Loss 0.009141    Objective Loss 0.009141                                        LR 0.001000    Time 0.305508    
2024-05-04 00:20:33,102 - Epoch: [68][  200/  217]    Overall Loss 0.008715    Objective Loss 0.008715                                        LR 0.001000    Time 0.282897    
2024-05-04 00:20:37,455 - Epoch: [68][  217/  217]    Overall Loss 0.008566    Objective Loss 0.008566    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.280786    
2024-05-04 00:20:37,940 - 

2024-05-04 00:20:37,941 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:21:10,244 - Epoch: [69][  100/  217]    Overall Loss 0.004968    Objective Loss 0.004968                                        LR 0.001000    Time 0.322927    
2024-05-04 00:21:36,336 - Epoch: [69][  200/  217]    Overall Loss 0.006084    Objective Loss 0.006084                                        LR 0.001000    Time 0.291878    
2024-05-04 00:21:40,371 - Epoch: [69][  217/  217]    Overall Loss 0.006211    Objective Loss 0.006211    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.287595    
2024-05-04 00:21:40,698 - 

2024-05-04 00:21:40,698 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:22:11,281 - Epoch: [70][  100/  217]    Overall Loss 0.007641    Objective Loss 0.007641                                        LR 0.001000    Time 0.305713    
2024-05-04 00:22:40,621 - Epoch: [70][  200/  217]    Overall Loss 0.007175    Objective Loss 0.007175                                        LR 0.001000    Time 0.299508    
2024-05-04 00:22:43,319 - Epoch: [70][  217/  217]    Overall Loss 0.006966    Objective Loss 0.006966    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.288466    
2024-05-04 00:22:44,240 - --- validate (epoch=70)-----------
2024-05-04 00:22:44,240 - 1736 samples (32 per mini-batch)
2024-05-04 00:22:59,163 - Epoch: [70][   55/   55]    Loss 2.769699    Top1 56.566820    Top5 72.811060    
2024-05-04 00:22:59,510 - ==> Top1: 56.567    Top5: 72.811    Loss: 2.770

2024-05-04 00:22:59,517 - ==> Best [Top1: 56.567   Top5: 72.811   Sparsity:0.00   Params: 381920 on epoch: 70]
2024-05-04 00:22:59,517 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 00:22:59,554 - 

2024-05-04 00:22:59,555 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:23:32,110 - Epoch: [71][  100/  217]    Overall Loss 0.005369    Objective Loss 0.005369                                        LR 0.001000    Time 0.325446    
2024-05-04 00:23:55,693 - Epoch: [71][  200/  217]    Overall Loss 0.005234    Objective Loss 0.005234                                        LR 0.001000    Time 0.280592    
2024-05-04 00:24:00,832 - Epoch: [71][  217/  217]    Overall Loss 0.005065    Objective Loss 0.005065    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.282281    
2024-05-04 00:24:01,267 - 

2024-05-04 00:24:01,268 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:24:31,768 - Epoch: [72][  100/  217]    Overall Loss 0.004666    Objective Loss 0.004666                                        LR 0.001000    Time 0.304910    
2024-05-04 00:24:58,532 - Epoch: [72][  200/  217]    Overall Loss 0.005205    Objective Loss 0.005205                                        LR 0.001000    Time 0.286234    
2024-05-04 00:25:02,154 - Epoch: [72][  217/  217]    Overall Loss 0.005098    Objective Loss 0.005098    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.280486    
2024-05-04 00:25:02,577 - 

2024-05-04 00:25:02,578 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:25:32,731 - Epoch: [73][  100/  217]    Overall Loss 0.005527    Objective Loss 0.005527                                        LR 0.001000    Time 0.301430    
2024-05-04 00:26:01,930 - Epoch: [73][  200/  217]    Overall Loss 0.005659    Objective Loss 0.005659                                        LR 0.001000    Time 0.296659    
2024-05-04 00:26:04,801 - Epoch: [73][  217/  217]    Overall Loss 0.005479    Objective Loss 0.005479    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.286640    
2024-05-04 00:26:05,438 - 

2024-05-04 00:26:05,439 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:26:33,539 - Epoch: [74][  100/  217]    Overall Loss 0.003950    Objective Loss 0.003950                                        LR 0.001000    Time 0.280899    
2024-05-04 00:27:02,914 - Epoch: [74][  200/  217]    Overall Loss 0.004065    Objective Loss 0.004065                                        LR 0.001000    Time 0.287281    
2024-05-04 00:27:08,369 - Epoch: [74][  217/  217]    Overall Loss 0.003957    Objective Loss 0.003957    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.289903    
2024-05-04 00:27:08,964 - 

2024-05-04 00:27:08,964 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:27:38,583 - Epoch: [75][  100/  217]    Overall Loss 0.003385    Objective Loss 0.003385                                        LR 0.001000    Time 0.296084    
2024-05-04 00:28:05,661 - Epoch: [75][  200/  217]    Overall Loss 0.004578    Objective Loss 0.004578                                        LR 0.001000    Time 0.283382    
2024-05-04 00:28:08,488 - Epoch: [75][  217/  217]    Overall Loss 0.004733    Objective Loss 0.004733    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.274198    
2024-05-04 00:28:09,388 - 

2024-05-04 00:28:09,389 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:28:37,410 - Epoch: [76][  100/  217]    Overall Loss 0.136090    Objective Loss 0.136090                                        LR 0.001000    Time 0.280096    
2024-05-04 00:29:03,563 - Epoch: [76][  200/  217]    Overall Loss 0.384642    Objective Loss 0.384642                                        LR 0.001000    Time 0.270771    
2024-05-04 00:29:09,289 - Epoch: [76][  217/  217]    Overall Loss 0.403543    Objective Loss 0.403543    Top1 83.606557    Top5 96.721311    LR 0.001000    Time 0.275930    
2024-05-04 00:29:09,958 - 

2024-05-04 00:29:09,959 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:29:40,332 - Epoch: [77][  100/  217]    Overall Loss 0.281422    Objective Loss 0.281422                                        LR 0.001000    Time 0.303613    
2024-05-04 00:30:07,306 - Epoch: [77][  200/  217]    Overall Loss 0.239065    Objective Loss 0.239065                                        LR 0.001000    Time 0.286629    
2024-05-04 00:30:12,967 - Epoch: [77][  217/  217]    Overall Loss 0.233705    Objective Loss 0.233705    Top1 93.442623    Top5 100.000000    LR 0.001000    Time 0.290247    
2024-05-04 00:30:13,468 - 

2024-05-04 00:30:13,469 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:30:40,146 - Epoch: [78][  100/  217]    Overall Loss 0.057182    Objective Loss 0.057182                                        LR 0.001000    Time 0.266665    
2024-05-04 00:31:04,577 - Epoch: [78][  200/  217]    Overall Loss 0.046875    Objective Loss 0.046875                                        LR 0.001000    Time 0.255439    
2024-05-04 00:31:07,396 - Epoch: [78][  217/  217]    Overall Loss 0.045663    Objective Loss 0.045663    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.248406    
2024-05-04 00:31:08,311 - 

2024-05-04 00:31:08,312 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:31:39,145 - Epoch: [79][  100/  217]    Overall Loss 0.017225    Objective Loss 0.017225                                        LR 0.001000    Time 0.308231    
2024-05-04 00:32:03,465 - Epoch: [79][  200/  217]    Overall Loss 0.017611    Objective Loss 0.017611                                        LR 0.001000    Time 0.275668    
2024-05-04 00:32:07,455 - Epoch: [79][  217/  217]    Overall Loss 0.017956    Objective Loss 0.017956    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.272452    
2024-05-04 00:32:07,897 - 

2024-05-04 00:32:07,898 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:32:43,019 - Epoch: [80][  100/  217]    Overall Loss 0.011284    Objective Loss 0.011284                                        LR 0.001000    Time 0.351119    
2024-05-04 00:33:09,887 - Epoch: [80][  200/  217]    Overall Loss 0.010602    Objective Loss 0.010602                                        LR 0.001000    Time 0.309849    
2024-05-04 00:33:15,073 - Epoch: [80][  217/  217]    Overall Loss 0.010845    Objective Loss 0.010845    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.309466    
2024-05-04 00:33:15,589 - --- validate (epoch=80)-----------
2024-05-04 00:33:15,590 - 1736 samples (32 per mini-batch)
2024-05-04 00:33:30,581 - Epoch: [80][   55/   55]    Loss 2.768203    Top1 55.587558    Top5 74.135945    
2024-05-04 00:33:30,868 - ==> Top1: 55.588    Top5: 74.136    Loss: 2.768

2024-05-04 00:33:30,877 - ==> Best [Top1: 56.567   Top5: 72.811   Sparsity:0.00   Params: 381920 on epoch: 70]
2024-05-04 00:33:30,878 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 00:33:30,930 - 

2024-05-04 00:33:30,931 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:33:59,526 - Epoch: [81][  100/  217]    Overall Loss 0.006230    Objective Loss 0.006230                                        LR 0.001000    Time 0.285843    
2024-05-04 00:34:27,014 - Epoch: [81][  200/  217]    Overall Loss 0.007318    Objective Loss 0.007318                                        LR 0.001000    Time 0.280320    
2024-05-04 00:34:30,805 - Epoch: [81][  217/  217]    Overall Loss 0.007547    Objective Loss 0.007547    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.275823    
2024-05-04 00:34:31,313 - 

2024-05-04 00:34:31,314 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:35:01,846 - Epoch: [82][  100/  217]    Overall Loss 0.006736    Objective Loss 0.006736                                        LR 0.001000    Time 0.305209    
2024-05-04 00:35:28,465 - Epoch: [82][  200/  217]    Overall Loss 0.006620    Objective Loss 0.006620                                        LR 0.001000    Time 0.285653    
2024-05-04 00:35:31,509 - Epoch: [82][  217/  217]    Overall Loss 0.006544    Objective Loss 0.006544    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.277289    
2024-05-04 00:35:31,979 - 

2024-05-04 00:35:31,980 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:36:02,992 - Epoch: [83][  100/  217]    Overall Loss 0.004818    Objective Loss 0.004818                                        LR 0.001000    Time 0.310014    
2024-05-04 00:36:30,243 - Epoch: [83][  200/  217]    Overall Loss 0.005396    Objective Loss 0.005396                                        LR 0.001000    Time 0.291220    
2024-05-04 00:36:33,816 - Epoch: [83][  217/  217]    Overall Loss 0.005221    Objective Loss 0.005221    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.284859    
2024-05-04 00:36:34,282 - 

2024-05-04 00:36:34,283 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:37:05,186 - Epoch: [84][  100/  217]    Overall Loss 0.003831    Objective Loss 0.003831                                        LR 0.001000    Time 0.308938    
2024-05-04 00:37:31,491 - Epoch: [84][  200/  217]    Overall Loss 0.004697    Objective Loss 0.004697                                        LR 0.001000    Time 0.285950    
2024-05-04 00:37:36,237 - Epoch: [84][  217/  217]    Overall Loss 0.004627    Objective Loss 0.004627    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.285411    
2024-05-04 00:37:36,588 - 

2024-05-04 00:37:36,588 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:38:04,968 - Epoch: [85][  100/  217]    Overall Loss 0.004805    Objective Loss 0.004805                                        LR 0.001000    Time 0.283700    
2024-05-04 00:38:29,152 - Epoch: [85][  200/  217]    Overall Loss 0.004379    Objective Loss 0.004379                                        LR 0.001000    Time 0.262724    
2024-05-04 00:38:33,626 - Epoch: [85][  217/  217]    Overall Loss 0.004516    Objective Loss 0.004516    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.262753    
2024-05-04 00:38:33,930 - 

2024-05-04 00:38:33,931 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:39:05,245 - Epoch: [86][  100/  217]    Overall Loss 0.004150    Objective Loss 0.004150                                        LR 0.001000    Time 0.313046    
2024-05-04 00:39:29,908 - Epoch: [86][  200/  217]    Overall Loss 0.004474    Objective Loss 0.004474                                        LR 0.001000    Time 0.279792    
2024-05-04 00:39:33,861 - Epoch: [86][  217/  217]    Overall Loss 0.004349    Objective Loss 0.004349    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.276080    
2024-05-04 00:39:34,168 - 

2024-05-04 00:39:34,168 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:40:02,668 - Epoch: [87][  100/  217]    Overall Loss 0.002938    Objective Loss 0.002938                                        LR 0.001000    Time 0.284904    
2024-05-04 00:40:29,757 - Epoch: [87][  200/  217]    Overall Loss 0.004239    Objective Loss 0.004239                                        LR 0.001000    Time 0.277857    
2024-05-04 00:40:33,220 - Epoch: [87][  217/  217]    Overall Loss 0.004137    Objective Loss 0.004137    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.272038    
2024-05-04 00:40:33,521 - 

2024-05-04 00:40:33,521 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:41:03,969 - Epoch: [88][  100/  217]    Overall Loss 0.002992    Objective Loss 0.002992                                        LR 0.001000    Time 0.304390    
2024-05-04 00:41:30,173 - Epoch: [88][  200/  217]    Overall Loss 0.003580    Objective Loss 0.003580                                        LR 0.001000    Time 0.283174    
2024-05-04 00:41:34,323 - Epoch: [88][  217/  217]    Overall Loss 0.003485    Objective Loss 0.003485    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.280107    
2024-05-04 00:41:34,608 - 

2024-05-04 00:41:34,609 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:42:05,361 - Epoch: [89][  100/  217]    Overall Loss 0.002822    Objective Loss 0.002822                                        LR 0.001000    Time 0.307422    
2024-05-04 00:42:31,869 - Epoch: [89][  200/  217]    Overall Loss 0.002852    Objective Loss 0.002852                                        LR 0.001000    Time 0.286216    
2024-05-04 00:42:34,482 - Epoch: [89][  217/  217]    Overall Loss 0.002774    Objective Loss 0.002774    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.275831    
2024-05-04 00:42:34,738 - 

2024-05-04 00:42:34,739 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:43:05,329 - Epoch: [90][  100/  217]    Overall Loss 0.003628    Objective Loss 0.003628                                        LR 0.001000    Time 0.305805    
2024-05-04 00:43:34,191 - Epoch: [90][  200/  217]    Overall Loss 0.003903    Objective Loss 0.003903                                        LR 0.001000    Time 0.297172    
2024-05-04 00:43:36,773 - Epoch: [90][  217/  217]    Overall Loss 0.003867    Objective Loss 0.003867    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.285779    
2024-05-04 00:43:37,127 - --- validate (epoch=90)-----------
2024-05-04 00:43:37,128 - 1736 samples (32 per mini-batch)
2024-05-04 00:43:53,162 - Epoch: [90][   55/   55]    Loss 2.871995    Top1 55.760369    Top5 73.099078    
2024-05-04 00:43:53,461 - ==> Top1: 55.760    Top5: 73.099    Loss: 2.872

2024-05-04 00:43:53,465 - ==> Best [Top1: 56.567   Top5: 72.811   Sparsity:0.00   Params: 381920 on epoch: 70]
2024-05-04 00:43:53,465 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 00:43:53,510 - 

2024-05-04 00:43:53,510 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:44:25,757 - Epoch: [91][  100/  217]    Overall Loss 0.004115    Objective Loss 0.004115                                        LR 0.001000    Time 0.322373    
2024-05-04 00:44:52,013 - Epoch: [91][  200/  217]    Overall Loss 0.004740    Objective Loss 0.004740                                        LR 0.001000    Time 0.292422    
2024-05-04 00:44:57,642 - Epoch: [91][  217/  217]    Overall Loss 0.004609    Objective Loss 0.004609    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.295449    
2024-05-04 00:44:58,244 - 

2024-05-04 00:44:58,244 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:45:26,384 - Epoch: [92][  100/  217]    Overall Loss 0.005213    Objective Loss 0.005213                                        LR 0.001000    Time 0.281304    
2024-05-04 00:45:51,733 - Epoch: [92][  200/  217]    Overall Loss 0.247782    Objective Loss 0.247782                                        LR 0.001000    Time 0.267356    
2024-05-04 00:45:57,305 - Epoch: [92][  217/  217]    Overall Loss 0.312646    Objective Loss 0.312646    Top1 80.327869    Top5 93.442623    LR 0.001000    Time 0.272081    
2024-05-04 00:45:57,950 - 

2024-05-04 00:45:57,951 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:46:29,675 - Epoch: [93][  100/  217]    Overall Loss 0.500568    Objective Loss 0.500568                                        LR 0.001000    Time 0.317136    
2024-05-04 00:46:56,884 - Epoch: [93][  200/  217]    Overall Loss 0.405977    Objective Loss 0.405977                                        LR 0.001000    Time 0.294569    
2024-05-04 00:46:59,589 - Epoch: [93][  217/  217]    Overall Loss 0.395471    Objective Loss 0.395471    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.283944    
2024-05-04 00:47:00,538 - 

2024-05-04 00:47:00,539 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:47:30,649 - Epoch: [94][  100/  217]    Overall Loss 0.082344    Objective Loss 0.082344                                        LR 0.001000    Time 0.301006    
2024-05-04 00:47:59,036 - Epoch: [94][  200/  217]    Overall Loss 0.069536    Objective Loss 0.069536                                        LR 0.001000    Time 0.292392    
2024-05-04 00:48:04,017 - Epoch: [94][  217/  217]    Overall Loss 0.068769    Objective Loss 0.068769    Top1 98.360656    Top5 100.000000    LR 0.001000    Time 0.292431    
2024-05-04 00:48:04,337 - 

2024-05-04 00:48:04,337 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:48:35,990 - Epoch: [95][  100/  217]    Overall Loss 0.023013    Objective Loss 0.023013                                        LR 0.001000    Time 0.316435    
2024-05-04 00:48:59,320 - Epoch: [95][  200/  217]    Overall Loss 0.022335    Objective Loss 0.022335                                        LR 0.001000    Time 0.274819    
2024-05-04 00:49:03,887 - Epoch: [95][  217/  217]    Overall Loss 0.022086    Objective Loss 0.022086    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.274328    
2024-05-04 00:49:04,302 - 

2024-05-04 00:49:04,303 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:49:33,480 - Epoch: [96][  100/  217]    Overall Loss 0.010734    Objective Loss 0.010734                                        LR 0.001000    Time 0.291672    
2024-05-04 00:49:57,920 - Epoch: [96][  200/  217]    Overall Loss 0.011339    Objective Loss 0.011339                                        LR 0.001000    Time 0.267992    
2024-05-04 00:50:03,435 - Epoch: [96][  217/  217]    Overall Loss 0.011300    Objective Loss 0.011300    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.272409    
2024-05-04 00:50:03,684 - 

2024-05-04 00:50:03,685 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:50:33,777 - Epoch: [97][  100/  217]    Overall Loss 0.009743    Objective Loss 0.009743                                        LR 0.001000    Time 0.300828    
2024-05-04 00:50:59,401 - Epoch: [97][  200/  217]    Overall Loss 0.008748    Objective Loss 0.008748                                        LR 0.001000    Time 0.278486    
2024-05-04 00:51:04,984 - Epoch: [97][  217/  217]    Overall Loss 0.008979    Objective Loss 0.008979    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.282386    
2024-05-04 00:51:05,255 - 

2024-05-04 00:51:05,256 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:51:35,510 - Epoch: [98][  100/  217]    Overall Loss 0.005772    Objective Loss 0.005772                                        LR 0.001000    Time 0.302439    
2024-05-04 00:52:01,368 - Epoch: [98][  200/  217]    Overall Loss 0.006255    Objective Loss 0.006255                                        LR 0.001000    Time 0.280356    
2024-05-04 00:52:05,129 - Epoch: [98][  217/  217]    Overall Loss 0.006258    Objective Loss 0.006258    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.275717    
2024-05-04 00:52:05,510 - 

2024-05-04 00:52:05,510 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:52:39,722 - Epoch: [99][  100/  217]    Overall Loss 0.004800    Objective Loss 0.004800                                        LR 0.001000    Time 0.342012    
2024-05-04 00:53:05,932 - Epoch: [99][  200/  217]    Overall Loss 0.004723    Objective Loss 0.004723                                        LR 0.001000    Time 0.301955    
2024-05-04 00:53:10,008 - Epoch: [99][  217/  217]    Overall Loss 0.004622    Objective Loss 0.004622    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.297074    
2024-05-04 00:53:10,335 - 

2024-05-04 00:53:10,335 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:53:40,747 - Epoch: [100][  100/  217]    Overall Loss 0.003878    Objective Loss 0.003878                                        LR 0.000250    Time 0.304019    
2024-05-04 00:54:13,155 - Epoch: [100][  200/  217]    Overall Loss 0.003989    Objective Loss 0.003989                                        LR 0.000250    Time 0.314006    
2024-05-04 00:54:16,853 - Epoch: [100][  217/  217]    Overall Loss 0.004251    Objective Loss 0.004251    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.306437    
2024-05-04 00:54:17,320 - --- validate (epoch=100)-----------
2024-05-04 00:54:17,320 - 1736 samples (32 per mini-batch)
2024-05-04 00:54:35,207 - Epoch: [100][   55/   55]    Loss 2.796548    Top1 56.278802    Top5 73.502304    
2024-05-04 00:54:35,779 - ==> Top1: 56.279    Top5: 73.502    Loss: 2.797

2024-05-04 00:54:35,782 - ==> Best [Top1: 56.567   Top5: 72.811   Sparsity:0.00   Params: 381920 on epoch: 70]
2024-05-04 00:54:35,782 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 00:54:35,811 - 

2024-05-04 00:54:35,811 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:55:04,708 - Epoch: [101][  100/  217]    Overall Loss 0.003995    Objective Loss 0.003995                                        LR 0.000250    Time 0.288869    
2024-05-04 00:55:35,126 - Epoch: [101][  200/  217]    Overall Loss 0.003763    Objective Loss 0.003763                                        LR 0.000250    Time 0.296473    
2024-05-04 00:55:39,350 - Epoch: [101][  217/  217]    Overall Loss 0.003807    Objective Loss 0.003807    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.292698    
2024-05-04 00:55:39,693 - 

2024-05-04 00:55:39,693 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:56:07,966 - Epoch: [102][  100/  217]    Overall Loss 0.003423    Objective Loss 0.003423                                        LR 0.000250    Time 0.282639    
2024-05-04 00:56:37,720 - Epoch: [102][  200/  217]    Overall Loss 0.003713    Objective Loss 0.003713                                        LR 0.000250    Time 0.290048    
2024-05-04 00:56:41,993 - Epoch: [102][  217/  217]    Overall Loss 0.003788    Objective Loss 0.003788    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.287006    
2024-05-04 00:56:42,251 - 

2024-05-04 00:56:42,252 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:57:12,089 - Epoch: [103][  100/  217]    Overall Loss 0.003059    Objective Loss 0.003059                                        LR 0.000250    Time 0.298293    
2024-05-04 00:57:38,014 - Epoch: [103][  200/  217]    Overall Loss 0.003053    Objective Loss 0.003053                                        LR 0.000250    Time 0.278724    
2024-05-04 00:57:42,366 - Epoch: [103][  217/  217]    Overall Loss 0.003147    Objective Loss 0.003147    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.276935    
2024-05-04 00:57:42,649 - 

2024-05-04 00:57:42,651 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:58:12,826 - Epoch: [104][  100/  217]    Overall Loss 0.003202    Objective Loss 0.003202                                        LR 0.000250    Time 0.301655    
2024-05-04 00:58:40,884 - Epoch: [104][  200/  217]    Overall Loss 0.003354    Objective Loss 0.003354                                        LR 0.000250    Time 0.291074    
2024-05-04 00:58:44,826 - Epoch: [104][  217/  217]    Overall Loss 0.003293    Objective Loss 0.003293    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.286428    
2024-05-04 00:58:45,136 - 

2024-05-04 00:58:45,137 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 00:59:16,001 - Epoch: [105][  100/  217]    Overall Loss 0.003110    Objective Loss 0.003110                                        LR 0.000250    Time 0.308548    
2024-05-04 00:59:42,514 - Epoch: [105][  200/  217]    Overall Loss 0.003356    Objective Loss 0.003356                                        LR 0.000250    Time 0.286794    
2024-05-04 00:59:45,461 - Epoch: [105][  217/  217]    Overall Loss 0.003281    Objective Loss 0.003281    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.277899    
2024-05-04 00:59:45,847 - 

2024-05-04 00:59:45,848 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:00:18,232 - Epoch: [106][  100/  217]    Overall Loss 0.002845    Objective Loss 0.002845                                        LR 0.000250    Time 0.323664    
2024-05-04 01:00:45,798 - Epoch: [106][  200/  217]    Overall Loss 0.003105    Objective Loss 0.003105                                        LR 0.000250    Time 0.299619    
2024-05-04 01:00:48,628 - Epoch: [106][  217/  217]    Overall Loss 0.003065    Objective Loss 0.003065    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.289179    
2024-05-04 01:00:49,101 - 

2024-05-04 01:00:49,102 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:01:22,201 - Epoch: [107][  100/  217]    Overall Loss 0.002823    Objective Loss 0.002823                                        LR 0.000250    Time 0.330886    
2024-05-04 01:01:49,281 - Epoch: [107][  200/  217]    Overall Loss 0.002980    Objective Loss 0.002980                                        LR 0.000250    Time 0.300791    
2024-05-04 01:01:52,609 - Epoch: [107][  217/  217]    Overall Loss 0.002899    Objective Loss 0.002899    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.292556    
2024-05-04 01:01:52,928 - 

2024-05-04 01:01:52,929 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:02:21,264 - Epoch: [108][  100/  217]    Overall Loss 0.003098    Objective Loss 0.003098                                        LR 0.000250    Time 0.283249    
2024-05-04 01:02:48,897 - Epoch: [108][  200/  217]    Overall Loss 0.002687    Objective Loss 0.002687                                        LR 0.000250    Time 0.279739    
2024-05-04 01:02:52,617 - Epoch: [108][  217/  217]    Overall Loss 0.002623    Objective Loss 0.002623    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.274959    
2024-05-04 01:02:53,170 - 

2024-05-04 01:02:53,170 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:03:21,931 - Epoch: [109][  100/  217]    Overall Loss 0.003371    Objective Loss 0.003371                                        LR 0.000250    Time 0.287501    
2024-05-04 01:03:49,124 - Epoch: [109][  200/  217]    Overall Loss 0.002759    Objective Loss 0.002759                                        LR 0.000250    Time 0.279675    
2024-05-04 01:03:54,027 - Epoch: [109][  217/  217]    Overall Loss 0.002687    Objective Loss 0.002687    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.280354    
2024-05-04 01:03:54,708 - 

2024-05-04 01:03:54,709 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:04:21,338 - Epoch: [110][  100/  217]    Overall Loss 0.002870    Objective Loss 0.002870                                        LR 0.000250    Time 0.266193    
2024-05-04 01:04:48,223 - Epoch: [110][  200/  217]    Overall Loss 0.002731    Objective Loss 0.002731                                        LR 0.000250    Time 0.267483    
2024-05-04 01:04:51,241 - Epoch: [110][  217/  217]    Overall Loss 0.002691    Objective Loss 0.002691    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.260424    
2024-05-04 01:04:51,838 - --- validate (epoch=110)-----------
2024-05-04 01:04:51,839 - 1736 samples (32 per mini-batch)
2024-05-04 01:05:08,471 - Epoch: [110][   55/   55]    Loss 2.836985    Top1 56.451613    Top5 73.041475    
2024-05-04 01:05:08,800 - ==> Top1: 56.452    Top5: 73.041    Loss: 2.837

2024-05-04 01:05:08,804 - ==> Best [Top1: 56.567   Top5: 72.811   Sparsity:0.00   Params: 381920 on epoch: 70]
2024-05-04 01:05:08,804 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 01:05:08,842 - 

2024-05-04 01:05:08,843 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:05:37,472 - Epoch: [111][  100/  217]    Overall Loss 0.002905    Objective Loss 0.002905                                        LR 0.000250    Time 0.286179    
2024-05-04 01:06:01,065 - Epoch: [111][  200/  217]    Overall Loss 0.002457    Objective Loss 0.002457                                        LR 0.000250    Time 0.261012    
2024-05-04 01:06:05,590 - Epoch: [111][  217/  217]    Overall Loss 0.002390    Objective Loss 0.002390    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.261410    
2024-05-04 01:06:05,988 - 

2024-05-04 01:06:05,988 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:06:36,629 - Epoch: [112][  100/  217]    Overall Loss 0.002344    Objective Loss 0.002344                                        LR 0.000250    Time 0.306305    
2024-05-04 01:07:02,522 - Epoch: [112][  200/  217]    Overall Loss 0.002402    Objective Loss 0.002402                                        LR 0.000250    Time 0.282571    
2024-05-04 01:07:05,384 - Epoch: [112][  217/  217]    Overall Loss 0.002531    Objective Loss 0.002531    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.273615    
2024-05-04 01:07:05,636 - 

2024-05-04 01:07:05,637 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:07:35,553 - Epoch: [113][  100/  217]    Overall Loss 0.002655    Objective Loss 0.002655                                        LR 0.000250    Time 0.299057    
2024-05-04 01:08:00,332 - Epoch: [113][  200/  217]    Overall Loss 0.002396    Objective Loss 0.002396                                        LR 0.000250    Time 0.273381    
2024-05-04 01:08:02,880 - Epoch: [113][  217/  217]    Overall Loss 0.002484    Objective Loss 0.002484    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.263702    
2024-05-04 01:08:03,115 - 

2024-05-04 01:08:03,116 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:08:35,569 - Epoch: [114][  100/  217]    Overall Loss 0.002980    Objective Loss 0.002980                                        LR 0.000250    Time 0.324441    
2024-05-04 01:09:00,216 - Epoch: [114][  200/  217]    Overall Loss 0.002542    Objective Loss 0.002542                                        LR 0.000250    Time 0.285414    
2024-05-04 01:09:03,643 - Epoch: [114][  217/  217]    Overall Loss 0.002453    Objective Loss 0.002453    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.278840    
2024-05-04 01:09:03,985 - 

2024-05-04 01:09:03,986 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:09:35,356 - Epoch: [115][  100/  217]    Overall Loss 0.002939    Objective Loss 0.002939                                        LR 0.000250    Time 0.313579    
2024-05-04 01:09:56,986 - Epoch: [115][  200/  217]    Overall Loss 0.002539    Objective Loss 0.002539                                        LR 0.000250    Time 0.264899    
2024-05-04 01:09:59,723 - Epoch: [115][  217/  217]    Overall Loss 0.002494    Objective Loss 0.002494    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.256745    
2024-05-04 01:09:59,949 - 

2024-05-04 01:09:59,949 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:10:32,264 - Epoch: [116][  100/  217]    Overall Loss 0.002214    Objective Loss 0.002214                                        LR 0.000250    Time 0.323049    
2024-05-04 01:10:56,774 - Epoch: [116][  200/  217]    Overall Loss 0.002013    Objective Loss 0.002013                                        LR 0.000250    Time 0.284035    
2024-05-04 01:11:01,737 - Epoch: [116][  217/  217]    Overall Loss 0.001958    Objective Loss 0.001958    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.284647    
2024-05-04 01:11:01,967 - 

2024-05-04 01:11:01,968 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:11:33,085 - Epoch: [117][  100/  217]    Overall Loss 0.001417    Objective Loss 0.001417                                        LR 0.000250    Time 0.311078    
2024-05-04 01:11:58,050 - Epoch: [117][  200/  217]    Overall Loss 0.001809    Objective Loss 0.001809                                        LR 0.000250    Time 0.280315    
2024-05-04 01:12:02,301 - Epoch: [117][  217/  217]    Overall Loss 0.002084    Objective Loss 0.002084    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.277939    
2024-05-04 01:12:02,544 - 

2024-05-04 01:12:02,545 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:12:34,527 - Epoch: [118][  100/  217]    Overall Loss 0.002662    Objective Loss 0.002662                                        LR 0.000250    Time 0.319724    
2024-05-04 01:12:57,973 - Epoch: [118][  200/  217]    Overall Loss 0.002125    Objective Loss 0.002125                                        LR 0.000250    Time 0.277055    
2024-05-04 01:13:02,096 - Epoch: [118][  217/  217]    Overall Loss 0.002102    Objective Loss 0.002102    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.274339    
2024-05-04 01:13:02,740 - 

2024-05-04 01:13:02,740 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:13:34,102 - Epoch: [119][  100/  217]    Overall Loss 0.001542    Objective Loss 0.001542                                        LR 0.000250    Time 0.313472    
2024-05-04 01:13:59,151 - Epoch: [119][  200/  217]    Overall Loss 0.001872    Objective Loss 0.001872                                        LR 0.000250    Time 0.281941    
2024-05-04 01:14:03,952 - Epoch: [119][  217/  217]    Overall Loss 0.002145    Objective Loss 0.002145    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.281970    
2024-05-04 01:14:04,232 - 

2024-05-04 01:14:04,233 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:14:35,198 - Epoch: [120][  100/  217]    Overall Loss 0.002851    Objective Loss 0.002851                                        LR 0.000250    Time 0.309564    
2024-05-04 01:15:04,422 - Epoch: [120][  200/  217]    Overall Loss 0.002526    Objective Loss 0.002526                                        LR 0.000250    Time 0.300857    
2024-05-04 01:15:08,102 - Epoch: [120][  217/  217]    Overall Loss 0.002573    Objective Loss 0.002573    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.294237    
2024-05-04 01:15:08,346 - --- validate (epoch=120)-----------
2024-05-04 01:15:08,347 - 1736 samples (32 per mini-batch)
2024-05-04 01:15:24,627 - Epoch: [120][   55/   55]    Loss 2.924215    Top1 56.278802    Top5 74.193548    
2024-05-04 01:15:24,875 - ==> Top1: 56.279    Top5: 74.194    Loss: 2.924

2024-05-04 01:15:24,879 - ==> Best [Top1: 56.567   Top5: 72.811   Sparsity:0.00   Params: 381920 on epoch: 70]
2024-05-04 01:15:24,879 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 01:15:24,922 - 

2024-05-04 01:15:24,923 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:15:56,296 - Epoch: [121][  100/  217]    Overall Loss 0.002943    Objective Loss 0.002943                                        LR 0.000250    Time 0.313629    
2024-05-04 01:16:23,223 - Epoch: [121][  200/  217]    Overall Loss 0.002628    Objective Loss 0.002628                                        LR 0.000250    Time 0.291387    
2024-05-04 01:16:25,915 - Epoch: [121][  217/  217]    Overall Loss 0.002512    Objective Loss 0.002512    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.280953    
2024-05-04 01:16:26,490 - 

2024-05-04 01:16:26,490 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:16:53,721 - Epoch: [122][  100/  217]    Overall Loss 0.001596    Objective Loss 0.001596                                        LR 0.000250    Time 0.272169    
2024-05-04 01:17:18,709 - Epoch: [122][  200/  217]    Overall Loss 0.001653    Objective Loss 0.001653                                        LR 0.000250    Time 0.260976    
2024-05-04 01:17:22,488 - Epoch: [122][  217/  217]    Overall Loss 0.001609    Objective Loss 0.001609    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.257939    
2024-05-04 01:17:23,153 - 

2024-05-04 01:17:23,154 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:17:53,987 - Epoch: [123][  100/  217]    Overall Loss 0.001381    Objective Loss 0.001381                                        LR 0.000250    Time 0.308217    
2024-05-04 01:18:16,390 - Epoch: [123][  200/  217]    Overall Loss 0.001642    Objective Loss 0.001642                                        LR 0.000250    Time 0.266080    
2024-05-04 01:18:21,347 - Epoch: [123][  217/  217]    Overall Loss 0.001586    Objective Loss 0.001586    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.268067    
2024-05-04 01:18:21,641 - 

2024-05-04 01:18:21,641 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:18:48,586 - Epoch: [124][  100/  217]    Overall Loss 0.001996    Objective Loss 0.001996                                        LR 0.000250    Time 0.269357    
2024-05-04 01:19:13,860 - Epoch: [124][  200/  217]    Overall Loss 0.001723    Objective Loss 0.001723                                        LR 0.000250    Time 0.260993    
2024-05-04 01:19:18,986 - Epoch: [124][  217/  217]    Overall Loss 0.001651    Objective Loss 0.001651    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.264159    
2024-05-04 01:19:19,267 - 

2024-05-04 01:19:19,268 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:19:50,916 - Epoch: [125][  100/  217]    Overall Loss 0.002104    Objective Loss 0.002104                                        LR 0.000250    Time 0.316405    
2024-05-04 01:20:18,216 - Epoch: [125][  200/  217]    Overall Loss 0.001491    Objective Loss 0.001491                                        LR 0.000250    Time 0.294649    
2024-05-04 01:20:22,969 - Epoch: [125][  217/  217]    Overall Loss 0.001449    Objective Loss 0.001449    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.293459    
2024-05-04 01:20:23,392 - 

2024-05-04 01:20:23,392 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:20:54,259 - Epoch: [126][  100/  217]    Overall Loss 0.001479    Objective Loss 0.001479                                        LR 0.000250    Time 0.308530    
2024-05-04 01:21:16,726 - Epoch: [126][  200/  217]    Overall Loss 0.001384    Objective Loss 0.001384                                        LR 0.000250    Time 0.266556    
2024-05-04 01:21:20,635 - Epoch: [126][  217/  217]    Overall Loss 0.001488    Objective Loss 0.001488    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.263678    
2024-05-04 01:21:21,064 - 

2024-05-04 01:21:21,065 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:21:52,385 - Epoch: [127][  100/  217]    Overall Loss 0.001496    Objective Loss 0.001496                                        LR 0.000250    Time 0.313098    
2024-05-04 01:22:16,425 - Epoch: [127][  200/  217]    Overall Loss 0.001392    Objective Loss 0.001392                                        LR 0.000250    Time 0.276684    
2024-05-04 01:22:19,902 - Epoch: [127][  217/  217]    Overall Loss 0.001333    Objective Loss 0.001333    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.271020    
2024-05-04 01:22:20,189 - 

2024-05-04 01:22:20,190 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:22:50,708 - Epoch: [128][  100/  217]    Overall Loss 0.001501    Objective Loss 0.001501                                        LR 0.000250    Time 0.305088    
2024-05-04 01:23:15,598 - Epoch: [128][  200/  217]    Overall Loss 0.001396    Objective Loss 0.001396                                        LR 0.000250    Time 0.276952    
2024-05-04 01:23:20,562 - Epoch: [128][  217/  217]    Overall Loss 0.001587    Objective Loss 0.001587    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.278121    
2024-05-04 01:23:20,831 - 

2024-05-04 01:23:20,832 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:23:50,833 - Epoch: [129][  100/  217]    Overall Loss 0.002865    Objective Loss 0.002865                                        LR 0.000250    Time 0.299916    
2024-05-04 01:24:16,820 - Epoch: [129][  200/  217]    Overall Loss 0.002536    Objective Loss 0.002536                                        LR 0.000250    Time 0.279846    
2024-05-04 01:24:20,943 - Epoch: [129][  217/  217]    Overall Loss 0.002583    Objective Loss 0.002583    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.276914    
2024-05-04 01:24:21,252 - 

2024-05-04 01:24:21,253 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:24:53,324 - Epoch: [130][  100/  217]    Overall Loss 0.001897    Objective Loss 0.001897                                        LR 0.000250    Time 0.320631    
2024-05-04 01:25:17,643 - Epoch: [130][  200/  217]    Overall Loss 0.003761    Objective Loss 0.003761                                        LR 0.000250    Time 0.281866    
2024-05-04 01:25:22,424 - Epoch: [130][  217/  217]    Overall Loss 0.003811    Objective Loss 0.003811    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.281808    
2024-05-04 01:25:22,883 - --- validate (epoch=130)-----------
2024-05-04 01:25:22,884 - 1736 samples (32 per mini-batch)
2024-05-04 01:25:40,715 - Epoch: [130][   55/   55]    Loss 3.046748    Top1 55.760369    Top5 72.811060    
2024-05-04 01:25:40,895 - ==> Top1: 55.760    Top5: 72.811    Loss: 3.047

2024-05-04 01:25:40,901 - ==> Best [Top1: 56.567   Top5: 72.811   Sparsity:0.00   Params: 381920 on epoch: 70]
2024-05-04 01:25:40,901 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 01:25:40,944 - 

2024-05-04 01:25:40,944 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:26:09,405 - Epoch: [131][  100/  217]    Overall Loss 0.001865    Objective Loss 0.001865                                        LR 0.000250    Time 0.284504    
2024-05-04 01:26:35,730 - Epoch: [131][  200/  217]    Overall Loss 0.001927    Objective Loss 0.001927                                        LR 0.000250    Time 0.273830    
2024-05-04 01:26:41,020 - Epoch: [131][  217/  217]    Overall Loss 0.002033    Objective Loss 0.002033    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.276744    
2024-05-04 01:26:41,431 - 

2024-05-04 01:26:41,432 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:27:09,947 - Epoch: [132][  100/  217]    Overall Loss 0.002421    Objective Loss 0.002421                                        LR 0.000250    Time 0.285061    
2024-05-04 01:27:38,033 - Epoch: [132][  200/  217]    Overall Loss 0.002772    Objective Loss 0.002772                                        LR 0.000250    Time 0.282908    
2024-05-04 01:27:40,689 - Epoch: [132][  217/  217]    Overall Loss 0.002653    Objective Loss 0.002653    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.272979    
2024-05-04 01:27:41,301 - 

2024-05-04 01:27:41,302 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:28:13,369 - Epoch: [133][  100/  217]    Overall Loss 0.001711    Objective Loss 0.001711                                        LR 0.000250    Time 0.320581    
2024-05-04 01:28:38,948 - Epoch: [133][  200/  217]    Overall Loss 0.001567    Objective Loss 0.001567                                        LR 0.000250    Time 0.288143    
2024-05-04 01:28:42,655 - Epoch: [133][  217/  217]    Overall Loss 0.001684    Objective Loss 0.001684    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.282639    
2024-05-04 01:28:42,984 - 

2024-05-04 01:28:42,985 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:29:12,364 - Epoch: [134][  100/  217]    Overall Loss 0.001789    Objective Loss 0.001789                                        LR 0.000250    Time 0.293675    
2024-05-04 01:29:40,468 - Epoch: [134][  200/  217]    Overall Loss 0.001323    Objective Loss 0.001323                                        LR 0.000250    Time 0.287315    
2024-05-04 01:29:44,120 - Epoch: [134][  217/  217]    Overall Loss 0.001424    Objective Loss 0.001424    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.281630    
2024-05-04 01:29:44,342 - 

2024-05-04 01:29:44,343 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:30:14,573 - Epoch: [135][  100/  217]    Overall Loss 0.001139    Objective Loss 0.001139                                        LR 0.000250    Time 0.302211    
2024-05-04 01:30:40,878 - Epoch: [135][  200/  217]    Overall Loss 0.001001    Objective Loss 0.001001                                        LR 0.000250    Time 0.282588    
2024-05-04 01:30:45,277 - Epoch: [135][  217/  217]    Overall Loss 0.001110    Objective Loss 0.001110    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.280708    
2024-05-04 01:30:45,522 - 

2024-05-04 01:30:45,522 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:31:14,249 - Epoch: [136][  100/  217]    Overall Loss 0.001122    Objective Loss 0.001122                                        LR 0.000250    Time 0.287182    
2024-05-04 01:31:39,693 - Epoch: [136][  200/  217]    Overall Loss 0.001142    Objective Loss 0.001142                                        LR 0.000250    Time 0.270763    
2024-05-04 01:31:43,151 - Epoch: [136][  217/  217]    Overall Loss 0.001092    Objective Loss 0.001092    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.265477    
2024-05-04 01:31:44,065 - 

2024-05-04 01:31:44,066 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:32:13,709 - Epoch: [137][  100/  217]    Overall Loss 0.000829    Objective Loss 0.000829                                        LR 0.000250    Time 0.296324    
2024-05-04 01:32:37,438 - Epoch: [137][  200/  217]    Overall Loss 0.001076    Objective Loss 0.001076                                        LR 0.000250    Time 0.266766    
2024-05-04 01:32:42,643 - Epoch: [137][  217/  217]    Overall Loss 0.001054    Objective Loss 0.001054    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.269848    
2024-05-04 01:32:42,985 - 

2024-05-04 01:32:42,985 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:33:15,021 - Epoch: [138][  100/  217]    Overall Loss 0.001117    Objective Loss 0.001117                                        LR 0.000250    Time 0.320259    
2024-05-04 01:33:40,121 - Epoch: [138][  200/  217]    Overall Loss 0.001005    Objective Loss 0.001005                                        LR 0.000250    Time 0.285580    
2024-05-04 01:33:43,870 - Epoch: [138][  217/  217]    Overall Loss 0.000963    Objective Loss 0.000963    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.280476    
2024-05-04 01:33:44,618 - 

2024-05-04 01:33:44,618 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:34:11,518 - Epoch: [139][  100/  217]    Overall Loss 0.001207    Objective Loss 0.001207                                        LR 0.000250    Time 0.268906    
2024-05-04 01:34:38,957 - Epoch: [139][  200/  217]    Overall Loss 0.001029    Objective Loss 0.001029                                        LR 0.000250    Time 0.271606    
2024-05-04 01:34:44,461 - Epoch: [139][  217/  217]    Overall Loss 0.000989    Objective Loss 0.000989    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.275685    
2024-05-04 01:34:44,673 - 

2024-05-04 01:34:44,674 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:35:13,898 - Epoch: [140][  100/  217]    Overall Loss 0.001453    Objective Loss 0.001453                                        LR 0.000250    Time 0.292141    
2024-05-04 01:35:38,878 - Epoch: [140][  200/  217]    Overall Loss 0.001018    Objective Loss 0.001018                                        LR 0.000250    Time 0.270930    
2024-05-04 01:35:43,233 - Epoch: [140][  217/  217]    Overall Loss 0.000973    Objective Loss 0.000973    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.269762    
2024-05-04 01:35:43,495 - --- validate (epoch=140)-----------
2024-05-04 01:35:43,496 - 1736 samples (32 per mini-batch)
2024-05-04 01:36:01,384 - Epoch: [140][   55/   55]    Loss 3.121439    Top1 56.566820    Top5 73.329493    
2024-05-04 01:36:01,719 - ==> Top1: 56.567    Top5: 73.329    Loss: 3.121

2024-05-04 01:36:01,723 - ==> Best [Top1: 56.567   Top5: 73.329   Sparsity:0.00   Params: 381920 on epoch: 140]
2024-05-04 01:36:01,723 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 01:36:01,786 - 

2024-05-04 01:36:01,788 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:36:31,051 - Epoch: [141][  100/  217]    Overall Loss 0.000973    Objective Loss 0.000973                                        LR 0.000250    Time 0.292517    
2024-05-04 01:36:58,640 - Epoch: [141][  200/  217]    Overall Loss 0.001037    Objective Loss 0.001037                                        LR 0.000250    Time 0.284160    
2024-05-04 01:37:02,132 - Epoch: [141][  217/  217]    Overall Loss 0.000989    Objective Loss 0.000989    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.277981    
2024-05-04 01:37:02,422 - 

2024-05-04 01:37:02,422 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:37:29,409 - Epoch: [142][  100/  217]    Overall Loss 0.000942    Objective Loss 0.000942                                        LR 0.000250    Time 0.269774    
2024-05-04 01:37:54,608 - Epoch: [142][  200/  217]    Overall Loss 0.001055    Objective Loss 0.001055                                        LR 0.000250    Time 0.260840    
2024-05-04 01:37:57,970 - Epoch: [142][  217/  217]    Overall Loss 0.001013    Objective Loss 0.001013    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.255889    
2024-05-04 01:37:58,197 - 

2024-05-04 01:37:58,198 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:38:26,675 - Epoch: [143][  100/  217]    Overall Loss 0.000585    Objective Loss 0.000585                                        LR 0.000250    Time 0.284685    
2024-05-04 01:38:51,045 - Epoch: [143][  200/  217]    Overall Loss 0.001668    Objective Loss 0.001668                                        LR 0.000250    Time 0.264152    
2024-05-04 01:38:55,892 - Epoch: [143][  217/  217]    Overall Loss 0.001799    Objective Loss 0.001799    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.265784    
2024-05-04 01:38:56,147 - 

2024-05-04 01:38:56,148 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:39:25,168 - Epoch: [144][  100/  217]    Overall Loss 0.004754    Objective Loss 0.004754                                        LR 0.000250    Time 0.290111    
2024-05-04 01:39:51,315 - Epoch: [144][  200/  217]    Overall Loss 0.009263    Objective Loss 0.009263                                        LR 0.000250    Time 0.275751    
2024-05-04 01:39:55,841 - Epoch: [144][  217/  217]    Overall Loss 0.009348    Objective Loss 0.009348    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.274992    
2024-05-04 01:39:56,099 - 

2024-05-04 01:39:56,100 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:40:25,601 - Epoch: [145][  100/  217]    Overall Loss 0.011832    Objective Loss 0.011832                                        LR 0.000250    Time 0.294912    
2024-05-04 01:40:51,714 - Epoch: [145][  200/  217]    Overall Loss 0.009182    Objective Loss 0.009182                                        LR 0.000250    Time 0.277980    
2024-05-04 01:40:55,771 - Epoch: [145][  217/  217]    Overall Loss 0.008836    Objective Loss 0.008836    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.274891    
2024-05-04 01:40:56,007 - 

2024-05-04 01:40:56,008 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:41:25,849 - Epoch: [146][  100/  217]    Overall Loss 0.003821    Objective Loss 0.003821                                        LR 0.000250    Time 0.298310    
2024-05-04 01:41:51,254 - Epoch: [146][  200/  217]    Overall Loss 0.002796    Objective Loss 0.002796                                        LR 0.000250    Time 0.276138    
2024-05-04 01:41:54,867 - Epoch: [146][  217/  217]    Overall Loss 0.002740    Objective Loss 0.002740    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.271145    
2024-05-04 01:41:55,241 - 

2024-05-04 01:41:55,243 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:42:24,883 - Epoch: [147][  100/  217]    Overall Loss 0.001870    Objective Loss 0.001870                                        LR 0.000250    Time 0.296285    
2024-05-04 01:42:49,857 - Epoch: [147][  200/  217]    Overall Loss 0.001520    Objective Loss 0.001520                                        LR 0.000250    Time 0.272974    
2024-05-04 01:42:53,653 - Epoch: [147][  217/  217]    Overall Loss 0.001637    Objective Loss 0.001637    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.269072    
2024-05-04 01:42:53,935 - 

2024-05-04 01:42:53,935 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:43:20,623 - Epoch: [148][  100/  217]    Overall Loss 0.001115    Objective Loss 0.001115                                        LR 0.000250    Time 0.266769    
2024-05-04 01:43:45,875 - Epoch: [148][  200/  217]    Overall Loss 0.001313    Objective Loss 0.001313                                        LR 0.000250    Time 0.259602    
2024-05-04 01:43:51,438 - Epoch: [148][  217/  217]    Overall Loss 0.001385    Objective Loss 0.001385    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.264889    
2024-05-04 01:43:51,866 - 

2024-05-04 01:43:51,866 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:44:17,391 - Epoch: [149][  100/  217]    Overall Loss 0.001134    Objective Loss 0.001134                                        LR 0.000250    Time 0.255162    
2024-05-04 01:44:41,141 - Epoch: [149][  200/  217]    Overall Loss 0.001381    Objective Loss 0.001381                                        LR 0.000250    Time 0.246288    
2024-05-04 01:44:45,212 - Epoch: [149][  217/  217]    Overall Loss 0.001319    Objective Loss 0.001319    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.245745    
2024-05-04 01:44:45,529 - 

2024-05-04 01:44:45,530 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:45:13,304 - Epoch: [150][  100/  217]    Overall Loss 0.001020    Objective Loss 0.001020                                        LR 0.000063    Time 0.277655    
2024-05-04 01:45:38,428 - Epoch: [150][  200/  217]    Overall Loss 0.001072    Objective Loss 0.001072                                        LR 0.000063    Time 0.264405    
2024-05-04 01:45:43,108 - Epoch: [150][  217/  217]    Overall Loss 0.001051    Objective Loss 0.001051    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.265251    
2024-05-04 01:45:43,427 - --- validate (epoch=150)-----------
2024-05-04 01:45:43,427 - 1736 samples (32 per mini-batch)
2024-05-04 01:46:00,531 - Epoch: [150][   55/   55]    Loss 3.149564    Top1 56.394009    Top5 72.983871    
2024-05-04 01:46:00,825 - ==> Top1: 56.394    Top5: 72.984    Loss: 3.150

2024-05-04 01:46:00,829 - ==> Best [Top1: 56.567   Top5: 73.329   Sparsity:0.00   Params: 381920 on epoch: 140]
2024-05-04 01:46:00,829 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 01:46:00,882 - 

2024-05-04 01:46:00,883 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:46:30,046 - Epoch: [151][  100/  217]    Overall Loss 0.000905    Objective Loss 0.000905                                        LR 0.000063    Time 0.291517    
2024-05-04 01:46:59,180 - Epoch: [151][  200/  217]    Overall Loss 0.000900    Objective Loss 0.000900                                        LR 0.000063    Time 0.291384    
2024-05-04 01:47:02,513 - Epoch: [151][  217/  217]    Overall Loss 0.001097    Objective Loss 0.001097    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.283908    
2024-05-04 01:47:02,913 - 

2024-05-04 01:47:02,914 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:47:34,093 - Epoch: [152][  100/  217]    Overall Loss 0.001069    Objective Loss 0.001069                                        LR 0.000063    Time 0.311684    
2024-05-04 01:48:02,201 - Epoch: [152][  200/  217]    Overall Loss 0.000948    Objective Loss 0.000948                                        LR 0.000063    Time 0.296337    
2024-05-04 01:48:06,663 - Epoch: [152][  217/  217]    Overall Loss 0.001016    Objective Loss 0.001016    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.293673    
2024-05-04 01:48:06,982 - 

2024-05-04 01:48:06,982 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:48:35,013 - Epoch: [153][  100/  217]    Overall Loss 0.000967    Objective Loss 0.000967                                        LR 0.000063    Time 0.280206    
2024-05-04 01:48:58,798 - Epoch: [153][  200/  217]    Overall Loss 0.001193    Objective Loss 0.001193                                        LR 0.000063    Time 0.258982    
2024-05-04 01:49:04,780 - Epoch: [153][  217/  217]    Overall Loss 0.001146    Objective Loss 0.001146    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.266250    
2024-05-04 01:49:05,125 - 

2024-05-04 01:49:05,126 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:49:34,858 - Epoch: [154][  100/  217]    Overall Loss 0.001264    Objective Loss 0.001264                                        LR 0.000063    Time 0.297225    
2024-05-04 01:49:59,643 - Epoch: [154][  200/  217]    Overall Loss 0.001069    Objective Loss 0.001069                                        LR 0.000063    Time 0.272498    
2024-05-04 01:50:02,770 - Epoch: [154][  217/  217]    Overall Loss 0.001050    Objective Loss 0.001050    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.265553    
2024-05-04 01:50:03,029 - 

2024-05-04 01:50:03,030 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:50:34,574 - Epoch: [155][  100/  217]    Overall Loss 0.000776    Objective Loss 0.000776                                        LR 0.000063    Time 0.315346    
2024-05-04 01:51:01,350 - Epoch: [155][  200/  217]    Overall Loss 0.000875    Objective Loss 0.000875                                        LR 0.000063    Time 0.291515    
2024-05-04 01:51:04,332 - Epoch: [155][  217/  217]    Overall Loss 0.000950    Objective Loss 0.000950    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.282405    
2024-05-04 01:51:04,617 - 

2024-05-04 01:51:04,618 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:51:32,419 - Epoch: [156][  100/  217]    Overall Loss 0.000521    Objective Loss 0.000521                                        LR 0.000063    Time 0.277897    
2024-05-04 01:52:01,128 - Epoch: [156][  200/  217]    Overall Loss 0.001012    Objective Loss 0.001012                                        LR 0.000063    Time 0.282449    
2024-05-04 01:52:03,634 - Epoch: [156][  217/  217]    Overall Loss 0.000978    Objective Loss 0.000978    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.271858    
2024-05-04 01:52:03,851 - 

2024-05-04 01:52:03,852 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:52:30,335 - Epoch: [157][  100/  217]    Overall Loss 0.001061    Objective Loss 0.001061                                        LR 0.000063    Time 0.264744    
2024-05-04 01:52:58,631 - Epoch: [157][  200/  217]    Overall Loss 0.001021    Objective Loss 0.001021                                        LR 0.000063    Time 0.273807    
2024-05-04 01:53:03,061 - Epoch: [157][  217/  217]    Overall Loss 0.000982    Objective Loss 0.000982    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.272761    
2024-05-04 01:53:03,318 - 

2024-05-04 01:53:03,318 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:53:30,882 - Epoch: [158][  100/  217]    Overall Loss 0.000520    Objective Loss 0.000520                                        LR 0.000063    Time 0.275526    
2024-05-04 01:53:59,328 - Epoch: [158][  200/  217]    Overall Loss 0.000839    Objective Loss 0.000839                                        LR 0.000063    Time 0.279945    
2024-05-04 01:54:03,727 - Epoch: [158][  217/  217]    Overall Loss 0.000921    Objective Loss 0.000921    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.278280    
2024-05-04 01:54:04,172 - 

2024-05-04 01:54:04,173 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:54:33,035 - Epoch: [159][  100/  217]    Overall Loss 0.000900    Objective Loss 0.000900                                        LR 0.000063    Time 0.288529    
2024-05-04 01:55:00,447 - Epoch: [159][  200/  217]    Overall Loss 0.000954    Objective Loss 0.000954                                        LR 0.000063    Time 0.281281    
2024-05-04 01:55:04,668 - Epoch: [159][  217/  217]    Overall Loss 0.000918    Objective Loss 0.000918    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.278686    
2024-05-04 01:55:04,942 - 

2024-05-04 01:55:04,942 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:55:35,344 - Epoch: [160][  100/  217]    Overall Loss 0.001202    Objective Loss 0.001202                                        LR 0.000063    Time 0.303913    
2024-05-04 01:56:01,258 - Epoch: [160][  200/  217]    Overall Loss 0.000965    Objective Loss 0.000965                                        LR 0.000063    Time 0.281483    
2024-05-04 01:56:06,236 - Epoch: [160][  217/  217]    Overall Loss 0.000928    Objective Loss 0.000928    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.282361    
2024-05-04 01:56:06,789 - --- validate (epoch=160)-----------
2024-05-04 01:56:06,790 - 1736 samples (32 per mini-batch)
2024-05-04 01:56:23,879 - Epoch: [160][   55/   55]    Loss 3.128914    Top1 56.624424    Top5 72.811060    
2024-05-04 01:56:24,402 - ==> Top1: 56.624    Top5: 72.811    Loss: 3.129

2024-05-04 01:56:24,407 - ==> Best [Top1: 56.624   Top5: 72.811   Sparsity:0.00   Params: 381920 on epoch: 160]
2024-05-04 01:56:24,408 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 01:56:24,462 - 

2024-05-04 01:56:24,462 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:56:59,585 - Epoch: [161][  100/  217]    Overall Loss 0.000883    Objective Loss 0.000883                                        LR 0.000063    Time 0.351116    
2024-05-04 01:57:26,097 - Epoch: [161][  200/  217]    Overall Loss 0.000931    Objective Loss 0.000931                                        LR 0.000063    Time 0.308073    
2024-05-04 01:57:29,737 - Epoch: [161][  217/  217]    Overall Loss 0.000891    Objective Loss 0.000891    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.300707    
2024-05-04 01:57:30,322 - 

2024-05-04 01:57:30,323 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:58:01,226 - Epoch: [162][  100/  217]    Overall Loss 0.000609    Objective Loss 0.000609                                        LR 0.000063    Time 0.308929    
2024-05-04 01:58:26,146 - Epoch: [162][  200/  217]    Overall Loss 0.000917    Objective Loss 0.000917                                        LR 0.000063    Time 0.279017    
2024-05-04 01:58:30,356 - Epoch: [162][  217/  217]    Overall Loss 0.000881    Objective Loss 0.000881    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.276523    
2024-05-04 01:58:30,647 - 

2024-05-04 01:58:30,648 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:58:57,470 - Epoch: [163][  100/  217]    Overall Loss 0.000851    Objective Loss 0.000851                                        LR 0.000063    Time 0.268126    
2024-05-04 01:59:22,463 - Epoch: [163][  200/  217]    Overall Loss 0.000867    Objective Loss 0.000867                                        LR 0.000063    Time 0.258985    
2024-05-04 01:59:27,220 - Epoch: [163][  217/  217]    Overall Loss 0.000823    Objective Loss 0.000823    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.260606    
2024-05-04 01:59:27,559 - 

2024-05-04 01:59:27,560 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 01:59:56,486 - Epoch: [164][  100/  217]    Overall Loss 0.000884    Objective Loss 0.000884                                        LR 0.000063    Time 0.289169    
2024-05-04 02:00:21,028 - Epoch: [164][  200/  217]    Overall Loss 0.000762    Objective Loss 0.000762                                        LR 0.000063    Time 0.267253    
2024-05-04 02:00:24,428 - Epoch: [164][  217/  217]    Overall Loss 0.000832    Objective Loss 0.000832    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.261978    
2024-05-04 02:00:24,663 - 

2024-05-04 02:00:24,663 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:00:54,025 - Epoch: [165][  100/  217]    Overall Loss 0.000772    Objective Loss 0.000772                                        LR 0.000063    Time 0.293529    
2024-05-04 02:01:16,155 - Epoch: [165][  200/  217]    Overall Loss 0.000821    Objective Loss 0.000821                                        LR 0.000063    Time 0.257375    
2024-05-04 02:01:18,872 - Epoch: [165][  217/  217]    Overall Loss 0.000789    Objective Loss 0.000789    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.249725    
2024-05-04 02:01:19,121 - 

2024-05-04 02:01:19,122 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:01:48,969 - Epoch: [166][  100/  217]    Overall Loss 0.000830    Objective Loss 0.000830                                        LR 0.000063    Time 0.298371    
2024-05-04 02:02:13,741 - Epoch: [166][  200/  217]    Overall Loss 0.000814    Objective Loss 0.000814                                        LR 0.000063    Time 0.273006    
2024-05-04 02:02:18,478 - Epoch: [166][  217/  217]    Overall Loss 0.000783    Objective Loss 0.000783    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.273436    
2024-05-04 02:02:18,832 - 

2024-05-04 02:02:18,832 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:02:47,501 - Epoch: [167][  100/  217]    Overall Loss 0.000335    Objective Loss 0.000335                                        LR 0.000063    Time 0.286611    
2024-05-04 02:03:14,165 - Epoch: [167][  200/  217]    Overall Loss 0.000685    Objective Loss 0.000685                                        LR 0.000063    Time 0.276577    
2024-05-04 02:03:17,283 - Epoch: [167][  217/  217]    Overall Loss 0.000775    Objective Loss 0.000775    Top1 98.360656    Top5 100.000000    LR 0.000063    Time 0.269270    
2024-05-04 02:03:17,624 - 

2024-05-04 02:03:17,624 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:03:47,434 - Epoch: [168][  100/  217]    Overall Loss 0.000298    Objective Loss 0.000298                                        LR 0.000063    Time 0.298005    
2024-05-04 02:04:15,187 - Epoch: [168][  200/  217]    Overall Loss 0.000776    Objective Loss 0.000776                                        LR 0.000063    Time 0.287734    
2024-05-04 02:04:19,848 - Epoch: [168][  217/  217]    Overall Loss 0.000742    Objective Loss 0.000742    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.286663    
2024-05-04 02:04:20,148 - 

2024-05-04 02:04:20,149 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:04:50,219 - Epoch: [169][  100/  217]    Overall Loss 0.001061    Objective Loss 0.001061                                        LR 0.000063    Time 0.300616    
2024-05-04 02:05:13,236 - Epoch: [169][  200/  217]    Overall Loss 0.000794    Objective Loss 0.000794                                        LR 0.000063    Time 0.265349    
2024-05-04 02:05:16,756 - Epoch: [169][  217/  217]    Overall Loss 0.000753    Objective Loss 0.000753    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.260776    
2024-05-04 02:05:17,121 - 

2024-05-04 02:05:17,122 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:05:49,036 - Epoch: [170][  100/  217]    Overall Loss 0.000952    Objective Loss 0.000952                                        LR 0.000063    Time 0.319044    
2024-05-04 02:06:17,706 - Epoch: [170][  200/  217]    Overall Loss 0.000801    Objective Loss 0.000801                                        LR 0.000063    Time 0.302798    
2024-05-04 02:06:22,758 - Epoch: [170][  217/  217]    Overall Loss 0.000759    Objective Loss 0.000759    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.302348    
2024-05-04 02:06:23,193 - --- validate (epoch=170)-----------
2024-05-04 02:06:23,194 - 1736 samples (32 per mini-batch)
2024-05-04 02:06:41,063 - Epoch: [170][   55/   55]    Loss 3.171661    Top1 56.854839    Top5 72.523041    
2024-05-04 02:06:41,663 - ==> Top1: 56.855    Top5: 72.523    Loss: 3.172

2024-05-04 02:06:41,670 - ==> Best [Top1: 56.855   Top5: 72.523   Sparsity:0.00   Params: 381920 on epoch: 170]
2024-05-04 02:06:41,670 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 02:06:41,721 - 

2024-05-04 02:06:41,722 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:07:12,156 - Epoch: [171][  100/  217]    Overall Loss 0.000520    Objective Loss 0.000520                                        LR 0.000063    Time 0.304231    
2024-05-04 02:07:38,989 - Epoch: [171][  200/  217]    Overall Loss 0.000620    Objective Loss 0.000620                                        LR 0.000063    Time 0.286235    
2024-05-04 02:07:42,849 - Epoch: [171][  217/  217]    Overall Loss 0.000708    Objective Loss 0.000708    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.281586    
2024-05-04 02:07:43,206 - 

2024-05-04 02:07:43,207 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:08:14,663 - Epoch: [172][  100/  217]    Overall Loss 0.000670    Objective Loss 0.000670                                        LR 0.000063    Time 0.314461    
2024-05-04 02:08:42,061 - Epoch: [172][  200/  217]    Overall Loss 0.000732    Objective Loss 0.000732                                        LR 0.000063    Time 0.294177    
2024-05-04 02:08:47,207 - Epoch: [172][  217/  217]    Overall Loss 0.000694    Objective Loss 0.000694    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.294835    
2024-05-04 02:08:47,857 - 

2024-05-04 02:08:47,858 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:09:15,741 - Epoch: [173][  100/  217]    Overall Loss 0.000973    Objective Loss 0.000973                                        LR 0.000063    Time 0.278729    
2024-05-04 02:09:36,751 - Epoch: [173][  200/  217]    Overall Loss 0.000793    Objective Loss 0.000793                                        LR 0.000063    Time 0.244372    
2024-05-04 02:09:40,514 - Epoch: [173][  217/  217]    Overall Loss 0.000745    Objective Loss 0.000745    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.242558    
2024-05-04 02:09:40,950 - 

2024-05-04 02:09:40,952 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:10:09,560 - Epoch: [174][  100/  217]    Overall Loss 0.000749    Objective Loss 0.000749                                        LR 0.000063    Time 0.285973    
2024-05-04 02:10:36,504 - Epoch: [174][  200/  217]    Overall Loss 0.000653    Objective Loss 0.000653                                        LR 0.000063    Time 0.277655    
2024-05-04 02:10:39,158 - Epoch: [174][  217/  217]    Overall Loss 0.000737    Objective Loss 0.000737    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.268117    
2024-05-04 02:10:39,517 - 

2024-05-04 02:10:39,518 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:11:11,437 - Epoch: [175][  100/  217]    Overall Loss 0.000965    Objective Loss 0.000965                                        LR 0.000063    Time 0.319093    
2024-05-04 02:11:38,118 - Epoch: [175][  200/  217]    Overall Loss 0.000737    Objective Loss 0.000737                                        LR 0.000063    Time 0.292913    
2024-05-04 02:11:43,132 - Epoch: [175][  217/  217]    Overall Loss 0.000697    Objective Loss 0.000697    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.293061    
2024-05-04 02:11:43,947 - 

2024-05-04 02:11:43,948 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:12:13,981 - Epoch: [176][  100/  217]    Overall Loss 0.000478    Objective Loss 0.000478                                        LR 0.000063    Time 0.300248    
2024-05-04 02:12:35,853 - Epoch: [176][  200/  217]    Overall Loss 0.000445    Objective Loss 0.000445                                        LR 0.000063    Time 0.259442    
2024-05-04 02:12:39,886 - Epoch: [176][  217/  217]    Overall Loss 0.000693    Objective Loss 0.000693    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.257697    
2024-05-04 02:12:40,106 - 

2024-05-04 02:12:40,107 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:13:09,139 - Epoch: [177][  100/  217]    Overall Loss 0.000425    Objective Loss 0.000425                                        LR 0.000063    Time 0.290219    
2024-05-04 02:13:34,976 - Epoch: [177][  200/  217]    Overall Loss 0.000610    Objective Loss 0.000610                                        LR 0.000063    Time 0.274251    
2024-05-04 02:13:39,864 - Epoch: [177][  217/  217]    Overall Loss 0.000682    Objective Loss 0.000682    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.275283    
2024-05-04 02:13:40,222 - 

2024-05-04 02:13:40,223 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:14:08,716 - Epoch: [178][  100/  217]    Overall Loss 0.000675    Objective Loss 0.000675                                        LR 0.000063    Time 0.284843    
2024-05-04 02:14:34,374 - Epoch: [178][  200/  217]    Overall Loss 0.000709    Objective Loss 0.000709                                        LR 0.000063    Time 0.270669    
2024-05-04 02:14:38,112 - Epoch: [178][  217/  217]    Overall Loss 0.000672    Objective Loss 0.000672    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.266682    
2024-05-04 02:14:38,861 - 

2024-05-04 02:14:38,862 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:15:10,157 - Epoch: [179][  100/  217]    Overall Loss 0.000799    Objective Loss 0.000799                                        LR 0.000063    Time 0.312871    
2024-05-04 02:15:36,621 - Epoch: [179][  200/  217]    Overall Loss 0.000737    Objective Loss 0.000737                                        LR 0.000063    Time 0.288713    
2024-05-04 02:15:41,118 - Epoch: [179][  217/  217]    Overall Loss 0.000698    Objective Loss 0.000698    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.286805    
2024-05-04 02:15:41,692 - 

2024-05-04 02:15:41,693 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:16:11,577 - Epoch: [180][  100/  217]    Overall Loss 0.000914    Objective Loss 0.000914                                        LR 0.000063    Time 0.298741    
2024-05-04 02:16:38,184 - Epoch: [180][  200/  217]    Overall Loss 0.000563    Objective Loss 0.000563                                        LR 0.000063    Time 0.282365    
2024-05-04 02:16:42,206 - Epoch: [180][  217/  217]    Overall Loss 0.000661    Objective Loss 0.000661    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.278770    
2024-05-04 02:16:43,170 - --- validate (epoch=180)-----------
2024-05-04 02:16:43,171 - 1736 samples (32 per mini-batch)
2024-05-04 02:17:00,692 - Epoch: [180][   55/   55]    Loss 3.231048    Top1 56.970046    Top5 73.099078    
2024-05-04 02:17:01,266 - ==> Top1: 56.970    Top5: 73.099    Loss: 3.231

2024-05-04 02:17:01,271 - ==> Best [Top1: 56.970   Top5: 73.099   Sparsity:0.00   Params: 381920 on epoch: 180]
2024-05-04 02:17:01,272 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 02:17:01,327 - 

2024-05-04 02:17:01,327 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:17:29,019 - Epoch: [181][  100/  217]    Overall Loss 0.000852    Objective Loss 0.000852                                        LR 0.000063    Time 0.276833    
2024-05-04 02:17:54,536 - Epoch: [181][  200/  217]    Overall Loss 0.000641    Objective Loss 0.000641                                        LR 0.000063    Time 0.265967    
2024-05-04 02:17:58,138 - Epoch: [181][  217/  217]    Overall Loss 0.000607    Objective Loss 0.000607    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.261688    
2024-05-04 02:17:58,678 - 

2024-05-04 02:17:58,679 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:18:28,431 - Epoch: [182][  100/  217]    Overall Loss 0.001078    Objective Loss 0.001078                                        LR 0.000063    Time 0.297424    
2024-05-04 02:18:54,075 - Epoch: [182][  200/  217]    Overall Loss 0.000772    Objective Loss 0.000772                                        LR 0.000063    Time 0.276880    
2024-05-04 02:18:58,431 - Epoch: [182][  217/  217]    Overall Loss 0.000728    Objective Loss 0.000728    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.275250    
2024-05-04 02:18:59,221 - 

2024-05-04 02:18:59,221 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:19:27,258 - Epoch: [183][  100/  217]    Overall Loss 0.000350    Objective Loss 0.000350                                        LR 0.000063    Time 0.280273    
2024-05-04 02:19:53,416 - Epoch: [183][  200/  217]    Overall Loss 0.000986    Objective Loss 0.000986                                        LR 0.000063    Time 0.270884    
2024-05-04 02:19:56,294 - Epoch: [183][  217/  217]    Overall Loss 0.000925    Objective Loss 0.000925    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.262918    
2024-05-04 02:19:56,501 - 

2024-05-04 02:19:56,502 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:20:25,885 - Epoch: [184][  100/  217]    Overall Loss 0.000400    Objective Loss 0.000400                                        LR 0.000063    Time 0.293749    
2024-05-04 02:20:52,925 - Epoch: [184][  200/  217]    Overall Loss 0.000754    Objective Loss 0.000754                                        LR 0.000063    Time 0.282030    
2024-05-04 02:20:56,323 - Epoch: [184][  217/  217]    Overall Loss 0.000710    Objective Loss 0.000710    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.275585    
2024-05-04 02:20:56,616 - 

2024-05-04 02:20:56,618 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:21:25,988 - Epoch: [185][  100/  217]    Overall Loss 0.000394    Objective Loss 0.000394                                        LR 0.000063    Time 0.293603    
2024-05-04 02:21:51,319 - Epoch: [185][  200/  217]    Overall Loss 0.000627    Objective Loss 0.000627                                        LR 0.000063    Time 0.273412    
2024-05-04 02:21:54,265 - Epoch: [185][  217/  217]    Overall Loss 0.000591    Objective Loss 0.000591    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.265565    
2024-05-04 02:21:54,507 - 

2024-05-04 02:21:54,508 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:22:21,729 - Epoch: [186][  100/  217]    Overall Loss 0.000846    Objective Loss 0.000846                                        LR 0.000063    Time 0.272119    
2024-05-04 02:22:48,553 - Epoch: [186][  200/  217]    Overall Loss 0.000741    Objective Loss 0.000741                                        LR 0.000063    Time 0.270132    
2024-05-04 02:22:52,302 - Epoch: [186][  217/  217]    Overall Loss 0.000697    Objective Loss 0.000697    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.266238    
2024-05-04 02:22:53,245 - 

2024-05-04 02:22:53,246 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:23:21,532 - Epoch: [187][  100/  217]    Overall Loss 0.000965    Objective Loss 0.000965                                        LR 0.000063    Time 0.282704    
2024-05-04 02:23:49,126 - Epoch: [187][  200/  217]    Overall Loss 0.000694    Objective Loss 0.000694                                        LR 0.000063    Time 0.279277    
2024-05-04 02:23:53,899 - Epoch: [187][  217/  217]    Overall Loss 0.000653    Objective Loss 0.000653    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.279387    
2024-05-04 02:23:54,290 - 

2024-05-04 02:23:54,291 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:24:23,599 - Epoch: [188][  100/  217]    Overall Loss 0.000624    Objective Loss 0.000624                                        LR 0.000063    Time 0.292988    
2024-05-04 02:24:51,791 - Epoch: [188][  200/  217]    Overall Loss 0.000521    Objective Loss 0.000521                                        LR 0.000063    Time 0.287363    
2024-05-04 02:24:55,424 - Epoch: [188][  217/  217]    Overall Loss 0.000614    Objective Loss 0.000614    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.281586    
2024-05-04 02:24:55,656 - 

2024-05-04 02:24:55,656 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:25:24,456 - Epoch: [189][  100/  217]    Overall Loss 0.000885    Objective Loss 0.000885                                        LR 0.000063    Time 0.287904    
2024-05-04 02:25:53,664 - Epoch: [189][  200/  217]    Overall Loss 0.000646    Objective Loss 0.000646                                        LR 0.000063    Time 0.289949    
2024-05-04 02:25:59,291 - Epoch: [189][  217/  217]    Overall Loss 0.000609    Objective Loss 0.000609    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.293153    
2024-05-04 02:25:59,840 - 

2024-05-04 02:25:59,841 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:26:29,511 - Epoch: [190][  100/  217]    Overall Loss 0.000833    Objective Loss 0.000833                                        LR 0.000063    Time 0.296614    
2024-05-04 02:26:56,872 - Epoch: [190][  200/  217]    Overall Loss 0.000619    Objective Loss 0.000619                                        LR 0.000063    Time 0.285065    
2024-05-04 02:27:02,608 - Epoch: [190][  217/  217]    Overall Loss 0.000581    Objective Loss 0.000581    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.289158    
2024-05-04 02:27:03,544 - --- validate (epoch=190)-----------
2024-05-04 02:27:03,544 - 1736 samples (32 per mini-batch)
2024-05-04 02:27:19,808 - Epoch: [190][   55/   55]    Loss 3.266984    Top1 56.970046    Top5 72.753456    
2024-05-04 02:27:20,056 - ==> Top1: 56.970    Top5: 72.753    Loss: 3.267

2024-05-04 02:27:20,060 - ==> Best [Top1: 56.970   Top5: 73.099   Sparsity:0.00   Params: 381920 on epoch: 180]
2024-05-04 02:27:20,060 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 02:27:20,099 - 

2024-05-04 02:27:20,099 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:27:51,073 - Epoch: [191][  100/  217]    Overall Loss 0.000351    Objective Loss 0.000351                                        LR 0.000063    Time 0.309655    
2024-05-04 02:28:15,803 - Epoch: [191][  200/  217]    Overall Loss 0.000655    Objective Loss 0.000655                                        LR 0.000063    Time 0.278438    
2024-05-04 02:28:21,259 - Epoch: [191][  217/  217]    Overall Loss 0.000614    Objective Loss 0.000614    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.281761    
2024-05-04 02:28:21,516 - 

2024-05-04 02:28:21,517 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:28:55,341 - Epoch: [192][  100/  217]    Overall Loss 0.000691    Objective Loss 0.000691                                        LR 0.000063    Time 0.338125    
2024-05-04 02:29:17,768 - Epoch: [192][  200/  217]    Overall Loss 0.000742    Objective Loss 0.000742                                        LR 0.000063    Time 0.281157    
2024-05-04 02:29:23,078 - Epoch: [192][  217/  217]    Overall Loss 0.000693    Objective Loss 0.000693    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.283591    
2024-05-04 02:29:23,415 - 

2024-05-04 02:29:23,415 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:29:57,219 - Epoch: [193][  100/  217]    Overall Loss 0.000779    Objective Loss 0.000779                                        LR 0.000063    Time 0.337958    
2024-05-04 02:30:18,891 - Epoch: [193][  200/  217]    Overall Loss 0.000631    Objective Loss 0.000631                                        LR 0.000063    Time 0.277288    
2024-05-04 02:30:23,694 - Epoch: [193][  217/  217]    Overall Loss 0.000591    Objective Loss 0.000591    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.277694    
2024-05-04 02:30:23,905 - 

2024-05-04 02:30:23,906 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:30:57,252 - Epoch: [194][  100/  217]    Overall Loss 0.000631    Objective Loss 0.000631                                        LR 0.000063    Time 0.333367    
2024-05-04 02:31:19,327 - Epoch: [194][  200/  217]    Overall Loss 0.000633    Objective Loss 0.000633                                        LR 0.000063    Time 0.277019    
2024-05-04 02:31:24,093 - Epoch: [194][  217/  217]    Overall Loss 0.000597    Objective Loss 0.000597    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.277273    
2024-05-04 02:31:24,314 - 

2024-05-04 02:31:24,314 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:31:51,126 - Epoch: [195][  100/  217]    Overall Loss 0.000877    Objective Loss 0.000877                                        LR 0.000063    Time 0.268032    
2024-05-04 02:32:16,836 - Epoch: [195][  200/  217]    Overall Loss 0.000624    Objective Loss 0.000624                                        LR 0.000063    Time 0.262523    
2024-05-04 02:32:20,616 - Epoch: [195][  217/  217]    Overall Loss 0.000585    Objective Loss 0.000585    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.259371    
2024-05-04 02:32:20,866 - 

2024-05-04 02:32:20,867 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:32:55,646 - Epoch: [196][  100/  217]    Overall Loss 0.000643    Objective Loss 0.000643                                        LR 0.000063    Time 0.347694    
2024-05-04 02:33:18,862 - Epoch: [196][  200/  217]    Overall Loss 0.000618    Objective Loss 0.000618                                        LR 0.000063    Time 0.289878    
2024-05-04 02:33:23,841 - Epoch: [196][  217/  217]    Overall Loss 0.000579    Objective Loss 0.000579    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.290100    
2024-05-04 02:33:24,283 - 

2024-05-04 02:33:24,283 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:33:54,444 - Epoch: [197][  100/  217]    Overall Loss 0.000328    Objective Loss 0.000328                                        LR 0.000063    Time 0.301501    
2024-05-04 02:34:20,701 - Epoch: [197][  200/  217]    Overall Loss 0.000586    Objective Loss 0.000586                                        LR 0.000063    Time 0.281999    
2024-05-04 02:34:25,055 - Epoch: [197][  217/  217]    Overall Loss 0.000548    Objective Loss 0.000548    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.279965    
2024-05-04 02:34:25,350 - 

2024-05-04 02:34:25,351 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:34:53,513 - Epoch: [198][  100/  217]    Overall Loss 0.000789    Objective Loss 0.000789                                        LR 0.000063    Time 0.281519    
2024-05-04 02:35:22,354 - Epoch: [198][  200/  217]    Overall Loss 0.000573    Objective Loss 0.000573                                        LR 0.000063    Time 0.284923    
2024-05-04 02:35:26,924 - Epoch: [198][  217/  217]    Overall Loss 0.000538    Objective Loss 0.000538    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.283655    
2024-05-04 02:35:27,230 - 

2024-05-04 02:35:27,231 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:35:56,672 - Epoch: [199][  100/  217]    Overall Loss 0.000335    Objective Loss 0.000335                                        LR 0.000063    Time 0.294317    
2024-05-04 02:36:23,108 - Epoch: [199][  200/  217]    Overall Loss 0.000646    Objective Loss 0.000646                                        LR 0.000063    Time 0.279297    
2024-05-04 02:36:28,692 - Epoch: [199][  217/  217]    Overall Loss 0.000603    Objective Loss 0.000603    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.283141    
2024-05-04 02:36:28,934 - 

2024-05-04 02:36:28,935 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:36:58,930 - Epoch: [200][  100/  217]    Overall Loss 0.000590    Objective Loss 0.000590                                        LR 0.000016    Time 0.299865    
2024-05-04 02:37:23,656 - Epoch: [200][  200/  217]    Overall Loss 0.000567    Objective Loss 0.000567                                        LR 0.000016    Time 0.273524    
2024-05-04 02:37:28,708 - Epoch: [200][  217/  217]    Overall Loss 0.000531    Objective Loss 0.000531    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.275367    
2024-05-04 02:37:28,922 - --- validate (epoch=200)-----------
2024-05-04 02:37:28,923 - 1736 samples (32 per mini-batch)
2024-05-04 02:37:44,732 - Epoch: [200][   55/   55]    Loss 3.340788    Top1 57.430876    Top5 72.523041    
2024-05-04 02:37:45,023 - ==> Top1: 57.431    Top5: 72.523    Loss: 3.341

2024-05-04 02:37:45,031 - ==> Best [Top1: 57.431   Top5: 72.523   Sparsity:0.00   Params: 381920 on epoch: 200]
2024-05-04 02:37:45,032 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 02:37:45,098 - 

2024-05-04 02:37:45,098 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:38:16,341 - Epoch: [201][  100/  217]    Overall Loss 0.000141    Objective Loss 0.000141                                        LR 0.000016    Time 0.312331    
2024-05-04 02:38:46,891 - Epoch: [201][  200/  217]    Overall Loss 0.000584    Objective Loss 0.000584                                        LR 0.000016    Time 0.308871    
2024-05-04 02:38:50,693 - Epoch: [201][  217/  217]    Overall Loss 0.000548    Objective Loss 0.000548    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.302184    
2024-05-04 02:38:51,220 - 

2024-05-04 02:38:51,221 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:39:22,738 - Epoch: [202][  100/  217]    Overall Loss 0.000778    Objective Loss 0.000778                                        LR 0.000016    Time 0.315072    
2024-05-04 02:39:48,679 - Epoch: [202][  200/  217]    Overall Loss 0.000563    Objective Loss 0.000563                                        LR 0.000016    Time 0.287198    
2024-05-04 02:39:52,658 - Epoch: [202][  217/  217]    Overall Loss 0.000528    Objective Loss 0.000528    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.283026    
2024-05-04 02:39:52,881 - 

2024-05-04 02:39:52,881 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:40:25,553 - Epoch: [203][  100/  217]    Overall Loss 0.000325    Objective Loss 0.000325                                        LR 0.000016    Time 0.326616    
2024-05-04 02:40:51,981 - Epoch: [203][  200/  217]    Overall Loss 0.000576    Objective Loss 0.000576                                        LR 0.000016    Time 0.295402    
2024-05-04 02:40:58,776 - Epoch: [203][  217/  217]    Overall Loss 0.000541    Objective Loss 0.000541    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.303563    
2024-05-04 02:40:59,347 - 

2024-05-04 02:40:59,348 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:41:30,032 - Epoch: [204][  100/  217]    Overall Loss 0.000577    Objective Loss 0.000577                                        LR 0.000016    Time 0.306747    
2024-05-04 02:41:57,639 - Epoch: [204][  200/  217]    Overall Loss 0.000440    Objective Loss 0.000440                                        LR 0.000016    Time 0.291352    
2024-05-04 02:42:01,740 - Epoch: [204][  217/  217]    Overall Loss 0.000529    Objective Loss 0.000529    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.287417    
2024-05-04 02:42:02,480 - 

2024-05-04 02:42:02,480 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:42:31,709 - Epoch: [205][  100/  217]    Overall Loss 0.000538    Objective Loss 0.000538                                        LR 0.000016    Time 0.292186    
2024-05-04 02:42:56,611 - Epoch: [205][  200/  217]    Overall Loss 0.000565    Objective Loss 0.000565                                        LR 0.000016    Time 0.270548    
2024-05-04 02:43:01,646 - Epoch: [205][  217/  217]    Overall Loss 0.000533    Objective Loss 0.000533    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.272551    
2024-05-04 02:43:02,193 - 

2024-05-04 02:43:02,194 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:43:27,659 - Epoch: [206][  100/  217]    Overall Loss 0.000767    Objective Loss 0.000767                                        LR 0.000016    Time 0.254548    
2024-05-04 02:43:53,271 - Epoch: [206][  200/  217]    Overall Loss 0.000555    Objective Loss 0.000555                                        LR 0.000016    Time 0.255292    
2024-05-04 02:43:59,391 - Epoch: [206][  217/  217]    Overall Loss 0.000521    Objective Loss 0.000521    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.263483    
2024-05-04 02:43:59,889 - 

2024-05-04 02:43:59,889 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:44:28,018 - Epoch: [207][  100/  217]    Overall Loss 0.000314    Objective Loss 0.000314                                        LR 0.000016    Time 0.281179    
2024-05-04 02:44:52,944 - Epoch: [207][  200/  217]    Overall Loss 0.000526    Objective Loss 0.000526                                        LR 0.000016    Time 0.265173    
2024-05-04 02:44:57,258 - Epoch: [207][  217/  217]    Overall Loss 0.000492    Objective Loss 0.000492    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.264267    
2024-05-04 02:44:57,785 - 

2024-05-04 02:44:57,786 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:45:23,407 - Epoch: [208][  100/  217]    Overall Loss 0.000109    Objective Loss 0.000109                                        LR 0.000016    Time 0.256094    
2024-05-04 02:45:49,182 - Epoch: [208][  200/  217]    Overall Loss 0.000553    Objective Loss 0.000553                                        LR 0.000016    Time 0.256870    
2024-05-04 02:45:54,210 - Epoch: [208][  217/  217]    Overall Loss 0.000518    Objective Loss 0.000518    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.259903    
2024-05-04 02:45:54,639 - 

2024-05-04 02:45:54,640 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:46:23,266 - Epoch: [209][  100/  217]    Overall Loss 0.000387    Objective Loss 0.000387                                        LR 0.000016    Time 0.286148    
2024-05-04 02:46:51,647 - Epoch: [209][  200/  217]    Overall Loss 0.000582    Objective Loss 0.000582                                        LR 0.000016    Time 0.284926    
2024-05-04 02:46:55,155 - Epoch: [209][  217/  217]    Overall Loss 0.000544    Objective Loss 0.000544    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.278758    
2024-05-04 02:46:55,568 - 

2024-05-04 02:46:55,568 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:47:23,569 - Epoch: [210][  100/  217]    Overall Loss 0.000505    Objective Loss 0.000505                                        LR 0.000016    Time 0.279898    
2024-05-04 02:47:48,024 - Epoch: [210][  200/  217]    Overall Loss 0.000554    Objective Loss 0.000554                                        LR 0.000016    Time 0.262179    
2024-05-04 02:47:53,145 - Epoch: [210][  217/  217]    Overall Loss 0.000520    Objective Loss 0.000520    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.265225    
2024-05-04 02:47:53,486 - --- validate (epoch=210)-----------
2024-05-04 02:47:53,487 - 1736 samples (32 per mini-batch)
2024-05-04 02:48:12,525 - Epoch: [210][   55/   55]    Loss 3.295422    Top1 57.200461    Top5 72.868664    
2024-05-04 02:48:12,775 - ==> Top1: 57.200    Top5: 72.869    Loss: 3.295

2024-05-04 02:48:12,782 - ==> Best [Top1: 57.431   Top5: 72.523   Sparsity:0.00   Params: 381920 on epoch: 200]
2024-05-04 02:48:12,782 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 02:48:12,821 - 

2024-05-04 02:48:12,822 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:48:37,800 - Epoch: [211][  100/  217]    Overall Loss 0.000561    Objective Loss 0.000561                                        LR 0.000016    Time 0.249683    
2024-05-04 02:49:06,000 - Epoch: [211][  200/  217]    Overall Loss 0.000423    Objective Loss 0.000423                                        LR 0.000016    Time 0.265791    
2024-05-04 02:49:08,863 - Epoch: [211][  217/  217]    Overall Loss 0.000522    Objective Loss 0.000522    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.258154    
2024-05-04 02:49:09,450 - 

2024-05-04 02:49:09,451 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:49:39,008 - Epoch: [212][  100/  217]    Overall Loss 0.000495    Objective Loss 0.000495                                        LR 0.000016    Time 0.295465    
2024-05-04 02:50:00,954 - Epoch: [212][  200/  217]    Overall Loss 0.000530    Objective Loss 0.000530                                        LR 0.000016    Time 0.257409    
2024-05-04 02:50:04,546 - Epoch: [212][  217/  217]    Overall Loss 0.000497    Objective Loss 0.000497    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.253787    
2024-05-04 02:50:04,900 - 

2024-05-04 02:50:04,901 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:50:35,119 - Epoch: [213][  100/  217]    Overall Loss 0.000824    Objective Loss 0.000824                                        LR 0.000016    Time 0.302086    
2024-05-04 02:51:00,056 - Epoch: [213][  200/  217]    Overall Loss 0.000574    Objective Loss 0.000574                                        LR 0.000016    Time 0.275679    
2024-05-04 02:51:03,859 - Epoch: [213][  217/  217]    Overall Loss 0.000538    Objective Loss 0.000538    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.271596    
2024-05-04 02:51:04,224 - 

2024-05-04 02:51:04,225 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:51:33,441 - Epoch: [214][  100/  217]    Overall Loss 0.000561    Objective Loss 0.000561                                        LR 0.000016    Time 0.292056    
2024-05-04 02:51:59,022 - Epoch: [214][  200/  217]    Overall Loss 0.000554    Objective Loss 0.000554                                        LR 0.000016    Time 0.273883    
2024-05-04 02:52:05,410 - Epoch: [214][  217/  217]    Overall Loss 0.000517    Objective Loss 0.000517    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.281855    
2024-05-04 02:52:05,678 - 

2024-05-04 02:52:05,678 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:52:35,487 - Epoch: [215][  100/  217]    Overall Loss 0.000569    Objective Loss 0.000569                                        LR 0.000016    Time 0.297985    
2024-05-04 02:52:59,817 - Epoch: [215][  200/  217]    Overall Loss 0.000456    Objective Loss 0.000456                                        LR 0.000016    Time 0.270592    
2024-05-04 02:53:04,553 - Epoch: [215][  217/  217]    Overall Loss 0.000526    Objective Loss 0.000526    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.271212    
2024-05-04 02:53:04,900 - 

2024-05-04 02:53:04,900 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:53:32,974 - Epoch: [216][  100/  217]    Overall Loss 0.000548    Objective Loss 0.000548                                        LR 0.000016    Time 0.280637    
2024-05-04 02:53:59,231 - Epoch: [216][  200/  217]    Overall Loss 0.000548    Objective Loss 0.000548                                        LR 0.000016    Time 0.271558    
2024-05-04 02:54:02,111 - Epoch: [216][  217/  217]    Overall Loss 0.000511    Objective Loss 0.000511    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.263547    
2024-05-04 02:54:02,473 - 

2024-05-04 02:54:02,473 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:54:31,496 - Epoch: [217][  100/  217]    Overall Loss 0.000764    Objective Loss 0.000764                                        LR 0.000016    Time 0.290130    
2024-05-04 02:54:57,518 - Epoch: [217][  200/  217]    Overall Loss 0.000538    Objective Loss 0.000538                                        LR 0.000016    Time 0.275126    
2024-05-04 02:55:01,498 - Epoch: [217][  217/  217]    Overall Loss 0.000507    Objective Loss 0.000507    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.271905    
2024-05-04 02:55:01,885 - 

2024-05-04 02:55:01,886 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:55:29,356 - Epoch: [218][  100/  217]    Overall Loss 0.000734    Objective Loss 0.000734                                        LR 0.000016    Time 0.274584    
2024-05-04 02:55:57,084 - Epoch: [218][  200/  217]    Overall Loss 0.000542    Objective Loss 0.000542                                        LR 0.000016    Time 0.275882    
2024-05-04 02:56:00,401 - Epoch: [218][  217/  217]    Overall Loss 0.000512    Objective Loss 0.000512    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.269550    
2024-05-04 02:56:00,954 - 

2024-05-04 02:56:00,955 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:56:30,649 - Epoch: [219][  100/  217]    Overall Loss 0.000568    Objective Loss 0.000568                                        LR 0.000016    Time 0.296846    
2024-05-04 02:56:58,887 - Epoch: [219][  200/  217]    Overall Loss 0.000446    Objective Loss 0.000446                                        LR 0.000016    Time 0.289560    
2024-05-04 02:57:02,622 - Epoch: [219][  217/  217]    Overall Loss 0.000521    Objective Loss 0.000521    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.284079    
2024-05-04 02:57:02,975 - 

2024-05-04 02:57:02,975 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:57:32,917 - Epoch: [220][  100/  217]    Overall Loss 0.000547    Objective Loss 0.000547                                        LR 0.000016    Time 0.299329    
2024-05-04 02:57:58,900 - Epoch: [220][  200/  217]    Overall Loss 0.000421    Objective Loss 0.000421                                        LR 0.000016    Time 0.279531    
2024-05-04 02:58:02,767 - Epoch: [220][  217/  217]    Overall Loss 0.000511    Objective Loss 0.000511    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.275442    
2024-05-04 02:58:03,119 - --- validate (epoch=220)-----------
2024-05-04 02:58:03,120 - 1736 samples (32 per mini-batch)
2024-05-04 02:58:18,932 - Epoch: [220][   55/   55]    Loss 3.325997    Top1 57.142857    Top5 72.695853    
2024-05-04 02:58:19,257 - ==> Top1: 57.143    Top5: 72.696    Loss: 3.326

2024-05-04 02:58:19,263 - ==> Best [Top1: 57.431   Top5: 72.523   Sparsity:0.00   Params: 381920 on epoch: 200]
2024-05-04 02:58:19,263 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 02:58:19,299 - 

2024-05-04 02:58:19,299 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:58:49,901 - Epoch: [221][  100/  217]    Overall Loss 0.000732    Objective Loss 0.000732                                        LR 0.000016    Time 0.305856    
2024-05-04 02:59:17,345 - Epoch: [221][  200/  217]    Overall Loss 0.000551    Objective Loss 0.000551                                        LR 0.000016    Time 0.290098    
2024-05-04 02:59:22,395 - Epoch: [221][  217/  217]    Overall Loss 0.000515    Objective Loss 0.000515    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.290635    
2024-05-04 02:59:22,969 - 

2024-05-04 02:59:22,969 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 02:59:51,526 - Epoch: [222][  100/  217]    Overall Loss 0.000564    Objective Loss 0.000564                                        LR 0.000016    Time 0.285469    
2024-05-04 03:00:18,546 - Epoch: [222][  200/  217]    Overall Loss 0.000551    Objective Loss 0.000551                                        LR 0.000016    Time 0.277789    
2024-05-04 03:00:22,091 - Epoch: [222][  217/  217]    Overall Loss 0.000515    Objective Loss 0.000515    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.272353    
2024-05-04 03:00:22,502 - 

2024-05-04 03:00:22,503 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:00:51,636 - Epoch: [223][  100/  217]    Overall Loss 0.000296    Objective Loss 0.000296                                        LR 0.000016    Time 0.291219    
2024-05-04 03:01:21,189 - Epoch: [223][  200/  217]    Overall Loss 0.000300    Objective Loss 0.000300                                        LR 0.000016    Time 0.293330    
2024-05-04 03:01:24,627 - Epoch: [223][  217/  217]    Overall Loss 0.000501    Objective Loss 0.000501    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.286182    
2024-05-04 03:01:25,340 - 

2024-05-04 03:01:25,340 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:01:56,123 - Epoch: [224][  100/  217]    Overall Loss 0.000236    Objective Loss 0.000236                                        LR 0.000016    Time 0.307717    
2024-05-04 03:02:18,576 - Epoch: [224][  200/  217]    Overall Loss 0.000551    Objective Loss 0.000551                                        LR 0.000016    Time 0.266076    
2024-05-04 03:02:25,217 - Epoch: [224][  217/  217]    Overall Loss 0.000513    Objective Loss 0.000513    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.275826    
2024-05-04 03:02:25,454 - 

2024-05-04 03:02:25,455 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:02:55,568 - Epoch: [225][  100/  217]    Overall Loss 0.000331    Objective Loss 0.000331                                        LR 0.000016    Time 0.301022    
2024-05-04 03:03:19,789 - Epoch: [225][  200/  217]    Overall Loss 0.000424    Objective Loss 0.000424                                        LR 0.000016    Time 0.271570    
2024-05-04 03:03:23,762 - Epoch: [225][  217/  217]    Overall Loss 0.000498    Objective Loss 0.000498    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.268595    
2024-05-04 03:03:24,073 - 

2024-05-04 03:03:24,073 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:03:54,971 - Epoch: [226][  100/  217]    Overall Loss 0.000554    Objective Loss 0.000554                                        LR 0.000016    Time 0.308873    
2024-05-04 03:04:24,297 - Epoch: [226][  200/  217]    Overall Loss 0.000528    Objective Loss 0.000528                                        LR 0.000016    Time 0.301015    
2024-05-04 03:04:29,787 - Epoch: [226][  217/  217]    Overall Loss 0.000492    Objective Loss 0.000492    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.302726    
2024-05-04 03:04:30,726 - 

2024-05-04 03:04:30,727 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:04:58,444 - Epoch: [227][  100/  217]    Overall Loss 0.000561    Objective Loss 0.000561                                        LR 0.000016    Time 0.277064    
2024-05-04 03:05:24,363 - Epoch: [227][  200/  217]    Overall Loss 0.000547    Objective Loss 0.000547                                        LR 0.000016    Time 0.268038    
2024-05-04 03:05:29,209 - Epoch: [227][  217/  217]    Overall Loss 0.000511    Objective Loss 0.000511    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.269361    
2024-05-04 03:05:30,262 - 

2024-05-04 03:05:30,262 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:05:59,670 - Epoch: [228][  100/  217]    Overall Loss 0.000566    Objective Loss 0.000566                                        LR 0.000016    Time 0.293967    
2024-05-04 03:06:24,791 - Epoch: [228][  200/  217]    Overall Loss 0.000534    Objective Loss 0.000534                                        LR 0.000016    Time 0.272546    
2024-05-04 03:06:28,403 - Epoch: [228][  217/  217]    Overall Loss 0.000499    Objective Loss 0.000499    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.267827    
2024-05-04 03:06:28,855 - 

2024-05-04 03:06:28,856 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:06:55,502 - Epoch: [229][  100/  217]    Overall Loss 0.000810    Objective Loss 0.000810                                        LR 0.000016    Time 0.266349    
2024-05-04 03:07:25,014 - Epoch: [229][  200/  217]    Overall Loss 0.000548    Objective Loss 0.000548                                        LR 0.000016    Time 0.280614    
2024-05-04 03:07:30,178 - Epoch: [229][  217/  217]    Overall Loss 0.000510    Objective Loss 0.000510    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.282415    
2024-05-04 03:07:30,773 - 

2024-05-04 03:07:30,774 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:07:57,436 - Epoch: [230][  100/  217]    Overall Loss 0.000334    Objective Loss 0.000334                                        LR 0.000016    Time 0.266517    
2024-05-04 03:08:24,871 - Epoch: [230][  200/  217]    Overall Loss 0.000544    Objective Loss 0.000544                                        LR 0.000016    Time 0.270385    
2024-05-04 03:08:30,191 - Epoch: [230][  217/  217]    Overall Loss 0.000508    Objective Loss 0.000508    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.273710    
2024-05-04 03:08:30,723 - --- validate (epoch=230)-----------
2024-05-04 03:08:30,723 - 1736 samples (32 per mini-batch)
2024-05-04 03:08:46,428 - Epoch: [230][   55/   55]    Loss 3.278545    Top1 56.912442    Top5 72.465438    
2024-05-04 03:08:46,914 - ==> Top1: 56.912    Top5: 72.465    Loss: 3.279

2024-05-04 03:08:46,923 - ==> Best [Top1: 57.431   Top5: 72.523   Sparsity:0.00   Params: 381920 on epoch: 200]
2024-05-04 03:08:46,924 - Saving checkpoint to: logs/2024.05.03-230945/checkpoint.pth.tar
2024-05-04 03:08:46,972 - 

2024-05-04 03:08:46,973 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:09:19,561 - Epoch: [231][  100/  217]    Overall Loss 0.000560    Objective Loss 0.000560                                        LR 0.000016    Time 0.325765    
2024-05-04 03:09:44,435 - Epoch: [231][  200/  217]    Overall Loss 0.000529    Objective Loss 0.000529                                        LR 0.000016    Time 0.287203    
2024-05-04 03:09:47,546 - Epoch: [231][  217/  217]    Overall Loss 0.000493    Objective Loss 0.000493    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.279026    
2024-05-04 03:09:47,931 - 

2024-05-04 03:09:47,931 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:10:21,144 - Epoch: [232][  100/  217]    Overall Loss 0.000718    Objective Loss 0.000718                                        LR 0.000016    Time 0.332030    
2024-05-04 03:10:46,806 - Epoch: [232][  200/  217]    Overall Loss 0.000531    Objective Loss 0.000531                                        LR 0.000016    Time 0.294280    
2024-05-04 03:10:50,476 - Epoch: [232][  217/  217]    Overall Loss 0.000495    Objective Loss 0.000495    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.288125    
2024-05-04 03:10:51,302 - 

2024-05-04 03:10:51,303 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:11:23,422 - Epoch: [233][  100/  217]    Overall Loss 0.000723    Objective Loss 0.000723                                        LR 0.000016    Time 0.321091    
2024-05-04 03:11:46,352 - Epoch: [233][  200/  217]    Overall Loss 0.000518    Objective Loss 0.000518                                        LR 0.000016    Time 0.275150    
2024-05-04 03:11:49,576 - Epoch: [233][  217/  217]    Overall Loss 0.000485    Objective Loss 0.000485    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.268438    
2024-05-04 03:11:50,100 - 

2024-05-04 03:11:50,100 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:12:24,860 - Epoch: [234][  100/  217]    Overall Loss 0.000773    Objective Loss 0.000773                                        LR 0.000016    Time 0.347498    
2024-05-04 03:12:52,033 - Epoch: [234][  200/  217]    Overall Loss 0.000434    Objective Loss 0.000434                                        LR 0.000016    Time 0.309572    
2024-05-04 03:12:54,965 - Epoch: [234][  217/  217]    Overall Loss 0.000504    Objective Loss 0.000504    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.298821    
2024-05-04 03:12:55,464 - 

2024-05-04 03:12:55,464 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:13:25,315 - Epoch: [235][  100/  217]    Overall Loss 0.000742    Objective Loss 0.000742                                        LR 0.000016    Time 0.298414    
2024-05-04 03:13:51,709 - Epoch: [235][  200/  217]    Overall Loss 0.000514    Objective Loss 0.000514                                        LR 0.000016    Time 0.281095    
2024-05-04 03:13:57,812 - Epoch: [235][  217/  217]    Overall Loss 0.000483    Objective Loss 0.000483    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.287190    
2024-05-04 03:13:58,169 - 

2024-05-04 03:13:58,172 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:14:25,857 - Epoch: [236][  100/  217]    Overall Loss 0.000498    Objective Loss 0.000498                                        LR 0.000016    Time 0.276743    
2024-05-04 03:14:52,485 - Epoch: [236][  200/  217]    Overall Loss 0.000390    Objective Loss 0.000390                                        LR 0.000016    Time 0.271461    
2024-05-04 03:14:57,276 - Epoch: [236][  217/  217]    Overall Loss 0.000478    Objective Loss 0.000478    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.272266    
2024-05-04 03:14:57,703 - 

2024-05-04 03:14:57,703 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:15:28,760 - Epoch: [237][  100/  217]    Overall Loss 0.000714    Objective Loss 0.000714                                        LR 0.000016    Time 0.310451    
2024-05-04 03:15:56,063 - Epoch: [237][  200/  217]    Overall Loss 0.000525    Objective Loss 0.000525                                        LR 0.000016    Time 0.291692    
2024-05-04 03:16:00,997 - Epoch: [237][  217/  217]    Overall Loss 0.000490    Objective Loss 0.000490    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.291573    
2024-05-04 03:16:01,563 - 

2024-05-04 03:16:01,563 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:16:30,950 - Epoch: [238][  100/  217]    Overall Loss 0.000782    Objective Loss 0.000782                                        LR 0.000016    Time 0.293760    
2024-05-04 03:16:58,196 - Epoch: [238][  200/  217]    Overall Loss 0.000538    Objective Loss 0.000538                                        LR 0.000016    Time 0.283063    
2024-05-04 03:17:02,720 - Epoch: [238][  217/  217]    Overall Loss 0.000503    Objective Loss 0.000503    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.281728    
2024-05-04 03:17:03,144 - 

2024-05-04 03:17:03,144 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:17:30,979 - Epoch: [239][  100/  217]    Overall Loss 0.000505    Objective Loss 0.000505                                        LR 0.000016    Time 0.278241    
2024-05-04 03:17:58,074 - Epoch: [239][  200/  217]    Overall Loss 0.000503    Objective Loss 0.000503                                        LR 0.000016    Time 0.274550    
2024-05-04 03:18:01,075 - Epoch: [239][  217/  217]    Overall Loss 0.000472    Objective Loss 0.000472    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.266860    
2024-05-04 03:18:01,694 - 

2024-05-04 03:18:01,695 - Initiating quantization aware training (QAT)...
2024-05-04 03:18:01,757 - 

2024-05-04 03:18:01,758 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:18:30,921 - Epoch: [240][  100/  217]    Overall Loss 2.107902    Objective Loss 2.107902                                        LR 0.000016    Time 0.291529    
2024-05-04 03:18:58,621 - Epoch: [240][  200/  217]    Overall Loss 1.590863    Objective Loss 1.590863                                        LR 0.000016    Time 0.284217    
2024-05-04 03:19:01,606 - Epoch: [240][  217/  217]    Overall Loss 1.526354    Objective Loss 1.526354    Top1 86.885246    Top5 98.360656    LR 0.000016    Time 0.275696    
2024-05-04 03:19:02,037 - --- validate (epoch=240)-----------
2024-05-04 03:19:02,038 - 1736 samples (32 per mini-batch)
2024-05-04 03:19:20,837 - Epoch: [240][   55/   55]    Loss 2.248506    Top1 51.440092    Top5 68.145161    
2024-05-04 03:19:21,099 - ==> Top1: 51.440    Top5: 68.145    Loss: 2.249

2024-05-04 03:19:21,103 - ==> Best [Top1: 51.440   Top5: 68.145   Sparsity:0.00   Params: 381920 on epoch: 240]
2024-05-04 03:19:21,104 - Saving checkpoint to: logs/2024.05.03-230945/qat_checkpoint.pth.tar
2024-05-04 03:19:21,142 - 

2024-05-04 03:19:21,142 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:19:54,113 - Epoch: [241][  100/  217]    Overall Loss 0.785785    Objective Loss 0.785785                                        LR 0.000016    Time 0.329612    
2024-05-04 03:20:19,259 - Epoch: [241][  200/  217]    Overall Loss 0.745030    Objective Loss 0.745030                                        LR 0.000016    Time 0.290493    
2024-05-04 03:20:22,528 - Epoch: [241][  217/  217]    Overall Loss 0.735744    Objective Loss 0.735744    Top1 88.524590    Top5 98.360656    LR 0.000016    Time 0.282790    
2024-05-04 03:20:22,968 - 

2024-05-04 03:20:22,968 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:20:52,100 - Epoch: [242][  100/  217]    Overall Loss 0.594560    Objective Loss 0.594560                                        LR 0.000016    Time 0.291217    
2024-05-04 03:21:17,288 - Epoch: [242][  200/  217]    Overall Loss 0.588767    Objective Loss 0.588767                                        LR 0.000016    Time 0.271504    
2024-05-04 03:21:21,526 - Epoch: [242][  217/  217]    Overall Loss 0.584369    Objective Loss 0.584369    Top1 81.967213    Top5 98.360656    LR 0.000016    Time 0.269754    
2024-05-04 03:21:21,988 - 

2024-05-04 03:21:21,988 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:21:48,572 - Epoch: [243][  100/  217]    Overall Loss 0.507268    Objective Loss 0.507268                                        LR 0.000016    Time 0.265750    
2024-05-04 03:22:17,238 - Epoch: [243][  200/  217]    Overall Loss 0.497549    Objective Loss 0.497549                                        LR 0.000016    Time 0.276162    
2024-05-04 03:22:21,285 - Epoch: [243][  217/  217]    Overall Loss 0.498826    Objective Loss 0.498826    Top1 91.803279    Top5 98.360656    LR 0.000016    Time 0.273164    
2024-05-04 03:22:21,899 - 

2024-05-04 03:22:21,900 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:22:49,617 - Epoch: [244][  100/  217]    Overall Loss 0.432711    Objective Loss 0.432711                                        LR 0.000016    Time 0.277066    
2024-05-04 03:23:18,695 - Epoch: [244][  200/  217]    Overall Loss 0.438825    Objective Loss 0.438825                                        LR 0.000016    Time 0.283873    
2024-05-04 03:23:22,679 - Epoch: [244][  217/  217]    Overall Loss 0.439061    Objective Loss 0.439061    Top1 91.803279    Top5 98.360656    LR 0.000016    Time 0.279985    
2024-05-04 03:23:23,327 - 

2024-05-04 03:23:23,328 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:23:55,064 - Epoch: [245][  100/  217]    Overall Loss 0.388522    Objective Loss 0.388522                                        LR 0.000016    Time 0.317270    
2024-05-04 03:24:19,528 - Epoch: [245][  200/  217]    Overall Loss 0.399202    Objective Loss 0.399202                                        LR 0.000016    Time 0.280906    
2024-05-04 03:24:24,182 - Epoch: [245][  217/  217]    Overall Loss 0.401089    Objective Loss 0.401089    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.280342    
2024-05-04 03:24:24,820 - 

2024-05-04 03:24:24,820 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:24:57,100 - Epoch: [246][  100/  217]    Overall Loss 0.362331    Objective Loss 0.362331                                        LR 0.000016    Time 0.322699    
2024-05-04 03:25:22,473 - Epoch: [246][  200/  217]    Overall Loss 0.373315    Objective Loss 0.373315                                        LR 0.000016    Time 0.288169    
2024-05-04 03:25:25,838 - Epoch: [246][  217/  217]    Overall Loss 0.370411    Objective Loss 0.370411    Top1 88.524590    Top5 100.000000    LR 0.000016    Time 0.281092    
2024-05-04 03:25:26,464 - 

2024-05-04 03:25:26,465 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:25:57,165 - Epoch: [247][  100/  217]    Overall Loss 0.328592    Objective Loss 0.328592                                        LR 0.000016    Time 0.306897    
2024-05-04 03:26:20,814 - Epoch: [247][  200/  217]    Overall Loss 0.339292    Objective Loss 0.339292                                        LR 0.000016    Time 0.271650    
2024-05-04 03:26:23,961 - Epoch: [247][  217/  217]    Overall Loss 0.339079    Objective Loss 0.339079    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.264863    
2024-05-04 03:26:24,197 - 

2024-05-04 03:26:24,197 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:26:55,399 - Epoch: [248][  100/  217]    Overall Loss 0.307698    Objective Loss 0.307698                                        LR 0.000016    Time 0.311921    
2024-05-04 03:27:18,261 - Epoch: [248][  200/  217]    Overall Loss 0.311309    Objective Loss 0.311309                                        LR 0.000016    Time 0.270227    
2024-05-04 03:27:23,864 - Epoch: [248][  217/  217]    Overall Loss 0.310650    Objective Loss 0.310650    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.274872    
2024-05-04 03:27:24,201 - 

2024-05-04 03:27:24,202 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:27:53,802 - Epoch: [249][  100/  217]    Overall Loss 0.297489    Objective Loss 0.297489                                        LR 0.000016    Time 0.295891    
2024-05-04 03:28:18,222 - Epoch: [249][  200/  217]    Overall Loss 0.302548    Objective Loss 0.302548                                        LR 0.000016    Time 0.270000    
2024-05-04 03:28:22,526 - Epoch: [249][  217/  217]    Overall Loss 0.304082    Objective Loss 0.304082    Top1 93.442623    Top5 100.000000    LR 0.000016    Time 0.268672    
2024-05-04 03:28:23,264 - 

2024-05-04 03:28:23,265 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:28:53,552 - Epoch: [250][  100/  217]    Overall Loss 0.282157    Objective Loss 0.282157                                        LR 0.000016    Time 0.302772    
2024-05-04 03:29:16,709 - Epoch: [250][  200/  217]    Overall Loss 0.295888    Objective Loss 0.295888                                        LR 0.000016    Time 0.267124    
2024-05-04 03:29:22,320 - Epoch: [250][  217/  217]    Overall Loss 0.294802    Objective Loss 0.294802    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.272043    
2024-05-04 03:29:22,794 - --- validate (epoch=250)-----------
2024-05-04 03:29:22,794 - 1736 samples (32 per mini-batch)
2024-05-04 03:29:41,733 - Epoch: [250][   55/   55]    Loss 2.232703    Top1 53.283410    Top5 71.082949    
2024-05-04 03:29:42,424 - ==> Top1: 53.283    Top5: 71.083    Loss: 2.233

2024-05-04 03:29:42,428 - ==> Best [Top1: 53.283   Top5: 71.083   Sparsity:0.00   Params: 381920 on epoch: 250]
2024-05-04 03:29:42,429 - Saving checkpoint to: logs/2024.05.03-230945/qat_checkpoint.pth.tar
2024-05-04 03:29:42,474 - 

2024-05-04 03:29:42,475 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:30:13,521 - Epoch: [251][  100/  217]    Overall Loss 0.277339    Objective Loss 0.277339                                        LR 0.000016    Time 0.310365    
2024-05-04 03:30:34,699 - Epoch: [251][  200/  217]    Overall Loss 0.278687    Objective Loss 0.278687                                        LR 0.000016    Time 0.261026    
2024-05-04 03:30:39,169 - Epoch: [251][  217/  217]    Overall Loss 0.281301    Objective Loss 0.281301    Top1 93.442623    Top5 98.360656    LR 0.000016    Time 0.261167    
2024-05-04 03:30:39,548 - 

2024-05-04 03:30:39,549 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:31:13,382 - Epoch: [252][  100/  217]    Overall Loss 0.258758    Objective Loss 0.258758                                        LR 0.000016    Time 0.338239    
2024-05-04 03:31:41,808 - Epoch: [252][  200/  217]    Overall Loss 0.259346    Objective Loss 0.259346                                        LR 0.000016    Time 0.311202    
2024-05-04 03:31:47,326 - Epoch: [252][  217/  217]    Overall Loss 0.261747    Objective Loss 0.261747    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.312244    
2024-05-04 03:31:47,603 - 

2024-05-04 03:31:47,604 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:32:16,021 - Epoch: [253][  100/  217]    Overall Loss 0.241689    Objective Loss 0.241689                                        LR 0.000016    Time 0.284058    
2024-05-04 03:32:36,299 - Epoch: [253][  200/  217]    Overall Loss 0.252374    Objective Loss 0.252374                                        LR 0.000016    Time 0.243375    
2024-05-04 03:32:40,242 - Epoch: [253][  217/  217]    Overall Loss 0.257966    Objective Loss 0.257966    Top1 91.803279    Top5 100.000000    LR 0.000016    Time 0.242469    
2024-05-04 03:32:40,645 - 

2024-05-04 03:32:40,646 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:33:10,096 - Epoch: [254][  100/  217]    Overall Loss 0.238640    Objective Loss 0.238640                                        LR 0.000016    Time 0.294400    
2024-05-04 03:33:35,687 - Epoch: [254][  200/  217]    Overall Loss 0.246870    Objective Loss 0.246870                                        LR 0.000016    Time 0.275109    
2024-05-04 03:33:40,065 - Epoch: [254][  217/  217]    Overall Loss 0.251247    Objective Loss 0.251247    Top1 93.442623    Top5 98.360656    LR 0.000016    Time 0.273723    
2024-05-04 03:33:40,402 - 

2024-05-04 03:33:40,403 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:34:10,511 - Epoch: [255][  100/  217]    Overall Loss 0.225407    Objective Loss 0.225407                                        LR 0.000016    Time 0.300950    
2024-05-04 03:34:34,780 - Epoch: [255][  200/  217]    Overall Loss 0.229257    Objective Loss 0.229257                                        LR 0.000016    Time 0.271774    
2024-05-04 03:34:37,849 - Epoch: [255][  217/  217]    Overall Loss 0.231852    Objective Loss 0.231852    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.264618    
2024-05-04 03:34:38,193 - 

2024-05-04 03:34:38,194 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:35:05,570 - Epoch: [256][  100/  217]    Overall Loss 0.218741    Objective Loss 0.218741                                        LR 0.000016    Time 0.273667    
2024-05-04 03:35:28,102 - Epoch: [256][  200/  217]    Overall Loss 0.223092    Objective Loss 0.223092                                        LR 0.000016    Time 0.249452    
2024-05-04 03:35:31,751 - Epoch: [256][  217/  217]    Overall Loss 0.224545    Objective Loss 0.224545    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.246716    
2024-05-04 03:35:32,047 - 

2024-05-04 03:35:32,048 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:35:59,451 - Epoch: [257][  100/  217]    Overall Loss 0.214061    Objective Loss 0.214061                                        LR 0.000016    Time 0.273932    
2024-05-04 03:36:25,436 - Epoch: [257][  200/  217]    Overall Loss 0.215873    Objective Loss 0.215873                                        LR 0.000016    Time 0.266848    
2024-05-04 03:36:28,129 - Epoch: [257][  217/  217]    Overall Loss 0.215745    Objective Loss 0.215745    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.258341    
2024-05-04 03:36:28,457 - 

2024-05-04 03:36:28,458 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:36:55,485 - Epoch: [258][  100/  217]    Overall Loss 0.210465    Objective Loss 0.210465                                        LR 0.000016    Time 0.270152    
2024-05-04 03:37:23,905 - Epoch: [258][  200/  217]    Overall Loss 0.214825    Objective Loss 0.214825                                        LR 0.000016    Time 0.277128    
2024-05-04 03:37:28,036 - Epoch: [258][  217/  217]    Overall Loss 0.218503    Objective Loss 0.218503    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.274444    
2024-05-04 03:37:28,463 - 

2024-05-04 03:37:28,463 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:37:58,093 - Epoch: [259][  100/  217]    Overall Loss 0.195091    Objective Loss 0.195091                                        LR 0.000016    Time 0.296187    
2024-05-04 03:38:24,591 - Epoch: [259][  200/  217]    Overall Loss 0.205431    Objective Loss 0.205431                                        LR 0.000016    Time 0.280534    
2024-05-04 03:38:27,132 - Epoch: [259][  217/  217]    Overall Loss 0.204841    Objective Loss 0.204841    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.270257    
2024-05-04 03:38:27,474 - 

2024-05-04 03:38:27,475 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:39:00,092 - Epoch: [260][  100/  217]    Overall Loss 0.181575    Objective Loss 0.181575                                        LR 0.000016    Time 0.326075    
2024-05-04 03:39:25,034 - Epoch: [260][  200/  217]    Overall Loss 0.194744    Objective Loss 0.194744                                        LR 0.000016    Time 0.287703    
2024-05-04 03:39:27,504 - Epoch: [260][  217/  217]    Overall Loss 0.196393    Objective Loss 0.196393    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.276538    
2024-05-04 03:39:27,862 - --- validate (epoch=260)-----------
2024-05-04 03:39:27,862 - 1736 samples (32 per mini-batch)
2024-05-04 03:39:43,793 - Epoch: [260][   55/   55]    Loss 2.300265    Top1 52.649770    Top5 70.910138    
2024-05-04 03:39:44,056 - ==> Top1: 52.650    Top5: 70.910    Loss: 2.300

2024-05-04 03:39:44,060 - ==> Best [Top1: 53.283   Top5: 71.083   Sparsity:0.00   Params: 381920 on epoch: 250]
2024-05-04 03:39:44,061 - Saving checkpoint to: logs/2024.05.03-230945/qat_checkpoint.pth.tar
2024-05-04 03:39:44,095 - 

2024-05-04 03:39:44,096 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:40:14,309 - Epoch: [261][  100/  217]    Overall Loss 0.194378    Objective Loss 0.194378                                        LR 0.000016    Time 0.302027    
2024-05-04 03:40:36,573 - Epoch: [261][  200/  217]    Overall Loss 0.195215    Objective Loss 0.195215                                        LR 0.000016    Time 0.262292    
2024-05-04 03:40:41,296 - Epoch: [261][  217/  217]    Overall Loss 0.196739    Objective Loss 0.196739    Top1 95.081967    Top5 100.000000    LR 0.000016    Time 0.263497    
2024-05-04 03:40:41,579 - 

2024-05-04 03:40:41,580 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:41:14,042 - Epoch: [262][  100/  217]    Overall Loss 0.182998    Objective Loss 0.182998                                        LR 0.000016    Time 0.324525    
2024-05-04 03:41:41,134 - Epoch: [262][  200/  217]    Overall Loss 0.191609    Objective Loss 0.191609                                        LR 0.000016    Time 0.297671    
2024-05-04 03:41:46,006 - Epoch: [262][  217/  217]    Overall Loss 0.191549    Objective Loss 0.191549    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.296793    
2024-05-04 03:41:46,439 - 

2024-05-04 03:41:46,440 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:42:17,231 - Epoch: [263][  100/  217]    Overall Loss 0.175269    Objective Loss 0.175269                                        LR 0.000016    Time 0.307808    
2024-05-04 03:42:38,601 - Epoch: [263][  200/  217]    Overall Loss 0.186406    Objective Loss 0.186406                                        LR 0.000016    Time 0.260709    
2024-05-04 03:42:42,684 - Epoch: [263][  217/  217]    Overall Loss 0.184284    Objective Loss 0.184284    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.259090    
2024-05-04 03:42:43,034 - 

2024-05-04 03:42:43,035 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:43:14,265 - Epoch: [264][  100/  217]    Overall Loss 0.175481    Objective Loss 0.175481                                        LR 0.000016    Time 0.312208    
2024-05-04 03:43:42,047 - Epoch: [264][  200/  217]    Overall Loss 0.178086    Objective Loss 0.178086                                        LR 0.000016    Time 0.294969    
2024-05-04 03:43:44,779 - Epoch: [264][  217/  217]    Overall Loss 0.178788    Objective Loss 0.178788    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.284438    
2024-05-04 03:43:45,099 - 

2024-05-04 03:43:45,099 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:44:15,356 - Epoch: [265][  100/  217]    Overall Loss 0.164095    Objective Loss 0.164095                                        LR 0.000016    Time 0.302468    
2024-05-04 03:44:39,435 - Epoch: [265][  200/  217]    Overall Loss 0.176774    Objective Loss 0.176774                                        LR 0.000016    Time 0.271583    
2024-05-04 03:44:43,387 - Epoch: [265][  217/  217]    Overall Loss 0.176814    Objective Loss 0.176814    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.268511    
2024-05-04 03:44:43,888 - 

2024-05-04 03:44:43,889 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:45:14,186 - Epoch: [266][  100/  217]    Overall Loss 0.164588    Objective Loss 0.164588                                        LR 0.000016    Time 0.302873    
2024-05-04 03:45:41,404 - Epoch: [266][  200/  217]    Overall Loss 0.173543    Objective Loss 0.173543                                        LR 0.000016    Time 0.287480    
2024-05-04 03:45:46,223 - Epoch: [266][  217/  217]    Overall Loss 0.172200    Objective Loss 0.172200    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.287156    
2024-05-04 03:45:46,683 - 

2024-05-04 03:45:46,684 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:46:18,027 - Epoch: [267][  100/  217]    Overall Loss 0.157735    Objective Loss 0.157735                                        LR 0.000016    Time 0.313333    
2024-05-04 03:46:44,218 - Epoch: [267][  200/  217]    Overall Loss 0.170750    Objective Loss 0.170750                                        LR 0.000016    Time 0.287576    
2024-05-04 03:46:47,528 - Epoch: [267][  217/  217]    Overall Loss 0.170777    Objective Loss 0.170777    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.280286    
2024-05-04 03:46:47,902 - 

2024-05-04 03:46:47,903 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:47:19,143 - Epoch: [268][  100/  217]    Overall Loss 0.151943    Objective Loss 0.151943                                        LR 0.000016    Time 0.312300    
2024-05-04 03:47:45,181 - Epoch: [268][  200/  217]    Overall Loss 0.165344    Objective Loss 0.165344                                        LR 0.000016    Time 0.286295    
2024-05-04 03:47:50,067 - Epoch: [268][  217/  217]    Overall Loss 0.165015    Objective Loss 0.165015    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.286376    
2024-05-04 03:47:50,385 - 

2024-05-04 03:47:50,386 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:48:20,041 - Epoch: [269][  100/  217]    Overall Loss 0.165603    Objective Loss 0.165603                                        LR 0.000016    Time 0.296455    
2024-05-04 03:48:40,844 - Epoch: [269][  200/  217]    Overall Loss 0.172982    Objective Loss 0.172982                                        LR 0.000016    Time 0.252198    
2024-05-04 03:48:44,836 - Epoch: [269][  217/  217]    Overall Loss 0.173605    Objective Loss 0.173605    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.250832    
2024-05-04 03:48:45,308 - 

2024-05-04 03:48:45,308 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:49:14,809 - Epoch: [270][  100/  217]    Overall Loss 0.153298    Objective Loss 0.153298                                        LR 0.000016    Time 0.294901    
2024-05-04 03:49:41,716 - Epoch: [270][  200/  217]    Overall Loss 0.153593    Objective Loss 0.153593                                        LR 0.000016    Time 0.281940    
2024-05-04 03:49:46,654 - Epoch: [270][  217/  217]    Overall Loss 0.155205    Objective Loss 0.155205    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.282603    
2024-05-04 03:49:47,104 - --- validate (epoch=270)-----------
2024-05-04 03:49:47,104 - 1736 samples (32 per mini-batch)
2024-05-04 03:50:02,237 - Epoch: [270][   55/   55]    Loss 2.257860    Top1 53.283410    Top5 71.198157    
2024-05-04 03:50:02,533 - ==> Top1: 53.283    Top5: 71.198    Loss: 2.258

2024-05-04 03:50:02,537 - ==> Best [Top1: 53.283   Top5: 71.198   Sparsity:0.00   Params: 381920 on epoch: 270]
2024-05-04 03:50:02,537 - Saving checkpoint to: logs/2024.05.03-230945/qat_checkpoint.pth.tar
2024-05-04 03:50:02,593 - 

2024-05-04 03:50:02,594 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:50:35,508 - Epoch: [271][  100/  217]    Overall Loss 0.154950    Objective Loss 0.154950                                        LR 0.000016    Time 0.329034    
2024-05-04 03:50:58,356 - Epoch: [271][  200/  217]    Overall Loss 0.154919    Objective Loss 0.154919                                        LR 0.000016    Time 0.278710    
2024-05-04 03:51:02,879 - Epoch: [271][  217/  217]    Overall Loss 0.156483    Objective Loss 0.156483    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.277707    
2024-05-04 03:51:03,185 - 

2024-05-04 03:51:03,185 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:51:32,840 - Epoch: [272][  100/  217]    Overall Loss 0.154823    Objective Loss 0.154823                                        LR 0.000016    Time 0.296450    
2024-05-04 03:51:58,730 - Epoch: [272][  200/  217]    Overall Loss 0.159674    Objective Loss 0.159674                                        LR 0.000016    Time 0.277631    
2024-05-04 03:52:02,389 - Epoch: [272][  217/  217]    Overall Loss 0.159075    Objective Loss 0.159075    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.272736    
2024-05-04 03:52:02,894 - 

2024-05-04 03:52:02,895 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:52:32,404 - Epoch: [273][  100/  217]    Overall Loss 0.172931    Objective Loss 0.172931                                        LR 0.000016    Time 0.294987    
2024-05-04 03:52:54,384 - Epoch: [273][  200/  217]    Overall Loss 0.169127    Objective Loss 0.169127                                        LR 0.000016    Time 0.257343    
2024-05-04 03:52:58,939 - Epoch: [273][  217/  217]    Overall Loss 0.168239    Objective Loss 0.168239    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.258169    
2024-05-04 03:52:59,296 - 

2024-05-04 03:52:59,297 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:53:29,015 - Epoch: [274][  100/  217]    Overall Loss 0.143131    Objective Loss 0.143131                                        LR 0.000016    Time 0.297089    
2024-05-04 03:53:52,293 - Epoch: [274][  200/  217]    Overall Loss 0.150996    Objective Loss 0.150996                                        LR 0.000016    Time 0.264890    
2024-05-04 03:53:57,663 - Epoch: [274][  217/  217]    Overall Loss 0.152910    Objective Loss 0.152910    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.268874    
2024-05-04 03:53:58,011 - 

2024-05-04 03:53:58,012 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:54:27,660 - Epoch: [275][  100/  217]    Overall Loss 0.163035    Objective Loss 0.163035                                        LR 0.000016    Time 0.296369    
2024-05-04 03:54:55,088 - Epoch: [275][  200/  217]    Overall Loss 0.155349    Objective Loss 0.155349                                        LR 0.000016    Time 0.285282    
2024-05-04 03:54:58,544 - Epoch: [275][  217/  217]    Overall Loss 0.153641    Objective Loss 0.153641    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.278846    
2024-05-04 03:54:59,342 - 

2024-05-04 03:54:59,343 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:55:30,462 - Epoch: [276][  100/  217]    Overall Loss 0.137571    Objective Loss 0.137571                                        LR 0.000016    Time 0.311079    
2024-05-04 03:55:55,647 - Epoch: [276][  200/  217]    Overall Loss 0.141127    Objective Loss 0.141127                                        LR 0.000016    Time 0.281418    
2024-05-04 03:55:59,486 - Epoch: [276][  217/  217]    Overall Loss 0.142389    Objective Loss 0.142389    Top1 96.721311    Top5 100.000000    LR 0.000016    Time 0.277050    
2024-05-04 03:56:00,103 - 

2024-05-04 03:56:00,104 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:56:27,129 - Epoch: [277][  100/  217]    Overall Loss 0.151285    Objective Loss 0.151285                                        LR 0.000016    Time 0.270117    
2024-05-04 03:56:54,611 - Epoch: [277][  200/  217]    Overall Loss 0.156106    Objective Loss 0.156106                                        LR 0.000016    Time 0.272422    
2024-05-04 03:56:59,261 - Epoch: [277][  217/  217]    Overall Loss 0.154457    Objective Loss 0.154457    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.272499    
2024-05-04 03:56:59,737 - 

2024-05-04 03:56:59,737 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:57:28,834 - Epoch: [278][  100/  217]    Overall Loss 0.138211    Objective Loss 0.138211                                        LR 0.000016    Time 0.290865    
2024-05-04 03:57:51,789 - Epoch: [278][  200/  217]    Overall Loss 0.139617    Objective Loss 0.139617                                        LR 0.000016    Time 0.260167    
2024-05-04 03:57:56,015 - Epoch: [278][  217/  217]    Overall Loss 0.142439    Objective Loss 0.142439    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.259248    
2024-05-04 03:57:56,466 - 

2024-05-04 03:57:56,467 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:58:26,553 - Epoch: [279][  100/  217]    Overall Loss 0.148713    Objective Loss 0.148713                                        LR 0.000016    Time 0.300754    
2024-05-04 03:58:54,795 - Epoch: [279][  200/  217]    Overall Loss 0.150574    Objective Loss 0.150574                                        LR 0.000016    Time 0.291536    
2024-05-04 03:58:57,916 - Epoch: [279][  217/  217]    Overall Loss 0.151993    Objective Loss 0.151993    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.283071    
2024-05-04 03:58:58,226 - 

2024-05-04 03:58:58,226 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 03:59:26,737 - Epoch: [280][  100/  217]    Overall Loss 0.126281    Objective Loss 0.126281                                        LR 0.000016    Time 0.284998    
2024-05-04 03:59:49,181 - Epoch: [280][  200/  217]    Overall Loss 0.137559    Objective Loss 0.137559                                        LR 0.000016    Time 0.254677    
2024-05-04 03:59:54,569 - Epoch: [280][  217/  217]    Overall Loss 0.140405    Objective Loss 0.140405    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.259542    
2024-05-04 03:59:55,033 - --- validate (epoch=280)-----------
2024-05-04 03:59:55,034 - 1736 samples (32 per mini-batch)
2024-05-04 04:00:11,788 - Epoch: [280][   55/   55]    Loss 2.276001    Top1 53.686636    Top5 71.486175    
2024-05-04 04:00:12,022 - ==> Top1: 53.687    Top5: 71.486    Loss: 2.276

2024-05-04 04:00:12,027 - ==> Best [Top1: 53.687   Top5: 71.486   Sparsity:0.00   Params: 381920 on epoch: 280]
2024-05-04 04:00:12,027 - Saving checkpoint to: logs/2024.05.03-230945/qat_checkpoint.pth.tar
2024-05-04 04:00:12,073 - 

2024-05-04 04:00:12,073 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:00:40,152 - Epoch: [281][  100/  217]    Overall Loss 0.127083    Objective Loss 0.127083                                        LR 0.000016    Time 0.280683    
2024-05-04 04:01:03,889 - Epoch: [281][  200/  217]    Overall Loss 0.139398    Objective Loss 0.139398                                        LR 0.000016    Time 0.258977    
2024-05-04 04:01:07,481 - Epoch: [281][  217/  217]    Overall Loss 0.139132    Objective Loss 0.139132    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.255233    
2024-05-04 04:01:07,908 - 

2024-05-04 04:01:07,909 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:01:36,699 - Epoch: [282][  100/  217]    Overall Loss 0.127801    Objective Loss 0.127801                                        LR 0.000016    Time 0.287805    
2024-05-04 04:01:58,014 - Epoch: [282][  200/  217]    Overall Loss 0.133353    Objective Loss 0.133353                                        LR 0.000016    Time 0.250432    
2024-05-04 04:02:02,286 - Epoch: [282][  217/  217]    Overall Loss 0.136620    Objective Loss 0.136620    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.250490    
2024-05-04 04:02:02,792 - 

2024-05-04 04:02:02,793 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:02:32,246 - Epoch: [283][  100/  217]    Overall Loss 0.123188    Objective Loss 0.123188                                        LR 0.000016    Time 0.294428    
2024-05-04 04:02:52,777 - Epoch: [283][  200/  217]    Overall Loss 0.130299    Objective Loss 0.130299                                        LR 0.000016    Time 0.249822    
2024-05-04 04:02:56,149 - Epoch: [283][  217/  217]    Overall Loss 0.131787    Objective Loss 0.131787    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.245785    
2024-05-04 04:02:56,622 - 

2024-05-04 04:02:56,623 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:03:25,178 - Epoch: [284][  100/  217]    Overall Loss 0.134540    Objective Loss 0.134540                                        LR 0.000016    Time 0.285444    
2024-05-04 04:03:45,673 - Epoch: [284][  200/  217]    Overall Loss 0.132802    Objective Loss 0.132802                                        LR 0.000016    Time 0.245152    
2024-05-04 04:03:49,479 - Epoch: [284][  217/  217]    Overall Loss 0.134444    Objective Loss 0.134444    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.243476    
2024-05-04 04:03:49,988 - 

2024-05-04 04:03:49,988 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:04:15,536 - Epoch: [285][  100/  217]    Overall Loss 0.132935    Objective Loss 0.132935                                        LR 0.000016    Time 0.255381    
2024-05-04 04:04:38,759 - Epoch: [285][  200/  217]    Overall Loss 0.137608    Objective Loss 0.137608                                        LR 0.000016    Time 0.243757    
2024-05-04 04:04:41,758 - Epoch: [285][  217/  217]    Overall Loss 0.138601    Objective Loss 0.138601    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.238472    
2024-05-04 04:04:42,098 - 

2024-05-04 04:04:42,099 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:05:04,904 - Epoch: [286][  100/  217]    Overall Loss 0.112258    Objective Loss 0.112258                                        LR 0.000016    Time 0.227967    
2024-05-04 04:05:30,976 - Epoch: [286][  200/  217]    Overall Loss 0.128098    Objective Loss 0.128098                                        LR 0.000016    Time 0.244297    
2024-05-04 04:05:33,853 - Epoch: [286][  217/  217]    Overall Loss 0.128917    Objective Loss 0.128917    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.238409    
2024-05-04 04:05:34,191 - 

2024-05-04 04:05:34,191 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:05:56,132 - Epoch: [287][  100/  217]    Overall Loss 0.128505    Objective Loss 0.128505                                        LR 0.000016    Time 0.219304    
2024-05-04 04:06:16,528 - Epoch: [287][  200/  217]    Overall Loss 0.129998    Objective Loss 0.129998                                        LR 0.000016    Time 0.211590    
2024-05-04 04:06:20,147 - Epoch: [287][  217/  217]    Overall Loss 0.129501    Objective Loss 0.129501    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.211684    
2024-05-04 04:06:20,518 - 

2024-05-04 04:06:20,519 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:06:46,299 - Epoch: [288][  100/  217]    Overall Loss 0.122684    Objective Loss 0.122684                                        LR 0.000016    Time 0.257711    
2024-05-04 04:07:05,447 - Epoch: [288][  200/  217]    Overall Loss 0.127031    Objective Loss 0.127031                                        LR 0.000016    Time 0.224546    
2024-05-04 04:07:09,923 - Epoch: [288][  217/  217]    Overall Loss 0.127642    Objective Loss 0.127642    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.227574    
2024-05-04 04:07:10,225 - 

2024-05-04 04:07:10,226 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:07:34,421 - Epoch: [289][  100/  217]    Overall Loss 0.127273    Objective Loss 0.127273                                        LR 0.000016    Time 0.241836    
2024-05-04 04:07:55,385 - Epoch: [289][  200/  217]    Overall Loss 0.133098    Objective Loss 0.133098                                        LR 0.000016    Time 0.225671    
2024-05-04 04:07:58,620 - Epoch: [289][  217/  217]    Overall Loss 0.131338    Objective Loss 0.131338    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.222891    
2024-05-04 04:07:58,956 - 

2024-05-04 04:07:58,957 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:08:24,980 - Epoch: [290][  100/  217]    Overall Loss 0.114022    Objective Loss 0.114022                                        LR 0.000016    Time 0.260133    
2024-05-04 04:08:48,799 - Epoch: [290][  200/  217]    Overall Loss 0.121709    Objective Loss 0.121709                                        LR 0.000016    Time 0.249113    
2024-05-04 04:08:51,668 - Epoch: [290][  217/  217]    Overall Loss 0.121476    Objective Loss 0.121476    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.242808    
2024-05-04 04:08:52,032 - --- validate (epoch=290)-----------
2024-05-04 04:08:52,032 - 1736 samples (32 per mini-batch)
2024-05-04 04:09:06,727 - Epoch: [290][   55/   55]    Loss 2.291530    Top1 53.571429    Top5 71.486175    
2024-05-04 04:09:07,175 - ==> Top1: 53.571    Top5: 71.486    Loss: 2.292

2024-05-04 04:09:07,182 - ==> Best [Top1: 53.687   Top5: 71.486   Sparsity:0.00   Params: 381920 on epoch: 280]
2024-05-04 04:09:07,183 - Saving checkpoint to: logs/2024.05.03-230945/qat_checkpoint.pth.tar
2024-05-04 04:09:07,214 - 

2024-05-04 04:09:07,214 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:09:34,978 - Epoch: [291][  100/  217]    Overall Loss 0.120641    Objective Loss 0.120641                                        LR 0.000016    Time 0.277539    
2024-05-04 04:09:55,997 - Epoch: [291][  200/  217]    Overall Loss 0.124948    Objective Loss 0.124948                                        LR 0.000016    Time 0.243827    
2024-05-04 04:09:58,875 - Epoch: [291][  217/  217]    Overall Loss 0.125861    Objective Loss 0.125861    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.237980    
2024-05-04 04:09:59,270 - 

2024-05-04 04:09:59,271 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:10:19,435 - Epoch: [292][  100/  217]    Overall Loss 0.128008    Objective Loss 0.128008                                        LR 0.000016    Time 0.201541    
2024-05-04 04:10:39,050 - Epoch: [292][  200/  217]    Overall Loss 0.130625    Objective Loss 0.130625                                        LR 0.000016    Time 0.198797    
2024-05-04 04:10:41,497 - Epoch: [292][  217/  217]    Overall Loss 0.128423    Objective Loss 0.128423    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.194492    
2024-05-04 04:10:42,016 - 

2024-05-04 04:10:42,017 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:11:04,942 - Epoch: [293][  100/  217]    Overall Loss 0.116536    Objective Loss 0.116536                                        LR 0.000016    Time 0.229153    
2024-05-04 04:11:23,907 - Epoch: [293][  200/  217]    Overall Loss 0.122148    Objective Loss 0.122148                                        LR 0.000016    Time 0.209354    
2024-05-04 04:11:26,359 - Epoch: [293][  217/  217]    Overall Loss 0.123387    Objective Loss 0.123387    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.204246    
2024-05-04 04:11:26,754 - 

2024-05-04 04:11:26,755 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:11:50,826 - Epoch: [294][  100/  217]    Overall Loss 0.123704    Objective Loss 0.123704                                        LR 0.000016    Time 0.240611    
2024-05-04 04:12:11,622 - Epoch: [294][  200/  217]    Overall Loss 0.124261    Objective Loss 0.124261                                        LR 0.000016    Time 0.224238    
2024-05-04 04:12:14,157 - Epoch: [294][  217/  217]    Overall Loss 0.123220    Objective Loss 0.123220    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.218346    
2024-05-04 04:12:14,603 - 

2024-05-04 04:12:14,604 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:12:35,064 - Epoch: [295][  100/  217]    Overall Loss 0.107406    Objective Loss 0.107406                                        LR 0.000016    Time 0.204514    
2024-05-04 04:12:52,893 - Epoch: [295][  200/  217]    Overall Loss 0.117875    Objective Loss 0.117875                                        LR 0.000016    Time 0.191361    
2024-05-04 04:12:55,485 - Epoch: [295][  217/  217]    Overall Loss 0.118884    Objective Loss 0.118884    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.188308    
2024-05-04 04:12:55,750 - 

2024-05-04 04:12:55,750 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:13:14,549 - Epoch: [296][  100/  217]    Overall Loss 0.108674    Objective Loss 0.108674                                        LR 0.000016    Time 0.187894    
2024-05-04 04:13:32,574 - Epoch: [296][  200/  217]    Overall Loss 0.117702    Objective Loss 0.117702                                        LR 0.000016    Time 0.184027    
2024-05-04 04:13:35,559 - Epoch: [296][  217/  217]    Overall Loss 0.117097    Objective Loss 0.117097    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.183354    
2024-05-04 04:13:35,956 - 

2024-05-04 04:13:35,956 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:13:55,154 - Epoch: [297][  100/  217]    Overall Loss 0.118516    Objective Loss 0.118516                                        LR 0.000016    Time 0.191890    
2024-05-04 04:14:11,180 - Epoch: [297][  200/  217]    Overall Loss 0.116051    Objective Loss 0.116051                                        LR 0.000016    Time 0.176035    
2024-05-04 04:14:13,658 - Epoch: [297][  217/  217]    Overall Loss 0.116357    Objective Loss 0.116357    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.173655    
2024-05-04 04:14:14,096 - 

2024-05-04 04:14:14,096 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:14:32,348 - Epoch: [298][  100/  217]    Overall Loss 0.113463    Objective Loss 0.113463                                        LR 0.000016    Time 0.182446    
2024-05-04 04:14:47,779 - Epoch: [298][  200/  217]    Overall Loss 0.119125    Objective Loss 0.119125                                        LR 0.000016    Time 0.168341    
2024-05-04 04:14:50,265 - Epoch: [298][  217/  217]    Overall Loss 0.119611    Objective Loss 0.119611    Top1 100.000000    Top5 100.000000    LR 0.000016    Time 0.166603    
2024-05-04 04:14:50,787 - 

2024-05-04 04:14:50,788 - Training epoch: 6941 samples (32 per mini-batch)
2024-05-04 04:15:09,076 - Epoch: [299][  100/  217]    Overall Loss 0.100232    Objective Loss 0.100232                                        LR 0.000016    Time 0.182801    
2024-05-04 04:15:24,647 - Epoch: [299][  200/  217]    Overall Loss 0.107855    Objective Loss 0.107855                                        LR 0.000016    Time 0.169218    
2024-05-04 04:15:26,938 - Epoch: [299][  217/  217]    Overall Loss 0.110924    Objective Loss 0.110924    Top1 98.360656    Top5 100.000000    LR 0.000016    Time 0.166512    
2024-05-04 04:15:27,317 - --- test ---------------------
2024-05-04 04:15:27,317 - 1736 samples (32 per mini-batch)
2024-05-04 04:15:38,536 - Test: [   55/   55]    Loss 2.343586    Top1 53.225806    Top5 69.758065    
2024-05-04 04:15:38,965 - ==> Top1: 53.226    Top5: 69.758    Loss: 2.344

2024-05-04 04:15:38,969 - 
2024-05-04 04:15:38,969 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.03-230945/2024.05.03-230945.log
