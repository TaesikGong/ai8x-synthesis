2024-05-03 23:09:49,625 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.03-230949/2024.05.03-230949.log
2024-05-03 23:09:54,362 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2024-05-03 23:09:54,363 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2024-05-03 23:09:54,558 - Dataset sizes:
	training=6941
	validation=1736
	test=1736
2024-05-03 23:09:54,558 - Reading compression schedule from: policies/schedule-cifar100-effnet2.yaml
2024-05-03 23:09:54,567 - 

2024-05-03 23:09:54,567 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:10:46,841 - Epoch: [0][   70/   70]    Overall Loss 3.743293    Objective Loss 3.743293    Top1 26.950355    Top5 41.843972    LR 0.001000    Time 0.746639    
2024-05-03 23:10:47,791 - --- validate (epoch=0)-----------
2024-05-03 23:10:47,794 - 1736 samples (100 per mini-batch)
2024-05-03 23:11:03,244 - Epoch: [0][   18/   18]    Loss 4.594494    Top1 8.698157    Top5 14.343318    
2024-05-03 23:11:03,580 - ==> Top1: 8.698    Top5: 14.343    Loss: 4.594

2024-05-03 23:11:03,590 - ==> Best [Top1: 8.698   Top5: 14.343   Sparsity:0.00   Params: 754976 on epoch: 0]
2024-05-03 23:11:03,590 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-03 23:11:03,662 - 

2024-05-03 23:11:03,663 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:11:56,024 - Epoch: [1][   70/   70]    Overall Loss 3.276564    Objective Loss 3.276564    Top1 29.078014    Top5 44.680851    LR 0.001000    Time 0.747883    
2024-05-03 23:11:56,208 - 

2024-05-03 23:11:56,209 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:12:51,693 - Epoch: [2][   70/   70]    Overall Loss 3.103983    Objective Loss 3.103983    Top1 33.333333    Top5 43.971631    LR 0.001000    Time 0.792500    
2024-05-03 23:12:52,042 - 

2024-05-03 23:12:52,043 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:13:47,791 - Epoch: [3][   70/   70]    Overall Loss 2.953604    Objective Loss 2.953604    Top1 31.914894    Top5 46.808511    LR 0.001000    Time 0.796264    
2024-05-03 23:13:48,562 - 

2024-05-03 23:13:48,563 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:14:45,869 - Epoch: [4][   70/   70]    Overall Loss 2.816146    Objective Loss 2.816146    Top1 33.333333    Top5 53.191489    LR 0.001000    Time 0.818530    
2024-05-03 23:14:46,597 - 

2024-05-03 23:14:46,597 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:15:42,636 - Epoch: [5][   70/   70]    Overall Loss 2.704226    Objective Loss 2.704226    Top1 29.787234    Top5 47.517730    LR 0.001000    Time 0.800398    
2024-05-03 23:15:44,396 - 

2024-05-03 23:15:44,397 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:16:38,992 - Epoch: [6][   70/   70]    Overall Loss 2.578284    Objective Loss 2.578284    Top1 45.390071    Top5 60.283688    LR 0.001000    Time 0.779803    
2024-05-03 23:16:39,357 - 

2024-05-03 23:16:39,358 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:17:36,395 - Epoch: [7][   70/   70]    Overall Loss 2.455152    Objective Loss 2.455152    Top1 43.971631    Top5 56.737589    LR 0.001000    Time 0.814695    
2024-05-03 23:17:36,710 - 

2024-05-03 23:17:36,711 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:18:35,028 - Epoch: [8][   70/   70]    Overall Loss 2.352607    Objective Loss 2.352607    Top1 46.099291    Top5 63.829787    LR 0.001000    Time 0.832960    
2024-05-03 23:18:35,411 - 

2024-05-03 23:18:35,412 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:19:30,966 - Epoch: [9][   70/   70]    Overall Loss 2.250263    Objective Loss 2.250263    Top1 58.865248    Top5 72.340426    LR 0.001000    Time 0.793526    
2024-05-03 23:19:31,277 - 

2024-05-03 23:19:31,278 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:20:22,931 - Epoch: [10][   70/   70]    Overall Loss 2.130392    Objective Loss 2.130392    Top1 48.226950    Top5 68.085106    LR 0.001000    Time 0.737771    
2024-05-03 23:20:23,212 - --- validate (epoch=10)-----------
2024-05-03 23:20:23,213 - 1736 samples (100 per mini-batch)
2024-05-03 23:20:40,075 - Epoch: [10][   18/   18]    Loss 3.845512    Top1 32.661290    Top5 48.675115    
2024-05-03 23:20:40,298 - ==> Top1: 32.661    Top5: 48.675    Loss: 3.846

2024-05-03 23:20:40,306 - ==> Best [Top1: 32.661   Top5: 48.675   Sparsity:0.00   Params: 754976 on epoch: 10]
2024-05-03 23:20:40,306 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-03 23:20:40,415 - 

2024-05-03 23:20:40,416 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:21:43,713 - Epoch: [11][   70/   70]    Overall Loss 1.990583    Objective Loss 1.990583    Top1 44.680851    Top5 63.120567    LR 0.001000    Time 0.904117    
2024-05-03 23:21:44,141 - 

2024-05-03 23:21:44,142 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:22:37,590 - Epoch: [12][   70/   70]    Overall Loss 1.842636    Objective Loss 1.842636    Top1 43.971631    Top5 73.049645    LR 0.001000    Time 0.763392    
2024-05-03 23:22:38,335 - 

2024-05-03 23:22:38,335 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:23:32,604 - Epoch: [13][   70/   70]    Overall Loss 1.762238    Objective Loss 1.762238    Top1 48.226950    Top5 74.468085    LR 0.001000    Time 0.775114    
2024-05-03 23:23:33,010 - 

2024-05-03 23:23:33,011 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:24:26,608 - Epoch: [14][   70/   70]    Overall Loss 1.578598    Objective Loss 1.578598    Top1 59.574468    Top5 81.560284    LR 0.001000    Time 0.765526    
2024-05-03 23:24:27,024 - 

2024-05-03 23:24:27,025 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:25:21,319 - Epoch: [15][   70/   70]    Overall Loss 1.429565    Objective Loss 1.429565    Top1 56.737589    Top5 75.177305    LR 0.001000    Time 0.775517    
2024-05-03 23:25:21,631 - 

2024-05-03 23:25:21,632 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:26:12,542 - Epoch: [16][   70/   70]    Overall Loss 1.293491    Objective Loss 1.293491    Top1 63.120567    Top5 79.432624    LR 0.001000    Time 0.727165    
2024-05-03 23:26:13,695 - 

2024-05-03 23:26:13,696 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:27:11,593 - Epoch: [17][   70/   70]    Overall Loss 1.161628    Objective Loss 1.161628    Top1 70.921986    Top5 89.361702    LR 0.001000    Time 0.826946    
2024-05-03 23:27:11,851 - 

2024-05-03 23:27:11,852 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:28:07,326 - Epoch: [18][   70/   70]    Overall Loss 0.980335    Objective Loss 0.980335    Top1 69.503546    Top5 91.489362    LR 0.001000    Time 0.792342    
2024-05-03 23:28:07,783 - 

2024-05-03 23:28:07,784 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:29:05,979 - Epoch: [19][   70/   70]    Overall Loss 0.856103    Objective Loss 0.856103    Top1 74.468085    Top5 92.198582    LR 0.001000    Time 0.831210    
2024-05-03 23:29:07,000 - 

2024-05-03 23:29:07,000 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:30:05,427 - Epoch: [20][   70/   70]    Overall Loss 0.736660    Objective Loss 0.736660    Top1 80.141844    Top5 97.163121    LR 0.001000    Time 0.834534    
2024-05-03 23:30:05,789 - --- validate (epoch=20)-----------
2024-05-03 23:30:05,790 - 1736 samples (100 per mini-batch)
2024-05-03 23:30:26,079 - Epoch: [20][   18/   18]    Loss 5.167106    Top1 28.974654    Top5 43.605991    
2024-05-03 23:30:26,889 - ==> Top1: 28.975    Top5: 43.606    Loss: 5.167

2024-05-03 23:30:26,896 - ==> Best [Top1: 32.661   Top5: 48.675   Sparsity:0.00   Params: 754976 on epoch: 10]
2024-05-03 23:30:26,896 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-03 23:30:26,984 - 

2024-05-03 23:30:26,984 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:31:20,178 - Epoch: [21][   70/   70]    Overall Loss 0.687605    Objective Loss 0.687605    Top1 78.014184    Top5 95.035461    LR 0.001000    Time 0.759762    
2024-05-03 23:31:20,603 - 

2024-05-03 23:31:20,603 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:32:15,208 - Epoch: [22][   70/   70]    Overall Loss 0.557588    Objective Loss 0.557588    Top1 88.652482    Top5 98.581560    LR 0.001000    Time 0.779931    
2024-05-03 23:32:15,767 - 

2024-05-03 23:32:15,768 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:33:09,228 - Epoch: [23][   70/   70]    Overall Loss 0.445604    Objective Loss 0.445604    Top1 90.780142    Top5 97.163121    LR 0.001000    Time 0.763567    
2024-05-03 23:33:09,570 - 

2024-05-03 23:33:09,571 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:34:04,741 - Epoch: [24][   70/   70]    Overall Loss 0.367859    Objective Loss 0.367859    Top1 92.907801    Top5 99.290780    LR 0.001000    Time 0.788039    
2024-05-03 23:34:05,492 - 

2024-05-03 23:34:05,492 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:35:04,900 - Epoch: [25][   70/   70]    Overall Loss 0.276407    Objective Loss 0.276407    Top1 97.163121    Top5 99.290780    LR 0.001000    Time 0.848546    
2024-05-03 23:35:05,718 - 

2024-05-03 23:35:05,719 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:36:04,867 - Epoch: [26][   70/   70]    Overall Loss 0.212974    Objective Loss 0.212974    Top1 96.453901    Top5 100.000000    LR 0.001000    Time 0.844837    
2024-05-03 23:36:05,166 - 

2024-05-03 23:36:05,167 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:37:04,791 - Epoch: [27][   70/   70]    Overall Loss 0.189042    Objective Loss 0.189042    Top1 98.581560    Top5 100.000000    LR 0.001000    Time 0.851653    
2024-05-03 23:37:05,545 - 

2024-05-03 23:37:05,546 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:38:00,136 - Epoch: [28][   70/   70]    Overall Loss 0.134742    Objective Loss 0.134742    Top1 97.872340    Top5 98.581560    LR 0.001000    Time 0.779743    
2024-05-03 23:38:00,827 - 

2024-05-03 23:38:00,827 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:38:56,167 - Epoch: [29][   70/   70]    Overall Loss 0.108075    Objective Loss 0.108075    Top1 98.581560    Top5 100.000000    LR 0.001000    Time 0.790467    
2024-05-03 23:38:56,376 - 

2024-05-03 23:38:56,377 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:39:54,057 - Epoch: [30][   70/   70]    Overall Loss 0.087357    Objective Loss 0.087357    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.823905    
2024-05-03 23:39:54,273 - --- validate (epoch=30)-----------
2024-05-03 23:39:54,274 - 1736 samples (100 per mini-batch)
2024-05-03 23:40:11,068 - Epoch: [30][   18/   18]    Loss 4.205768    Top1 35.195853    Top5 53.052995    
2024-05-03 23:40:11,716 - ==> Top1: 35.196    Top5: 53.053    Loss: 4.206

2024-05-03 23:40:11,748 - ==> Best [Top1: 35.196   Top5: 53.053   Sparsity:0.00   Params: 754976 on epoch: 30]
2024-05-03 23:40:11,748 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-03 23:40:11,888 - 

2024-05-03 23:40:11,889 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:41:12,598 - Epoch: [31][   70/   70]    Overall Loss 0.065402    Objective Loss 0.065402    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.867153    
2024-05-03 23:41:12,856 - 

2024-05-03 23:41:12,856 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:42:11,524 - Epoch: [32][   70/   70]    Overall Loss 0.053277    Objective Loss 0.053277    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.837987    
2024-05-03 23:42:11,892 - 

2024-05-03 23:42:11,893 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:43:07,524 - Epoch: [33][   70/   70]    Overall Loss 0.052400    Objective Loss 0.052400    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.794611    
2024-05-03 23:43:07,900 - 

2024-05-03 23:43:07,900 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:43:59,595 - Epoch: [34][   70/   70]    Overall Loss 0.042433    Objective Loss 0.042433    Top1 99.290780    Top5 99.290780    LR 0.001000    Time 0.738387    
2024-05-03 23:43:59,812 - 

2024-05-03 23:43:59,813 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:44:51,608 - Epoch: [35][   70/   70]    Overall Loss 0.038282    Objective Loss 0.038282    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.739827    
2024-05-03 23:44:52,216 - 

2024-05-03 23:44:52,217 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:45:47,111 - Epoch: [36][   70/   70]    Overall Loss 0.033276    Objective Loss 0.033276    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.784095    
2024-05-03 23:45:47,338 - 

2024-05-03 23:45:47,339 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:46:43,174 - Epoch: [37][   70/   70]    Overall Loss 0.031466    Objective Loss 0.031466    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.797537    
2024-05-03 23:46:44,077 - 

2024-05-03 23:46:44,078 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:47:38,625 - Epoch: [38][   70/   70]    Overall Loss 0.028409    Objective Loss 0.028409    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.779119    
2024-05-03 23:47:39,349 - 

2024-05-03 23:47:39,349 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:48:45,192 - Epoch: [39][   70/   70]    Overall Loss 0.025591    Objective Loss 0.025591    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.940475    
2024-05-03 23:48:45,644 - 

2024-05-03 23:48:45,646 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:49:41,078 - Epoch: [40][   70/   70]    Overall Loss 0.025621    Objective Loss 0.025621    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.791723    
2024-05-03 23:49:41,524 - --- validate (epoch=40)-----------
2024-05-03 23:49:41,525 - 1736 samples (100 per mini-batch)
2024-05-03 23:49:58,138 - Epoch: [40][   18/   18]    Loss 4.721247    Top1 36.405530    Top5 51.843318    
2024-05-03 23:49:58,506 - ==> Top1: 36.406    Top5: 51.843    Loss: 4.721

2024-05-03 23:49:58,513 - ==> Best [Top1: 36.406   Top5: 51.843   Sparsity:0.00   Params: 754976 on epoch: 40]
2024-05-03 23:49:58,513 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-03 23:49:58,591 - 

2024-05-03 23:49:58,592 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:50:51,254 - Epoch: [41][   70/   70]    Overall Loss 0.023447    Objective Loss 0.023447    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 0.752178    
2024-05-03 23:50:51,666 - 

2024-05-03 23:50:51,667 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:51:46,527 - Epoch: [42][   70/   70]    Overall Loss 0.022849    Objective Loss 0.022849    Top1 99.290780    Top5 100.000000    LR 0.001000    Time 0.783561    
2024-05-03 23:51:47,068 - 

2024-05-03 23:51:47,069 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:52:36,581 - Epoch: [43][   70/   70]    Overall Loss 0.107892    Objective Loss 0.107892    Top1 80.851064    Top5 99.290780    LR 0.001000    Time 0.707208    
2024-05-03 23:52:37,308 - 

2024-05-03 23:52:37,309 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:53:34,239 - Epoch: [44][   70/   70]    Overall Loss 2.201325    Objective Loss 2.201325    Top1 42.553191    Top5 70.212766    LR 0.001000    Time 0.813171    
2024-05-03 23:53:35,284 - 

2024-05-03 23:53:35,285 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:54:31,376 - Epoch: [45][   70/   70]    Overall Loss 1.618251    Objective Loss 1.618251    Top1 61.702128    Top5 85.815603    LR 0.001000    Time 0.801196    
2024-05-03 23:54:32,352 - 

2024-05-03 23:54:32,354 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:55:25,805 - Epoch: [46][   70/   70]    Overall Loss 1.074755    Objective Loss 1.074755    Top1 73.758865    Top5 89.361702    LR 0.001000    Time 0.763444    
2024-05-03 23:55:26,178 - 

2024-05-03 23:55:26,179 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:56:17,987 - Epoch: [47][   70/   70]    Overall Loss 0.783537    Objective Loss 0.783537    Top1 68.794326    Top5 90.780142    LR 0.001000    Time 0.740011    
2024-05-03 23:56:18,438 - 

2024-05-03 23:56:18,439 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:57:18,059 - Epoch: [48][   70/   70]    Overall Loss 0.555236    Objective Loss 0.555236    Top1 82.269504    Top5 97.872340    LR 0.001000    Time 0.851617    
2024-05-03 23:57:18,448 - 

2024-05-03 23:57:18,449 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:58:16,432 - Epoch: [49][   70/   70]    Overall Loss 0.325799    Objective Loss 0.325799    Top1 89.361702    Top5 98.581560    LR 0.001000    Time 0.828209    
2024-05-03 23:58:17,086 - 

2024-05-03 23:58:17,087 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-03 23:59:21,811 - Epoch: [50][   70/   70]    Overall Loss 0.142021    Objective Loss 0.142021    Top1 99.290780    Top5 100.000000    LR 0.000500    Time 0.924500    
2024-05-03 23:59:22,189 - --- validate (epoch=50)-----------
2024-05-03 23:59:22,189 - 1736 samples (100 per mini-batch)
2024-05-03 23:59:39,031 - Epoch: [50][   18/   18]    Loss 3.964175    Top1 36.635945    Top5 54.032258    
2024-05-03 23:59:39,576 - ==> Top1: 36.636    Top5: 54.032    Loss: 3.964

2024-05-03 23:59:39,627 - ==> Best [Top1: 36.636   Top5: 54.032   Sparsity:0.00   Params: 754976 on epoch: 50]
2024-05-03 23:59:39,627 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-03 23:59:39,765 - 

2024-05-03 23:59:39,766 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:00:35,863 - Epoch: [51][   70/   70]    Overall Loss 0.077035    Objective Loss 0.077035    Top1 99.290780    Top5 100.000000    LR 0.000500    Time 0.801263    
2024-05-04 00:00:36,304 - 

2024-05-04 00:00:36,304 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:01:33,070 - Epoch: [52][   70/   70]    Overall Loss 0.058802    Objective Loss 0.058802    Top1 99.290780    Top5 100.000000    LR 0.000500    Time 0.810815    
2024-05-04 00:01:33,752 - 

2024-05-04 00:01:33,753 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:02:30,146 - Epoch: [53][   70/   70]    Overall Loss 0.053002    Objective Loss 0.053002    Top1 99.290780    Top5 100.000000    LR 0.000500    Time 0.805479    
2024-05-04 00:02:30,327 - 

2024-05-04 00:02:30,328 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:03:23,258 - Epoch: [54][   70/   70]    Overall Loss 0.045776    Objective Loss 0.045776    Top1 97.872340    Top5 100.000000    LR 0.000500    Time 0.755991    
2024-05-04 00:03:23,978 - 

2024-05-04 00:03:23,978 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:04:19,162 - Epoch: [55][   70/   70]    Overall Loss 0.041030    Objective Loss 0.041030    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.788211    
2024-05-04 00:04:19,878 - 

2024-05-04 00:04:19,879 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:05:19,907 - Epoch: [56][   70/   70]    Overall Loss 0.038273    Objective Loss 0.038273    Top1 99.290780    Top5 99.290780    LR 0.000500    Time 0.857431    
2024-05-04 00:05:20,297 - 

2024-05-04 00:05:20,298 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:06:12,260 - Epoch: [57][   70/   70]    Overall Loss 0.035977    Objective Loss 0.035977    Top1 99.290780    Top5 100.000000    LR 0.000500    Time 0.742204    
2024-05-04 00:06:12,572 - 

2024-05-04 00:06:12,573 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:07:07,121 - Epoch: [58][   70/   70]    Overall Loss 0.033689    Objective Loss 0.033689    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.779124    
2024-05-04 00:07:07,420 - 

2024-05-04 00:07:07,421 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:08:01,525 - Epoch: [59][   70/   70]    Overall Loss 0.032351    Objective Loss 0.032351    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.772813    
2024-05-04 00:08:01,840 - 

2024-05-04 00:08:01,842 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:09:01,273 - Epoch: [60][   70/   70]    Overall Loss 0.030098    Objective Loss 0.030098    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.848882    
2024-05-04 00:09:01,531 - --- validate (epoch=60)-----------
2024-05-04 00:09:01,532 - 1736 samples (100 per mini-batch)
2024-05-04 00:09:19,462 - Epoch: [60][   18/   18]    Loss 4.290312    Top1 38.133641    Top5 54.781106    
2024-05-04 00:09:20,203 - ==> Top1: 38.134    Top5: 54.781    Loss: 4.290

2024-05-04 00:09:20,211 - ==> Best [Top1: 38.134   Top5: 54.781   Sparsity:0.00   Params: 754976 on epoch: 60]
2024-05-04 00:09:20,211 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 00:09:20,322 - 

2024-05-04 00:09:20,322 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:10:16,178 - Epoch: [61][   70/   70]    Overall Loss 0.027522    Objective Loss 0.027522    Top1 98.581560    Top5 100.000000    LR 0.000500    Time 0.797776    
2024-05-04 00:10:16,470 - 

2024-05-04 00:10:16,471 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:11:14,466 - Epoch: [62][   70/   70]    Overall Loss 0.028172    Objective Loss 0.028172    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.828369    
2024-05-04 00:11:14,939 - 

2024-05-04 00:11:14,940 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:12:12,399 - Epoch: [63][   70/   70]    Overall Loss 0.027177    Objective Loss 0.027177    Top1 98.581560    Top5 100.000000    LR 0.000500    Time 0.820734    
2024-05-04 00:12:12,836 - 

2024-05-04 00:12:12,837 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:13:07,223 - Epoch: [64][   70/   70]    Overall Loss 0.025777    Objective Loss 0.025777    Top1 99.290780    Top5 99.290780    LR 0.000500    Time 0.776815    
2024-05-04 00:13:07,487 - 

2024-05-04 00:13:07,487 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:14:01,175 - Epoch: [65][   70/   70]    Overall Loss 0.025510    Objective Loss 0.025510    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.766852    
2024-05-04 00:14:01,440 - 

2024-05-04 00:14:01,441 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:14:57,441 - Epoch: [66][   70/   70]    Overall Loss 0.023494    Objective Loss 0.023494    Top1 99.290780    Top5 99.290780    LR 0.000500    Time 0.799862    
2024-05-04 00:14:57,954 - 

2024-05-04 00:14:57,956 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:15:49,208 - Epoch: [67][   70/   70]    Overall Loss 0.023701    Objective Loss 0.023701    Top1 99.290780    Top5 99.290780    LR 0.000500    Time 0.732053    
2024-05-04 00:15:49,551 - 

2024-05-04 00:15:49,552 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:16:49,342 - Epoch: [68][   70/   70]    Overall Loss 0.022886    Objective Loss 0.022886    Top1 98.581560    Top5 100.000000    LR 0.000500    Time 0.854028    
2024-05-04 00:16:50,079 - 

2024-05-04 00:16:50,079 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:17:49,511 - Epoch: [69][   70/   70]    Overall Loss 0.022046    Objective Loss 0.022046    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.848920    
2024-05-04 00:17:49,874 - 

2024-05-04 00:17:49,875 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:18:47,407 - Epoch: [70][   70/   70]    Overall Loss 0.019636    Objective Loss 0.019636    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.821775    
2024-05-04 00:18:47,670 - --- validate (epoch=70)-----------
2024-05-04 00:18:47,671 - 1736 samples (100 per mini-batch)
2024-05-04 00:19:07,492 - Epoch: [70][   18/   18]    Loss 4.431302    Top1 37.327189    Top5 54.838710    
2024-05-04 00:19:07,826 - ==> Top1: 37.327    Top5: 54.839    Loss: 4.431

2024-05-04 00:19:07,836 - ==> Best [Top1: 38.134   Top5: 54.781   Sparsity:0.00   Params: 754976 on epoch: 60]
2024-05-04 00:19:07,836 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 00:19:07,899 - 

2024-05-04 00:19:07,900 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:19:56,788 - Epoch: [71][   70/   70]    Overall Loss 0.019036    Objective Loss 0.019036    Top1 99.290780    Top5 100.000000    LR 0.000500    Time 0.698266    
2024-05-04 00:19:57,037 - 

2024-05-04 00:19:57,038 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:20:50,613 - Epoch: [72][   70/   70]    Overall Loss 0.018987    Objective Loss 0.018987    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.765223    
2024-05-04 00:20:50,870 - 

2024-05-04 00:20:50,870 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:21:51,043 - Epoch: [73][   70/   70]    Overall Loss 0.018413    Objective Loss 0.018413    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.859487    
2024-05-04 00:21:51,514 - 

2024-05-04 00:21:51,515 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:22:46,569 - Epoch: [74][   70/   70]    Overall Loss 0.018415    Objective Loss 0.018415    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.786355    
2024-05-04 00:22:47,177 - 

2024-05-04 00:22:47,178 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:23:42,756 - Epoch: [75][   70/   70]    Overall Loss 0.016451    Objective Loss 0.016451    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.793848    
2024-05-04 00:23:42,998 - 

2024-05-04 00:23:42,999 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:24:34,073 - Epoch: [76][   70/   70]    Overall Loss 0.016867    Objective Loss 0.016867    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.729518    
2024-05-04 00:24:34,330 - 

2024-05-04 00:24:34,331 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:25:29,684 - Epoch: [77][   70/   70]    Overall Loss 0.017208    Objective Loss 0.017208    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.790621    
2024-05-04 00:25:30,217 - 

2024-05-04 00:25:30,219 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:26:23,121 - Epoch: [78][   70/   70]    Overall Loss 0.016471    Objective Loss 0.016471    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.755610    
2024-05-04 00:26:23,316 - 

2024-05-04 00:26:23,316 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:27:20,844 - Epoch: [79][   70/   70]    Overall Loss 0.014831    Objective Loss 0.014831    Top1 99.290780    Top5 99.290780    LR 0.000500    Time 0.821720    
2024-05-04 00:27:21,104 - 

2024-05-04 00:27:21,106 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:28:17,030 - Epoch: [80][   70/   70]    Overall Loss 0.015633    Objective Loss 0.015633    Top1 99.290780    Top5 99.290780    LR 0.000500    Time 0.798781    
2024-05-04 00:28:17,455 - --- validate (epoch=80)-----------
2024-05-04 00:28:17,456 - 1736 samples (100 per mini-batch)
2024-05-04 00:28:33,566 - Epoch: [80][   18/   18]    Loss 4.535035    Top1 38.133641    Top5 55.817972    
2024-05-04 00:28:34,377 - ==> Top1: 38.134    Top5: 55.818    Loss: 4.535

2024-05-04 00:28:34,386 - ==> Best [Top1: 38.134   Top5: 55.818   Sparsity:0.00   Params: 754976 on epoch: 80]
2024-05-04 00:28:34,386 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 00:28:34,500 - 

2024-05-04 00:28:34,500 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:29:24,492 - Epoch: [81][   70/   70]    Overall Loss 0.014342    Objective Loss 0.014342    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.714037    
2024-05-04 00:29:25,117 - 

2024-05-04 00:29:25,118 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:30:18,693 - Epoch: [82][   70/   70]    Overall Loss 0.014293    Objective Loss 0.014293    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.765215    
2024-05-04 00:30:19,022 - 

2024-05-04 00:30:19,023 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:31:16,625 - Epoch: [83][   70/   70]    Overall Loss 0.014701    Objective Loss 0.014701    Top1 99.290780    Top5 100.000000    LR 0.000500    Time 0.822756    
2024-05-04 00:31:17,354 - 

2024-05-04 00:31:17,355 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:32:08,207 - Epoch: [84][   70/   70]    Overall Loss 0.015221    Objective Loss 0.015221    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.726333    
2024-05-04 00:32:08,552 - 

2024-05-04 00:32:08,552 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:33:04,507 - Epoch: [85][   70/   70]    Overall Loss 0.014481    Objective Loss 0.014481    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.799226    
2024-05-04 00:33:04,845 - 

2024-05-04 00:33:04,847 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:33:58,793 - Epoch: [86][   70/   70]    Overall Loss 0.013844    Objective Loss 0.013844    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.770536    
2024-05-04 00:33:59,253 - 

2024-05-04 00:33:59,254 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:34:51,117 - Epoch: [87][   70/   70]    Overall Loss 0.012647    Objective Loss 0.012647    Top1 98.581560    Top5 99.290780    LR 0.000500    Time 0.740793    
2024-05-04 00:34:51,369 - 

2024-05-04 00:34:51,370 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:35:41,162 - Epoch: [88][   70/   70]    Overall Loss 0.014416    Objective Loss 0.014416    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.711214    
2024-05-04 00:35:41,476 - 

2024-05-04 00:35:41,477 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:36:38,171 - Epoch: [89][   70/   70]    Overall Loss 0.012660    Objective Loss 0.012660    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.809775    
2024-05-04 00:36:38,603 - 

2024-05-04 00:36:38,603 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:37:33,442 - Epoch: [90][   70/   70]    Overall Loss 0.014414    Objective Loss 0.014414    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.783290    
2024-05-04 00:37:33,728 - --- validate (epoch=90)-----------
2024-05-04 00:37:33,728 - 1736 samples (100 per mini-batch)
2024-05-04 00:37:52,582 - Epoch: [90][   18/   18]    Loss 4.744118    Top1 37.500000    Top5 55.011521    
2024-05-04 00:37:52,958 - ==> Top1: 37.500    Top5: 55.012    Loss: 4.744

2024-05-04 00:37:52,966 - ==> Best [Top1: 38.134   Top5: 55.818   Sparsity:0.00   Params: 754976 on epoch: 80]
2024-05-04 00:37:52,967 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 00:37:53,056 - 

2024-05-04 00:37:53,057 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:38:47,966 - Epoch: [91][   70/   70]    Overall Loss 0.017213    Objective Loss 0.017213    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.784281    
2024-05-04 00:38:48,369 - 

2024-05-04 00:38:48,370 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:39:49,138 - Epoch: [92][   70/   70]    Overall Loss 0.020355    Objective Loss 0.020355    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.867983    
2024-05-04 00:39:49,563 - 

2024-05-04 00:39:49,563 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:40:44,776 - Epoch: [93][   70/   70]    Overall Loss 0.021029    Objective Loss 0.021029    Top1 97.872340    Top5 100.000000    LR 0.000500    Time 0.788626    
2024-05-04 00:40:45,212 - 

2024-05-04 00:40:45,212 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:41:48,182 - Epoch: [94][   70/   70]    Overall Loss 0.057914    Objective Loss 0.057914    Top1 97.163121    Top5 100.000000    LR 0.000500    Time 0.899432    
2024-05-04 00:41:48,704 - 

2024-05-04 00:41:48,705 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:42:49,182 - Epoch: [95][   70/   70]    Overall Loss 0.439573    Objective Loss 0.439573    Top1 70.212766    Top5 95.035461    LR 0.000500    Time 0.863827    
2024-05-04 00:42:49,512 - 

2024-05-04 00:42:49,513 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:43:45,586 - Epoch: [96][   70/   70]    Overall Loss 0.558533    Objective Loss 0.558533    Top1 85.815603    Top5 97.872340    LR 0.000500    Time 0.800931    
2024-05-04 00:43:45,879 - 

2024-05-04 00:43:45,880 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:44:37,421 - Epoch: [97][   70/   70]    Overall Loss 0.174886    Objective Loss 0.174886    Top1 95.035461    Top5 100.000000    LR 0.000500    Time 0.736167    
2024-05-04 00:44:37,666 - 

2024-05-04 00:44:37,666 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:45:34,487 - Epoch: [98][   70/   70]    Overall Loss 0.066189    Objective Loss 0.066189    Top1 99.290780    Top5 99.290780    LR 0.000500    Time 0.811608    
2024-05-04 00:45:35,204 - 

2024-05-04 00:45:35,205 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:46:34,240 - Epoch: [99][   70/   70]    Overall Loss 0.034018    Objective Loss 0.034018    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.843227    
2024-05-04 00:46:34,665 - 

2024-05-04 00:46:34,666 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:47:34,707 - Epoch: [100][   70/   70]    Overall Loss 0.021122    Objective Loss 0.021122    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.857598    
2024-05-04 00:47:35,353 - --- validate (epoch=100)-----------
2024-05-04 00:47:35,354 - 1736 samples (100 per mini-batch)
2024-05-04 00:47:55,069 - Epoch: [100][   18/   18]    Loss 4.531344    Top1 38.709677    Top5 55.933180    
2024-05-04 00:47:55,603 - ==> Top1: 38.710    Top5: 55.933    Loss: 4.531

2024-05-04 00:47:55,616 - ==> Best [Top1: 38.710   Top5: 55.933   Sparsity:0.00   Params: 754976 on epoch: 100]
2024-05-04 00:47:55,616 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 00:47:55,734 - 

2024-05-04 00:47:55,735 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:48:48,302 - Epoch: [101][   70/   70]    Overall Loss 0.019193    Objective Loss 0.019193    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.750836    
2024-05-04 00:48:48,873 - 

2024-05-04 00:48:48,874 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:49:42,017 - Epoch: [102][   70/   70]    Overall Loss 0.019106    Objective Loss 0.019106    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.759061    
2024-05-04 00:49:42,964 - 

2024-05-04 00:49:42,965 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:50:38,172 - Epoch: [103][   70/   70]    Overall Loss 0.016301    Objective Loss 0.016301    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.788527    
2024-05-04 00:50:38,557 - 

2024-05-04 00:50:38,558 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:51:36,591 - Epoch: [104][   70/   70]    Overall Loss 0.016837    Objective Loss 0.016837    Top1 99.290780    Top5 99.290780    LR 0.000250    Time 0.828900    
2024-05-04 00:51:36,978 - 

2024-05-04 00:51:36,979 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:52:32,506 - Epoch: [105][   70/   70]    Overall Loss 0.015842    Objective Loss 0.015842    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.793124    
2024-05-04 00:52:33,465 - 

2024-05-04 00:52:33,466 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:53:26,076 - Epoch: [106][   70/   70]    Overall Loss 0.015172    Objective Loss 0.015172    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.751443    
2024-05-04 00:53:26,625 - 

2024-05-04 00:53:26,626 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:54:18,693 - Epoch: [107][   70/   70]    Overall Loss 0.014682    Objective Loss 0.014682    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.743692    
2024-05-04 00:54:18,988 - 

2024-05-04 00:54:18,989 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:55:16,949 - Epoch: [108][   70/   70]    Overall Loss 0.013942    Objective Loss 0.013942    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.827849    
2024-05-04 00:55:17,213 - 

2024-05-04 00:55:17,214 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:56:11,284 - Epoch: [109][   70/   70]    Overall Loss 0.014433    Objective Loss 0.014433    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.772311    
2024-05-04 00:56:11,709 - 

2024-05-04 00:56:11,710 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:57:09,194 - Epoch: [110][   70/   70]    Overall Loss 0.015291    Objective Loss 0.015291    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.821088    
2024-05-04 00:57:09,428 - --- validate (epoch=110)-----------
2024-05-04 00:57:09,428 - 1736 samples (100 per mini-batch)
2024-05-04 00:57:27,724 - Epoch: [110][   18/   18]    Loss 4.740756    Top1 38.594470    Top5 55.702765    
2024-05-04 00:57:27,990 - ==> Top1: 38.594    Top5: 55.703    Loss: 4.741

2024-05-04 00:57:28,004 - ==> Best [Top1: 38.710   Top5: 55.933   Sparsity:0.00   Params: 754976 on epoch: 100]
2024-05-04 00:57:28,004 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 00:57:28,089 - 

2024-05-04 00:57:28,090 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:58:24,665 - Epoch: [111][   70/   70]    Overall Loss 0.013853    Objective Loss 0.013853    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.808089    
2024-05-04 00:58:25,174 - 

2024-05-04 00:58:25,174 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 00:59:16,964 - Epoch: [112][   70/   70]    Overall Loss 0.012951    Objective Loss 0.012951    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.739717    
2024-05-04 00:59:17,384 - 

2024-05-04 00:59:17,384 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:00:17,831 - Epoch: [113][   70/   70]    Overall Loss 0.012875    Objective Loss 0.012875    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.863404    
2024-05-04 01:00:19,096 - 

2024-05-04 01:00:19,097 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:01:16,625 - Epoch: [114][   70/   70]    Overall Loss 0.012038    Objective Loss 0.012038    Top1 99.290780    Top5 99.290780    LR 0.000250    Time 0.821691    
2024-05-04 01:01:17,360 - 

2024-05-04 01:01:17,360 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:02:11,014 - Epoch: [115][   70/   70]    Overall Loss 0.012091    Objective Loss 0.012091    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.766360    
2024-05-04 01:02:11,485 - 

2024-05-04 01:02:11,485 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:03:08,121 - Epoch: [116][   70/   70]    Overall Loss 0.011829    Objective Loss 0.011829    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.808956    
2024-05-04 01:03:08,468 - 

2024-05-04 01:03:08,470 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:04:04,837 - Epoch: [117][   70/   70]    Overall Loss 0.011923    Objective Loss 0.011923    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.805109    
2024-05-04 01:04:05,126 - 

2024-05-04 01:04:05,126 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:05:00,976 - Epoch: [118][   70/   70]    Overall Loss 0.011323    Objective Loss 0.011323    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.797742    
2024-05-04 01:05:01,209 - 

2024-05-04 01:05:01,210 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:05:54,239 - Epoch: [119][   70/   70]    Overall Loss 0.011459    Objective Loss 0.011459    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.757436    
2024-05-04 01:05:54,521 - 

2024-05-04 01:05:54,522 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:06:52,832 - Epoch: [120][   70/   70]    Overall Loss 0.011538    Objective Loss 0.011538    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.832867    
2024-05-04 01:06:53,093 - --- validate (epoch=120)-----------
2024-05-04 01:06:53,093 - 1736 samples (100 per mini-batch)
2024-05-04 01:07:08,459 - Epoch: [120][   18/   18]    Loss 4.774995    Top1 38.940092    Top5 56.105991    
2024-05-04 01:07:08,686 - ==> Top1: 38.940    Top5: 56.106    Loss: 4.775

2024-05-04 01:07:08,694 - ==> Best [Top1: 38.940   Top5: 56.106   Sparsity:0.00   Params: 754976 on epoch: 120]
2024-05-04 01:07:08,694 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 01:07:08,821 - 

2024-05-04 01:07:08,822 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:08:04,900 - Epoch: [121][   70/   70]    Overall Loss 0.010355    Objective Loss 0.010355    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.801015    
2024-05-04 01:08:05,091 - 

2024-05-04 01:08:05,092 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:08:59,185 - Epoch: [122][   70/   70]    Overall Loss 0.010733    Objective Loss 0.010733    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.772660    
2024-05-04 01:08:59,398 - 

2024-05-04 01:08:59,399 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:09:56,975 - Epoch: [123][   70/   70]    Overall Loss 0.011100    Objective Loss 0.011100    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.822408    
2024-05-04 01:09:57,271 - 

2024-05-04 01:09:57,272 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:10:56,003 - Epoch: [124][   70/   70]    Overall Loss 0.010742    Objective Loss 0.010742    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.838869    
2024-05-04 01:10:56,617 - 

2024-05-04 01:10:56,617 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:11:50,076 - Epoch: [125][   70/   70]    Overall Loss 0.010392    Objective Loss 0.010392    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.763557    
2024-05-04 01:11:50,485 - 

2024-05-04 01:11:50,486 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:12:44,013 - Epoch: [126][   70/   70]    Overall Loss 0.010873    Objective Loss 0.010873    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.764556    
2024-05-04 01:12:44,399 - 

2024-05-04 01:12:44,399 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:13:38,637 - Epoch: [127][   70/   70]    Overall Loss 0.010183    Objective Loss 0.010183    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.774720    
2024-05-04 01:13:38,926 - 

2024-05-04 01:13:38,927 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:14:34,994 - Epoch: [128][   70/   70]    Overall Loss 0.010289    Objective Loss 0.010289    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.800823    
2024-05-04 01:14:35,325 - 

2024-05-04 01:14:35,326 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:15:30,924 - Epoch: [129][   70/   70]    Overall Loss 0.010100    Objective Loss 0.010100    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.794129    
2024-05-04 01:15:31,251 - 

2024-05-04 01:15:31,252 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:16:26,806 - Epoch: [130][   70/   70]    Overall Loss 0.010762    Objective Loss 0.010762    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.793512    
2024-05-04 01:16:27,419 - --- validate (epoch=130)-----------
2024-05-04 01:16:27,420 - 1736 samples (100 per mini-batch)
2024-05-04 01:16:50,072 - Epoch: [130][   18/   18]    Loss 4.858480    Top1 38.824885    Top5 55.472350    
2024-05-04 01:16:50,529 - ==> Top1: 38.825    Top5: 55.472    Loss: 4.858

2024-05-04 01:16:50,549 - ==> Best [Top1: 38.940   Top5: 56.106   Sparsity:0.00   Params: 754976 on epoch: 120]
2024-05-04 01:16:50,549 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 01:16:50,628 - 

2024-05-04 01:16:50,629 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:17:47,335 - Epoch: [131][   70/   70]    Overall Loss 0.009956    Objective Loss 0.009956    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.809971    
2024-05-04 01:17:47,722 - 

2024-05-04 01:17:47,723 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:18:47,848 - Epoch: [132][   70/   70]    Overall Loss 0.009442    Objective Loss 0.009442    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.858827    
2024-05-04 01:18:48,821 - 

2024-05-04 01:18:48,822 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:19:43,595 - Epoch: [133][   70/   70]    Overall Loss 0.009394    Objective Loss 0.009394    Top1 99.290780    Top5 99.290780    LR 0.000250    Time 0.782358    
2024-05-04 01:19:43,867 - 

2024-05-04 01:19:43,868 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:20:39,998 - Epoch: [134][   70/   70]    Overall Loss 0.009942    Objective Loss 0.009942    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.801715    
2024-05-04 01:20:40,298 - 

2024-05-04 01:20:40,299 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:21:33,724 - Epoch: [135][   70/   70]    Overall Loss 0.009218    Objective Loss 0.009218    Top1 98.581560    Top5 99.290780    LR 0.000250    Time 0.763085    
2024-05-04 01:21:34,318 - 

2024-05-04 01:21:34,319 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:22:26,954 - Epoch: [136][   70/   70]    Overall Loss 0.008667    Objective Loss 0.008667    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.751792    
2024-05-04 01:22:27,511 - 

2024-05-04 01:22:27,512 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:23:23,111 - Epoch: [137][   70/   70]    Overall Loss 0.008935    Objective Loss 0.008935    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.794161    
2024-05-04 01:23:23,650 - 

2024-05-04 01:23:23,651 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:24:15,614 - Epoch: [138][   70/   70]    Overall Loss 0.009259    Objective Loss 0.009259    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.742218    
2024-05-04 01:24:16,438 - 

2024-05-04 01:24:16,438 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:25:12,661 - Epoch: [139][   70/   70]    Overall Loss 0.009376    Objective Loss 0.009376    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.803070    
2024-05-04 01:25:13,298 - 

2024-05-04 01:25:13,298 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:26:07,577 - Epoch: [140][   70/   70]    Overall Loss 0.009224    Objective Loss 0.009224    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.775280    
2024-05-04 01:26:08,330 - --- validate (epoch=140)-----------
2024-05-04 01:26:08,330 - 1736 samples (100 per mini-batch)
2024-05-04 01:26:27,838 - Epoch: [140][   18/   18]    Loss 5.087727    Top1 38.018433    Top5 55.933180    
2024-05-04 01:26:28,354 - ==> Top1: 38.018    Top5: 55.933    Loss: 5.088

2024-05-04 01:26:28,361 - ==> Best [Top1: 38.940   Top5: 56.106   Sparsity:0.00   Params: 754976 on epoch: 120]
2024-05-04 01:26:28,362 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 01:26:28,451 - 

2024-05-04 01:26:28,452 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:27:21,904 - Epoch: [141][   70/   70]    Overall Loss 0.008467    Objective Loss 0.008467    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.763445    
2024-05-04 01:27:22,729 - 

2024-05-04 01:27:22,731 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:28:20,516 - Epoch: [142][   70/   70]    Overall Loss 0.008727    Objective Loss 0.008727    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.825353    
2024-05-04 01:28:20,818 - 

2024-05-04 01:28:20,820 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:29:16,912 - Epoch: [143][   70/   70]    Overall Loss 0.008496    Objective Loss 0.008496    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.801180    
2024-05-04 01:29:17,254 - 

2024-05-04 01:29:17,254 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:30:11,421 - Epoch: [144][   70/   70]    Overall Loss 0.008321    Objective Loss 0.008321    Top1 99.290780    Top5 100.000000    LR 0.000250    Time 0.773698    
2024-05-04 01:30:11,814 - 

2024-05-04 01:30:11,815 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:31:10,105 - Epoch: [145][   70/   70]    Overall Loss 0.008194    Objective Loss 0.008194    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.832567    
2024-05-04 01:31:10,913 - 

2024-05-04 01:31:10,913 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:32:04,210 - Epoch: [146][   70/   70]    Overall Loss 0.008362    Objective Loss 0.008362    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.761261    
2024-05-04 01:32:05,065 - 

2024-05-04 01:32:05,066 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:33:00,717 - Epoch: [147][   70/   70]    Overall Loss 0.008429    Objective Loss 0.008429    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.794889    
2024-05-04 01:33:01,025 - 

2024-05-04 01:33:01,026 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:33:56,267 - Epoch: [148][   70/   70]    Overall Loss 0.008959    Objective Loss 0.008959    Top1 99.290780    Top5 99.290780    LR 0.000250    Time 0.789012    
2024-05-04 01:33:56,473 - 

2024-05-04 01:33:56,474 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:34:50,076 - Epoch: [149][   70/   70]    Overall Loss 0.008702    Objective Loss 0.008702    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 0.765626    
2024-05-04 01:34:50,277 - 

2024-05-04 01:34:50,278 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:35:47,222 - Epoch: [150][   70/   70]    Overall Loss 0.008205    Objective Loss 0.008205    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.813359    
2024-05-04 01:35:47,802 - --- validate (epoch=150)-----------
2024-05-04 01:35:47,802 - 1736 samples (100 per mini-batch)
2024-05-04 01:36:11,569 - Epoch: [150][   18/   18]    Loss 5.082534    Top1 38.191244    Top5 55.645161    
2024-05-04 01:36:11,812 - ==> Top1: 38.191    Top5: 55.645    Loss: 5.083

2024-05-04 01:36:11,821 - ==> Best [Top1: 38.940   Top5: 56.106   Sparsity:0.00   Params: 754976 on epoch: 120]
2024-05-04 01:36:11,821 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 01:36:11,912 - 

2024-05-04 01:36:11,913 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:37:10,490 - Epoch: [151][   70/   70]    Overall Loss 0.007652    Objective Loss 0.007652    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.836691    
2024-05-04 01:37:10,860 - 

2024-05-04 01:37:10,861 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:38:10,258 - Epoch: [152][   70/   70]    Overall Loss 0.007960    Objective Loss 0.007960    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.848410    
2024-05-04 01:38:10,797 - 

2024-05-04 01:38:10,798 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:39:04,284 - Epoch: [153][   70/   70]    Overall Loss 0.007701    Objective Loss 0.007701    Top1 99.290780    Top5 99.290780    LR 0.000125    Time 0.763928    
2024-05-04 01:39:04,889 - 

2024-05-04 01:39:04,890 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:40:01,776 - Epoch: [154][   70/   70]    Overall Loss 0.007838    Objective Loss 0.007838    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.812526    
2024-05-04 01:40:02,173 - 

2024-05-04 01:40:02,173 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:40:56,216 - Epoch: [155][   70/   70]    Overall Loss 0.007269    Objective Loss 0.007269    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.771905    
2024-05-04 01:40:56,712 - 

2024-05-04 01:40:56,713 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:41:54,705 - Epoch: [156][   70/   70]    Overall Loss 0.007525    Objective Loss 0.007525    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.828318    
2024-05-04 01:41:55,091 - 

2024-05-04 01:41:55,092 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:42:53,922 - Epoch: [157][   70/   70]    Overall Loss 0.007322    Objective Loss 0.007322    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.840294    
2024-05-04 01:42:54,221 - 

2024-05-04 01:42:54,222 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:43:50,475 - Epoch: [158][   70/   70]    Overall Loss 0.007563    Objective Loss 0.007563    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.803506    
2024-05-04 01:43:50,979 - 

2024-05-04 01:43:50,980 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:44:41,809 - Epoch: [159][   70/   70]    Overall Loss 0.007519    Objective Loss 0.007519    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.725977    
2024-05-04 01:44:42,341 - 

2024-05-04 01:44:42,342 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:45:35,021 - Epoch: [160][   70/   70]    Overall Loss 0.008040    Objective Loss 0.008040    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.752413    
2024-05-04 01:45:35,296 - --- validate (epoch=160)-----------
2024-05-04 01:45:35,297 - 1736 samples (100 per mini-batch)
2024-05-04 01:45:54,228 - Epoch: [160][   18/   18]    Loss 5.044160    Top1 38.652074    Top5 55.241935    
2024-05-04 01:45:54,597 - ==> Top1: 38.652    Top5: 55.242    Loss: 5.044

2024-05-04 01:45:54,605 - ==> Best [Top1: 38.940   Top5: 56.106   Sparsity:0.00   Params: 754976 on epoch: 120]
2024-05-04 01:45:54,606 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 01:45:54,690 - 

2024-05-04 01:45:54,691 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:46:49,149 - Epoch: [161][   70/   70]    Overall Loss 0.007486    Objective Loss 0.007486    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.777851    
2024-05-04 01:46:49,974 - 

2024-05-04 01:46:49,975 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:47:45,848 - Epoch: [162][   70/   70]    Overall Loss 0.007511    Objective Loss 0.007511    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.798068    
2024-05-04 01:47:46,223 - 

2024-05-04 01:47:46,224 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:48:38,065 - Epoch: [163][   70/   70]    Overall Loss 0.007058    Objective Loss 0.007058    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.740467    
2024-05-04 01:48:39,128 - 

2024-05-04 01:48:39,130 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:49:38,747 - Epoch: [164][   70/   70]    Overall Loss 0.007171    Objective Loss 0.007171    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.851519    
2024-05-04 01:49:39,176 - 

2024-05-04 01:49:39,177 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:50:36,930 - Epoch: [165][   70/   70]    Overall Loss 0.007189    Objective Loss 0.007189    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.824935    
2024-05-04 01:50:37,351 - 

2024-05-04 01:50:37,352 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:51:26,494 - Epoch: [166][   70/   70]    Overall Loss 0.007798    Objective Loss 0.007798    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.701903    
2024-05-04 01:51:27,160 - 

2024-05-04 01:51:27,161 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:52:21,450 - Epoch: [167][   70/   70]    Overall Loss 0.007762    Objective Loss 0.007762    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.775434    
2024-05-04 01:52:21,852 - 

2024-05-04 01:52:21,853 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:53:14,300 - Epoch: [168][   70/   70]    Overall Loss 0.007897    Objective Loss 0.007897    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.749122    
2024-05-04 01:53:14,549 - 

2024-05-04 01:53:14,550 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:54:16,587 - Epoch: [169][   70/   70]    Overall Loss 0.007875    Objective Loss 0.007875    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.886126    
2024-05-04 01:54:16,945 - 

2024-05-04 01:54:16,946 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:55:10,289 - Epoch: [170][   70/   70]    Overall Loss 0.007482    Objective Loss 0.007482    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.761929    
2024-05-04 01:55:10,530 - --- validate (epoch=170)-----------
2024-05-04 01:55:10,531 - 1736 samples (100 per mini-batch)
2024-05-04 01:55:29,101 - Epoch: [170][   18/   18]    Loss 5.153814    Top1 38.191244    Top5 55.702765    
2024-05-04 01:55:29,305 - ==> Top1: 38.191    Top5: 55.703    Loss: 5.154

2024-05-04 01:55:29,314 - ==> Best [Top1: 38.940   Top5: 56.106   Sparsity:0.00   Params: 754976 on epoch: 120]
2024-05-04 01:55:29,315 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 01:55:29,397 - 

2024-05-04 01:55:29,398 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:56:24,788 - Epoch: [171][   70/   70]    Overall Loss 0.007101    Objective Loss 0.007101    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.791154    
2024-05-04 01:56:25,520 - 

2024-05-04 01:56:25,521 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:57:19,023 - Epoch: [172][   70/   70]    Overall Loss 0.007282    Objective Loss 0.007282    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.764169    
2024-05-04 01:57:19,667 - 

2024-05-04 01:57:19,668 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:58:10,799 - Epoch: [173][   70/   70]    Overall Loss 0.007137    Objective Loss 0.007137    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.730318    
2024-05-04 01:58:11,476 - 

2024-05-04 01:58:11,477 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:59:05,117 - Epoch: [174][   70/   70]    Overall Loss 0.007140    Objective Loss 0.007140    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.766154    
2024-05-04 01:59:05,746 - 

2024-05-04 01:59:05,747 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 01:59:57,447 - Epoch: [175][   70/   70]    Overall Loss 0.007002    Objective Loss 0.007002    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.738447    
2024-05-04 01:59:57,704 - 

2024-05-04 01:59:57,704 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:00:50,619 - Epoch: [176][   70/   70]    Overall Loss 0.006959    Objective Loss 0.006959    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.755805    
2024-05-04 02:00:50,825 - 

2024-05-04 02:00:50,825 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:01:47,198 - Epoch: [177][   70/   70]    Overall Loss 0.006977    Objective Loss 0.006977    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.805230    
2024-05-04 02:01:47,912 - 

2024-05-04 02:01:47,913 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:02:41,688 - Epoch: [178][   70/   70]    Overall Loss 0.007118    Objective Loss 0.007118    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.768092    
2024-05-04 02:02:42,308 - 

2024-05-04 02:02:42,309 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:03:37,648 - Epoch: [179][   70/   70]    Overall Loss 0.007298    Objective Loss 0.007298    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.790398    
2024-05-04 02:03:38,014 - 

2024-05-04 02:03:38,015 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:04:34,329 - Epoch: [180][   70/   70]    Overall Loss 0.007096    Objective Loss 0.007096    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.804352    
2024-05-04 02:04:34,562 - --- validate (epoch=180)-----------
2024-05-04 02:04:34,562 - 1736 samples (100 per mini-batch)
2024-05-04 02:04:54,038 - Epoch: [180][   18/   18]    Loss 5.090702    Top1 38.997696    Top5 55.702765    
2024-05-04 02:04:54,382 - ==> Top1: 38.998    Top5: 55.703    Loss: 5.091

2024-05-04 02:04:54,433 - ==> Best [Top1: 38.998   Top5: 55.703   Sparsity:0.00   Params: 754976 on epoch: 180]
2024-05-04 02:04:54,434 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 02:04:54,542 - 

2024-05-04 02:04:54,544 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:05:48,765 - Epoch: [181][   70/   70]    Overall Loss 0.006908    Objective Loss 0.006908    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.774433    
2024-05-04 02:05:49,121 - 

2024-05-04 02:05:49,122 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:06:45,991 - Epoch: [182][   70/   70]    Overall Loss 0.006979    Objective Loss 0.006979    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.812282    
2024-05-04 02:06:46,572 - 

2024-05-04 02:06:46,572 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:07:42,437 - Epoch: [183][   70/   70]    Overall Loss 0.007040    Objective Loss 0.007040    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.797961    
2024-05-04 02:07:42,773 - 

2024-05-04 02:07:42,774 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:08:37,607 - Epoch: [184][   70/   70]    Overall Loss 0.007381    Objective Loss 0.007381    Top1 99.290780    Top5 99.290780    LR 0.000125    Time 0.783233    
2024-05-04 02:08:37,889 - 

2024-05-04 02:08:37,891 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:09:34,003 - Epoch: [185][   70/   70]    Overall Loss 0.006898    Objective Loss 0.006898    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.801488    
2024-05-04 02:09:34,294 - 

2024-05-04 02:09:34,295 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:10:33,698 - Epoch: [186][   70/   70]    Overall Loss 0.013266    Objective Loss 0.013266    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.848507    
2024-05-04 02:10:33,982 - 

2024-05-04 02:10:33,983 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:11:30,814 - Epoch: [187][   70/   70]    Overall Loss 0.010028    Objective Loss 0.010028    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.811767    
2024-05-04 02:11:31,004 - 

2024-05-04 02:11:31,004 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:12:26,909 - Epoch: [188][   70/   70]    Overall Loss 0.007879    Objective Loss 0.007879    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.798527    
2024-05-04 02:12:27,119 - 

2024-05-04 02:12:27,120 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:13:20,052 - Epoch: [189][   70/   70]    Overall Loss 0.007859    Objective Loss 0.007859    Top1 99.290780    Top5 99.290780    LR 0.000125    Time 0.756048    
2024-05-04 02:13:20,308 - 

2024-05-04 02:13:20,309 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:14:16,074 - Epoch: [190][   70/   70]    Overall Loss 0.006932    Objective Loss 0.006932    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.796518    
2024-05-04 02:14:16,256 - --- validate (epoch=190)-----------
2024-05-04 02:14:16,256 - 1736 samples (100 per mini-batch)
2024-05-04 02:14:32,197 - Epoch: [190][   18/   18]    Loss 5.145634    Top1 38.364055    Top5 55.011521    
2024-05-04 02:14:32,506 - ==> Top1: 38.364    Top5: 55.012    Loss: 5.146

2024-05-04 02:14:32,516 - ==> Best [Top1: 38.998   Top5: 55.703   Sparsity:0.00   Params: 754976 on epoch: 180]
2024-05-04 02:14:32,517 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 02:14:32,601 - 

2024-05-04 02:14:32,602 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:15:27,858 - Epoch: [191][   70/   70]    Overall Loss 0.006960    Objective Loss 0.006960    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.789259    
2024-05-04 02:15:28,053 - 

2024-05-04 02:15:28,053 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:16:24,139 - Epoch: [192][   70/   70]    Overall Loss 0.007015    Objective Loss 0.007015    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.801127    
2024-05-04 02:16:24,348 - 

2024-05-04 02:16:24,349 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:17:19,681 - Epoch: [193][   70/   70]    Overall Loss 0.006946    Objective Loss 0.006946    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.790339    
2024-05-04 02:17:19,887 - 

2024-05-04 02:17:19,888 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:18:11,688 - Epoch: [194][   70/   70]    Overall Loss 0.006876    Objective Loss 0.006876    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.739896    
2024-05-04 02:18:11,875 - 

2024-05-04 02:18:11,876 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:19:10,182 - Epoch: [195][   70/   70]    Overall Loss 0.006647    Objective Loss 0.006647    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.832810    
2024-05-04 02:19:10,663 - 

2024-05-04 02:19:10,664 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:20:03,301 - Epoch: [196][   70/   70]    Overall Loss 0.006609    Objective Loss 0.006609    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.751847    
2024-05-04 02:20:03,738 - 

2024-05-04 02:20:03,739 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:20:58,116 - Epoch: [197][   70/   70]    Overall Loss 0.006580    Objective Loss 0.006580    Top1 99.290780    Top5 99.290780    LR 0.000125    Time 0.776677    
2024-05-04 02:20:58,475 - 

2024-05-04 02:20:58,475 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:21:52,692 - Epoch: [198][   70/   70]    Overall Loss 0.006604    Objective Loss 0.006604    Top1 100.000000    Top5 100.000000    LR 0.000125    Time 0.774411    
2024-05-04 02:21:53,058 - 

2024-05-04 02:21:53,059 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:22:45,357 - Epoch: [199][   70/   70]    Overall Loss 0.006619    Objective Loss 0.006619    Top1 99.290780    Top5 100.000000    LR 0.000125    Time 0.746992    
2024-05-04 02:22:45,569 - 

2024-05-04 02:22:45,569 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:23:35,348 - Epoch: [200][   70/   70]    Overall Loss 0.006435    Objective Loss 0.006435    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.711026    
2024-05-04 02:23:35,648 - --- validate (epoch=200)-----------
2024-05-04 02:23:35,649 - 1736 samples (100 per mini-batch)
2024-05-04 02:23:50,300 - Epoch: [200][   18/   18]    Loss 5.253344    Top1 39.055300    Top5 55.069124    
2024-05-04 02:23:50,773 - ==> Top1: 39.055    Top5: 55.069    Loss: 5.253

2024-05-04 02:23:50,785 - ==> Best [Top1: 39.055   Top5: 55.069   Sparsity:0.00   Params: 754976 on epoch: 200]
2024-05-04 02:23:50,785 - Saving checkpoint to: logs/2024.05.03-230949/checkpoint.pth.tar
2024-05-04 02:23:50,878 - 

2024-05-04 02:23:50,879 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:24:41,901 - Epoch: [201][   70/   70]    Overall Loss 0.006347    Objective Loss 0.006347    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.728768    
2024-05-04 02:24:42,108 - 

2024-05-04 02:24:42,109 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:25:33,702 - Epoch: [202][   70/   70]    Overall Loss 0.006245    Objective Loss 0.006245    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.736933    
2024-05-04 02:25:33,982 - 

2024-05-04 02:25:33,983 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:26:29,969 - Epoch: [203][   70/   70]    Overall Loss 0.006252    Objective Loss 0.006252    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.799681    
2024-05-04 02:26:30,316 - 

2024-05-04 02:26:30,316 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:27:25,196 - Epoch: [204][   70/   70]    Overall Loss 0.006363    Objective Loss 0.006363    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.783876    
2024-05-04 02:27:25,564 - 

2024-05-04 02:27:25,564 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:28:19,073 - Epoch: [205][   70/   70]    Overall Loss 0.006219    Objective Loss 0.006219    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.764294    
2024-05-04 02:28:19,332 - 

2024-05-04 02:28:19,333 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:29:14,686 - Epoch: [206][   70/   70]    Overall Loss 0.006396    Objective Loss 0.006396    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.790633    
2024-05-04 02:29:14,856 - 

2024-05-04 02:29:14,857 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:30:07,475 - Epoch: [207][   70/   70]    Overall Loss 0.006280    Objective Loss 0.006280    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.751579    
2024-05-04 02:30:07,716 - 

2024-05-04 02:30:07,717 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:31:07,101 - Epoch: [208][   70/   70]    Overall Loss 0.006372    Objective Loss 0.006372    Top1 98.581560    Top5 100.000000    LR 0.000063    Time 0.848218    
2024-05-04 02:31:07,460 - 

2024-05-04 02:31:07,461 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:31:59,345 - Epoch: [209][   70/   70]    Overall Loss 0.006462    Objective Loss 0.006462    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.741094    
2024-05-04 02:31:59,647 - 

2024-05-04 02:31:59,648 - Initiating quantization aware training (QAT)...
2024-05-04 02:31:59,709 - 

2024-05-04 02:31:59,710 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:33:03,403 - Epoch: [210][   70/   70]    Overall Loss 0.271335    Objective Loss 0.271335    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.909801    
2024-05-04 02:33:03,676 - --- validate (epoch=210)-----------
2024-05-04 02:33:03,677 - 1736 samples (100 per mini-batch)
2024-05-04 02:33:20,485 - Epoch: [210][   18/   18]    Loss 5.263853    Top1 37.845622    Top5 55.126728    
2024-05-04 02:33:21,503 - ==> Top1: 37.846    Top5: 55.127    Loss: 5.264

2024-05-04 02:33:21,524 - ==> Best [Top1: 37.846   Top5: 55.127   Sparsity:0.00   Params: 754976 on epoch: 210]
2024-05-04 02:33:21,524 - Saving checkpoint to: logs/2024.05.03-230949/qat_checkpoint.pth.tar
2024-05-04 02:33:21,577 - 

2024-05-04 02:33:21,577 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:34:21,998 - Epoch: [211][   70/   70]    Overall Loss 0.008346    Objective Loss 0.008346    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.863052    
2024-05-04 02:34:22,584 - 

2024-05-04 02:34:22,585 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:35:16,523 - Epoch: [212][   70/   70]    Overall Loss 0.007510    Objective Loss 0.007510    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.770444    
2024-05-04 02:35:17,050 - 

2024-05-04 02:35:17,050 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:36:08,868 - Epoch: [213][   70/   70]    Overall Loss 0.007634    Objective Loss 0.007634    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.740150    
2024-05-04 02:36:09,149 - 

2024-05-04 02:36:09,150 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:37:00,419 - Epoch: [214][   70/   70]    Overall Loss 0.007243    Objective Loss 0.007243    Top1 99.290780    Top5 99.290780    LR 0.000063    Time 0.732308    
2024-05-04 02:37:00,685 - 

2024-05-04 02:37:00,686 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:37:56,113 - Epoch: [215][   70/   70]    Overall Loss 0.006829    Objective Loss 0.006829    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.791703    
2024-05-04 02:37:56,470 - 

2024-05-04 02:37:56,471 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:38:49,658 - Epoch: [216][   70/   70]    Overall Loss 0.006718    Objective Loss 0.006718    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.759690    
2024-05-04 02:38:50,020 - 

2024-05-04 02:38:50,021 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:39:48,144 - Epoch: [217][   70/   70]    Overall Loss 0.006965    Objective Loss 0.006965    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.830217    
2024-05-04 02:39:48,352 - 

2024-05-04 02:39:48,352 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:40:42,991 - Epoch: [218][   70/   70]    Overall Loss 0.006554    Objective Loss 0.006554    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.780443    
2024-05-04 02:40:43,939 - 

2024-05-04 02:40:43,940 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:41:41,087 - Epoch: [219][   70/   70]    Overall Loss 0.006438    Objective Loss 0.006438    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.816266    
2024-05-04 02:41:42,101 - 

2024-05-04 02:41:42,103 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:42:42,410 - Epoch: [220][   70/   70]    Overall Loss 0.006418    Objective Loss 0.006418    Top1 98.581560    Top5 99.290780    LR 0.000063    Time 0.861351    
2024-05-04 02:42:43,122 - --- validate (epoch=220)-----------
2024-05-04 02:42:43,122 - 1736 samples (100 per mini-batch)
2024-05-04 02:43:00,237 - Epoch: [220][   18/   18]    Loss 5.599453    Top1 38.133641    Top5 55.472350    
2024-05-04 02:43:00,810 - ==> Top1: 38.134    Top5: 55.472    Loss: 5.599

2024-05-04 02:43:00,817 - ==> Best [Top1: 38.134   Top5: 55.472   Sparsity:0.00   Params: 754976 on epoch: 220]
2024-05-04 02:43:00,817 - Saving checkpoint to: logs/2024.05.03-230949/qat_checkpoint.pth.tar
2024-05-04 02:43:00,915 - 

2024-05-04 02:43:00,916 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:43:56,045 - Epoch: [221][   70/   70]    Overall Loss 0.006356    Objective Loss 0.006356    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.787408    
2024-05-04 02:43:56,286 - 

2024-05-04 02:43:56,287 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:44:55,899 - Epoch: [222][   70/   70]    Overall Loss 0.007121    Objective Loss 0.007121    Top1 98.581560    Top5 100.000000    LR 0.000063    Time 0.851471    
2024-05-04 02:44:56,447 - 

2024-05-04 02:44:56,448 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:45:53,354 - Epoch: [223][   70/   70]    Overall Loss 0.006314    Objective Loss 0.006314    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.812803    
2024-05-04 02:45:53,734 - 

2024-05-04 02:45:53,735 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:46:49,098 - Epoch: [224][   70/   70]    Overall Loss 0.006274    Objective Loss 0.006274    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.790762    
2024-05-04 02:46:49,401 - 

2024-05-04 02:46:49,401 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:47:48,465 - Epoch: [225][   70/   70]    Overall Loss 0.006294    Objective Loss 0.006294    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.843650    
2024-05-04 02:47:49,035 - 

2024-05-04 02:47:49,036 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:48:44,410 - Epoch: [226][   70/   70]    Overall Loss 0.006230    Objective Loss 0.006230    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.790942    
2024-05-04 02:48:45,085 - 

2024-05-04 02:48:45,085 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:49:39,988 - Epoch: [227][   70/   70]    Overall Loss 0.006180    Objective Loss 0.006180    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.784148    
2024-05-04 02:49:40,775 - 

2024-05-04 02:49:40,776 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:50:38,474 - Epoch: [228][   70/   70]    Overall Loss 0.006162    Objective Loss 0.006162    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.824093    
2024-05-04 02:50:39,219 - 

2024-05-04 02:50:39,220 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:51:32,782 - Epoch: [229][   70/   70]    Overall Loss 0.006163    Objective Loss 0.006163    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.765053    
2024-05-04 02:51:33,264 - 

2024-05-04 02:51:33,265 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:52:26,927 - Epoch: [230][   70/   70]    Overall Loss 0.006134    Objective Loss 0.006134    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.766426    
2024-05-04 02:52:27,381 - --- validate (epoch=230)-----------
2024-05-04 02:52:27,382 - 1736 samples (100 per mini-batch)
2024-05-04 02:52:44,364 - Epoch: [230][   18/   18]    Loss 5.700256    Top1 38.652074    Top5 55.587558    
2024-05-04 02:52:44,772 - ==> Top1: 38.652    Top5: 55.588    Loss: 5.700

2024-05-04 02:52:44,780 - ==> Best [Top1: 38.652   Top5: 55.588   Sparsity:0.00   Params: 754976 on epoch: 230]
2024-05-04 02:52:44,780 - Saving checkpoint to: logs/2024.05.03-230949/qat_checkpoint.pth.tar
2024-05-04 02:52:44,873 - 

2024-05-04 02:52:44,874 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:53:43,254 - Epoch: [231][   70/   70]    Overall Loss 0.006117    Objective Loss 0.006117    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.833847    
2024-05-04 02:53:43,746 - 

2024-05-04 02:53:43,748 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:54:35,194 - Epoch: [232][   70/   70]    Overall Loss 0.006080    Objective Loss 0.006080    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.734801    
2024-05-04 02:54:35,503 - 

2024-05-04 02:54:35,504 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:55:31,383 - Epoch: [233][   70/   70]    Overall Loss 0.006100    Objective Loss 0.006100    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.798140    
2024-05-04 02:55:31,944 - 

2024-05-04 02:55:31,945 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:56:25,748 - Epoch: [234][   70/   70]    Overall Loss 0.006652    Objective Loss 0.006652    Top1 99.290780    Top5 99.290780    LR 0.000063    Time 0.768475    
2024-05-04 02:56:26,448 - 

2024-05-04 02:56:26,449 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:57:25,153 - Epoch: [235][   70/   70]    Overall Loss 0.006061    Objective Loss 0.006061    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.838490    
2024-05-04 02:57:25,396 - 

2024-05-04 02:57:25,397 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:58:17,414 - Epoch: [236][   70/   70]    Overall Loss 0.006050    Objective Loss 0.006050    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.742968    
2024-05-04 02:58:17,770 - 

2024-05-04 02:58:17,771 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 02:59:14,516 - Epoch: [237][   70/   70]    Overall Loss 0.006053    Objective Loss 0.006053    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.810516    
2024-05-04 02:59:14,771 - 

2024-05-04 02:59:14,772 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:00:11,466 - Epoch: [238][   70/   70]    Overall Loss 0.006027    Objective Loss 0.006027    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.809797    
2024-05-04 03:00:11,990 - 

2024-05-04 03:00:11,991 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:01:03,792 - Epoch: [239][   70/   70]    Overall Loss 0.006028    Objective Loss 0.006028    Top1 99.290780    Top5 99.290780    LR 0.000063    Time 0.739886    
2024-05-04 03:01:04,123 - 

2024-05-04 03:01:04,124 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:01:55,896 - Epoch: [240][   70/   70]    Overall Loss 0.006015    Objective Loss 0.006015    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.739478    
2024-05-04 03:01:56,231 - --- validate (epoch=240)-----------
2024-05-04 03:01:56,231 - 1736 samples (100 per mini-batch)
2024-05-04 03:02:14,045 - Epoch: [240][   18/   18]    Loss 5.775672    Top1 38.652074    Top5 54.953917    
2024-05-04 03:02:14,273 - ==> Top1: 38.652    Top5: 54.954    Loss: 5.776

2024-05-04 03:02:14,278 - ==> Best [Top1: 38.652   Top5: 55.588   Sparsity:0.00   Params: 754976 on epoch: 230]
2024-05-04 03:02:14,278 - Saving checkpoint to: logs/2024.05.03-230949/qat_checkpoint.pth.tar
2024-05-04 03:02:14,317 - 

2024-05-04 03:02:14,318 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:03:09,552 - Epoch: [241][   70/   70]    Overall Loss 0.006031    Objective Loss 0.006031    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.788942    
2024-05-04 03:03:09,789 - 

2024-05-04 03:03:09,790 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:04:02,930 - Epoch: [242][   70/   70]    Overall Loss 0.006011    Objective Loss 0.006011    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.759014    
2024-05-04 03:04:03,243 - 

2024-05-04 03:04:03,244 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:04:57,906 - Epoch: [243][   70/   70]    Overall Loss 0.006024    Objective Loss 0.006024    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.780748    
2024-05-04 03:04:58,271 - 

2024-05-04 03:04:58,272 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:05:49,674 - Epoch: [244][   70/   70]    Overall Loss 0.006006    Objective Loss 0.006006    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.734180    
2024-05-04 03:05:49,942 - 

2024-05-04 03:05:49,942 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:06:46,907 - Epoch: [245][   70/   70]    Overall Loss 0.006010    Objective Loss 0.006010    Top1 99.290780    Top5 99.290780    LR 0.000063    Time 0.813655    
2024-05-04 03:06:47,262 - 

2024-05-04 03:06:47,262 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:07:44,866 - Epoch: [246][   70/   70]    Overall Loss 0.005980    Objective Loss 0.005980    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.822765    
2024-05-04 03:07:45,527 - 

2024-05-04 03:07:45,528 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:08:42,148 - Epoch: [247][   70/   70]    Overall Loss 0.006763    Objective Loss 0.006763    Top1 98.581560    Top5 99.290780    LR 0.000063    Time 0.808720    
2024-05-04 03:08:42,450 - 

2024-05-04 03:08:42,451 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:09:40,084 - Epoch: [248][   70/   70]    Overall Loss 0.005995    Objective Loss 0.005995    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.823210    
2024-05-04 03:09:40,530 - 

2024-05-04 03:09:40,531 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:10:34,603 - Epoch: [249][   70/   70]    Overall Loss 0.005968    Objective Loss 0.005968    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.772345    
2024-05-04 03:10:35,066 - 

2024-05-04 03:10:35,067 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:11:30,998 - Epoch: [250][   70/   70]    Overall Loss 0.006596    Objective Loss 0.006596    Top1 99.290780    Top5 99.290780    LR 0.000063    Time 0.798889    
2024-05-04 03:11:31,370 - --- validate (epoch=250)-----------
2024-05-04 03:11:31,371 - 1736 samples (100 per mini-batch)
2024-05-04 03:11:51,748 - Epoch: [250][   18/   18]    Loss 5.886903    Top1 38.709677    Top5 54.838710    
2024-05-04 03:11:52,239 - ==> Top1: 38.710    Top5: 54.839    Loss: 5.887

2024-05-04 03:11:52,249 - ==> Best [Top1: 38.710   Top5: 54.839   Sparsity:0.00   Params: 754976 on epoch: 250]
2024-05-04 03:11:52,249 - Saving checkpoint to: logs/2024.05.03-230949/qat_checkpoint.pth.tar
2024-05-04 03:11:52,336 - 

2024-05-04 03:11:52,337 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:12:46,495 - Epoch: [251][   70/   70]    Overall Loss 0.006017    Objective Loss 0.006017    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.773519    
2024-05-04 03:12:46,770 - 

2024-05-04 03:12:46,770 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:13:42,714 - Epoch: [252][   70/   70]    Overall Loss 0.005980    Objective Loss 0.005980    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.799074    
2024-05-04 03:13:43,024 - 

2024-05-04 03:13:43,026 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:14:39,156 - Epoch: [253][   70/   70]    Overall Loss 0.005967    Objective Loss 0.005967    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.801726    
2024-05-04 03:14:39,669 - 

2024-05-04 03:14:39,670 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:15:29,280 - Epoch: [254][   70/   70]    Overall Loss 0.005946    Objective Loss 0.005946    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.708612    
2024-05-04 03:15:29,549 - 

2024-05-04 03:15:29,550 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:16:27,076 - Epoch: [255][   70/   70]    Overall Loss 0.005976    Objective Loss 0.005976    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.821694    
2024-05-04 03:16:27,421 - 

2024-05-04 03:16:27,422 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:17:29,493 - Epoch: [256][   70/   70]    Overall Loss 0.006107    Objective Loss 0.006107    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.886596    
2024-05-04 03:17:30,062 - 

2024-05-04 03:17:30,062 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:18:24,712 - Epoch: [257][   70/   70]    Overall Loss 0.005982    Objective Loss 0.005982    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.780587    
2024-05-04 03:18:25,582 - 

2024-05-04 03:18:25,582 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:19:21,817 - Epoch: [258][   70/   70]    Overall Loss 0.006156    Objective Loss 0.006156    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.803218    
2024-05-04 03:19:22,306 - 

2024-05-04 03:19:22,306 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:20:16,766 - Epoch: [259][   70/   70]    Overall Loss 0.006006    Objective Loss 0.006006    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.777890    
2024-05-04 03:20:17,150 - 

2024-05-04 03:20:17,151 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:21:13,586 - Epoch: [260][   70/   70]    Overall Loss 0.006316    Objective Loss 0.006316    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.806066    
2024-05-04 03:21:13,961 - --- validate (epoch=260)-----------
2024-05-04 03:21:13,962 - 1736 samples (100 per mini-batch)
2024-05-04 03:21:31,074 - Epoch: [260][   18/   18]    Loss 5.886612    Top1 38.594470    Top5 54.781106    
2024-05-04 03:21:31,611 - ==> Top1: 38.594    Top5: 54.781    Loss: 5.887

2024-05-04 03:21:31,620 - ==> Best [Top1: 38.710   Top5: 54.839   Sparsity:0.00   Params: 754976 on epoch: 250]
2024-05-04 03:21:31,621 - Saving checkpoint to: logs/2024.05.03-230949/qat_checkpoint.pth.tar
2024-05-04 03:21:31,678 - 

2024-05-04 03:21:31,679 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:22:29,325 - Epoch: [261][   70/   70]    Overall Loss 0.005930    Objective Loss 0.005930    Top1 98.581560    Top5 98.581560    LR 0.000063    Time 0.823393    
2024-05-04 03:22:29,702 - 

2024-05-04 03:22:29,703 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:23:22,386 - Epoch: [262][   70/   70]    Overall Loss 0.005913    Objective Loss 0.005913    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.752429    
2024-05-04 03:23:22,657 - 

2024-05-04 03:23:22,659 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:24:15,855 - Epoch: [263][   70/   70]    Overall Loss 0.005927    Objective Loss 0.005927    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.759813    
2024-05-04 03:24:16,065 - 

2024-05-04 03:24:16,066 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:25:15,685 - Epoch: [264][   70/   70]    Overall Loss 0.005927    Objective Loss 0.005927    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.851602    
2024-05-04 03:25:15,945 - 

2024-05-04 03:25:15,946 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:26:08,090 - Epoch: [265][   70/   70]    Overall Loss 0.006489    Objective Loss 0.006489    Top1 99.290780    Top5 99.290780    LR 0.000063    Time 0.744795    
2024-05-04 03:26:08,422 - 

2024-05-04 03:26:08,423 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:27:04,547 - Epoch: [266][   70/   70]    Overall Loss 0.005951    Objective Loss 0.005951    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.801651    
2024-05-04 03:27:04,876 - 

2024-05-04 03:27:04,876 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:27:59,106 - Epoch: [267][   70/   70]    Overall Loss 0.005876    Objective Loss 0.005876    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.774592    
2024-05-04 03:27:59,389 - 

2024-05-04 03:27:59,389 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:28:57,230 - Epoch: [268][   70/   70]    Overall Loss 0.005956    Objective Loss 0.005956    Top1 98.581560    Top5 100.000000    LR 0.000063    Time 0.826166    
2024-05-04 03:28:57,492 - 

2024-05-04 03:28:57,493 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:29:51,992 - Epoch: [269][   70/   70]    Overall Loss 0.006358    Objective Loss 0.006358    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.778409    
2024-05-04 03:29:52,680 - 

2024-05-04 03:29:52,682 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:30:45,268 - Epoch: [270][   70/   70]    Overall Loss 0.005908    Objective Loss 0.005908    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.751108    
2024-05-04 03:30:45,575 - --- validate (epoch=270)-----------
2024-05-04 03:30:45,576 - 1736 samples (100 per mini-batch)
2024-05-04 03:31:01,537 - Epoch: [270][   18/   18]    Loss 5.988862    Top1 38.306452    Top5 54.953917    
2024-05-04 03:31:01,885 - ==> Top1: 38.306    Top5: 54.954    Loss: 5.989

2024-05-04 03:31:01,895 - ==> Best [Top1: 38.710   Top5: 54.839   Sparsity:0.00   Params: 754976 on epoch: 250]
2024-05-04 03:31:01,896 - Saving checkpoint to: logs/2024.05.03-230949/qat_checkpoint.pth.tar
2024-05-04 03:31:01,979 - 

2024-05-04 03:31:01,980 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:31:56,799 - Epoch: [271][   70/   70]    Overall Loss 0.005887    Objective Loss 0.005887    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.782997    
2024-05-04 03:31:57,052 - 

2024-05-04 03:31:57,053 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:32:53,439 - Epoch: [272][   70/   70]    Overall Loss 0.005937    Objective Loss 0.005937    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.805392    
2024-05-04 03:32:54,066 - 

2024-05-04 03:32:54,068 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:33:49,287 - Epoch: [273][   70/   70]    Overall Loss 0.005858    Objective Loss 0.005858    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.788686    
2024-05-04 03:33:49,728 - 

2024-05-04 03:33:49,729 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:34:44,891 - Epoch: [274][   70/   70]    Overall Loss 0.006217    Objective Loss 0.006217    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.787911    
2024-05-04 03:34:45,104 - 

2024-05-04 03:34:45,105 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:35:38,980 - Epoch: [275][   70/   70]    Overall Loss 0.005911    Objective Loss 0.005911    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.769511    
2024-05-04 03:35:39,228 - 

2024-05-04 03:35:39,229 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:36:32,059 - Epoch: [276][   70/   70]    Overall Loss 0.005917    Objective Loss 0.005917    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.754582    
2024-05-04 03:36:32,390 - 

2024-05-04 03:36:32,390 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:37:25,738 - Epoch: [277][   70/   70]    Overall Loss 0.005881    Objective Loss 0.005881    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.762007    
2024-05-04 03:37:26,159 - 

2024-05-04 03:37:26,161 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:38:19,396 - Epoch: [278][   70/   70]    Overall Loss 0.005886    Objective Loss 0.005886    Top1 98.581560    Top5 100.000000    LR 0.000063    Time 0.760343    
2024-05-04 03:38:20,418 - 

2024-05-04 03:38:20,419 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:39:21,423 - Epoch: [279][   70/   70]    Overall Loss 0.006072    Objective Loss 0.006072    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.871362    
2024-05-04 03:39:21,701 - 

2024-05-04 03:39:21,702 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:40:18,475 - Epoch: [280][   70/   70]    Overall Loss 0.005831    Objective Loss 0.005831    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.810913    
2024-05-04 03:40:19,411 - --- validate (epoch=280)-----------
2024-05-04 03:40:19,412 - 1736 samples (100 per mini-batch)
2024-05-04 03:40:34,011 - Epoch: [280][   18/   18]    Loss 5.988070    Top1 38.882488    Top5 54.953917    
2024-05-04 03:40:34,348 - ==> Top1: 38.882    Top5: 54.954    Loss: 5.988

2024-05-04 03:40:34,357 - ==> Best [Top1: 38.882   Top5: 54.954   Sparsity:0.00   Params: 754976 on epoch: 280]
2024-05-04 03:40:34,357 - Saving checkpoint to: logs/2024.05.03-230949/qat_checkpoint.pth.tar
2024-05-04 03:40:34,453 - 

2024-05-04 03:40:34,454 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:41:30,694 - Epoch: [281][   70/   70]    Overall Loss 0.005905    Objective Loss 0.005905    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.803295    
2024-05-04 03:41:31,056 - 

2024-05-04 03:41:31,057 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:42:26,793 - Epoch: [282][   70/   70]    Overall Loss 0.005843    Objective Loss 0.005843    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.796096    
2024-05-04 03:42:27,136 - 

2024-05-04 03:42:27,136 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:43:23,424 - Epoch: [283][   70/   70]    Overall Loss 0.005939    Objective Loss 0.005939    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.803987    
2024-05-04 03:43:23,804 - 

2024-05-04 03:43:23,805 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:44:23,138 - Epoch: [284][   70/   70]    Overall Loss 0.005912    Objective Loss 0.005912    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.847484    
2024-05-04 03:44:23,637 - 

2024-05-04 03:44:23,637 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:45:18,357 - Epoch: [285][   70/   70]    Overall Loss 0.005966    Objective Loss 0.005966    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.781579    
2024-05-04 03:45:18,747 - 

2024-05-04 03:45:18,749 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:46:12,136 - Epoch: [286][   70/   70]    Overall Loss 0.005884    Objective Loss 0.005884    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.762545    
2024-05-04 03:46:13,160 - 

2024-05-04 03:46:13,161 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:47:09,680 - Epoch: [287][   70/   70]    Overall Loss 0.005919    Objective Loss 0.005919    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.807292    
2024-05-04 03:47:10,132 - 

2024-05-04 03:47:10,132 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:48:02,977 - Epoch: [288][   70/   70]    Overall Loss 0.005852    Objective Loss 0.005852    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.754807    
2024-05-04 03:48:03,259 - 

2024-05-04 03:48:03,261 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:48:59,812 - Epoch: [289][   70/   70]    Overall Loss 0.005937    Objective Loss 0.005937    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.807727    
2024-05-04 03:49:00,177 - 

2024-05-04 03:49:00,178 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:49:55,650 - Epoch: [290][   70/   70]    Overall Loss 0.005843    Objective Loss 0.005843    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.792335    
2024-05-04 03:49:55,890 - --- validate (epoch=290)-----------
2024-05-04 03:49:55,891 - 1736 samples (100 per mini-batch)
2024-05-04 03:50:12,142 - Epoch: [290][   18/   18]    Loss 6.092543    Top1 38.824885    Top5 55.875576    
2024-05-04 03:50:12,437 - ==> Top1: 38.825    Top5: 55.876    Loss: 6.093

2024-05-04 03:50:12,443 - ==> Best [Top1: 38.882   Top5: 54.954   Sparsity:0.00   Params: 754976 on epoch: 280]
2024-05-04 03:50:12,443 - Saving checkpoint to: logs/2024.05.03-230949/qat_checkpoint.pth.tar
2024-05-04 03:50:12,487 - 

2024-05-04 03:50:12,488 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:51:07,351 - Epoch: [291][   70/   70]    Overall Loss 0.005950    Objective Loss 0.005950    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.783637    
2024-05-04 03:51:07,583 - 

2024-05-04 03:51:07,584 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:52:06,745 - Epoch: [292][   70/   70]    Overall Loss 0.005821    Objective Loss 0.005821    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.845012    
2024-05-04 03:52:07,042 - 

2024-05-04 03:52:07,043 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:53:06,102 - Epoch: [293][   70/   70]    Overall Loss 0.006070    Objective Loss 0.006070    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.843571    
2024-05-04 03:53:06,584 - 

2024-05-04 03:53:06,584 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:54:03,216 - Epoch: [294][   70/   70]    Overall Loss 0.005840    Objective Loss 0.005840    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.808900    
2024-05-04 03:54:03,661 - 

2024-05-04 03:54:03,661 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:55:00,419 - Epoch: [295][   70/   70]    Overall Loss 0.005959    Objective Loss 0.005959    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.810707    
2024-05-04 03:55:00,818 - 

2024-05-04 03:55:00,819 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:56:06,055 - Epoch: [296][   70/   70]    Overall Loss 0.005855    Objective Loss 0.005855    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.931833    
2024-05-04 03:56:06,269 - 

2024-05-04 03:56:06,270 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:57:05,253 - Epoch: [297][   70/   70]    Overall Loss 0.005839    Objective Loss 0.005839    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.842498    
2024-05-04 03:57:05,527 - 

2024-05-04 03:57:05,528 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:58:00,815 - Epoch: [298][   70/   70]    Overall Loss 0.005878    Objective Loss 0.005878    Top1 100.000000    Top5 100.000000    LR 0.000063    Time 0.789707    
2024-05-04 03:58:01,042 - 

2024-05-04 03:58:01,043 - Training epoch: 6941 samples (100 per mini-batch)
2024-05-04 03:58:53,876 - Epoch: [299][   70/   70]    Overall Loss 0.005836    Objective Loss 0.005836    Top1 99.290780    Top5 100.000000    LR 0.000063    Time 0.754657    
2024-05-04 03:58:54,196 - --- test ---------------------
2024-05-04 03:58:54,197 - 1736 samples (100 per mini-batch)
2024-05-04 03:59:15,832 - Test: [   18/   18]    Loss 6.157216    Top1 38.364055    Top5 55.126728    
2024-05-04 03:59:16,067 - ==> Top1: 38.364    Top5: 55.127    Loss: 6.157

2024-05-04 03:59:16,073 - 
2024-05-04 03:59:16,073 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.05.03-230949/2024.05.03-230949.log
