2024-02-17 10:52:34,003 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.02.17-105234/2024.02.17-105234.log
2024-02-17 10:52:37,666 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2024-02-17 10:52:37,667 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}
2024-02-17 10:52:39,742 - Dataset sizes:
	training=50000
	validation=10000
	test=10000
2024-02-17 10:52:39,742 - Reading compression schedule from: policies/schedule-cifar100-mobilenetv2.yaml
2024-02-17 10:52:39,752 - 

2024-02-17 10:52:39,753 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:52:50,323 - Epoch: [0][  100/  391]    Overall Loss 4.416315    Objective Loss 4.416315                                        LR 0.100000    Time 0.105637    
2024-02-17 10:52:59,655 - Epoch: [0][  200/  391]    Overall Loss 4.286143    Objective Loss 4.286143                                        LR 0.100000    Time 0.099452    
2024-02-17 10:53:08,996 - Epoch: [0][  300/  391]    Overall Loss 4.195390    Objective Loss 4.195390                                        LR 0.100000    Time 0.097424    
2024-02-17 10:53:17,515 - Epoch: [0][  391/  391]    Overall Loss 4.128643    Objective Loss 4.128643    Top1 8.173077    Top5 30.769231    LR 0.100000    Time 0.096525    
2024-02-17 10:53:17,660 - --- validate (epoch=0)-----------
2024-02-17 10:53:17,660 - 10000 samples (128 per mini-batch)
2024-02-17 10:53:20,390 - Epoch: [0][   79/   79]    Loss 4.294045    Top1 5.940000    Top5 21.100000    
2024-02-17 10:53:20,575 - ==> Top1: 5.940    Top5: 21.100    Loss: 4.294

2024-02-17 10:53:20,916 - ==> Best [Top1: 5.940   Top5: 21.100   Sparsity:0.00   Params: 1341960 on epoch: 0]
2024-02-17 10:53:20,917 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 10:53:21,024 - 

2024-02-17 10:53:21,024 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:53:30,984 - Epoch: [1][  100/  391]    Overall Loss 3.804707    Objective Loss 3.804707                                        LR 0.100000    Time 0.099523    
2024-02-17 10:53:40,512 - Epoch: [1][  200/  391]    Overall Loss 3.776933    Objective Loss 3.776933                                        LR 0.100000    Time 0.097375    
2024-02-17 10:53:49,997 - Epoch: [1][  300/  391]    Overall Loss 3.746371    Objective Loss 3.746371                                        LR 0.100000    Time 0.096519    
2024-02-17 10:53:58,568 - Epoch: [1][  391/  391]    Overall Loss 3.722566    Objective Loss 3.722566    Top1 11.538462    Top5 36.538462    LR 0.100000    Time 0.095965    
2024-02-17 10:53:58,736 - --- validate (epoch=1)-----------
2024-02-17 10:53:58,736 - 10000 samples (128 per mini-batch)
2024-02-17 10:54:01,382 - Epoch: [1][   79/   79]    Loss 3.612136    Top1 11.770000    Top5 36.110000    
2024-02-17 10:54:01,550 - ==> Top1: 11.770    Top5: 36.110    Loss: 3.612

2024-02-17 10:54:01,569 - ==> Best [Top1: 11.770   Top5: 36.110   Sparsity:0.00   Params: 1341960 on epoch: 1]
2024-02-17 10:54:01,569 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 10:54:01,662 - 

2024-02-17 10:54:01,663 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:54:12,130 - Epoch: [2][  100/  391]    Overall Loss 3.567376    Objective Loss 3.567376                                        LR 0.100000    Time 0.104593    
2024-02-17 10:54:21,657 - Epoch: [2][  200/  391]    Overall Loss 3.529924    Objective Loss 3.529924                                        LR 0.100000    Time 0.099903    
2024-02-17 10:54:31,112 - Epoch: [2][  300/  391]    Overall Loss 3.513421    Objective Loss 3.513421                                        LR 0.100000    Time 0.098104    
2024-02-17 10:54:39,686 - Epoch: [2][  391/  391]    Overall Loss 3.484502    Objective Loss 3.484502    Top1 13.461538    Top5 45.192308    LR 0.100000    Time 0.097190    
2024-02-17 10:54:39,837 - --- validate (epoch=2)-----------
2024-02-17 10:54:39,838 - 10000 samples (128 per mini-batch)
2024-02-17 10:54:42,565 - Epoch: [2][   79/   79]    Loss 3.470122    Top1 14.090000    Top5 42.420000    
2024-02-17 10:54:42,698 - ==> Top1: 14.090    Top5: 42.420    Loss: 3.470

2024-02-17 10:54:42,716 - ==> Best [Top1: 14.090   Top5: 42.420   Sparsity:0.00   Params: 1341960 on epoch: 2]
2024-02-17 10:54:42,717 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 10:54:42,824 - 

2024-02-17 10:54:42,824 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:54:52,738 - Epoch: [3][  100/  391]    Overall Loss 3.324654    Objective Loss 3.324654                                        LR 0.100000    Time 0.099061    
2024-02-17 10:55:02,336 - Epoch: [3][  200/  391]    Overall Loss 3.292986    Objective Loss 3.292986                                        LR 0.100000    Time 0.097501    
2024-02-17 10:55:11,922 - Epoch: [3][  300/  391]    Overall Loss 3.274662    Objective Loss 3.274662                                        LR 0.100000    Time 0.096938    
2024-02-17 10:55:20,716 - Epoch: [3][  391/  391]    Overall Loss 3.251404    Objective Loss 3.251404    Top1 17.788462    Top5 50.000000    LR 0.100000    Time 0.096857    
2024-02-17 10:55:20,885 - --- validate (epoch=3)-----------
2024-02-17 10:55:20,886 - 10000 samples (128 per mini-batch)
2024-02-17 10:55:23,440 - Epoch: [3][   79/   79]    Loss 3.512974    Top1 16.070000    Top5 42.120000    
2024-02-17 10:55:23,548 - ==> Top1: 16.070    Top5: 42.120    Loss: 3.513

2024-02-17 10:55:23,567 - ==> Best [Top1: 16.070   Top5: 42.120   Sparsity:0.00   Params: 1341960 on epoch: 3]
2024-02-17 10:55:23,567 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 10:55:23,657 - 

2024-02-17 10:55:23,658 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:55:33,528 - Epoch: [4][  100/  391]    Overall Loss 3.115228    Objective Loss 3.115228                                        LR 0.100000    Time 0.098628    
2024-02-17 10:55:42,981 - Epoch: [4][  200/  391]    Overall Loss 3.098125    Objective Loss 3.098125                                        LR 0.100000    Time 0.096557    
2024-02-17 10:55:52,417 - Epoch: [4][  300/  391]    Overall Loss 3.082714    Objective Loss 3.082714                                        LR 0.100000    Time 0.095810    
2024-02-17 10:56:01,230 - Epoch: [4][  391/  391]    Overall Loss 3.057796    Objective Loss 3.057796    Top1 21.153846    Top5 56.730769    LR 0.100000    Time 0.096041    
2024-02-17 10:56:01,420 - --- validate (epoch=4)-----------
2024-02-17 10:56:01,420 - 10000 samples (128 per mini-batch)
2024-02-17 10:56:04,054 - Epoch: [4][   79/   79]    Loss 3.680597    Top1 15.720000    Top5 42.760000    
2024-02-17 10:56:04,153 - ==> Top1: 15.720    Top5: 42.760    Loss: 3.681

2024-02-17 10:56:04,171 - ==> Best [Top1: 16.070   Top5: 42.120   Sparsity:0.00   Params: 1341960 on epoch: 3]
2024-02-17 10:56:04,171 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 10:56:04,504 - 

2024-02-17 10:56:04,504 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:56:14,419 - Epoch: [5][  100/  391]    Overall Loss 2.921600    Objective Loss 2.921600                                        LR 0.100000    Time 0.099076    
2024-02-17 10:56:23,964 - Epoch: [5][  200/  391]    Overall Loss 2.925484    Objective Loss 2.925484                                        LR 0.100000    Time 0.097239    
2024-02-17 10:56:33,395 - Epoch: [5][  300/  391]    Overall Loss 2.903268    Objective Loss 2.903268                                        LR 0.100000    Time 0.096249    
2024-02-17 10:56:42,161 - Epoch: [5][  391/  391]    Overall Loss 2.888552    Objective Loss 2.888552    Top1 24.038462    Top5 55.769231    LR 0.100000    Time 0.096256    
2024-02-17 10:56:42,299 - --- validate (epoch=5)-----------
2024-02-17 10:56:42,300 - 10000 samples (128 per mini-batch)
2024-02-17 10:56:44,937 - Epoch: [5][   79/   79]    Loss 3.252491    Top1 20.500000    Top5 51.040000    
2024-02-17 10:56:45,077 - ==> Top1: 20.500    Top5: 51.040    Loss: 3.252

2024-02-17 10:56:45,094 - ==> Best [Top1: 20.500   Top5: 51.040   Sparsity:0.00   Params: 1341960 on epoch: 5]
2024-02-17 10:56:45,095 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 10:56:45,185 - 

2024-02-17 10:56:45,186 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:56:55,232 - Epoch: [6][  100/  391]    Overall Loss 2.782903    Objective Loss 2.782903                                        LR 0.100000    Time 0.100391    
2024-02-17 10:57:04,795 - Epoch: [6][  200/  391]    Overall Loss 2.783424    Objective Loss 2.783424                                        LR 0.100000    Time 0.097989    
2024-02-17 10:57:14,295 - Epoch: [6][  300/  391]    Overall Loss 2.762626    Objective Loss 2.762626                                        LR 0.100000    Time 0.096980    
2024-02-17 10:57:22,874 - Epoch: [6][  391/  391]    Overall Loss 2.749103    Objective Loss 2.749103    Top1 28.365385    Top5 61.057692    LR 0.100000    Time 0.096338    
2024-02-17 10:57:23,083 - --- validate (epoch=6)-----------
2024-02-17 10:57:23,083 - 10000 samples (128 per mini-batch)
2024-02-17 10:57:25,628 - Epoch: [6][   79/   79]    Loss 2.988540    Top1 23.640000    Top5 56.320000    
2024-02-17 10:57:25,729 - ==> Top1: 23.640    Top5: 56.320    Loss: 2.989

2024-02-17 10:57:25,747 - ==> Best [Top1: 23.640   Top5: 56.320   Sparsity:0.00   Params: 1341960 on epoch: 6]
2024-02-17 10:57:25,747 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 10:57:25,837 - 

2024-02-17 10:57:25,837 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:57:35,678 - Epoch: [7][  100/  391]    Overall Loss 2.656388    Objective Loss 2.656388                                        LR 0.100000    Time 0.098334    
2024-02-17 10:57:45,180 - Epoch: [7][  200/  391]    Overall Loss 2.651310    Objective Loss 2.651310                                        LR 0.100000    Time 0.096656    
2024-02-17 10:57:54,775 - Epoch: [7][  300/  391]    Overall Loss 2.638051    Objective Loss 2.638051                                        LR 0.100000    Time 0.096406    
2024-02-17 10:58:03,618 - Epoch: [7][  391/  391]    Overall Loss 2.628637    Objective Loss 2.628637    Top1 29.807692    Top5 65.865385    LR 0.100000    Time 0.096576    
2024-02-17 10:58:03,793 - --- validate (epoch=7)-----------
2024-02-17 10:58:03,795 - 10000 samples (128 per mini-batch)
2024-02-17 10:58:06,425 - Epoch: [7][   79/   79]    Loss 2.928358    Top1 26.420000    Top5 58.360000    
2024-02-17 10:58:06,522 - ==> Top1: 26.420    Top5: 58.360    Loss: 2.928

2024-02-17 10:58:06,542 - ==> Best [Top1: 26.420   Top5: 58.360   Sparsity:0.00   Params: 1341960 on epoch: 7]
2024-02-17 10:58:06,542 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 10:58:06,633 - 

2024-02-17 10:58:06,633 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:58:16,768 - Epoch: [8][  100/  391]    Overall Loss 2.557369    Objective Loss 2.557369                                        LR 0.100000    Time 0.101277    
2024-02-17 10:58:26,186 - Epoch: [8][  200/  391]    Overall Loss 2.552592    Objective Loss 2.552592                                        LR 0.100000    Time 0.097708    
2024-02-17 10:58:35,672 - Epoch: [8][  300/  391]    Overall Loss 2.547581    Objective Loss 2.547581                                        LR 0.100000    Time 0.096745    
2024-02-17 10:58:44,261 - Epoch: [8][  391/  391]    Overall Loss 2.535819    Objective Loss 2.535819    Top1 30.769231    Top5 67.788462    LR 0.100000    Time 0.096183    
2024-02-17 10:58:44,411 - --- validate (epoch=8)-----------
2024-02-17 10:58:44,411 - 10000 samples (128 per mini-batch)
2024-02-17 10:58:47,080 - Epoch: [8][   79/   79]    Loss 2.706972    Top1 29.560000    Top5 62.900000    
2024-02-17 10:58:47,253 - ==> Top1: 29.560    Top5: 62.900    Loss: 2.707

2024-02-17 10:58:47,274 - ==> Best [Top1: 29.560   Top5: 62.900   Sparsity:0.00   Params: 1341960 on epoch: 8]
2024-02-17 10:58:47,275 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 10:58:47,370 - 

2024-02-17 10:58:47,370 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:58:57,574 - Epoch: [9][  100/  391]    Overall Loss 2.443809    Objective Loss 2.443809                                        LR 0.100000    Time 0.101966    
2024-02-17 10:59:07,015 - Epoch: [9][  200/  391]    Overall Loss 2.461339    Objective Loss 2.461339                                        LR 0.100000    Time 0.098160    
2024-02-17 10:59:16,584 - Epoch: [9][  300/  391]    Overall Loss 2.454683    Objective Loss 2.454683                                        LR 0.100000    Time 0.097321    
2024-02-17 10:59:25,245 - Epoch: [9][  391/  391]    Overall Loss 2.455477    Objective Loss 2.455477    Top1 37.019231    Top5 63.942308    LR 0.100000    Time 0.096811    
2024-02-17 10:59:25,402 - --- validate (epoch=9)-----------
2024-02-17 10:59:25,403 - 10000 samples (128 per mini-batch)
2024-02-17 10:59:27,937 - Epoch: [9][   79/   79]    Loss 3.159537    Top1 24.220000    Top5 55.490000    
2024-02-17 10:59:28,093 - ==> Top1: 24.220    Top5: 55.490    Loss: 3.160

2024-02-17 10:59:28,112 - ==> Best [Top1: 29.560   Top5: 62.900   Sparsity:0.00   Params: 1341960 on epoch: 8]
2024-02-17 10:59:28,112 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 10:59:28,191 - 

2024-02-17 10:59:28,192 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:59:38,098 - Epoch: [10][  100/  391]    Overall Loss 2.413701    Objective Loss 2.413701                                        LR 0.100000    Time 0.098990    
2024-02-17 10:59:47,506 - Epoch: [10][  200/  391]    Overall Loss 2.394479    Objective Loss 2.394479                                        LR 0.100000    Time 0.096516    
2024-02-17 10:59:57,050 - Epoch: [10][  300/  391]    Overall Loss 2.385760    Objective Loss 2.385760                                        LR 0.100000    Time 0.096141    
2024-02-17 11:00:05,709 - Epoch: [10][  391/  391]    Overall Loss 2.385589    Objective Loss 2.385589    Top1 31.250000    Top5 69.711538    LR 0.100000    Time 0.095901    
2024-02-17 11:00:05,907 - --- validate (epoch=10)-----------
2024-02-17 11:00:05,907 - 10000 samples (128 per mini-batch)
2024-02-17 11:00:08,520 - Epoch: [10][   79/   79]    Loss 2.457716    Top1 33.660000    Top5 68.120000    
2024-02-17 11:00:08,706 - ==> Top1: 33.660    Top5: 68.120    Loss: 2.458

2024-02-17 11:00:08,725 - ==> Best [Top1: 33.660   Top5: 68.120   Sparsity:0.00   Params: 1341960 on epoch: 10]
2024-02-17 11:00:08,726 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:00:08,818 - 

2024-02-17 11:00:08,819 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:00:18,967 - Epoch: [11][  100/  391]    Overall Loss 2.344371    Objective Loss 2.344371                                        LR 0.100000    Time 0.101417    
2024-02-17 11:00:28,517 - Epoch: [11][  200/  391]    Overall Loss 2.329532    Objective Loss 2.329532                                        LR 0.100000    Time 0.098436    
2024-02-17 11:00:37,977 - Epoch: [11][  300/  391]    Overall Loss 2.320764    Objective Loss 2.320764                                        LR 0.100000    Time 0.097141    
2024-02-17 11:00:46,649 - Epoch: [11][  391/  391]    Overall Loss 2.318187    Objective Loss 2.318187    Top1 40.384615    Top5 67.307692    LR 0.100000    Time 0.096700    
2024-02-17 11:00:46,792 - --- validate (epoch=11)-----------
2024-02-17 11:00:46,793 - 10000 samples (128 per mini-batch)
2024-02-17 11:00:49,308 - Epoch: [11][   79/   79]    Loss 2.570210    Top1 32.130000    Top5 66.130000    
2024-02-17 11:00:49,502 - ==> Top1: 32.130    Top5: 66.130    Loss: 2.570

2024-02-17 11:00:49,521 - ==> Best [Top1: 33.660   Top5: 68.120   Sparsity:0.00   Params: 1341960 on epoch: 10]
2024-02-17 11:00:49,522 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:00:49,599 - 

2024-02-17 11:00:49,599 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:01:00,003 - Epoch: [12][  100/  391]    Overall Loss 2.279788    Objective Loss 2.279788                                        LR 0.100000    Time 0.103966    
2024-02-17 11:01:09,719 - Epoch: [12][  200/  391]    Overall Loss 2.269952    Objective Loss 2.269952                                        LR 0.100000    Time 0.100535    
2024-02-17 11:01:18,964 - Epoch: [12][  300/  391]    Overall Loss 2.270224    Objective Loss 2.270224                                        LR 0.100000    Time 0.097822    
2024-02-17 11:01:26,799 - Epoch: [12][  391/  391]    Overall Loss 2.271273    Objective Loss 2.271273    Top1 36.057692    Top5 73.076923    LR 0.100000    Time 0.095085    
2024-02-17 11:01:26,907 - --- validate (epoch=12)-----------
2024-02-17 11:01:26,908 - 10000 samples (128 per mini-batch)
2024-02-17 11:01:29,069 - Epoch: [12][   79/   79]    Loss 2.388325    Top1 36.210000    Top5 69.180000    
2024-02-17 11:01:29,211 - ==> Top1: 36.210    Top5: 69.180    Loss: 2.388

2024-02-17 11:01:29,220 - ==> Best [Top1: 36.210   Top5: 69.180   Sparsity:0.00   Params: 1341960 on epoch: 12]
2024-02-17 11:01:29,220 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:01:29,310 - 

2024-02-17 11:01:29,311 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:01:38,169 - Epoch: [13][  100/  391]    Overall Loss 2.193745    Objective Loss 2.193745                                        LR 0.100000    Time 0.088521    
2024-02-17 11:01:47,616 - Epoch: [13][  200/  391]    Overall Loss 2.201809    Objective Loss 2.201809                                        LR 0.100000    Time 0.091468    
2024-02-17 11:01:57,087 - Epoch: [13][  300/  391]    Overall Loss 2.204183    Objective Loss 2.204183                                        LR 0.100000    Time 0.092534    
2024-02-17 11:02:05,979 - Epoch: [13][  391/  391]    Overall Loss 2.202856    Objective Loss 2.202856    Top1 39.903846    Top5 68.750000    LR 0.100000    Time 0.093726    
2024-02-17 11:02:06,132 - --- validate (epoch=13)-----------
2024-02-17 11:02:06,133 - 10000 samples (128 per mini-batch)
2024-02-17 11:02:08,803 - Epoch: [13][   79/   79]    Loss 3.709286    Top1 22.120000    Top5 47.120000    
2024-02-17 11:02:08,982 - ==> Top1: 22.120    Top5: 47.120    Loss: 3.709

2024-02-17 11:02:08,999 - ==> Best [Top1: 36.210   Top5: 69.180   Sparsity:0.00   Params: 1341960 on epoch: 12]
2024-02-17 11:02:08,999 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:02:09,090 - 

2024-02-17 11:02:09,090 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:02:19,694 - Epoch: [14][  100/  391]    Overall Loss 2.149988    Objective Loss 2.149988                                        LR 0.100000    Time 0.105962    
2024-02-17 11:02:28,735 - Epoch: [14][  200/  391]    Overall Loss 2.156148    Objective Loss 2.156148                                        LR 0.100000    Time 0.098160    
2024-02-17 11:02:38,562 - Epoch: [14][  300/  391]    Overall Loss 2.161825    Objective Loss 2.161825                                        LR 0.100000    Time 0.098180    
2024-02-17 11:02:47,680 - Epoch: [14][  391/  391]    Overall Loss 2.157114    Objective Loss 2.157114    Top1 41.826923    Top5 75.000000    LR 0.100000    Time 0.098635    
2024-02-17 11:02:47,841 - --- validate (epoch=14)-----------
2024-02-17 11:02:47,842 - 10000 samples (128 per mini-batch)
2024-02-17 11:02:50,458 - Epoch: [14][   79/   79]    Loss 2.429733    Top1 36.470000    Top5 68.480000    
2024-02-17 11:02:50,642 - ==> Top1: 36.470    Top5: 68.480    Loss: 2.430

2024-02-17 11:02:50,660 - ==> Best [Top1: 36.470   Top5: 68.480   Sparsity:0.00   Params: 1341960 on epoch: 14]
2024-02-17 11:02:50,660 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:02:50,755 - 

2024-02-17 11:02:50,755 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:03:00,632 - Epoch: [15][  100/  391]    Overall Loss 2.120016    Objective Loss 2.120016                                        LR 0.100000    Time 0.098685    
2024-02-17 11:03:10,173 - Epoch: [15][  200/  391]    Overall Loss 2.116726    Objective Loss 2.116726                                        LR 0.100000    Time 0.097025    
2024-02-17 11:03:19,602 - Epoch: [15][  300/  391]    Overall Loss 2.114391    Objective Loss 2.114391                                        LR 0.100000    Time 0.096097    
2024-02-17 11:03:28,670 - Epoch: [15][  391/  391]    Overall Loss 2.116584    Objective Loss 2.116584    Top1 47.596154    Top5 78.846154    LR 0.100000    Time 0.096910    
2024-02-17 11:03:28,838 - --- validate (epoch=15)-----------
2024-02-17 11:03:28,839 - 10000 samples (128 per mini-batch)
2024-02-17 11:03:31,472 - Epoch: [15][   79/   79]    Loss 2.365633    Top1 37.020000    Top5 70.350000    
2024-02-17 11:03:31,623 - ==> Top1: 37.020    Top5: 70.350    Loss: 2.366

2024-02-17 11:03:31,633 - ==> Best [Top1: 37.020   Top5: 70.350   Sparsity:0.00   Params: 1341960 on epoch: 15]
2024-02-17 11:03:31,633 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:03:31,727 - 

2024-02-17 11:03:31,727 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:03:42,360 - Epoch: [16][  100/  391]    Overall Loss 2.047925    Objective Loss 2.047925                                        LR 0.100000    Time 0.106250    
2024-02-17 11:03:52,177 - Epoch: [16][  200/  391]    Overall Loss 2.072812    Objective Loss 2.072812                                        LR 0.100000    Time 0.102184    
2024-02-17 11:04:02,069 - Epoch: [16][  300/  391]    Overall Loss 2.064370    Objective Loss 2.064370                                        LR 0.100000    Time 0.101079    
2024-02-17 11:04:11,137 - Epoch: [16][  391/  391]    Overall Loss 2.064254    Objective Loss 2.064254    Top1 44.230769    Top5 77.403846    LR 0.100000    Time 0.100734    
2024-02-17 11:04:11,281 - --- validate (epoch=16)-----------
2024-02-17 11:04:11,281 - 10000 samples (128 per mini-batch)
2024-02-17 11:04:13,877 - Epoch: [16][   79/   79]    Loss 2.320833    Top1 38.740000    Top5 71.180000    
2024-02-17 11:04:13,979 - ==> Top1: 38.740    Top5: 71.180    Loss: 2.321

2024-02-17 11:04:13,996 - ==> Best [Top1: 38.740   Top5: 71.180   Sparsity:0.00   Params: 1341960 on epoch: 16]
2024-02-17 11:04:13,997 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:04:14,091 - 

2024-02-17 11:04:14,091 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:04:24,204 - Epoch: [17][  100/  391]    Overall Loss 2.010597    Objective Loss 2.010597                                        LR 0.100000    Time 0.101056    
2024-02-17 11:04:33,818 - Epoch: [17][  200/  391]    Overall Loss 2.013927    Objective Loss 2.013927                                        LR 0.100000    Time 0.098570    
2024-02-17 11:04:43,473 - Epoch: [17][  300/  391]    Overall Loss 2.022046    Objective Loss 2.022046                                        LR 0.100000    Time 0.097882    
2024-02-17 11:04:52,328 - Epoch: [17][  391/  391]    Overall Loss 2.025805    Objective Loss 2.025805    Top1 44.230769    Top5 77.403846    LR 0.100000    Time 0.097737    
2024-02-17 11:04:52,563 - --- validate (epoch=17)-----------
2024-02-17 11:04:52,565 - 10000 samples (128 per mini-batch)
2024-02-17 11:04:55,322 - Epoch: [17][   79/   79]    Loss 2.210307    Top1 40.720000    Top5 73.640000    
2024-02-17 11:04:55,448 - ==> Top1: 40.720    Top5: 73.640    Loss: 2.210

2024-02-17 11:04:55,466 - ==> Best [Top1: 40.720   Top5: 73.640   Sparsity:0.00   Params: 1341960 on epoch: 17]
2024-02-17 11:04:55,466 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:04:55,582 - 

2024-02-17 11:04:55,582 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:05:05,897 - Epoch: [18][  100/  391]    Overall Loss 2.002099    Objective Loss 2.002099                                        LR 0.100000    Time 0.103064    
2024-02-17 11:05:15,340 - Epoch: [18][  200/  391]    Overall Loss 1.996806    Objective Loss 1.996806                                        LR 0.100000    Time 0.098722    
2024-02-17 11:05:24,915 - Epoch: [18][  300/  391]    Overall Loss 2.003348    Objective Loss 2.003348                                        LR 0.100000    Time 0.097715    
2024-02-17 11:05:33,702 - Epoch: [18][  391/  391]    Overall Loss 2.002153    Objective Loss 2.002153    Top1 42.788462    Top5 75.000000    LR 0.100000    Time 0.097434    
2024-02-17 11:05:33,849 - --- validate (epoch=18)-----------
2024-02-17 11:05:33,850 - 10000 samples (128 per mini-batch)
2024-02-17 11:05:36,540 - Epoch: [18][   79/   79]    Loss 2.192672    Top1 41.320000    Top5 74.030000    
2024-02-17 11:05:36,656 - ==> Top1: 41.320    Top5: 74.030    Loss: 2.193

2024-02-17 11:05:36,673 - ==> Best [Top1: 41.320   Top5: 74.030   Sparsity:0.00   Params: 1341960 on epoch: 18]
2024-02-17 11:05:36,673 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:05:36,768 - 

2024-02-17 11:05:36,769 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:05:47,233 - Epoch: [19][  100/  391]    Overall Loss 1.957174    Objective Loss 1.957174                                        LR 0.100000    Time 0.104558    
2024-02-17 11:05:57,125 - Epoch: [19][  200/  391]    Overall Loss 1.959424    Objective Loss 1.959424                                        LR 0.100000    Time 0.101711    
2024-02-17 11:06:06,652 - Epoch: [19][  300/  391]    Overall Loss 1.953811    Objective Loss 1.953811                                        LR 0.100000    Time 0.099545    
2024-02-17 11:06:16,051 - Epoch: [19][  391/  391]    Overall Loss 1.957145    Objective Loss 1.957145    Top1 46.634615    Top5 79.807692    LR 0.100000    Time 0.100404    
2024-02-17 11:06:16,195 - --- validate (epoch=19)-----------
2024-02-17 11:06:16,196 - 10000 samples (128 per mini-batch)
2024-02-17 11:06:18,839 - Epoch: [19][   79/   79]    Loss 2.076991    Top1 42.880000    Top5 75.840000    
2024-02-17 11:06:18,937 - ==> Top1: 42.880    Top5: 75.840    Loss: 2.077

2024-02-17 11:06:18,947 - ==> Best [Top1: 42.880   Top5: 75.840   Sparsity:0.00   Params: 1341960 on epoch: 19]
2024-02-17 11:06:18,947 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:06:19,059 - 

2024-02-17 11:06:19,059 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:06:29,264 - Epoch: [20][  100/  391]    Overall Loss 1.917232    Objective Loss 1.917232                                        LR 0.100000    Time 0.101965    
2024-02-17 11:06:38,930 - Epoch: [20][  200/  391]    Overall Loss 1.918055    Objective Loss 1.918055                                        LR 0.100000    Time 0.099289    
2024-02-17 11:06:48,617 - Epoch: [20][  300/  391]    Overall Loss 1.924079    Objective Loss 1.924079                                        LR 0.100000    Time 0.098463    
2024-02-17 11:06:57,499 - Epoch: [20][  391/  391]    Overall Loss 1.920631    Objective Loss 1.920631    Top1 50.480769    Top5 82.692308    LR 0.100000    Time 0.098252    
2024-02-17 11:06:57,683 - --- validate (epoch=20)-----------
2024-02-17 11:06:57,684 - 10000 samples (128 per mini-batch)
2024-02-17 11:07:00,601 - Epoch: [20][   79/   79]    Loss 2.159016    Top1 40.930000    Top5 75.230000    
2024-02-17 11:07:00,748 - ==> Top1: 40.930    Top5: 75.230    Loss: 2.159

2024-02-17 11:07:00,766 - ==> Best [Top1: 42.880   Top5: 75.840   Sparsity:0.00   Params: 1341960 on epoch: 19]
2024-02-17 11:07:00,767 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:07:00,851 - 

2024-02-17 11:07:00,851 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:07:11,126 - Epoch: [21][  100/  391]    Overall Loss 1.908032    Objective Loss 1.908032                                        LR 0.100000    Time 0.102671    
2024-02-17 11:07:20,536 - Epoch: [21][  200/  391]    Overall Loss 1.902803    Objective Loss 1.902803                                        LR 0.100000    Time 0.098355    
2024-02-17 11:07:29,755 - Epoch: [21][  300/  391]    Overall Loss 1.902629    Objective Loss 1.902629                                        LR 0.100000    Time 0.096283    
2024-02-17 11:07:38,478 - Epoch: [21][  391/  391]    Overall Loss 1.899149    Objective Loss 1.899149    Top1 44.230769    Top5 75.961538    LR 0.100000    Time 0.096173    
2024-02-17 11:07:38,659 - --- validate (epoch=21)-----------
2024-02-17 11:07:38,660 - 10000 samples (128 per mini-batch)
2024-02-17 11:07:41,327 - Epoch: [21][   79/   79]    Loss 2.036169    Top1 44.570000    Top5 76.830000    
2024-02-17 11:07:41,482 - ==> Top1: 44.570    Top5: 76.830    Loss: 2.036

2024-02-17 11:07:41,503 - ==> Best [Top1: 44.570   Top5: 76.830   Sparsity:0.00   Params: 1341960 on epoch: 21]
2024-02-17 11:07:41,503 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:07:41,599 - 

2024-02-17 11:07:41,599 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:07:52,182 - Epoch: [22][  100/  391]    Overall Loss 1.867678    Objective Loss 1.867678                                        LR 0.100000    Time 0.105742    
2024-02-17 11:08:01,410 - Epoch: [22][  200/  391]    Overall Loss 1.858907    Objective Loss 1.858907                                        LR 0.100000    Time 0.098985    
2024-02-17 11:08:10,630 - Epoch: [22][  300/  391]    Overall Loss 1.868168    Objective Loss 1.868168                                        LR 0.100000    Time 0.096709    
2024-02-17 11:08:19,341 - Epoch: [22][  391/  391]    Overall Loss 1.867814    Objective Loss 1.867814    Top1 46.634615    Top5 80.288462    LR 0.100000    Time 0.096470    
2024-02-17 11:08:19,479 - --- validate (epoch=22)-----------
2024-02-17 11:08:19,480 - 10000 samples (128 per mini-batch)
2024-02-17 11:08:22,274 - Epoch: [22][   79/   79]    Loss 2.015076    Top1 44.960000    Top5 77.410000    
2024-02-17 11:08:22,403 - ==> Top1: 44.960    Top5: 77.410    Loss: 2.015

2024-02-17 11:08:22,422 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:08:22,423 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:08:22,517 - 

2024-02-17 11:08:22,517 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:08:32,681 - Epoch: [23][  100/  391]    Overall Loss 1.836784    Objective Loss 1.836784                                        LR 0.100000    Time 0.101558    
2024-02-17 11:08:42,258 - Epoch: [23][  200/  391]    Overall Loss 1.845296    Objective Loss 1.845296                                        LR 0.100000    Time 0.098640    
2024-02-17 11:08:49,500 - Epoch: [23][  300/  391]    Overall Loss 1.845424    Objective Loss 1.845424                                        LR 0.100000    Time 0.089888    
2024-02-17 11:08:56,036 - Epoch: [23][  391/  391]    Overall Loss 1.846385    Objective Loss 1.846385    Top1 42.788462    Top5 74.519231    LR 0.100000    Time 0.085674    
2024-02-17 11:08:56,183 - --- validate (epoch=23)-----------
2024-02-17 11:08:56,183 - 10000 samples (128 per mini-batch)
2024-02-17 11:08:58,751 - Epoch: [23][   79/   79]    Loss 2.083110    Top1 44.040000    Top5 76.330000    
2024-02-17 11:08:58,893 - ==> Top1: 44.040    Top5: 76.330    Loss: 2.083

2024-02-17 11:08:58,912 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:08:58,912 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:08:58,989 - 

2024-02-17 11:08:58,990 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:09:09,321 - Epoch: [24][  100/  391]    Overall Loss 1.795736    Objective Loss 1.795736                                        LR 0.100000    Time 0.103238    
2024-02-17 11:09:18,591 - Epoch: [24][  200/  391]    Overall Loss 1.800785    Objective Loss 1.800785                                        LR 0.100000    Time 0.097944    
2024-02-17 11:09:27,874 - Epoch: [24][  300/  391]    Overall Loss 1.814031    Objective Loss 1.814031                                        LR 0.100000    Time 0.096224    
2024-02-17 11:09:36,605 - Epoch: [24][  391/  391]    Overall Loss 1.819678    Objective Loss 1.819678    Top1 53.365385    Top5 83.653846    LR 0.100000    Time 0.096149    
2024-02-17 11:09:36,739 - --- validate (epoch=24)-----------
2024-02-17 11:09:36,740 - 10000 samples (128 per mini-batch)
2024-02-17 11:09:39,320 - Epoch: [24][   79/   79]    Loss 2.628696    Top1 35.390000    Top5 68.330000    
2024-02-17 11:09:39,452 - ==> Top1: 35.390    Top5: 68.330    Loss: 2.629

2024-02-17 11:09:39,471 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:09:39,471 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:09:39,549 - 

2024-02-17 11:09:39,550 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:09:49,721 - Epoch: [25][  100/  391]    Overall Loss 1.772238    Objective Loss 1.772238                                        LR 0.100000    Time 0.101633    
2024-02-17 11:09:58,941 - Epoch: [25][  200/  391]    Overall Loss 1.775472    Objective Loss 1.775472                                        LR 0.100000    Time 0.096895    
2024-02-17 11:10:08,202 - Epoch: [25][  300/  391]    Overall Loss 1.788799    Objective Loss 1.788799                                        LR 0.100000    Time 0.095451    
2024-02-17 11:10:16,916 - Epoch: [25][  391/  391]    Overall Loss 1.791222    Objective Loss 1.791222    Top1 48.076923    Top5 81.730769    LR 0.100000    Time 0.095513    
2024-02-17 11:10:17,094 - --- validate (epoch=25)-----------
2024-02-17 11:10:17,095 - 10000 samples (128 per mini-batch)
2024-02-17 11:10:19,693 - Epoch: [25][   79/   79]    Loss 2.358221    Top1 39.670000    Top5 71.360000    
2024-02-17 11:10:19,833 - ==> Top1: 39.670    Top5: 71.360    Loss: 2.358

2024-02-17 11:10:19,851 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:10:19,852 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:10:19,927 - 

2024-02-17 11:10:19,927 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:10:30,096 - Epoch: [26][  100/  391]    Overall Loss 1.733931    Objective Loss 1.733931                                        LR 0.100000    Time 0.101613    
2024-02-17 11:10:39,584 - Epoch: [26][  200/  391]    Overall Loss 1.755922    Objective Loss 1.755922                                        LR 0.100000    Time 0.098226    
2024-02-17 11:10:48,790 - Epoch: [26][  300/  391]    Overall Loss 1.762424    Objective Loss 1.762424                                        LR 0.100000    Time 0.096154    
2024-02-17 11:10:57,541 - Epoch: [26][  391/  391]    Overall Loss 1.766247    Objective Loss 1.766247    Top1 51.442308    Top5 82.692308    LR 0.100000    Time 0.096148    
2024-02-17 11:10:57,676 - --- validate (epoch=26)-----------
2024-02-17 11:10:57,677 - 10000 samples (128 per mini-batch)
2024-02-17 11:11:00,341 - Epoch: [26][   79/   79]    Loss 2.096929    Top1 43.990000    Top5 76.130000    
2024-02-17 11:11:00,475 - ==> Top1: 43.990    Top5: 76.130    Loss: 2.097

2024-02-17 11:11:00,495 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:11:00,496 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:11:00,579 - 

2024-02-17 11:11:00,579 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:11:10,510 - Epoch: [27][  100/  391]    Overall Loss 1.701968    Objective Loss 1.701968                                        LR 0.100000    Time 0.099227    
2024-02-17 11:11:19,911 - Epoch: [27][  200/  391]    Overall Loss 1.725346    Objective Loss 1.725346                                        LR 0.100000    Time 0.096600    
2024-02-17 11:11:29,404 - Epoch: [27][  300/  391]    Overall Loss 1.735624    Objective Loss 1.735624                                        LR 0.100000    Time 0.096026    
2024-02-17 11:11:38,561 - Epoch: [27][  391/  391]    Overall Loss 1.742071    Objective Loss 1.742071    Top1 48.076923    Top5 81.250000    LR 0.100000    Time 0.097085    
2024-02-17 11:11:38,733 - --- validate (epoch=27)-----------
2024-02-17 11:11:38,735 - 10000 samples (128 per mini-batch)
2024-02-17 11:11:41,349 - Epoch: [27][   79/   79]    Loss 2.224197    Top1 41.210000    Top5 73.400000    
2024-02-17 11:11:41,542 - ==> Top1: 41.210    Top5: 73.400    Loss: 2.224

2024-02-17 11:11:41,561 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:11:41,561 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:11:41,636 - 

2024-02-17 11:11:41,637 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:11:51,762 - Epoch: [28][  100/  391]    Overall Loss 1.722163    Objective Loss 1.722163                                        LR 0.100000    Time 0.101172    
2024-02-17 11:12:01,006 - Epoch: [28][  200/  391]    Overall Loss 1.715269    Objective Loss 1.715269                                        LR 0.100000    Time 0.096786    
2024-02-17 11:12:08,724 - Epoch: [28][  300/  391]    Overall Loss 1.721193    Objective Loss 1.721193                                        LR 0.100000    Time 0.090236    
2024-02-17 11:12:15,991 - Epoch: [28][  391/  391]    Overall Loss 1.723180    Objective Loss 1.723180    Top1 50.961538    Top5 80.769231    LR 0.100000    Time 0.087811    
2024-02-17 11:12:16,120 - --- validate (epoch=28)-----------
2024-02-17 11:12:16,121 - 10000 samples (128 per mini-batch)
2024-02-17 11:12:18,756 - Epoch: [28][   79/   79]    Loss 2.306765    Top1 40.140000    Top5 72.850000    
2024-02-17 11:12:18,940 - ==> Top1: 40.140    Top5: 72.850    Loss: 2.307

2024-02-17 11:12:18,961 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:12:18,961 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:12:19,035 - 

2024-02-17 11:12:19,036 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:12:28,987 - Epoch: [29][  100/  391]    Overall Loss 1.684613    Objective Loss 1.684613                                        LR 0.100000    Time 0.099441    
2024-02-17 11:12:38,504 - Epoch: [29][  200/  391]    Overall Loss 1.698510    Objective Loss 1.698510                                        LR 0.100000    Time 0.097280    
2024-02-17 11:12:47,830 - Epoch: [29][  300/  391]    Overall Loss 1.710720    Objective Loss 1.710720                                        LR 0.100000    Time 0.095924    
2024-02-17 11:12:56,482 - Epoch: [29][  391/  391]    Overall Loss 1.714815    Objective Loss 1.714815    Top1 50.000000    Top5 83.173077    LR 0.100000    Time 0.095718    
2024-02-17 11:12:56,625 - --- validate (epoch=29)-----------
2024-02-17 11:12:56,626 - 10000 samples (128 per mini-batch)
2024-02-17 11:12:59,158 - Epoch: [29][   79/   79]    Loss 2.234177    Top1 42.070000    Top5 73.000000    
2024-02-17 11:12:59,293 - ==> Top1: 42.070    Top5: 73.000    Loss: 2.234

2024-02-17 11:12:59,312 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:12:59,312 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:12:59,402 - 

2024-02-17 11:12:59,403 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:13:09,458 - Epoch: [30][  100/  391]    Overall Loss 1.644164    Objective Loss 1.644164                                        LR 0.100000    Time 0.100473    
2024-02-17 11:13:19,137 - Epoch: [30][  200/  391]    Overall Loss 1.668109    Objective Loss 1.668109                                        LR 0.100000    Time 0.098606    
2024-02-17 11:13:27,533 - Epoch: [30][  300/  391]    Overall Loss 1.679802    Objective Loss 1.679802                                        LR 0.100000    Time 0.093712    
2024-02-17 11:13:34,945 - Epoch: [30][  391/  391]    Overall Loss 1.683561    Objective Loss 1.683561    Top1 51.923077    Top5 85.096154    LR 0.100000    Time 0.090848    
2024-02-17 11:13:35,105 - --- validate (epoch=30)-----------
2024-02-17 11:13:35,106 - 10000 samples (128 per mini-batch)
2024-02-17 11:13:37,628 - Epoch: [30][   79/   79]    Loss 2.379153    Top1 39.670000    Top5 71.900000    
2024-02-17 11:13:37,807 - ==> Top1: 39.670    Top5: 71.900    Loss: 2.379

2024-02-17 11:13:37,826 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:13:37,827 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:13:38,112 - 

2024-02-17 11:13:38,113 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:13:48,029 - Epoch: [31][  100/  391]    Overall Loss 1.661652    Objective Loss 1.661652                                        LR 0.100000    Time 0.099096    
2024-02-17 11:13:57,564 - Epoch: [31][  200/  391]    Overall Loss 1.659721    Objective Loss 1.659721                                        LR 0.100000    Time 0.097198    
2024-02-17 11:14:06,987 - Epoch: [31][  300/  391]    Overall Loss 1.663134    Objective Loss 1.663134                                        LR 0.100000    Time 0.096192    
2024-02-17 11:14:15,664 - Epoch: [31][  391/  391]    Overall Loss 1.668387    Objective Loss 1.668387    Top1 56.730769    Top5 82.692308    LR 0.100000    Time 0.095987    
2024-02-17 11:14:15,852 - --- validate (epoch=31)-----------
2024-02-17 11:14:15,853 - 10000 samples (128 per mini-batch)
2024-02-17 11:14:18,441 - Epoch: [31][   79/   79]    Loss 1.868021    Top1 48.600000    Top5 80.400000    
2024-02-17 11:14:18,578 - ==> Top1: 48.600    Top5: 80.400    Loss: 1.868

2024-02-17 11:14:18,597 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:14:18,597 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:14:18,695 - 

2024-02-17 11:14:18,696 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:14:28,920 - Epoch: [32][  100/  391]    Overall Loss 1.627926    Objective Loss 1.627926                                        LR 0.100000    Time 0.102172    
2024-02-17 11:14:38,324 - Epoch: [32][  200/  391]    Overall Loss 1.643109    Objective Loss 1.643109                                        LR 0.100000    Time 0.098080    
2024-02-17 11:14:47,670 - Epoch: [32][  300/  391]    Overall Loss 1.644081    Objective Loss 1.644081                                        LR 0.100000    Time 0.096523    
2024-02-17 11:14:56,340 - Epoch: [32][  391/  391]    Overall Loss 1.646557    Objective Loss 1.646557    Top1 48.076923    Top5 83.653846    LR 0.100000    Time 0.096224    
2024-02-17 11:14:56,510 - --- validate (epoch=32)-----------
2024-02-17 11:14:56,511 - 10000 samples (128 per mini-batch)
2024-02-17 11:14:59,108 - Epoch: [32][   79/   79]    Loss 1.976561    Top1 45.680000    Top5 78.180000    
2024-02-17 11:14:59,243 - ==> Top1: 45.680    Top5: 78.180    Loss: 1.977

2024-02-17 11:14:59,260 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:14:59,261 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:14:59,336 - 

2024-02-17 11:14:59,336 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:15:09,314 - Epoch: [33][  100/  391]    Overall Loss 1.595683    Objective Loss 1.595683                                        LR 0.100000    Time 0.099706    
2024-02-17 11:15:18,760 - Epoch: [33][  200/  391]    Overall Loss 1.606158    Objective Loss 1.606158                                        LR 0.100000    Time 0.097057    
2024-02-17 11:15:27,930 - Epoch: [33][  300/  391]    Overall Loss 1.621585    Objective Loss 1.621585                                        LR 0.100000    Time 0.095259    
2024-02-17 11:15:36,602 - Epoch: [33][  391/  391]    Overall Loss 1.629625    Objective Loss 1.629625    Top1 54.807692    Top5 82.211538    LR 0.100000    Time 0.095256    
2024-02-17 11:15:36,839 - --- validate (epoch=33)-----------
2024-02-17 11:15:36,840 - 10000 samples (128 per mini-batch)
2024-02-17 11:15:39,421 - Epoch: [33][   79/   79]    Loss 1.999365    Top1 46.460000    Top5 78.210000    
2024-02-17 11:15:39,558 - ==> Top1: 46.460    Top5: 78.210    Loss: 1.999

2024-02-17 11:15:39,568 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:15:39,569 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:15:39,644 - 

2024-02-17 11:15:39,644 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:15:49,904 - Epoch: [34][  100/  391]    Overall Loss 1.594835    Objective Loss 1.594835                                        LR 0.100000    Time 0.102524    
2024-02-17 11:15:59,543 - Epoch: [34][  200/  391]    Overall Loss 1.604233    Objective Loss 1.604233                                        LR 0.100000    Time 0.099433    
2024-02-17 11:16:08,374 - Epoch: [34][  300/  391]    Overall Loss 1.605438    Objective Loss 1.605438                                        LR 0.100000    Time 0.095713    
2024-02-17 11:16:16,686 - Epoch: [34][  391/  391]    Overall Loss 1.612352    Objective Loss 1.612352    Top1 54.807692    Top5 87.500000    LR 0.100000    Time 0.094686    
2024-02-17 11:16:16,843 - --- validate (epoch=34)-----------
2024-02-17 11:16:16,843 - 10000 samples (128 per mini-batch)
2024-02-17 11:16:19,372 - Epoch: [34][   79/   79]    Loss 2.018327    Top1 46.060000    Top5 77.830000    
2024-02-17 11:16:19,486 - ==> Top1: 46.060    Top5: 77.830    Loss: 2.018

2024-02-17 11:16:19,504 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:16:19,504 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:16:19,580 - 

2024-02-17 11:16:19,581 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:16:29,154 - Epoch: [35][  100/  391]    Overall Loss 1.575489    Objective Loss 1.575489                                        LR 0.100000    Time 0.095660    
2024-02-17 11:16:38,636 - Epoch: [35][  200/  391]    Overall Loss 1.586312    Objective Loss 1.586312                                        LR 0.100000    Time 0.095219    
2024-02-17 11:16:47,851 - Epoch: [35][  300/  391]    Overall Loss 1.602771    Objective Loss 1.602771                                        LR 0.100000    Time 0.094180    
2024-02-17 11:16:56,286 - Epoch: [35][  391/  391]    Overall Loss 1.606899    Objective Loss 1.606899    Top1 51.442308    Top5 86.057692    LR 0.100000    Time 0.093822    
2024-02-17 11:16:56,466 - --- validate (epoch=35)-----------
2024-02-17 11:16:56,467 - 10000 samples (128 per mini-batch)
2024-02-17 11:16:59,095 - Epoch: [35][   79/   79]    Loss 1.989054    Top1 45.770000    Top5 77.170000    
2024-02-17 11:16:59,229 - ==> Top1: 45.770    Top5: 77.170    Loss: 1.989

2024-02-17 11:16:59,247 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:16:59,248 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:16:59,324 - 

2024-02-17 11:16:59,324 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:17:09,690 - Epoch: [36][  100/  391]    Overall Loss 1.539836    Objective Loss 1.539836                                        LR 0.100000    Time 0.103580    
2024-02-17 11:17:19,203 - Epoch: [36][  200/  391]    Overall Loss 1.562597    Objective Loss 1.562597                                        LR 0.100000    Time 0.099332    
2024-02-17 11:17:28,605 - Epoch: [36][  300/  391]    Overall Loss 1.576128    Objective Loss 1.576128                                        LR 0.100000    Time 0.097546    
2024-02-17 11:17:36,001 - Epoch: [36][  391/  391]    Overall Loss 1.586168    Objective Loss 1.586168    Top1 52.403846    Top5 86.057692    LR 0.100000    Time 0.093747    
2024-02-17 11:17:36,138 - --- validate (epoch=36)-----------
2024-02-17 11:17:36,139 - 10000 samples (128 per mini-batch)
2024-02-17 11:17:38,711 - Epoch: [36][   79/   79]    Loss 1.807605    Top1 50.040000    Top5 81.320000    
2024-02-17 11:17:38,825 - ==> Top1: 50.040    Top5: 81.320    Loss: 1.808

2024-02-17 11:17:38,842 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:17:38,842 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:17:38,935 - 

2024-02-17 11:17:38,935 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:17:49,075 - Epoch: [37][  100/  391]    Overall Loss 1.568262    Objective Loss 1.568262                                        LR 0.100000    Time 0.101325    
2024-02-17 11:17:58,318 - Epoch: [37][  200/  391]    Overall Loss 1.577324    Objective Loss 1.577324                                        LR 0.100000    Time 0.096852    
2024-02-17 11:18:07,782 - Epoch: [37][  300/  391]    Overall Loss 1.573395    Objective Loss 1.573395                                        LR 0.100000    Time 0.096099    
2024-02-17 11:18:16,477 - Epoch: [37][  391/  391]    Overall Loss 1.579745    Objective Loss 1.579745    Top1 52.884615    Top5 84.134615    LR 0.100000    Time 0.095960    
2024-02-17 11:18:16,606 - --- validate (epoch=37)-----------
2024-02-17 11:18:16,606 - 10000 samples (128 per mini-batch)
2024-02-17 11:18:19,192 - Epoch: [37][   79/   79]    Loss 2.064832    Top1 45.690000    Top5 76.750000    
2024-02-17 11:18:19,323 - ==> Top1: 45.690    Top5: 76.750    Loss: 2.065

2024-02-17 11:18:19,342 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:18:19,342 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:18:19,416 - 

2024-02-17 11:18:19,417 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:18:29,276 - Epoch: [38][  100/  391]    Overall Loss 1.527588    Objective Loss 1.527588                                        LR 0.100000    Time 0.098518    
2024-02-17 11:18:38,448 - Epoch: [38][  200/  391]    Overall Loss 1.537939    Objective Loss 1.537939                                        LR 0.100000    Time 0.095094    
2024-02-17 11:18:47,734 - Epoch: [38][  300/  391]    Overall Loss 1.550885    Objective Loss 1.550885                                        LR 0.100000    Time 0.094338    
2024-02-17 11:18:56,551 - Epoch: [38][  391/  391]    Overall Loss 1.551893    Objective Loss 1.551893    Top1 51.923077    Top5 85.096154    LR 0.100000    Time 0.094920    
2024-02-17 11:18:56,685 - --- validate (epoch=38)-----------
2024-02-17 11:18:56,686 - 10000 samples (128 per mini-batch)
2024-02-17 11:18:59,541 - Epoch: [38][   79/   79]    Loss 1.979522    Top1 47.710000    Top5 78.950000    
2024-02-17 11:18:59,672 - ==> Top1: 47.710    Top5: 78.950    Loss: 1.980

2024-02-17 11:18:59,691 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:18:59,691 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:18:59,766 - 

2024-02-17 11:18:59,767 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:19:09,695 - Epoch: [39][  100/  391]    Overall Loss 1.531712    Objective Loss 1.531712                                        LR 0.100000    Time 0.099211    
2024-02-17 11:19:19,280 - Epoch: [39][  200/  391]    Overall Loss 1.542680    Objective Loss 1.542680                                        LR 0.100000    Time 0.097504    
2024-02-17 11:19:28,827 - Epoch: [39][  300/  391]    Overall Loss 1.548814    Objective Loss 1.548814                                        LR 0.100000    Time 0.096812    
2024-02-17 11:19:37,144 - Epoch: [39][  391/  391]    Overall Loss 1.550180    Objective Loss 1.550180    Top1 59.134615    Top5 87.019231    LR 0.100000    Time 0.095542    
2024-02-17 11:19:37,333 - --- validate (epoch=39)-----------
2024-02-17 11:19:37,334 - 10000 samples (128 per mini-batch)
2024-02-17 11:19:39,973 - Epoch: [39][   79/   79]    Loss 1.908074    Top1 47.730000    Top5 79.360000    
2024-02-17 11:19:40,097 - ==> Top1: 47.730    Top5: 79.360    Loss: 1.908

2024-02-17 11:19:40,119 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:19:40,119 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:19:40,198 - 

2024-02-17 11:19:40,199 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:19:50,513 - Epoch: [40][  100/  391]    Overall Loss 1.499845    Objective Loss 1.499845                                        LR 0.100000    Time 0.103077    
2024-02-17 11:20:00,295 - Epoch: [40][  200/  391]    Overall Loss 1.531605    Objective Loss 1.531605                                        LR 0.100000    Time 0.100423    
2024-02-17 11:20:09,749 - Epoch: [40][  300/  391]    Overall Loss 1.545265    Objective Loss 1.545265                                        LR 0.100000    Time 0.098446    
2024-02-17 11:20:18,501 - Epoch: [40][  391/  391]    Overall Loss 1.539392    Objective Loss 1.539392    Top1 58.173077    Top5 89.903846    LR 0.100000    Time 0.097906    
2024-02-17 11:20:18,721 - --- validate (epoch=40)-----------
2024-02-17 11:20:18,722 - 10000 samples (128 per mini-batch)
2024-02-17 11:20:21,408 - Epoch: [40][   79/   79]    Loss 1.845986    Top1 48.800000    Top5 80.410000    
2024-02-17 11:20:21,509 - ==> Top1: 48.800    Top5: 80.410    Loss: 1.846

2024-02-17 11:20:21,532 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:20:21,532 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:20:21,612 - 

2024-02-17 11:20:21,613 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:20:31,713 - Epoch: [41][  100/  391]    Overall Loss 1.485648    Objective Loss 1.485648                                        LR 0.100000    Time 0.100926    
2024-02-17 11:20:41,194 - Epoch: [41][  200/  391]    Overall Loss 1.491010    Objective Loss 1.491010                                        LR 0.100000    Time 0.097843    
2024-02-17 11:20:49,963 - Epoch: [41][  300/  391]    Overall Loss 1.509338    Objective Loss 1.509338                                        LR 0.100000    Time 0.094445    
2024-02-17 11:20:58,683 - Epoch: [41][  391/  391]    Overall Loss 1.520214    Objective Loss 1.520214    Top1 59.134615    Top5 88.942308    LR 0.100000    Time 0.094755    
2024-02-17 11:20:58,867 - --- validate (epoch=41)-----------
2024-02-17 11:20:58,868 - 10000 samples (128 per mini-batch)
2024-02-17 11:21:01,577 - Epoch: [41][   79/   79]    Loss 1.984574    Top1 46.930000    Top5 77.890000    
2024-02-17 11:21:01,717 - ==> Top1: 46.930    Top5: 77.890    Loss: 1.985

2024-02-17 11:21:01,735 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:21:01,735 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:21:01,817 - 

2024-02-17 11:21:01,818 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:21:12,067 - Epoch: [42][  100/  391]    Overall Loss 1.489325    Objective Loss 1.489325                                        LR 0.100000    Time 0.102409    
2024-02-17 11:21:21,661 - Epoch: [42][  200/  391]    Overall Loss 1.483331    Objective Loss 1.483331                                        LR 0.100000    Time 0.099150    
2024-02-17 11:21:30,774 - Epoch: [42][  300/  391]    Overall Loss 1.500461    Objective Loss 1.500461                                        LR 0.100000    Time 0.096460    
2024-02-17 11:21:39,460 - Epoch: [42][  391/  391]    Overall Loss 1.510962    Objective Loss 1.510962    Top1 56.250000    Top5 87.019231    LR 0.100000    Time 0.096214    
2024-02-17 11:21:39,636 - --- validate (epoch=42)-----------
2024-02-17 11:21:39,637 - 10000 samples (128 per mini-batch)
2024-02-17 11:21:42,364 - Epoch: [42][   79/   79]    Loss 1.952823    Top1 47.340000    Top5 78.560000    
2024-02-17 11:21:42,563 - ==> Top1: 47.340    Top5: 78.560    Loss: 1.953

2024-02-17 11:21:42,584 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:21:42,584 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:21:42,666 - 

2024-02-17 11:21:42,667 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:21:52,634 - Epoch: [43][  100/  391]    Overall Loss 1.457503    Objective Loss 1.457503                                        LR 0.100000    Time 0.099587    
2024-02-17 11:22:02,274 - Epoch: [43][  200/  391]    Overall Loss 1.475390    Objective Loss 1.475390                                        LR 0.100000    Time 0.097971    
2024-02-17 11:22:11,438 - Epoch: [43][  300/  391]    Overall Loss 1.492215    Objective Loss 1.492215                                        LR 0.100000    Time 0.095845    
2024-02-17 11:22:20,116 - Epoch: [43][  391/  391]    Overall Loss 1.502405    Objective Loss 1.502405    Top1 54.807692    Top5 82.211538    LR 0.100000    Time 0.095723    
2024-02-17 11:22:20,254 - --- validate (epoch=43)-----------
2024-02-17 11:22:20,255 - 10000 samples (128 per mini-batch)
2024-02-17 11:22:22,833 - Epoch: [43][   79/   79]    Loss 1.839245    Top1 48.860000    Top5 81.150000    
2024-02-17 11:22:22,993 - ==> Top1: 48.860    Top5: 81.150    Loss: 1.839

2024-02-17 11:22:23,013 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:22:23,014 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:22:23,089 - 

2024-02-17 11:22:23,090 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:22:33,356 - Epoch: [44][  100/  391]    Overall Loss 1.465794    Objective Loss 1.465794                                        LR 0.100000    Time 0.102589    
2024-02-17 11:22:42,857 - Epoch: [44][  200/  391]    Overall Loss 1.475357    Objective Loss 1.475357                                        LR 0.100000    Time 0.098777    
2024-02-17 11:22:52,023 - Epoch: [44][  300/  391]    Overall Loss 1.490347    Objective Loss 1.490347                                        LR 0.100000    Time 0.096389    
2024-02-17 11:23:00,715 - Epoch: [44][  391/  391]    Overall Loss 1.492887    Objective Loss 1.492887    Top1 54.807692    Top5 79.326923    LR 0.100000    Time 0.096175    
2024-02-17 11:23:00,880 - --- validate (epoch=44)-----------
2024-02-17 11:23:00,881 - 10000 samples (128 per mini-batch)
2024-02-17 11:23:03,538 - Epoch: [44][   79/   79]    Loss 1.917619    Top1 48.810000    Top5 79.840000    
2024-02-17 11:23:03,704 - ==> Top1: 48.810    Top5: 79.840    Loss: 1.918

2024-02-17 11:23:03,724 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:23:03,725 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:23:03,802 - 

2024-02-17 11:23:03,802 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:23:13,747 - Epoch: [45][  100/  391]    Overall Loss 1.433537    Objective Loss 1.433537                                        LR 0.100000    Time 0.099379    
2024-02-17 11:23:23,272 - Epoch: [45][  200/  391]    Overall Loss 1.449299    Objective Loss 1.449299                                        LR 0.100000    Time 0.097289    
2024-02-17 11:23:32,336 - Epoch: [45][  300/  391]    Overall Loss 1.465140    Objective Loss 1.465140                                        LR 0.100000    Time 0.095057    
2024-02-17 11:23:40,730 - Epoch: [45][  391/  391]    Overall Loss 1.475613    Objective Loss 1.475613    Top1 58.173077    Top5 87.019231    LR 0.100000    Time 0.094393    
2024-02-17 11:23:40,902 - --- validate (epoch=45)-----------
2024-02-17 11:23:40,903 - 10000 samples (128 per mini-batch)
2024-02-17 11:23:43,531 - Epoch: [45][   79/   79]    Loss 1.833996    Top1 49.530000    Top5 80.410000    
2024-02-17 11:23:43,671 - ==> Top1: 49.530    Top5: 80.410    Loss: 1.834

2024-02-17 11:23:43,689 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:23:43,689 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:23:43,767 - 

2024-02-17 11:23:43,767 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:23:51,543 - Epoch: [46][  100/  391]    Overall Loss 1.404987    Objective Loss 1.404987                                        LR 0.100000    Time 0.077698    
2024-02-17 11:23:59,120 - Epoch: [46][  200/  391]    Overall Loss 1.428167    Objective Loss 1.428167                                        LR 0.100000    Time 0.076714    
2024-02-17 11:24:06,383 - Epoch: [46][  300/  391]    Overall Loss 1.445878    Objective Loss 1.445878                                        LR 0.100000    Time 0.075339    
2024-02-17 11:24:14,722 - Epoch: [46][  391/  391]    Overall Loss 1.459151    Objective Loss 1.459151    Top1 62.019231    Top5 93.269231    LR 0.100000    Time 0.079122    
2024-02-17 11:24:14,908 - --- validate (epoch=46)-----------
2024-02-17 11:24:14,909 - 10000 samples (128 per mini-batch)
2024-02-17 11:24:17,825 - Epoch: [46][   79/   79]    Loss 1.885629    Top1 50.330000    Top5 80.910000    
2024-02-17 11:24:17,973 - ==> Top1: 50.330    Top5: 80.910    Loss: 1.886

2024-02-17 11:24:17,990 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:24:17,991 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:24:18,084 - 

2024-02-17 11:24:18,085 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:24:27,972 - Epoch: [47][  100/  391]    Overall Loss 1.427855    Objective Loss 1.427855                                        LR 0.100000    Time 0.098789    
2024-02-17 11:24:37,394 - Epoch: [47][  200/  391]    Overall Loss 1.431328    Objective Loss 1.431328                                        LR 0.100000    Time 0.096478    
2024-02-17 11:24:46,927 - Epoch: [47][  300/  391]    Overall Loss 1.449821    Objective Loss 1.449821                                        LR 0.100000    Time 0.096078    
2024-02-17 11:24:55,467 - Epoch: [47][  391/  391]    Overall Loss 1.456628    Objective Loss 1.456628    Top1 54.807692    Top5 90.384615    LR 0.100000    Time 0.095548    
2024-02-17 11:24:55,601 - --- validate (epoch=47)-----------
2024-02-17 11:24:55,602 - 10000 samples (128 per mini-batch)
2024-02-17 11:24:58,194 - Epoch: [47][   79/   79]    Loss 2.009425    Top1 47.200000    Top5 77.860000    
2024-02-17 11:24:58,325 - ==> Top1: 47.200    Top5: 77.860    Loss: 2.009

2024-02-17 11:24:58,345 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:24:58,346 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:24:58,436 - 

2024-02-17 11:24:58,437 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:25:08,363 - Epoch: [48][  100/  391]    Overall Loss 1.426738    Objective Loss 1.426738                                        LR 0.100000    Time 0.099177    
2024-02-17 11:25:18,030 - Epoch: [48][  200/  391]    Overall Loss 1.439196    Objective Loss 1.439196                                        LR 0.100000    Time 0.097896    
2024-02-17 11:25:27,042 - Epoch: [48][  300/  391]    Overall Loss 1.440461    Objective Loss 1.440461                                        LR 0.100000    Time 0.095292    
2024-02-17 11:25:35,606 - Epoch: [48][  391/  391]    Overall Loss 1.447062    Objective Loss 1.447062    Top1 54.807692    Top5 85.096154    LR 0.100000    Time 0.095003    
2024-02-17 11:25:35,759 - --- validate (epoch=48)-----------
2024-02-17 11:25:35,759 - 10000 samples (128 per mini-batch)
2024-02-17 11:25:38,715 - Epoch: [48][   79/   79]    Loss 1.940782    Top1 48.330000    Top5 79.390000    
2024-02-17 11:25:38,890 - ==> Top1: 48.330    Top5: 79.390    Loss: 1.941

2024-02-17 11:25:38,908 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:25:38,909 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:25:38,985 - 

2024-02-17 11:25:38,985 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:25:48,987 - Epoch: [49][  100/  391]    Overall Loss 1.422912    Objective Loss 1.422912                                        LR 0.100000    Time 0.099940    
2024-02-17 11:25:58,553 - Epoch: [49][  200/  391]    Overall Loss 1.418644    Objective Loss 1.418644                                        LR 0.100000    Time 0.097776    
2024-02-17 11:26:08,058 - Epoch: [49][  300/  391]    Overall Loss 1.426263    Objective Loss 1.426263                                        LR 0.100000    Time 0.096853    
2024-02-17 11:26:16,561 - Epoch: [49][  391/  391]    Overall Loss 1.433422    Objective Loss 1.433422    Top1 62.019231    Top5 87.019231    LR 0.100000    Time 0.096047    
2024-02-17 11:26:16,743 - --- validate (epoch=49)-----------
2024-02-17 11:26:16,745 - 10000 samples (128 per mini-batch)
2024-02-17 11:26:19,382 - Epoch: [49][   79/   79]    Loss 2.283702    Top1 42.440000    Top5 73.860000    
2024-02-17 11:26:19,524 - ==> Top1: 42.440    Top5: 73.860    Loss: 2.284

2024-02-17 11:26:19,542 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:26:19,543 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:26:19,622 - 

2024-02-17 11:26:19,622 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:26:30,048 - Epoch: [50][  100/  391]    Overall Loss 1.403427    Objective Loss 1.403427                                        LR 0.100000    Time 0.104172    
2024-02-17 11:26:39,572 - Epoch: [50][  200/  391]    Overall Loss 1.417837    Objective Loss 1.417837                                        LR 0.100000    Time 0.099683    
2024-02-17 11:26:49,012 - Epoch: [50][  300/  391]    Overall Loss 1.419865    Objective Loss 1.419865                                        LR 0.100000    Time 0.097907    
2024-02-17 11:26:57,810 - Epoch: [50][  391/  391]    Overall Loss 1.429393    Objective Loss 1.429393    Top1 57.211538    Top5 84.615385    LR 0.100000    Time 0.097609    
2024-02-17 11:26:57,917 - --- validate (epoch=50)-----------
2024-02-17 11:26:57,920 - 10000 samples (128 per mini-batch)
2024-02-17 11:27:00,770 - Epoch: [50][   79/   79]    Loss 1.889087    Top1 50.290000    Top5 79.830000    
2024-02-17 11:27:00,873 - ==> Top1: 50.290    Top5: 79.830    Loss: 1.889

2024-02-17 11:27:00,889 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:27:00,890 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:27:00,968 - 

2024-02-17 11:27:00,968 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:27:10,998 - Epoch: [51][  100/  391]    Overall Loss 1.368844    Objective Loss 1.368844                                        LR 0.100000    Time 0.100214    
2024-02-17 11:27:20,557 - Epoch: [51][  200/  391]    Overall Loss 1.400591    Objective Loss 1.400591                                        LR 0.100000    Time 0.097880    
2024-02-17 11:27:30,107 - Epoch: [51][  300/  391]    Overall Loss 1.412388    Objective Loss 1.412388                                        LR 0.100000    Time 0.097072    
2024-02-17 11:27:38,968 - Epoch: [51][  391/  391]    Overall Loss 1.425627    Objective Loss 1.425627    Top1 52.403846    Top5 84.615385    LR 0.100000    Time 0.097130    
2024-02-17 11:27:39,098 - --- validate (epoch=51)-----------
2024-02-17 11:27:39,099 - 10000 samples (128 per mini-batch)
2024-02-17 11:27:41,862 - Epoch: [51][   79/   79]    Loss 1.779124    Top1 51.110000    Top5 81.550000    
2024-02-17 11:27:41,980 - ==> Top1: 51.110    Top5: 81.550    Loss: 1.779

2024-02-17 11:27:42,000 - ==> Best [Top1: 51.110   Top5: 81.550   Sparsity:0.00   Params: 1341960 on epoch: 51]
2024-02-17 11:27:42,000 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:27:42,098 - 

2024-02-17 11:27:42,098 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:27:51,431 - Epoch: [52][  100/  391]    Overall Loss 1.356018    Objective Loss 1.356018                                        LR 0.100000    Time 0.093255    
2024-02-17 11:27:58,921 - Epoch: [52][  200/  391]    Overall Loss 1.368702    Objective Loss 1.368702                                        LR 0.100000    Time 0.084056    
2024-02-17 11:28:06,266 - Epoch: [52][  300/  391]    Overall Loss 1.390229    Objective Loss 1.390229                                        LR 0.100000    Time 0.080508    
2024-02-17 11:28:13,557 - Epoch: [52][  391/  391]    Overall Loss 1.396170    Objective Loss 1.396170    Top1 62.980769    Top5 86.538462    LR 0.100000    Time 0.080409    
2024-02-17 11:28:13,684 - --- validate (epoch=52)-----------
2024-02-17 11:28:13,685 - 10000 samples (128 per mini-batch)
2024-02-17 11:28:16,536 - Epoch: [52][   79/   79]    Loss 1.752732    Top1 51.550000    Top5 81.970000    
2024-02-17 11:28:16,651 - ==> Top1: 51.550    Top5: 81.970    Loss: 1.753

2024-02-17 11:28:16,669 - ==> Best [Top1: 51.550   Top5: 81.970   Sparsity:0.00   Params: 1341960 on epoch: 52]
2024-02-17 11:28:16,669 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:28:16,778 - 

2024-02-17 11:28:16,779 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:28:27,212 - Epoch: [53][  100/  391]    Overall Loss 1.352209    Objective Loss 1.352209                                        LR 0.100000    Time 0.104224    
2024-02-17 11:28:36,825 - Epoch: [53][  200/  391]    Overall Loss 1.385189    Objective Loss 1.385189                                        LR 0.100000    Time 0.100154    
2024-02-17 11:28:46,508 - Epoch: [53][  300/  391]    Overall Loss 1.395419    Objective Loss 1.395419                                        LR 0.100000    Time 0.099032    
2024-02-17 11:28:55,200 - Epoch: [53][  391/  391]    Overall Loss 1.400876    Objective Loss 1.400876    Top1 61.057692    Top5 88.461538    LR 0.100000    Time 0.098203    
2024-02-17 11:28:55,365 - --- validate (epoch=53)-----------
2024-02-17 11:28:55,366 - 10000 samples (128 per mini-batch)
2024-02-17 11:28:58,307 - Epoch: [53][   79/   79]    Loss 1.756776    Top1 52.330000    Top5 82.320000    
2024-02-17 11:28:58,484 - ==> Top1: 52.330    Top5: 82.320    Loss: 1.757

2024-02-17 11:28:58,509 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:28:58,510 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:28:58,624 - 

2024-02-17 11:28:58,625 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:29:08,947 - Epoch: [54][  100/  391]    Overall Loss 1.365076    Objective Loss 1.365076                                        LR 0.100000    Time 0.103147    
2024-02-17 11:29:18,405 - Epoch: [54][  200/  391]    Overall Loss 1.377045    Objective Loss 1.377045                                        LR 0.100000    Time 0.098837    
2024-02-17 11:29:27,781 - Epoch: [54][  300/  391]    Overall Loss 1.386387    Objective Loss 1.386387                                        LR 0.100000    Time 0.097129    
2024-02-17 11:29:36,720 - Epoch: [54][  391/  391]    Overall Loss 1.388969    Objective Loss 1.388969    Top1 61.057692    Top5 86.538462    LR 0.100000    Time 0.097374    
2024-02-17 11:29:36,869 - --- validate (epoch=54)-----------
2024-02-17 11:29:36,870 - 10000 samples (128 per mini-batch)
2024-02-17 11:29:39,794 - Epoch: [54][   79/   79]    Loss 1.958111    Top1 48.140000    Top5 78.360000    
2024-02-17 11:29:39,916 - ==> Top1: 48.140    Top5: 78.360    Loss: 1.958

2024-02-17 11:29:39,935 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:29:39,936 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:29:40,010 - 

2024-02-17 11:29:40,011 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:29:49,943 - Epoch: [55][  100/  391]    Overall Loss 1.354112    Objective Loss 1.354112                                        LR 0.100000    Time 0.099243    
2024-02-17 11:29:59,507 - Epoch: [55][  200/  391]    Overall Loss 1.363370    Objective Loss 1.363370                                        LR 0.100000    Time 0.097421    
2024-02-17 11:30:09,127 - Epoch: [55][  300/  391]    Overall Loss 1.373081    Objective Loss 1.373081                                        LR 0.100000    Time 0.096996    
2024-02-17 11:30:17,520 - Epoch: [55][  391/  391]    Overall Loss 1.380583    Objective Loss 1.380583    Top1 61.538462    Top5 89.903846    LR 0.100000    Time 0.095876    
2024-02-17 11:30:17,663 - --- validate (epoch=55)-----------
2024-02-17 11:30:17,664 - 10000 samples (128 per mini-batch)
2024-02-17 11:30:20,423 - Epoch: [55][   79/   79]    Loss 1.765769    Top1 51.630000    Top5 82.110000    
2024-02-17 11:30:20,541 - ==> Top1: 51.630    Top5: 82.110    Loss: 1.766

2024-02-17 11:30:20,563 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:30:20,563 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:30:20,644 - 

2024-02-17 11:30:20,645 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:30:30,706 - Epoch: [56][  100/  391]    Overall Loss 1.326227    Objective Loss 1.326227                                        LR 0.100000    Time 0.100533    
2024-02-17 11:30:40,290 - Epoch: [56][  200/  391]    Overall Loss 1.348147    Objective Loss 1.348147                                        LR 0.100000    Time 0.098167    
2024-02-17 11:30:49,762 - Epoch: [56][  300/  391]    Overall Loss 1.367657    Objective Loss 1.367657                                        LR 0.100000    Time 0.097002    
2024-02-17 11:30:58,121 - Epoch: [56][  391/  391]    Overall Loss 1.379064    Objective Loss 1.379064    Top1 57.692308    Top5 86.538462    LR 0.100000    Time 0.095794    
2024-02-17 11:30:58,302 - --- validate (epoch=56)-----------
2024-02-17 11:30:58,303 - 10000 samples (128 per mini-batch)
2024-02-17 11:31:01,057 - Epoch: [56][   79/   79]    Loss 1.785426    Top1 51.600000    Top5 81.980000    
2024-02-17 11:31:01,177 - ==> Top1: 51.600    Top5: 81.980    Loss: 1.785

2024-02-17 11:31:01,193 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:31:01,193 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:31:01,275 - 

2024-02-17 11:31:01,276 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:31:10,805 - Epoch: [57][  100/  391]    Overall Loss 1.349260    Objective Loss 1.349260                                        LR 0.100000    Time 0.095210    
2024-02-17 11:31:20,341 - Epoch: [57][  200/  391]    Overall Loss 1.363172    Objective Loss 1.363172                                        LR 0.100000    Time 0.095262    
2024-02-17 11:31:29,855 - Epoch: [57][  300/  391]    Overall Loss 1.365457    Objective Loss 1.365457                                        LR 0.100000    Time 0.095210    
2024-02-17 11:31:38,324 - Epoch: [57][  391/  391]    Overall Loss 1.366706    Objective Loss 1.366706    Top1 62.019231    Top5 88.942308    LR 0.100000    Time 0.094698    
2024-02-17 11:31:38,462 - --- validate (epoch=57)-----------
2024-02-17 11:31:38,463 - 10000 samples (128 per mini-batch)
2024-02-17 11:31:41,146 - Epoch: [57][   79/   79]    Loss 1.894377    Top1 49.460000    Top5 80.050000    
2024-02-17 11:31:41,319 - ==> Top1: 49.460    Top5: 80.050    Loss: 1.894

2024-02-17 11:31:41,339 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:31:41,339 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:31:41,419 - 

2024-02-17 11:31:41,420 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:31:51,241 - Epoch: [58][  100/  391]    Overall Loss 1.325014    Objective Loss 1.325014                                        LR 0.100000    Time 0.098140    
2024-02-17 11:32:00,842 - Epoch: [58][  200/  391]    Overall Loss 1.345946    Objective Loss 1.345946                                        LR 0.100000    Time 0.097051    
2024-02-17 11:32:10,394 - Epoch: [58][  300/  391]    Overall Loss 1.349931    Objective Loss 1.349931                                        LR 0.100000    Time 0.096523    
2024-02-17 11:32:18,991 - Epoch: [58][  391/  391]    Overall Loss 1.360621    Objective Loss 1.360621    Top1 62.980769    Top5 87.019231    LR 0.100000    Time 0.096037    
2024-02-17 11:32:19,173 - --- validate (epoch=58)-----------
2024-02-17 11:32:19,174 - 10000 samples (128 per mini-batch)
2024-02-17 11:32:22,060 - Epoch: [58][   79/   79]    Loss 1.667708    Top1 54.190000    Top5 83.330000    
2024-02-17 11:32:22,199 - ==> Top1: 54.190    Top5: 83.330    Loss: 1.668

2024-02-17 11:32:22,219 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:32:22,219 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:32:22,325 - 

2024-02-17 11:32:22,326 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:32:31,775 - Epoch: [59][  100/  391]    Overall Loss 1.308791    Objective Loss 1.308791                                        LR 0.100000    Time 0.094412    
2024-02-17 11:32:41,318 - Epoch: [59][  200/  391]    Overall Loss 1.326215    Objective Loss 1.326215                                        LR 0.100000    Time 0.094897    
2024-02-17 11:32:50,861 - Epoch: [59][  300/  391]    Overall Loss 1.341077    Objective Loss 1.341077                                        LR 0.100000    Time 0.095057    
2024-02-17 11:32:59,550 - Epoch: [59][  391/  391]    Overall Loss 1.348947    Objective Loss 1.348947    Top1 62.500000    Top5 88.942308    LR 0.100000    Time 0.095145    
2024-02-17 11:32:59,690 - --- validate (epoch=59)-----------
2024-02-17 11:32:59,691 - 10000 samples (128 per mini-batch)
2024-02-17 11:33:02,537 - Epoch: [59][   79/   79]    Loss 1.826966    Top1 50.750000    Top5 81.150000    
2024-02-17 11:33:02,659 - ==> Top1: 50.750    Top5: 81.150    Loss: 1.827

2024-02-17 11:33:02,679 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:33:02,679 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:33:02,753 - 

2024-02-17 11:33:02,754 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:33:12,937 - Epoch: [60][  100/  391]    Overall Loss 1.305665    Objective Loss 1.305665                                        LR 0.100000    Time 0.101758    
2024-02-17 11:33:22,571 - Epoch: [60][  200/  391]    Overall Loss 1.311440    Objective Loss 1.311440                                        LR 0.100000    Time 0.099024    
2024-02-17 11:33:32,097 - Epoch: [60][  300/  391]    Overall Loss 1.328394    Objective Loss 1.328394                                        LR 0.100000    Time 0.097755    
2024-02-17 11:33:40,921 - Epoch: [60][  391/  391]    Overall Loss 1.339107    Objective Loss 1.339107    Top1 63.942308    Top5 88.461538    LR 0.100000    Time 0.097560    
2024-02-17 11:33:41,073 - --- validate (epoch=60)-----------
2024-02-17 11:33:41,074 - 10000 samples (128 per mini-batch)
2024-02-17 11:33:43,846 - Epoch: [60][   79/   79]    Loss 1.691084    Top1 52.590000    Top5 83.400000    
2024-02-17 11:33:43,958 - ==> Top1: 52.590    Top5: 83.400    Loss: 1.691

2024-02-17 11:33:43,978 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:33:43,978 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:33:44,057 - 

2024-02-17 11:33:44,057 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:33:54,131 - Epoch: [61][  100/  391]    Overall Loss 1.296610    Objective Loss 1.296610                                        LR 0.100000    Time 0.100654    
2024-02-17 11:34:03,989 - Epoch: [61][  200/  391]    Overall Loss 1.326394    Objective Loss 1.326394                                        LR 0.100000    Time 0.099594    
2024-02-17 11:34:13,483 - Epoch: [61][  300/  391]    Overall Loss 1.333381    Objective Loss 1.333381                                        LR 0.100000    Time 0.098025    
2024-02-17 11:34:22,395 - Epoch: [61][  391/  391]    Overall Loss 1.339205    Objective Loss 1.339205    Top1 61.538462    Top5 86.538462    LR 0.100000    Time 0.097994    
2024-02-17 11:34:22,541 - --- validate (epoch=61)-----------
2024-02-17 11:34:22,542 - 10000 samples (128 per mini-batch)
2024-02-17 11:34:25,325 - Epoch: [61][   79/   79]    Loss 1.818900    Top1 50.560000    Top5 81.340000    
2024-02-17 11:34:25,518 - ==> Top1: 50.560    Top5: 81.340    Loss: 1.819

2024-02-17 11:34:25,538 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:34:25,539 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:34:25,616 - 

2024-02-17 11:34:25,617 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:34:36,021 - Epoch: [62][  100/  391]    Overall Loss 1.288146    Objective Loss 1.288146                                        LR 0.100000    Time 0.103960    
2024-02-17 11:34:45,543 - Epoch: [62][  200/  391]    Overall Loss 1.302987    Objective Loss 1.302987                                        LR 0.100000    Time 0.099566    
2024-02-17 11:34:54,553 - Epoch: [62][  300/  391]    Overall Loss 1.328770    Objective Loss 1.328770                                        LR 0.100000    Time 0.096394    
2024-02-17 11:35:02,034 - Epoch: [62][  391/  391]    Overall Loss 1.332459    Objective Loss 1.332459    Top1 58.173077    Top5 86.538462    LR 0.100000    Time 0.093084    
2024-02-17 11:35:02,186 - --- validate (epoch=62)-----------
2024-02-17 11:35:02,186 - 10000 samples (128 per mini-batch)
2024-02-17 11:35:05,052 - Epoch: [62][   79/   79]    Loss 1.716351    Top1 52.900000    Top5 82.810000    
2024-02-17 11:35:05,166 - ==> Top1: 52.900    Top5: 82.810    Loss: 1.716

2024-02-17 11:35:05,177 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:35:05,177 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:35:05,259 - 

2024-02-17 11:35:05,260 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:35:15,121 - Epoch: [63][  100/  391]    Overall Loss 1.285306    Objective Loss 1.285306                                        LR 0.100000    Time 0.098527    
2024-02-17 11:35:24,614 - Epoch: [63][  200/  391]    Overall Loss 1.292782    Objective Loss 1.292782                                        LR 0.100000    Time 0.096704    
2024-02-17 11:35:34,132 - Epoch: [63][  300/  391]    Overall Loss 1.312682    Objective Loss 1.312682                                        LR 0.100000    Time 0.096181    
2024-02-17 11:35:42,691 - Epoch: [63][  391/  391]    Overall Loss 1.317608    Objective Loss 1.317608    Top1 61.057692    Top5 87.980769    LR 0.100000    Time 0.095674    
2024-02-17 11:35:42,827 - --- validate (epoch=63)-----------
2024-02-17 11:35:42,828 - 10000 samples (128 per mini-batch)
2024-02-17 11:35:45,456 - Epoch: [63][   79/   79]    Loss 1.956995    Top1 48.160000    Top5 78.860000    
2024-02-17 11:35:45,634 - ==> Top1: 48.160    Top5: 78.860    Loss: 1.957

2024-02-17 11:35:45,644 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:35:45,644 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:35:45,726 - 

2024-02-17 11:35:45,726 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:35:56,137 - Epoch: [64][  100/  391]    Overall Loss 1.297899    Objective Loss 1.297899                                        LR 0.100000    Time 0.104034    
2024-02-17 11:36:05,749 - Epoch: [64][  200/  391]    Overall Loss 1.311936    Objective Loss 1.311936                                        LR 0.100000    Time 0.100053    
2024-02-17 11:36:15,329 - Epoch: [64][  300/  391]    Overall Loss 1.316976    Objective Loss 1.316976                                        LR 0.100000    Time 0.098619    
2024-02-17 11:36:24,071 - Epoch: [64][  391/  391]    Overall Loss 1.320537    Objective Loss 1.320537    Top1 63.461538    Top5 92.307692    LR 0.100000    Time 0.098013    
2024-02-17 11:36:24,203 - --- validate (epoch=64)-----------
2024-02-17 11:36:24,204 - 10000 samples (128 per mini-batch)
2024-02-17 11:36:27,138 - Epoch: [64][   79/   79]    Loss 1.750812    Top1 51.930000    Top5 82.150000    
2024-02-17 11:36:27,281 - ==> Top1: 51.930    Top5: 82.150    Loss: 1.751

2024-02-17 11:36:27,297 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:36:27,297 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:36:27,386 - 

2024-02-17 11:36:27,386 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:36:37,301 - Epoch: [65][  100/  391]    Overall Loss 1.261279    Objective Loss 1.261279                                        LR 0.100000    Time 0.099060    
2024-02-17 11:36:46,899 - Epoch: [65][  200/  391]    Overall Loss 1.285149    Objective Loss 1.285149                                        LR 0.100000    Time 0.097496    
2024-02-17 11:36:56,414 - Epoch: [65][  300/  391]    Overall Loss 1.301239    Objective Loss 1.301239                                        LR 0.100000    Time 0.096698    
2024-02-17 11:37:05,133 - Epoch: [65][  391/  391]    Overall Loss 1.310330    Objective Loss 1.310330    Top1 60.096154    Top5 87.019231    LR 0.100000    Time 0.096480    
2024-02-17 11:37:05,315 - --- validate (epoch=65)-----------
2024-02-17 11:37:05,316 - 10000 samples (128 per mini-batch)
2024-02-17 11:37:08,151 - Epoch: [65][   79/   79]    Loss 1.815695    Top1 51.080000    Top5 80.620000    
2024-02-17 11:37:08,321 - ==> Top1: 51.080    Top5: 80.620    Loss: 1.816

2024-02-17 11:37:08,337 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:37:08,337 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:37:08,425 - 

2024-02-17 11:37:08,425 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:37:18,639 - Epoch: [66][  100/  391]    Overall Loss 1.250810    Objective Loss 1.250810                                        LR 0.100000    Time 0.102065    
2024-02-17 11:37:28,147 - Epoch: [66][  200/  391]    Overall Loss 1.267830    Objective Loss 1.267830                                        LR 0.100000    Time 0.098547    
2024-02-17 11:37:37,677 - Epoch: [66][  300/  391]    Overall Loss 1.283463    Objective Loss 1.283463                                        LR 0.100000    Time 0.097447    
2024-02-17 11:37:46,285 - Epoch: [66][  391/  391]    Overall Loss 1.292746    Objective Loss 1.292746    Top1 49.519231    Top5 80.769231    LR 0.100000    Time 0.096773    
2024-02-17 11:37:46,436 - --- validate (epoch=66)-----------
2024-02-17 11:37:46,437 - 10000 samples (128 per mini-batch)
2024-02-17 11:37:49,431 - Epoch: [66][   79/   79]    Loss 1.934755    Top1 49.240000    Top5 80.590000    
2024-02-17 11:37:49,619 - ==> Top1: 49.240    Top5: 80.590    Loss: 1.935

2024-02-17 11:37:49,638 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:37:49,638 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:37:49,715 - 

2024-02-17 11:37:49,715 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:37:59,983 - Epoch: [67][  100/  391]    Overall Loss 1.269191    Objective Loss 1.269191                                        LR 0.100000    Time 0.102602    
2024-02-17 11:38:09,517 - Epoch: [67][  200/  391]    Overall Loss 1.276007    Objective Loss 1.276007                                        LR 0.100000    Time 0.098944    
2024-02-17 11:38:19,096 - Epoch: [67][  300/  391]    Overall Loss 1.285233    Objective Loss 1.285233                                        LR 0.100000    Time 0.097878    
2024-02-17 11:38:27,725 - Epoch: [67][  391/  391]    Overall Loss 1.298565    Objective Loss 1.298565    Top1 66.346154    Top5 89.423077    LR 0.100000    Time 0.097154    
2024-02-17 11:38:27,896 - --- validate (epoch=67)-----------
2024-02-17 11:38:27,897 - 10000 samples (128 per mini-batch)
2024-02-17 11:38:30,545 - Epoch: [67][   79/   79]    Loss 1.758080    Top1 52.600000    Top5 82.260000    
2024-02-17 11:38:30,732 - ==> Top1: 52.600    Top5: 82.260    Loss: 1.758

2024-02-17 11:38:30,751 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:38:30,751 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:38:30,828 - 

2024-02-17 11:38:30,829 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:38:40,825 - Epoch: [68][  100/  391]    Overall Loss 1.250863    Objective Loss 1.250863                                        LR 0.100000    Time 0.099877    
2024-02-17 11:38:50,363 - Epoch: [68][  200/  391]    Overall Loss 1.268339    Objective Loss 1.268339                                        LR 0.100000    Time 0.097608    
2024-02-17 11:38:59,906 - Epoch: [68][  300/  391]    Overall Loss 1.273696    Objective Loss 1.273696                                        LR 0.100000    Time 0.096866    
2024-02-17 11:39:08,172 - Epoch: [68][  391/  391]    Overall Loss 1.285888    Objective Loss 1.285888    Top1 61.538462    Top5 90.865385    LR 0.100000    Time 0.095452    
2024-02-17 11:39:08,362 - --- validate (epoch=68)-----------
2024-02-17 11:39:08,362 - 10000 samples (128 per mini-batch)
2024-02-17 11:39:10,969 - Epoch: [68][   79/   79]    Loss 1.862298    Top1 50.420000    Top5 80.730000    
2024-02-17 11:39:11,086 - ==> Top1: 50.420    Top5: 80.730    Loss: 1.862

2024-02-17 11:39:11,098 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:39:11,098 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:39:11,180 - 

2024-02-17 11:39:11,180 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:39:21,153 - Epoch: [69][  100/  391]    Overall Loss 1.257213    Objective Loss 1.257213                                        LR 0.100000    Time 0.099655    
2024-02-17 11:39:30,733 - Epoch: [69][  200/  391]    Overall Loss 1.259300    Objective Loss 1.259300                                        LR 0.100000    Time 0.097707    
2024-02-17 11:39:40,210 - Epoch: [69][  300/  391]    Overall Loss 1.275271    Objective Loss 1.275271                                        LR 0.100000    Time 0.096710    
2024-02-17 11:39:48,524 - Epoch: [69][  391/  391]    Overall Loss 1.284235    Objective Loss 1.284235    Top1 63.942308    Top5 87.980769    LR 0.100000    Time 0.095456    
2024-02-17 11:39:48,650 - --- validate (epoch=69)-----------
2024-02-17 11:39:48,652 - 10000 samples (128 per mini-batch)
2024-02-17 11:39:51,241 - Epoch: [69][   79/   79]    Loss 1.776082    Top1 51.820000    Top5 82.050000    
2024-02-17 11:39:51,369 - ==> Top1: 51.820    Top5: 82.050    Loss: 1.776

2024-02-17 11:39:51,384 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:39:51,384 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:39:51,458 - 

2024-02-17 11:39:51,459 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:40:01,762 - Epoch: [70][  100/  391]    Overall Loss 1.246878    Objective Loss 1.246878                                        LR 0.100000    Time 0.102956    
2024-02-17 11:40:11,246 - Epoch: [70][  200/  391]    Overall Loss 1.248874    Objective Loss 1.248874                                        LR 0.100000    Time 0.098876    
2024-02-17 11:40:20,732 - Epoch: [70][  300/  391]    Overall Loss 1.265239    Objective Loss 1.265239                                        LR 0.100000    Time 0.097522    
2024-02-17 11:40:29,624 - Epoch: [70][  391/  391]    Overall Loss 1.268964    Objective Loss 1.268964    Top1 63.461538    Top5 88.461538    LR 0.100000    Time 0.097555    
2024-02-17 11:40:29,803 - --- validate (epoch=70)-----------
2024-02-17 11:40:29,804 - 10000 samples (128 per mini-batch)
2024-02-17 11:40:32,516 - Epoch: [70][   79/   79]    Loss 1.651100    Top1 54.200000    Top5 84.270000    
2024-02-17 11:40:32,633 - ==> Top1: 54.200    Top5: 84.270    Loss: 1.651

2024-02-17 11:40:32,655 - ==> Best [Top1: 54.200   Top5: 84.270   Sparsity:0.00   Params: 1341960 on epoch: 70]
2024-02-17 11:40:32,656 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:40:32,753 - 

2024-02-17 11:40:32,754 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:40:42,648 - Epoch: [71][  100/  391]    Overall Loss 1.214183    Objective Loss 1.214183                                        LR 0.100000    Time 0.098867    
2024-02-17 11:40:52,226 - Epoch: [71][  200/  391]    Overall Loss 1.237423    Objective Loss 1.237423                                        LR 0.100000    Time 0.097302    
2024-02-17 11:41:01,690 - Epoch: [71][  300/  391]    Overall Loss 1.252054    Objective Loss 1.252054                                        LR 0.100000    Time 0.096399    
2024-02-17 11:41:10,699 - Epoch: [71][  391/  391]    Overall Loss 1.269942    Objective Loss 1.269942    Top1 62.500000    Top5 92.307692    LR 0.100000    Time 0.096992    
2024-02-17 11:41:10,829 - --- validate (epoch=71)-----------
2024-02-17 11:41:10,830 - 10000 samples (128 per mini-batch)
2024-02-17 11:41:13,591 - Epoch: [71][   79/   79]    Loss 1.664598    Top1 54.580000    Top5 83.680000    
2024-02-17 11:41:13,779 - ==> Top1: 54.580    Top5: 83.680    Loss: 1.665

2024-02-17 11:41:13,800 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:41:13,800 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:41:13,898 - 

2024-02-17 11:41:13,898 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:41:23,917 - Epoch: [72][  100/  391]    Overall Loss 1.228445    Objective Loss 1.228445                                        LR 0.100000    Time 0.100110    
2024-02-17 11:41:33,514 - Epoch: [72][  200/  391]    Overall Loss 1.231338    Objective Loss 1.231338                                        LR 0.100000    Time 0.098016    
2024-02-17 11:41:43,019 - Epoch: [72][  300/  391]    Overall Loss 1.256302    Objective Loss 1.256302                                        LR 0.100000    Time 0.097012    
2024-02-17 11:41:51,806 - Epoch: [72][  391/  391]    Overall Loss 1.258400    Objective Loss 1.258400    Top1 60.576923    Top5 88.461538    LR 0.100000    Time 0.096896    
2024-02-17 11:41:51,926 - --- validate (epoch=72)-----------
2024-02-17 11:41:51,927 - 10000 samples (128 per mini-batch)
2024-02-17 11:41:54,856 - Epoch: [72][   79/   79]    Loss 1.766127    Top1 52.280000    Top5 81.960000    
2024-02-17 11:41:54,970 - ==> Top1: 52.280    Top5: 81.960    Loss: 1.766

2024-02-17 11:41:54,990 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:41:54,990 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:41:55,068 - 

2024-02-17 11:41:55,068 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:42:04,820 - Epoch: [73][  100/  391]    Overall Loss 1.235244    Objective Loss 1.235244                                        LR 0.100000    Time 0.097434    
2024-02-17 11:42:14,397 - Epoch: [73][  200/  391]    Overall Loss 1.225271    Objective Loss 1.225271                                        LR 0.100000    Time 0.096582    
2024-02-17 11:42:23,641 - Epoch: [73][  300/  391]    Overall Loss 1.248749    Objective Loss 1.248749                                        LR 0.100000    Time 0.095186    
2024-02-17 11:42:32,731 - Epoch: [73][  391/  391]    Overall Loss 1.258203    Objective Loss 1.258203    Top1 62.019231    Top5 92.307692    LR 0.100000    Time 0.096268    
2024-02-17 11:42:32,850 - --- validate (epoch=73)-----------
2024-02-17 11:42:32,851 - 10000 samples (128 per mini-batch)
2024-02-17 11:42:35,641 - Epoch: [73][   79/   79]    Loss 1.792526    Top1 52.330000    Top5 81.370000    
2024-02-17 11:42:35,820 - ==> Top1: 52.330    Top5: 81.370    Loss: 1.793

2024-02-17 11:42:35,841 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:42:35,841 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:42:35,918 - 

2024-02-17 11:42:35,918 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:42:46,487 - Epoch: [74][  100/  391]    Overall Loss 1.213928    Objective Loss 1.213928                                        LR 0.100000    Time 0.105616    
2024-02-17 11:42:55,905 - Epoch: [74][  200/  391]    Overall Loss 1.237431    Objective Loss 1.237431                                        LR 0.100000    Time 0.099874    
2024-02-17 11:43:05,142 - Epoch: [74][  300/  391]    Overall Loss 1.250356    Objective Loss 1.250356                                        LR 0.100000    Time 0.097357    
2024-02-17 11:43:13,787 - Epoch: [74][  391/  391]    Overall Loss 1.257349    Objective Loss 1.257349    Top1 62.980769    Top5 90.865385    LR 0.100000    Time 0.096797    
2024-02-17 11:43:13,945 - --- validate (epoch=74)-----------
2024-02-17 11:43:13,946 - 10000 samples (128 per mini-batch)
2024-02-17 11:43:16,782 - Epoch: [74][   79/   79]    Loss 1.893712    Top1 50.480000    Top5 80.250000    
2024-02-17 11:43:16,894 - ==> Top1: 50.480    Top5: 80.250    Loss: 1.894

2024-02-17 11:43:16,912 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:43:16,912 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:43:16,994 - 

2024-02-17 11:43:16,994 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:43:25,894 - Epoch: [75][  100/  391]    Overall Loss 1.223472    Objective Loss 1.223472                                        LR 0.100000    Time 0.088929    
2024-02-17 11:43:34,785 - Epoch: [75][  200/  391]    Overall Loss 1.242622    Objective Loss 1.242622                                        LR 0.100000    Time 0.088896    
2024-02-17 11:43:44,106 - Epoch: [75][  300/  391]    Overall Loss 1.249421    Objective Loss 1.249421                                        LR 0.100000    Time 0.090321    
2024-02-17 11:43:52,622 - Epoch: [75][  391/  391]    Overall Loss 1.251632    Objective Loss 1.251632    Top1 62.019231    Top5 87.500000    LR 0.100000    Time 0.091068    
2024-02-17 11:43:52,769 - --- validate (epoch=75)-----------
2024-02-17 11:43:52,770 - 10000 samples (128 per mini-batch)
2024-02-17 11:43:55,507 - Epoch: [75][   79/   79]    Loss 1.653270    Top1 54.870000    Top5 83.890000    
2024-02-17 11:43:55,626 - ==> Top1: 54.870    Top5: 83.890    Loss: 1.653

2024-02-17 11:43:55,646 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:43:55,647 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:43:55,744 - 

2024-02-17 11:43:55,745 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:44:05,496 - Epoch: [76][  100/  391]    Overall Loss 1.170750    Objective Loss 1.170750                                        LR 0.100000    Time 0.097441    
2024-02-17 11:44:14,930 - Epoch: [76][  200/  391]    Overall Loss 1.213522    Objective Loss 1.213522                                        LR 0.100000    Time 0.095864    
2024-02-17 11:44:24,306 - Epoch: [76][  300/  391]    Overall Loss 1.231686    Objective Loss 1.231686                                        LR 0.100000    Time 0.095146    
2024-02-17 11:44:31,242 - Epoch: [76][  391/  391]    Overall Loss 1.240826    Objective Loss 1.240826    Top1 67.788462    Top5 90.865385    LR 0.100000    Time 0.090734    
2024-02-17 11:44:31,374 - --- validate (epoch=76)-----------
2024-02-17 11:44:31,376 - 10000 samples (128 per mini-batch)
2024-02-17 11:44:34,381 - Epoch: [76][   79/   79]    Loss 1.814758    Top1 51.530000    Top5 81.440000    
2024-02-17 11:44:34,546 - ==> Top1: 51.530    Top5: 81.440    Loss: 1.815

2024-02-17 11:44:34,558 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:44:34,559 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:44:34,640 - 

2024-02-17 11:44:34,641 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:44:44,538 - Epoch: [77][  100/  391]    Overall Loss 1.180127    Objective Loss 1.180127                                        LR 0.100000    Time 0.098900    
2024-02-17 11:44:54,032 - Epoch: [77][  200/  391]    Overall Loss 1.209763    Objective Loss 1.209763                                        LR 0.100000    Time 0.096893    
2024-02-17 11:45:03,562 - Epoch: [77][  300/  391]    Overall Loss 1.223902    Objective Loss 1.223902                                        LR 0.100000    Time 0.096349    
2024-02-17 11:45:12,186 - Epoch: [77][  391/  391]    Overall Loss 1.233894    Objective Loss 1.233894    Top1 61.538462    Top5 92.307692    LR 0.100000    Time 0.095970    
2024-02-17 11:45:12,362 - --- validate (epoch=77)-----------
2024-02-17 11:45:12,363 - 10000 samples (128 per mini-batch)
2024-02-17 11:45:15,251 - Epoch: [77][   79/   79]    Loss 1.657645    Top1 54.100000    Top5 83.660000    
2024-02-17 11:45:15,382 - ==> Top1: 54.100    Top5: 83.660    Loss: 1.658

2024-02-17 11:45:15,397 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:45:15,397 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:45:15,477 - 

2024-02-17 11:45:15,477 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:45:25,465 - Epoch: [78][  100/  391]    Overall Loss 1.201644    Objective Loss 1.201644                                        LR 0.100000    Time 0.099803    
2024-02-17 11:45:35,072 - Epoch: [78][  200/  391]    Overall Loss 1.203288    Objective Loss 1.203288                                        LR 0.100000    Time 0.097913    
2024-02-17 11:45:44,000 - Epoch: [78][  300/  391]    Overall Loss 1.218297    Objective Loss 1.218297                                        LR 0.100000    Time 0.095019    
2024-02-17 11:45:52,662 - Epoch: [78][  391/  391]    Overall Loss 1.222866    Objective Loss 1.222866    Top1 61.057692    Top5 92.307692    LR 0.100000    Time 0.095048    
2024-02-17 11:45:52,804 - --- validate (epoch=78)-----------
2024-02-17 11:45:52,804 - 10000 samples (128 per mini-batch)
2024-02-17 11:45:55,641 - Epoch: [78][   79/   79]    Loss 1.797887    Top1 52.410000    Top5 81.490000    
2024-02-17 11:45:55,776 - ==> Top1: 52.410    Top5: 81.490    Loss: 1.798

2024-02-17 11:45:55,794 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:45:55,794 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:45:55,874 - 

2024-02-17 11:45:55,875 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:46:05,685 - Epoch: [79][  100/  391]    Overall Loss 1.185781    Objective Loss 1.185781                                        LR 0.100000    Time 0.098011    
2024-02-17 11:46:15,206 - Epoch: [79][  200/  391]    Overall Loss 1.206895    Objective Loss 1.206895                                        LR 0.100000    Time 0.096589    
2024-02-17 11:46:24,652 - Epoch: [79][  300/  391]    Overall Loss 1.223145    Objective Loss 1.223145                                        LR 0.100000    Time 0.095865    
2024-02-17 11:46:33,484 - Epoch: [79][  391/  391]    Overall Loss 1.228430    Objective Loss 1.228430    Top1 60.576923    Top5 89.903846    LR 0.100000    Time 0.096129    
2024-02-17 11:46:33,646 - --- validate (epoch=79)-----------
2024-02-17 11:46:33,647 - 10000 samples (128 per mini-batch)
2024-02-17 11:46:36,427 - Epoch: [79][   79/   79]    Loss 1.761844    Top1 52.140000    Top5 82.710000    
2024-02-17 11:46:36,567 - ==> Top1: 52.140    Top5: 82.710    Loss: 1.762

2024-02-17 11:46:36,584 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:46:36,584 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:46:36,664 - 

2024-02-17 11:46:36,664 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:46:47,242 - Epoch: [80][  100/  391]    Overall Loss 1.183672    Objective Loss 1.183672                                        LR 0.100000    Time 0.105700    
2024-02-17 11:46:56,735 - Epoch: [80][  200/  391]    Overall Loss 1.188290    Objective Loss 1.188290                                        LR 0.100000    Time 0.100288    
2024-02-17 11:47:06,759 - Epoch: [80][  300/  391]    Overall Loss 1.207868    Objective Loss 1.207868                                        LR 0.100000    Time 0.100258    
2024-02-17 11:47:15,480 - Epoch: [80][  391/  391]    Overall Loss 1.216519    Objective Loss 1.216519    Top1 59.615385    Top5 87.019231    LR 0.100000    Time 0.099218    
2024-02-17 11:47:15,598 - --- validate (epoch=80)-----------
2024-02-17 11:47:15,599 - 10000 samples (128 per mini-batch)
2024-02-17 11:47:18,396 - Epoch: [80][   79/   79]    Loss 1.780596    Top1 51.910000    Top5 82.270000    
2024-02-17 11:47:18,513 - ==> Top1: 51.910    Top5: 82.270    Loss: 1.781

2024-02-17 11:47:18,533 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:47:18,533 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:47:18,614 - 

2024-02-17 11:47:18,614 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:47:28,657 - Epoch: [81][  100/  391]    Overall Loss 1.154308    Objective Loss 1.154308                                        LR 0.100000    Time 0.100359    
2024-02-17 11:47:38,087 - Epoch: [81][  200/  391]    Overall Loss 1.192567    Objective Loss 1.192567                                        LR 0.100000    Time 0.097303    
2024-02-17 11:47:47,784 - Epoch: [81][  300/  391]    Overall Loss 1.212697    Objective Loss 1.212697                                        LR 0.100000    Time 0.097178    
2024-02-17 11:47:56,496 - Epoch: [81][  391/  391]    Overall Loss 1.216576    Objective Loss 1.216576    Top1 65.865385    Top5 88.461538    LR 0.100000    Time 0.096831    
2024-02-17 11:47:56,668 - --- validate (epoch=81)-----------
2024-02-17 11:47:56,669 - 10000 samples (128 per mini-batch)
2024-02-17 11:47:59,442 - Epoch: [81][   79/   79]    Loss 1.901255    Top1 50.050000    Top5 80.380000    
2024-02-17 11:47:59,574 - ==> Top1: 50.050    Top5: 80.380    Loss: 1.901

2024-02-17 11:47:59,596 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:47:59,596 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:47:59,677 - 

2024-02-17 11:47:59,677 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:48:09,969 - Epoch: [82][  100/  391]    Overall Loss 1.181685    Objective Loss 1.181685                                        LR 0.100000    Time 0.102848    
2024-02-17 11:48:19,525 - Epoch: [82][  200/  391]    Overall Loss 1.191637    Objective Loss 1.191637                                        LR 0.100000    Time 0.099176    
2024-02-17 11:48:29,166 - Epoch: [82][  300/  391]    Overall Loss 1.195987    Objective Loss 1.195987                                        LR 0.100000    Time 0.098239    
2024-02-17 11:48:37,956 - Epoch: [82][  391/  391]    Overall Loss 1.205439    Objective Loss 1.205439    Top1 65.384615    Top5 88.461538    LR 0.100000    Time 0.097844    
2024-02-17 11:48:38,111 - --- validate (epoch=82)-----------
2024-02-17 11:48:38,112 - 10000 samples (128 per mini-batch)
2024-02-17 11:48:40,829 - Epoch: [82][   79/   79]    Loss 1.780336    Top1 52.640000    Top5 82.950000    
2024-02-17 11:48:40,974 - ==> Top1: 52.640    Top5: 82.950    Loss: 1.780

2024-02-17 11:48:40,996 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:48:40,996 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:48:41,076 - 

2024-02-17 11:48:41,076 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:48:51,280 - Epoch: [83][  100/  391]    Overall Loss 1.189221    Objective Loss 1.189221                                        LR 0.100000    Time 0.101965    
2024-02-17 11:49:00,762 - Epoch: [83][  200/  391]    Overall Loss 1.189462    Objective Loss 1.189462                                        LR 0.100000    Time 0.098366    
2024-02-17 11:49:10,673 - Epoch: [83][  300/  391]    Overall Loss 1.210404    Objective Loss 1.210404                                        LR 0.100000    Time 0.098597    
2024-02-17 11:49:19,267 - Epoch: [83][  391/  391]    Overall Loss 1.212851    Objective Loss 1.212851    Top1 72.596154    Top5 90.865385    LR 0.100000    Time 0.097620    
2024-02-17 11:49:19,397 - --- validate (epoch=83)-----------
2024-02-17 11:49:19,398 - 10000 samples (128 per mini-batch)
2024-02-17 11:49:22,129 - Epoch: [83][   79/   79]    Loss 1.912212    Top1 49.810000    Top5 80.350000    
2024-02-17 11:49:22,258 - ==> Top1: 49.810    Top5: 80.350    Loss: 1.912

2024-02-17 11:49:22,277 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:49:22,278 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:49:22,357 - 

2024-02-17 11:49:22,357 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:49:32,444 - Epoch: [84][  100/  391]    Overall Loss 1.144011    Objective Loss 1.144011                                        LR 0.100000    Time 0.100787    
2024-02-17 11:49:41,975 - Epoch: [84][  200/  391]    Overall Loss 1.156792    Objective Loss 1.156792                                        LR 0.100000    Time 0.098029    
2024-02-17 11:49:51,490 - Epoch: [84][  300/  391]    Overall Loss 1.189619    Objective Loss 1.189619                                        LR 0.100000    Time 0.097053    
2024-02-17 11:50:00,283 - Epoch: [84][  391/  391]    Overall Loss 1.205060    Objective Loss 1.205060    Top1 65.384615    Top5 91.346154    LR 0.100000    Time 0.096942    
2024-02-17 11:50:00,408 - --- validate (epoch=84)-----------
2024-02-17 11:50:00,409 - 10000 samples (128 per mini-batch)
2024-02-17 11:50:03,188 - Epoch: [84][   79/   79]    Loss 1.722928    Top1 53.200000    Top5 83.280000    
2024-02-17 11:50:03,323 - ==> Top1: 53.200    Top5: 83.280    Loss: 1.723

2024-02-17 11:50:03,341 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:50:03,342 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:50:03,418 - 

2024-02-17 11:50:03,418 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:50:13,640 - Epoch: [85][  100/  391]    Overall Loss 1.142040    Objective Loss 1.142040                                        LR 0.100000    Time 0.102147    
2024-02-17 11:50:21,754 - Epoch: [85][  200/  391]    Overall Loss 1.177139    Objective Loss 1.177139                                        LR 0.100000    Time 0.091625    
2024-02-17 11:50:29,302 - Epoch: [85][  300/  391]    Overall Loss 1.182821    Objective Loss 1.182821                                        LR 0.100000    Time 0.086228    
2024-02-17 11:50:37,747 - Epoch: [85][  391/  391]    Overall Loss 1.191620    Objective Loss 1.191620    Top1 67.788462    Top5 89.423077    LR 0.100000    Time 0.087749    
2024-02-17 11:50:37,894 - --- validate (epoch=85)-----------
2024-02-17 11:50:37,895 - 10000 samples (128 per mini-batch)
2024-02-17 11:50:40,487 - Epoch: [85][   79/   79]    Loss 1.687304    Top1 54.170000    Top5 83.350000    
2024-02-17 11:50:40,594 - ==> Top1: 54.170    Top5: 83.350    Loss: 1.687

2024-02-17 11:50:40,610 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:50:40,611 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:50:40,691 - 

2024-02-17 11:50:40,691 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:50:50,985 - Epoch: [86][  100/  391]    Overall Loss 1.130520    Objective Loss 1.130520                                        LR 0.100000    Time 0.102862    
2024-02-17 11:51:00,445 - Epoch: [86][  200/  391]    Overall Loss 1.160144    Objective Loss 1.160144                                        LR 0.100000    Time 0.098704    
2024-02-17 11:51:09,854 - Epoch: [86][  300/  391]    Overall Loss 1.178367    Objective Loss 1.178367                                        LR 0.100000    Time 0.097150    
2024-02-17 11:51:17,801 - Epoch: [86][  391/  391]    Overall Loss 1.190103    Objective Loss 1.190103    Top1 63.942308    Top5 87.980769    LR 0.100000    Time 0.094854    
2024-02-17 11:51:17,986 - --- validate (epoch=86)-----------
2024-02-17 11:51:17,987 - 10000 samples (128 per mini-batch)
2024-02-17 11:51:20,672 - Epoch: [86][   79/   79]    Loss 1.617547    Top1 55.120000    Top5 84.970000    
2024-02-17 11:51:20,805 - ==> Top1: 55.120    Top5: 84.970    Loss: 1.618

2024-02-17 11:51:20,823 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:51:20,823 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:51:20,916 - 

2024-02-17 11:51:20,916 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:51:30,790 - Epoch: [87][  100/  391]    Overall Loss 1.147955    Objective Loss 1.147955                                        LR 0.100000    Time 0.098670    
2024-02-17 11:51:40,322 - Epoch: [87][  200/  391]    Overall Loss 1.160226    Objective Loss 1.160226                                        LR 0.100000    Time 0.096971    
2024-02-17 11:51:49,775 - Epoch: [87][  300/  391]    Overall Loss 1.174703    Objective Loss 1.174703                                        LR 0.100000    Time 0.096142    
2024-02-17 11:51:58,188 - Epoch: [87][  391/  391]    Overall Loss 1.181049    Objective Loss 1.181049    Top1 60.096154    Top5 89.423077    LR 0.100000    Time 0.095271    
2024-02-17 11:51:58,353 - --- validate (epoch=87)-----------
2024-02-17 11:51:58,354 - 10000 samples (128 per mini-batch)
2024-02-17 11:52:00,949 - Epoch: [87][   79/   79]    Loss 1.951335    Top1 49.510000    Top5 80.040000    
2024-02-17 11:52:01,083 - ==> Top1: 49.510    Top5: 80.040    Loss: 1.951

2024-02-17 11:52:01,093 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:52:01,093 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:52:01,189 - 

2024-02-17 11:52:01,189 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:52:11,345 - Epoch: [88][  100/  391]    Overall Loss 1.139977    Objective Loss 1.139977                                        LR 0.100000    Time 0.101487    
2024-02-17 11:52:20,941 - Epoch: [88][  200/  391]    Overall Loss 1.153389    Objective Loss 1.153389                                        LR 0.100000    Time 0.098695    
2024-02-17 11:52:30,291 - Epoch: [88][  300/  391]    Overall Loss 1.167938    Objective Loss 1.167938                                        LR 0.100000    Time 0.096949    
2024-02-17 11:52:38,853 - Epoch: [88][  391/  391]    Overall Loss 1.181036    Objective Loss 1.181036    Top1 69.230769    Top5 93.269231    LR 0.100000    Time 0.096274    
2024-02-17 11:52:38,996 - --- validate (epoch=88)-----------
2024-02-17 11:52:38,997 - 10000 samples (128 per mini-batch)
2024-02-17 11:52:41,649 - Epoch: [88][   79/   79]    Loss 1.785244    Top1 52.210000    Top5 82.760000    
2024-02-17 11:52:41,750 - ==> Top1: 52.210    Top5: 82.760    Loss: 1.785

2024-02-17 11:52:41,771 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:52:41,771 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:52:41,853 - 

2024-02-17 11:52:41,853 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:52:51,804 - Epoch: [89][  100/  391]    Overall Loss 1.127520    Objective Loss 1.127520                                        LR 0.100000    Time 0.099393    
2024-02-17 11:53:01,320 - Epoch: [89][  200/  391]    Overall Loss 1.147720    Objective Loss 1.147720                                        LR 0.100000    Time 0.097253    
2024-02-17 11:53:10,866 - Epoch: [89][  300/  391]    Overall Loss 1.166428    Objective Loss 1.166428                                        LR 0.100000    Time 0.096640    
2024-02-17 11:53:19,329 - Epoch: [89][  391/  391]    Overall Loss 1.177647    Objective Loss 1.177647    Top1 65.384615    Top5 89.423077    LR 0.100000    Time 0.095782    
2024-02-17 11:53:19,479 - --- validate (epoch=89)-----------
2024-02-17 11:53:19,480 - 10000 samples (128 per mini-batch)
2024-02-17 11:53:22,095 - Epoch: [89][   79/   79]    Loss 1.973575    Top1 50.370000    Top5 78.920000    
2024-02-17 11:53:22,210 - ==> Top1: 50.370    Top5: 78.920    Loss: 1.974

2024-02-17 11:53:22,230 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:53:22,230 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:53:22,307 - 

2024-02-17 11:53:22,307 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:53:32,599 - Epoch: [90][  100/  391]    Overall Loss 1.121073    Objective Loss 1.121073                                        LR 0.100000    Time 0.102838    
2024-02-17 11:53:42,046 - Epoch: [90][  200/  391]    Overall Loss 1.145805    Objective Loss 1.145805                                        LR 0.100000    Time 0.098630    
2024-02-17 11:53:51,451 - Epoch: [90][  300/  391]    Overall Loss 1.156484    Objective Loss 1.156484                                        LR 0.100000    Time 0.097089    
2024-02-17 11:54:00,105 - Epoch: [90][  391/  391]    Overall Loss 1.164850    Objective Loss 1.164850    Top1 64.423077    Top5 89.423077    LR 0.100000    Time 0.096613    
2024-02-17 11:54:00,280 - --- validate (epoch=90)-----------
2024-02-17 11:54:00,280 - 10000 samples (128 per mini-batch)
2024-02-17 11:54:02,924 - Epoch: [90][   79/   79]    Loss 1.671971    Top1 54.660000    Top5 83.970000    
2024-02-17 11:54:03,045 - ==> Top1: 54.660    Top5: 83.970    Loss: 1.672

2024-02-17 11:54:03,064 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:54:03,064 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:54:03,139 - 

2024-02-17 11:54:03,139 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:54:13,046 - Epoch: [91][  100/  391]    Overall Loss 1.106952    Objective Loss 1.106952                                        LR 0.100000    Time 0.098987    
2024-02-17 11:54:22,482 - Epoch: [91][  200/  391]    Overall Loss 1.141084    Objective Loss 1.141084                                        LR 0.100000    Time 0.096650    
2024-02-17 11:54:31,747 - Epoch: [91][  300/  391]    Overall Loss 1.156907    Objective Loss 1.156907                                        LR 0.100000    Time 0.095304    
2024-02-17 11:54:40,363 - Epoch: [91][  391/  391]    Overall Loss 1.162098    Objective Loss 1.162098    Top1 61.538462    Top5 86.538462    LR 0.100000    Time 0.095146    
2024-02-17 11:54:40,501 - --- validate (epoch=91)-----------
2024-02-17 11:54:40,501 - 10000 samples (128 per mini-batch)
2024-02-17 11:54:43,156 - Epoch: [91][   79/   79]    Loss 1.832772    Top1 52.330000    Top5 81.380000    
2024-02-17 11:54:43,301 - ==> Top1: 52.330    Top5: 81.380    Loss: 1.833

2024-02-17 11:54:43,320 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:54:43,320 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:54:43,397 - 

2024-02-17 11:54:43,397 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:54:53,384 - Epoch: [92][  100/  391]    Overall Loss 1.128679    Objective Loss 1.128679                                        LR 0.100000    Time 0.099798    
2024-02-17 11:55:02,988 - Epoch: [92][  200/  391]    Overall Loss 1.136649    Objective Loss 1.136649                                        LR 0.100000    Time 0.097895    
2024-02-17 11:55:12,273 - Epoch: [92][  300/  391]    Overall Loss 1.152592    Objective Loss 1.152592                                        LR 0.100000    Time 0.096196    
2024-02-17 11:55:20,897 - Epoch: [92][  391/  391]    Overall Loss 1.165580    Objective Loss 1.165580    Top1 61.057692    Top5 88.461538    LR 0.100000    Time 0.095852    
2024-02-17 11:55:21,039 - --- validate (epoch=92)-----------
2024-02-17 11:55:21,040 - 10000 samples (128 per mini-batch)
2024-02-17 11:55:23,965 - Epoch: [92][   79/   79]    Loss 1.663866    Top1 55.390000    Top5 84.270000    
2024-02-17 11:55:24,078 - ==> Top1: 55.390    Top5: 84.270    Loss: 1.664

2024-02-17 11:55:24,092 - ==> Best [Top1: 55.390   Top5: 84.270   Sparsity:0.00   Params: 1341960 on epoch: 92]
2024-02-17 11:55:24,092 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:55:24,182 - 

2024-02-17 11:55:24,183 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:55:34,320 - Epoch: [93][  100/  391]    Overall Loss 1.132603    Objective Loss 1.132603                                        LR 0.100000    Time 0.101292    
2024-02-17 11:55:43,796 - Epoch: [93][  200/  391]    Overall Loss 1.157660    Objective Loss 1.157660                                        LR 0.100000    Time 0.098005    
2024-02-17 11:55:53,042 - Epoch: [93][  300/  391]    Overall Loss 1.163294    Objective Loss 1.163294                                        LR 0.100000    Time 0.096142    
2024-02-17 11:56:01,270 - Epoch: [93][  391/  391]    Overall Loss 1.168071    Objective Loss 1.168071    Top1 64.903846    Top5 92.307692    LR 0.100000    Time 0.094800    
2024-02-17 11:56:01,406 - --- validate (epoch=93)-----------
2024-02-17 11:56:01,407 - 10000 samples (128 per mini-batch)
2024-02-17 11:56:04,343 - Epoch: [93][   79/   79]    Loss 1.778660    Top1 53.460000    Top5 82.040000    
2024-02-17 11:56:04,495 - ==> Top1: 53.460    Top5: 82.040    Loss: 1.779

2024-02-17 11:56:04,510 - ==> Best [Top1: 55.390   Top5: 84.270   Sparsity:0.00   Params: 1341960 on epoch: 92]
2024-02-17 11:56:04,510 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:56:04,589 - 

2024-02-17 11:56:04,589 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:56:14,649 - Epoch: [94][  100/  391]    Overall Loss 1.124040    Objective Loss 1.124040                                        LR 0.100000    Time 0.100522    
2024-02-17 11:56:24,093 - Epoch: [94][  200/  391]    Overall Loss 1.130566    Objective Loss 1.130566                                        LR 0.100000    Time 0.097456    
2024-02-17 11:56:33,389 - Epoch: [94][  300/  391]    Overall Loss 1.136517    Objective Loss 1.136517                                        LR 0.100000    Time 0.095941    
2024-02-17 11:56:41,684 - Epoch: [94][  391/  391]    Overall Loss 1.153137    Objective Loss 1.153137    Top1 62.019231    Top5 90.865385    LR 0.100000    Time 0.094816    
2024-02-17 11:56:41,801 - --- validate (epoch=94)-----------
2024-02-17 11:56:41,801 - 10000 samples (128 per mini-batch)
2024-02-17 11:56:44,502 - Epoch: [94][   79/   79]    Loss 1.611510    Top1 55.400000    Top5 84.510000    
2024-02-17 11:56:44,636 - ==> Top1: 55.400    Top5: 84.510    Loss: 1.612

2024-02-17 11:56:44,655 - ==> Best [Top1: 55.400   Top5: 84.510   Sparsity:0.00   Params: 1341960 on epoch: 94]
2024-02-17 11:56:44,655 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:56:44,750 - 

2024-02-17 11:56:44,750 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:56:54,935 - Epoch: [95][  100/  391]    Overall Loss 1.102432    Objective Loss 1.102432                                        LR 0.100000    Time 0.101775    
2024-02-17 11:57:04,374 - Epoch: [95][  200/  391]    Overall Loss 1.122185    Objective Loss 1.122185                                        LR 0.100000    Time 0.098060    
2024-02-17 11:57:13,865 - Epoch: [95][  300/  391]    Overall Loss 1.136620    Objective Loss 1.136620                                        LR 0.100000    Time 0.096994    
2024-02-17 11:57:21,160 - Epoch: [95][  391/  391]    Overall Loss 1.147500    Objective Loss 1.147500    Top1 66.826923    Top5 92.307692    LR 0.100000    Time 0.093066    
2024-02-17 11:57:21,291 - --- validate (epoch=95)-----------
2024-02-17 11:57:21,292 - 10000 samples (128 per mini-batch)
2024-02-17 11:57:24,110 - Epoch: [95][   79/   79]    Loss 1.925001    Top1 49.310000    Top5 79.670000    
2024-02-17 11:57:24,254 - ==> Top1: 49.310    Top5: 79.670    Loss: 1.925

2024-02-17 11:57:24,274 - ==> Best [Top1: 55.400   Top5: 84.510   Sparsity:0.00   Params: 1341960 on epoch: 94]
2024-02-17 11:57:24,274 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:57:24,355 - 

2024-02-17 11:57:24,355 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:57:33,284 - Epoch: [96][  100/  391]    Overall Loss 1.119518    Objective Loss 1.119518                                        LR 0.100000    Time 0.089173    
2024-02-17 11:57:42,810 - Epoch: [96][  200/  391]    Overall Loss 1.122402    Objective Loss 1.122402                                        LR 0.100000    Time 0.092197    
2024-02-17 11:57:52,300 - Epoch: [96][  300/  391]    Overall Loss 1.134191    Objective Loss 1.134191                                        LR 0.100000    Time 0.093082    
2024-02-17 11:58:00,929 - Epoch: [96][  391/  391]    Overall Loss 1.143472    Objective Loss 1.143472    Top1 58.653846    Top5 87.980769    LR 0.100000    Time 0.093478    
2024-02-17 11:58:01,070 - --- validate (epoch=96)-----------
2024-02-17 11:58:01,071 - 10000 samples (128 per mini-batch)
2024-02-17 11:58:03,916 - Epoch: [96][   79/   79]    Loss 1.566625    Top1 57.290000    Top5 85.610000    
2024-02-17 11:58:04,062 - ==> Top1: 57.290    Top5: 85.610    Loss: 1.567

2024-02-17 11:58:04,081 - ==> Best [Top1: 57.290   Top5: 85.610   Sparsity:0.00   Params: 1341960 on epoch: 96]
2024-02-17 11:58:04,081 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:58:04,173 - 

2024-02-17 11:58:04,174 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:58:14,370 - Epoch: [97][  100/  391]    Overall Loss 1.097953    Objective Loss 1.097953                                        LR 0.100000    Time 0.101889    
2024-02-17 11:58:23,746 - Epoch: [97][  200/  391]    Overall Loss 1.116361    Objective Loss 1.116361                                        LR 0.100000    Time 0.097799    
2024-02-17 11:58:33,266 - Epoch: [97][  300/  391]    Overall Loss 1.132750    Objective Loss 1.132750                                        LR 0.100000    Time 0.096919    
2024-02-17 11:58:41,565 - Epoch: [97][  391/  391]    Overall Loss 1.143397    Objective Loss 1.143397    Top1 67.788462    Top5 91.826923    LR 0.100000    Time 0.095576    
2024-02-17 11:58:41,701 - --- validate (epoch=97)-----------
2024-02-17 11:58:41,703 - 10000 samples (128 per mini-batch)
2024-02-17 11:58:44,489 - Epoch: [97][   79/   79]    Loss 1.555645    Top1 57.460000    Top5 84.990000    
2024-02-17 11:58:44,670 - ==> Top1: 57.460    Top5: 84.990    Loss: 1.556

2024-02-17 11:58:44,690 - ==> Best [Top1: 57.460   Top5: 84.990   Sparsity:0.00   Params: 1341960 on epoch: 97]
2024-02-17 11:58:44,690 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:58:44,787 - 

2024-02-17 11:58:44,787 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:58:55,064 - Epoch: [98][  100/  391]    Overall Loss 1.112636    Objective Loss 1.112636                                        LR 0.100000    Time 0.102688    
2024-02-17 11:59:04,555 - Epoch: [98][  200/  391]    Overall Loss 1.116951    Objective Loss 1.116951                                        LR 0.100000    Time 0.098775    
2024-02-17 11:59:14,159 - Epoch: [98][  300/  391]    Overall Loss 1.128124    Objective Loss 1.128124                                        LR 0.100000    Time 0.097849    
2024-02-17 11:59:22,555 - Epoch: [98][  391/  391]    Overall Loss 1.137896    Objective Loss 1.137896    Top1 58.653846    Top5 89.903846    LR 0.100000    Time 0.096538    
2024-02-17 11:59:22,688 - --- validate (epoch=98)-----------
2024-02-17 11:59:22,689 - 10000 samples (128 per mini-batch)
2024-02-17 11:59:25,418 - Epoch: [98][   79/   79]    Loss 1.759463    Top1 53.370000    Top5 83.050000    
2024-02-17 11:59:25,525 - ==> Top1: 53.370    Top5: 83.050    Loss: 1.759

2024-02-17 11:59:25,545 - ==> Best [Top1: 57.460   Top5: 84.990   Sparsity:0.00   Params: 1341960 on epoch: 97]
2024-02-17 11:59:25,545 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 11:59:25,622 - 

2024-02-17 11:59:25,622 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:59:35,261 - Epoch: [99][  100/  391]    Overall Loss 1.093764    Objective Loss 1.093764                                        LR 0.100000    Time 0.096318    
2024-02-17 11:59:44,746 - Epoch: [99][  200/  391]    Overall Loss 1.125451    Objective Loss 1.125451                                        LR 0.100000    Time 0.095560    
2024-02-17 11:59:54,258 - Epoch: [99][  300/  391]    Overall Loss 1.129275    Objective Loss 1.129275                                        LR 0.100000    Time 0.095396    
2024-02-17 12:00:02,613 - Epoch: [99][  391/  391]    Overall Loss 1.134985    Objective Loss 1.134985    Top1 65.384615    Top5 91.346154    LR 0.100000    Time 0.094552    
2024-02-17 12:00:02,744 - --- validate (epoch=99)-----------
2024-02-17 12:00:02,745 - 10000 samples (128 per mini-batch)
2024-02-17 12:00:05,531 - Epoch: [99][   79/   79]    Loss 1.652775    Top1 55.010000    Top5 84.220000    
2024-02-17 12:00:05,648 - ==> Top1: 55.010    Top5: 84.220    Loss: 1.653

2024-02-17 12:00:05,659 - ==> Best [Top1: 57.460   Top5: 84.990   Sparsity:0.00   Params: 1341960 on epoch: 97]
2024-02-17 12:00:05,660 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:00:05,738 - 

2024-02-17 12:00:05,739 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:00:15,834 - Epoch: [100][  100/  391]    Overall Loss 0.903404    Objective Loss 0.903404                                        LR 0.023500    Time 0.100874    
2024-02-17 12:00:25,346 - Epoch: [100][  200/  391]    Overall Loss 0.855386    Objective Loss 0.855386                                        LR 0.023500    Time 0.097976    
2024-02-17 12:00:34,399 - Epoch: [100][  300/  391]    Overall Loss 0.837106    Objective Loss 0.837106                                        LR 0.023500    Time 0.095477    
2024-02-17 12:00:43,000 - Epoch: [100][  391/  391]    Overall Loss 0.827087    Objective Loss 0.827087    Top1 75.480769    Top5 95.673077    LR 0.023500    Time 0.095243    
2024-02-17 12:00:43,147 - --- validate (epoch=100)-----------
2024-02-17 12:00:43,147 - 10000 samples (128 per mini-batch)
2024-02-17 12:00:45,960 - Epoch: [100][   79/   79]    Loss 1.230817    Top1 65.550000    Top5 90.210000    
2024-02-17 12:00:46,069 - ==> Top1: 65.550    Top5: 90.210    Loss: 1.231

2024-02-17 12:00:46,079 - ==> Best [Top1: 65.550   Top5: 90.210   Sparsity:0.00   Params: 1341960 on epoch: 100]
2024-02-17 12:00:46,079 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:00:46,196 - 

2024-02-17 12:00:46,196 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:00:56,246 - Epoch: [101][  100/  391]    Overall Loss 0.744482    Objective Loss 0.744482                                        LR 0.023500    Time 0.100421    
2024-02-17 12:01:05,743 - Epoch: [101][  200/  391]    Overall Loss 0.752669    Objective Loss 0.752669                                        LR 0.023500    Time 0.097669    
2024-02-17 12:01:15,269 - Epoch: [101][  300/  391]    Overall Loss 0.749218    Objective Loss 0.749218                                        LR 0.023500    Time 0.096852    
2024-02-17 12:01:23,714 - Epoch: [101][  391/  391]    Overall Loss 0.749171    Objective Loss 0.749171    Top1 78.365385    Top5 96.153846    LR 0.023500    Time 0.095897    
2024-02-17 12:01:23,898 - --- validate (epoch=101)-----------
2024-02-17 12:01:23,899 - 10000 samples (128 per mini-batch)
2024-02-17 12:01:26,687 - Epoch: [101][   79/   79]    Loss 1.232268    Top1 65.080000    Top5 90.050000    
2024-02-17 12:01:26,839 - ==> Top1: 65.080    Top5: 90.050    Loss: 1.232

2024-02-17 12:01:26,858 - ==> Best [Top1: 65.550   Top5: 90.210   Sparsity:0.00   Params: 1341960 on epoch: 100]
2024-02-17 12:01:26,858 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:01:26,932 - 

2024-02-17 12:01:26,933 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:01:37,249 - Epoch: [102][  100/  391]    Overall Loss 0.727950    Objective Loss 0.727950                                        LR 0.023500    Time 0.103085    
2024-02-17 12:01:46,758 - Epoch: [102][  200/  391]    Overall Loss 0.724930    Objective Loss 0.724930                                        LR 0.023500    Time 0.099067    
2024-02-17 12:01:56,159 - Epoch: [102][  300/  391]    Overall Loss 0.721203    Objective Loss 0.721203                                        LR 0.023500    Time 0.097365    
2024-02-17 12:02:04,589 - Epoch: [102][  391/  391]    Overall Loss 0.721563    Objective Loss 0.721563    Top1 74.038462    Top5 95.192308    LR 0.023500    Time 0.096254    
2024-02-17 12:02:04,762 - --- validate (epoch=102)-----------
2024-02-17 12:02:04,763 - 10000 samples (128 per mini-batch)
2024-02-17 12:02:07,851 - Epoch: [102][   79/   79]    Loss 1.217137    Top1 65.640000    Top5 90.050000    
2024-02-17 12:02:08,046 - ==> Top1: 65.640    Top5: 90.050    Loss: 1.217

2024-02-17 12:02:08,064 - ==> Best [Top1: 65.640   Top5: 90.050   Sparsity:0.00   Params: 1341960 on epoch: 102]
2024-02-17 12:02:08,064 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:02:08,164 - 

2024-02-17 12:02:08,165 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:02:18,076 - Epoch: [103][  100/  391]    Overall Loss 0.680840    Objective Loss 0.680840                                        LR 0.023500    Time 0.099021    
2024-02-17 12:02:27,620 - Epoch: [103][  200/  391]    Overall Loss 0.682802    Objective Loss 0.682802                                        LR 0.023500    Time 0.097207    
2024-02-17 12:02:37,200 - Epoch: [103][  300/  391]    Overall Loss 0.691215    Objective Loss 0.691215                                        LR 0.023500    Time 0.096725    
2024-02-17 12:02:45,811 - Epoch: [103][  391/  391]    Overall Loss 0.697680    Objective Loss 0.697680    Top1 85.096154    Top5 95.673077    LR 0.023500    Time 0.096224    
2024-02-17 12:02:46,007 - --- validate (epoch=103)-----------
2024-02-17 12:02:46,008 - 10000 samples (128 per mini-batch)
2024-02-17 12:02:48,976 - Epoch: [103][   79/   79]    Loss 1.242191    Top1 65.060000    Top5 89.730000    
2024-02-17 12:02:49,085 - ==> Top1: 65.060    Top5: 89.730    Loss: 1.242

2024-02-17 12:02:49,100 - ==> Best [Top1: 65.640   Top5: 90.050   Sparsity:0.00   Params: 1341960 on epoch: 102]
2024-02-17 12:02:49,101 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:02:49,190 - 

2024-02-17 12:02:49,190 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:02:59,157 - Epoch: [104][  100/  391]    Overall Loss 0.668947    Objective Loss 0.668947                                        LR 0.023500    Time 0.099590    
2024-02-17 12:03:08,640 - Epoch: [104][  200/  391]    Overall Loss 0.680357    Objective Loss 0.680357                                        LR 0.023500    Time 0.097186    
2024-02-17 12:03:18,113 - Epoch: [104][  300/  391]    Overall Loss 0.679509    Objective Loss 0.679509                                        LR 0.023500    Time 0.096353    
2024-02-17 12:03:26,819 - Epoch: [104][  391/  391]    Overall Loss 0.687346    Objective Loss 0.687346    Top1 76.442308    Top5 95.192308    LR 0.023500    Time 0.096182    
2024-02-17 12:03:27,017 - --- validate (epoch=104)-----------
2024-02-17 12:03:27,018 - 10000 samples (128 per mini-batch)
2024-02-17 12:03:30,226 - Epoch: [104][   79/   79]    Loss 1.212613    Top1 66.040000    Top5 90.370000    
2024-02-17 12:03:30,351 - ==> Top1: 66.040    Top5: 90.370    Loss: 1.213

2024-02-17 12:03:30,376 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:03:30,377 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:03:30,483 - 

2024-02-17 12:03:30,484 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:03:40,453 - Epoch: [105][  100/  391]    Overall Loss 0.649788    Objective Loss 0.649788                                        LR 0.023500    Time 0.099599    
2024-02-17 12:03:49,957 - Epoch: [105][  200/  391]    Overall Loss 0.655619    Objective Loss 0.655619                                        LR 0.023500    Time 0.097298    
2024-02-17 12:03:59,396 - Epoch: [105][  300/  391]    Overall Loss 0.663308    Objective Loss 0.663308                                        LR 0.023500    Time 0.096312    
2024-02-17 12:04:08,088 - Epoch: [105][  391/  391]    Overall Loss 0.667088    Objective Loss 0.667088    Top1 77.884615    Top5 98.557692    LR 0.023500    Time 0.096115    
2024-02-17 12:04:08,215 - --- validate (epoch=105)-----------
2024-02-17 12:04:08,215 - 10000 samples (128 per mini-batch)
2024-02-17 12:04:11,075 - Epoch: [105][   79/   79]    Loss 1.222828    Top1 65.370000    Top5 90.160000    
2024-02-17 12:04:11,203 - ==> Top1: 65.370    Top5: 90.160    Loss: 1.223

2024-02-17 12:04:11,219 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:04:11,219 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:04:11,300 - 

2024-02-17 12:04:11,300 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:04:21,402 - Epoch: [106][  100/  391]    Overall Loss 0.647922    Objective Loss 0.647922                                        LR 0.023500    Time 0.100934    
2024-02-17 12:04:30,488 - Epoch: [106][  200/  391]    Overall Loss 0.656212    Objective Loss 0.656212                                        LR 0.023500    Time 0.095875    
2024-02-17 12:04:40,134 - Epoch: [106][  300/  391]    Overall Loss 0.650680    Objective Loss 0.650680                                        LR 0.023500    Time 0.096052    
2024-02-17 12:04:48,828 - Epoch: [106][  391/  391]    Overall Loss 0.652795    Objective Loss 0.652795    Top1 80.288462    Top5 96.634615    LR 0.023500    Time 0.095919    
2024-02-17 12:04:49,015 - --- validate (epoch=106)-----------
2024-02-17 12:04:49,016 - 10000 samples (128 per mini-batch)
2024-02-17 12:04:52,153 - Epoch: [106][   79/   79]    Loss 1.237146    Top1 65.390000    Top5 90.110000    
2024-02-17 12:04:52,275 - ==> Top1: 65.390    Top5: 90.110    Loss: 1.237

2024-02-17 12:04:52,288 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:04:52,288 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:04:52,370 - 

2024-02-17 12:04:52,370 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:05:02,567 - Epoch: [107][  100/  391]    Overall Loss 0.628644    Objective Loss 0.628644                                        LR 0.023500    Time 0.101889    
2024-02-17 12:05:11,939 - Epoch: [107][  200/  391]    Overall Loss 0.638219    Objective Loss 0.638219                                        LR 0.023500    Time 0.097780    
2024-02-17 12:05:21,558 - Epoch: [107][  300/  391]    Overall Loss 0.640107    Objective Loss 0.640107                                        LR 0.023500    Time 0.097231    
2024-02-17 12:05:30,195 - Epoch: [107][  391/  391]    Overall Loss 0.648960    Objective Loss 0.648960    Top1 79.326923    Top5 98.076923    LR 0.023500    Time 0.096682    
2024-02-17 12:05:30,332 - --- validate (epoch=107)-----------
2024-02-17 12:05:30,333 - 10000 samples (128 per mini-batch)
2024-02-17 12:05:33,199 - Epoch: [107][   79/   79]    Loss 1.260121    Top1 65.170000    Top5 89.580000    
2024-02-17 12:05:33,338 - ==> Top1: 65.170    Top5: 89.580    Loss: 1.260

2024-02-17 12:05:33,358 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:05:33,358 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:05:33,437 - 

2024-02-17 12:05:33,438 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:05:43,757 - Epoch: [108][  100/  391]    Overall Loss 0.618427    Objective Loss 0.618427                                        LR 0.023500    Time 0.103120    
2024-02-17 12:05:53,268 - Epoch: [108][  200/  391]    Overall Loss 0.626711    Objective Loss 0.626711                                        LR 0.023500    Time 0.099093    
2024-02-17 12:06:02,738 - Epoch: [108][  300/  391]    Overall Loss 0.635810    Objective Loss 0.635810                                        LR 0.023500    Time 0.097613    
2024-02-17 12:06:11,290 - Epoch: [108][  391/  391]    Overall Loss 0.640143    Objective Loss 0.640143    Top1 82.211538    Top5 99.519231    LR 0.023500    Time 0.096754    
2024-02-17 12:06:11,438 - --- validate (epoch=108)-----------
2024-02-17 12:06:11,439 - 10000 samples (128 per mini-batch)
2024-02-17 12:06:14,291 - Epoch: [108][   79/   79]    Loss 1.233425    Top1 65.550000    Top5 90.400000    
2024-02-17 12:06:14,407 - ==> Top1: 65.550    Top5: 90.400    Loss: 1.233

2024-02-17 12:06:14,423 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:06:14,423 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:06:14,503 - 

2024-02-17 12:06:14,504 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:06:24,656 - Epoch: [109][  100/  391]    Overall Loss 0.608839    Objective Loss 0.608839                                        LR 0.023500    Time 0.101418    
2024-02-17 12:06:34,145 - Epoch: [109][  200/  391]    Overall Loss 0.617831    Objective Loss 0.617831                                        LR 0.023500    Time 0.098129    
2024-02-17 12:06:42,928 - Epoch: [109][  300/  391]    Overall Loss 0.625404    Objective Loss 0.625404                                        LR 0.023500    Time 0.094685    
2024-02-17 12:06:51,568 - Epoch: [109][  391/  391]    Overall Loss 0.631449    Objective Loss 0.631449    Top1 78.365385    Top5 96.153846    LR 0.023500    Time 0.094734    
2024-02-17 12:06:51,749 - --- validate (epoch=109)-----------
2024-02-17 12:06:51,750 - 10000 samples (128 per mini-batch)
2024-02-17 12:06:54,484 - Epoch: [109][   79/   79]    Loss 1.257949    Top1 65.170000    Top5 89.880000    
2024-02-17 12:06:54,625 - ==> Top1: 65.170    Top5: 89.880    Loss: 1.258

2024-02-17 12:06:54,642 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:06:54,642 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:06:54,734 - 

2024-02-17 12:06:54,734 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:07:04,989 - Epoch: [110][  100/  391]    Overall Loss 0.593725    Objective Loss 0.593725                                        LR 0.023500    Time 0.102470    
2024-02-17 12:07:14,474 - Epoch: [110][  200/  391]    Overall Loss 0.605866    Objective Loss 0.605866                                        LR 0.023500    Time 0.098638    
2024-02-17 12:07:24,144 - Epoch: [110][  300/  391]    Overall Loss 0.614432    Objective Loss 0.614432                                        LR 0.023500    Time 0.097977    
2024-02-17 12:07:33,017 - Epoch: [110][  391/  391]    Overall Loss 0.621633    Objective Loss 0.621633    Top1 75.480769    Top5 95.192308    LR 0.023500    Time 0.097855    
2024-02-17 12:07:33,198 - --- validate (epoch=110)-----------
2024-02-17 12:07:33,199 - 10000 samples (128 per mini-batch)
2024-02-17 12:07:35,811 - Epoch: [110][   79/   79]    Loss 1.219577    Top1 66.110000    Top5 90.090000    
2024-02-17 12:07:35,930 - ==> Top1: 66.110    Top5: 90.090    Loss: 1.220

2024-02-17 12:07:35,942 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:07:35,942 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:07:36,042 - 

2024-02-17 12:07:36,042 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:07:45,852 - Epoch: [111][  100/  391]    Overall Loss 0.599360    Objective Loss 0.599360                                        LR 0.023500    Time 0.098018    
2024-02-17 12:07:55,349 - Epoch: [111][  200/  391]    Overall Loss 0.600780    Objective Loss 0.600780                                        LR 0.023500    Time 0.096473    
2024-02-17 12:08:04,705 - Epoch: [111][  300/  391]    Overall Loss 0.612345    Objective Loss 0.612345                                        LR 0.023500    Time 0.095485    
2024-02-17 12:08:13,584 - Epoch: [111][  391/  391]    Overall Loss 0.621217    Objective Loss 0.621217    Top1 82.211538    Top5 97.115385    LR 0.023500    Time 0.095959    
2024-02-17 12:08:13,744 - --- validate (epoch=111)-----------
2024-02-17 12:08:13,745 - 10000 samples (128 per mini-batch)
2024-02-17 12:08:16,321 - Epoch: [111][   79/   79]    Loss 1.307443    Top1 64.350000    Top5 89.100000    
2024-02-17 12:08:16,450 - ==> Top1: 64.350    Top5: 89.100    Loss: 1.307

2024-02-17 12:08:16,469 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:08:16,469 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:08:16,547 - 

2024-02-17 12:08:16,547 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:08:26,692 - Epoch: [112][  100/  391]    Overall Loss 0.589680    Objective Loss 0.589680                                        LR 0.023500    Time 0.101375    
2024-02-17 12:08:36,214 - Epoch: [112][  200/  391]    Overall Loss 0.594518    Objective Loss 0.594518                                        LR 0.023500    Time 0.098269    
2024-02-17 12:08:45,525 - Epoch: [112][  300/  391]    Overall Loss 0.599510    Objective Loss 0.599510                                        LR 0.023500    Time 0.096534    
2024-02-17 12:08:53,824 - Epoch: [112][  391/  391]    Overall Loss 0.609625    Objective Loss 0.609625    Top1 76.442308    Top5 96.153846    LR 0.023500    Time 0.095283    
2024-02-17 12:08:53,974 - --- validate (epoch=112)-----------
2024-02-17 12:08:53,975 - 10000 samples (128 per mini-batch)
2024-02-17 12:08:56,476 - Epoch: [112][   79/   79]    Loss 1.298559    Top1 64.930000    Top5 89.460000    
2024-02-17 12:08:56,653 - ==> Top1: 64.930    Top5: 89.460    Loss: 1.299

2024-02-17 12:08:56,672 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:08:56,672 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:08:56,748 - 

2024-02-17 12:08:56,748 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:09:06,478 - Epoch: [113][  100/  391]    Overall Loss 0.592065    Objective Loss 0.592065                                        LR 0.023500    Time 0.097221    
2024-02-17 12:09:15,989 - Epoch: [113][  200/  391]    Overall Loss 0.593936    Objective Loss 0.593936                                        LR 0.023500    Time 0.096142    
2024-02-17 12:09:25,257 - Epoch: [113][  300/  391]    Overall Loss 0.600268    Objective Loss 0.600268                                        LR 0.023500    Time 0.094973    
2024-02-17 12:09:34,012 - Epoch: [113][  391/  391]    Overall Loss 0.606185    Objective Loss 0.606185    Top1 76.442308    Top5 98.076923    LR 0.023500    Time 0.095251    
2024-02-17 12:09:34,197 - --- validate (epoch=113)-----------
2024-02-17 12:09:34,198 - 10000 samples (128 per mini-batch)
2024-02-17 12:09:36,805 - Epoch: [113][   79/   79]    Loss 1.317711    Top1 64.190000    Top5 89.450000    
2024-02-17 12:09:36,950 - ==> Top1: 64.190    Top5: 89.450    Loss: 1.318

2024-02-17 12:09:36,968 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:09:36,968 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:09:37,043 - 

2024-02-17 12:09:37,043 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:09:46,795 - Epoch: [114][  100/  391]    Overall Loss 0.574343    Objective Loss 0.574343                                        LR 0.023500    Time 0.097444    
2024-02-17 12:09:56,171 - Epoch: [114][  200/  391]    Overall Loss 0.586967    Objective Loss 0.586967                                        LR 0.023500    Time 0.095578    
2024-02-17 12:10:05,355 - Epoch: [114][  300/  391]    Overall Loss 0.596132    Objective Loss 0.596132                                        LR 0.023500    Time 0.094319    
2024-02-17 12:10:14,032 - Epoch: [114][  391/  391]    Overall Loss 0.605783    Objective Loss 0.605783    Top1 80.769231    Top5 96.153846    LR 0.023500    Time 0.094547    
2024-02-17 12:10:14,178 - --- validate (epoch=114)-----------
2024-02-17 12:10:14,178 - 10000 samples (128 per mini-batch)
2024-02-17 12:10:16,955 - Epoch: [114][   79/   79]    Loss 1.309465    Top1 64.210000    Top5 88.980000    
2024-02-17 12:10:17,154 - ==> Top1: 64.210    Top5: 88.980    Loss: 1.309

2024-02-17 12:10:17,174 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:10:17,175 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:10:17,250 - 

2024-02-17 12:10:17,250 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:10:26,364 - Epoch: [115][  100/  391]    Overall Loss 0.574954    Objective Loss 0.574954                                        LR 0.023500    Time 0.091063    
2024-02-17 12:10:34,137 - Epoch: [115][  200/  391]    Overall Loss 0.579106    Objective Loss 0.579106                                        LR 0.023500    Time 0.084379    
2024-02-17 12:10:43,347 - Epoch: [115][  300/  391]    Overall Loss 0.592322    Objective Loss 0.592322                                        LR 0.023500    Time 0.086939    
2024-02-17 12:10:52,000 - Epoch: [115][  391/  391]    Overall Loss 0.601133    Objective Loss 0.601133    Top1 81.730769    Top5 96.153846    LR 0.023500    Time 0.088824    
2024-02-17 12:10:52,141 - --- validate (epoch=115)-----------
2024-02-17 12:10:52,142 - 10000 samples (128 per mini-batch)
2024-02-17 12:10:54,708 - Epoch: [115][   79/   79]    Loss 1.320970    Top1 64.180000    Top5 88.950000    
2024-02-17 12:10:54,822 - ==> Top1: 64.180    Top5: 88.950    Loss: 1.321

2024-02-17 12:10:54,842 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:10:54,843 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:10:54,919 - 

2024-02-17 12:10:54,919 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:11:04,601 - Epoch: [116][  100/  391]    Overall Loss 0.590266    Objective Loss 0.590266                                        LR 0.023500    Time 0.096743    
2024-02-17 12:11:13,986 - Epoch: [116][  200/  391]    Overall Loss 0.591161    Objective Loss 0.591161                                        LR 0.023500    Time 0.095274    
2024-02-17 12:11:23,770 - Epoch: [116][  300/  391]    Overall Loss 0.597120    Objective Loss 0.597120                                        LR 0.023500    Time 0.096112    
2024-02-17 12:11:32,457 - Epoch: [116][  391/  391]    Overall Loss 0.602517    Objective Loss 0.602517    Top1 77.403846    Top5 96.634615    LR 0.023500    Time 0.095951    
2024-02-17 12:11:32,600 - --- validate (epoch=116)-----------
2024-02-17 12:11:32,601 - 10000 samples (128 per mini-batch)
2024-02-17 12:11:35,473 - Epoch: [116][   79/   79]    Loss 1.299381    Top1 65.010000    Top5 89.490000    
2024-02-17 12:11:35,645 - ==> Top1: 65.010    Top5: 89.490    Loss: 1.299

2024-02-17 12:11:35,664 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:11:35,665 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:11:35,740 - 

2024-02-17 12:11:35,740 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:11:45,391 - Epoch: [117][  100/  391]    Overall Loss 0.571592    Objective Loss 0.571592                                        LR 0.023500    Time 0.096438    
2024-02-17 12:11:54,603 - Epoch: [117][  200/  391]    Overall Loss 0.580793    Objective Loss 0.580793                                        LR 0.023500    Time 0.094240    
2024-02-17 12:12:03,100 - Epoch: [117][  300/  391]    Overall Loss 0.587905    Objective Loss 0.587905                                        LR 0.023500    Time 0.091137    
2024-02-17 12:12:11,814 - Epoch: [117][  391/  391]    Overall Loss 0.594131    Objective Loss 0.594131    Top1 77.403846    Top5 95.673077    LR 0.023500    Time 0.092202    
2024-02-17 12:12:12,000 - --- validate (epoch=117)-----------
2024-02-17 12:12:12,001 - 10000 samples (128 per mini-batch)
2024-02-17 12:12:14,643 - Epoch: [117][   79/   79]    Loss 1.343025    Top1 64.040000    Top5 88.790000    
2024-02-17 12:12:14,823 - ==> Top1: 64.040    Top5: 88.790    Loss: 1.343

2024-02-17 12:12:14,841 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:12:14,842 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:12:14,918 - 

2024-02-17 12:12:14,918 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:12:24,948 - Epoch: [118][  100/  391]    Overall Loss 0.565083    Objective Loss 0.565083                                        LR 0.023500    Time 0.100211    
2024-02-17 12:12:34,212 - Epoch: [118][  200/  391]    Overall Loss 0.568931    Objective Loss 0.568931                                        LR 0.023500    Time 0.096399    
2024-02-17 12:12:43,511 - Epoch: [118][  300/  391]    Overall Loss 0.579081    Objective Loss 0.579081                                        LR 0.023500    Time 0.095249    
2024-02-17 12:12:52,178 - Epoch: [118][  391/  391]    Overall Loss 0.589991    Objective Loss 0.589991    Top1 79.326923    Top5 97.596154    LR 0.023500    Time 0.095235    
2024-02-17 12:12:52,358 - --- validate (epoch=118)-----------
2024-02-17 12:12:52,359 - 10000 samples (128 per mini-batch)
2024-02-17 12:12:55,030 - Epoch: [118][   79/   79]    Loss 1.285393    Top1 65.080000    Top5 89.680000    
2024-02-17 12:12:55,150 - ==> Top1: 65.080    Top5: 89.680    Loss: 1.285

2024-02-17 12:12:55,170 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:12:55,170 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:12:55,247 - 

2024-02-17 12:12:55,248 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:13:05,013 - Epoch: [119][  100/  391]    Overall Loss 0.555229    Objective Loss 0.555229                                        LR 0.023500    Time 0.097576    
2024-02-17 12:13:14,369 - Epoch: [119][  200/  391]    Overall Loss 0.563886    Objective Loss 0.563886                                        LR 0.023500    Time 0.095544    
2024-02-17 12:13:23,564 - Epoch: [119][  300/  391]    Overall Loss 0.574306    Objective Loss 0.574306                                        LR 0.023500    Time 0.094332    
2024-02-17 12:13:32,201 - Epoch: [119][  391/  391]    Overall Loss 0.585766    Objective Loss 0.585766    Top1 77.403846    Top5 98.076923    LR 0.023500    Time 0.094457    
2024-02-17 12:13:32,338 - --- validate (epoch=119)-----------
2024-02-17 12:13:32,339 - 10000 samples (128 per mini-batch)
2024-02-17 12:13:34,899 - Epoch: [119][   79/   79]    Loss 1.281214    Top1 65.070000    Top5 89.780000    
2024-02-17 12:13:35,085 - ==> Top1: 65.070    Top5: 89.780    Loss: 1.281

2024-02-17 12:13:35,102 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:13:35,103 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:13:35,176 - 

2024-02-17 12:13:35,176 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:13:45,092 - Epoch: [120][  100/  391]    Overall Loss 0.544596    Objective Loss 0.544596                                        LR 0.023500    Time 0.099088    
2024-02-17 12:13:54,489 - Epoch: [120][  200/  391]    Overall Loss 0.561226    Objective Loss 0.561226                                        LR 0.023500    Time 0.096506    
2024-02-17 12:14:03,687 - Epoch: [120][  300/  391]    Overall Loss 0.570281    Objective Loss 0.570281                                        LR 0.023500    Time 0.094982    
2024-02-17 12:14:12,346 - Epoch: [120][  391/  391]    Overall Loss 0.578666    Objective Loss 0.578666    Top1 80.769231    Top5 98.557692    LR 0.023500    Time 0.095009    
2024-02-17 12:14:12,484 - --- validate (epoch=120)-----------
2024-02-17 12:14:12,484 - 10000 samples (128 per mini-batch)
2024-02-17 12:14:15,130 - Epoch: [120][   79/   79]    Loss 1.283224    Top1 64.780000    Top5 89.990000    
2024-02-17 12:14:15,244 - ==> Top1: 64.780    Top5: 89.990    Loss: 1.283

2024-02-17 12:14:15,265 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:14:15,265 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:14:15,342 - 

2024-02-17 12:14:15,342 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:14:24,979 - Epoch: [121][  100/  391]    Overall Loss 0.539578    Objective Loss 0.539578                                        LR 0.023500    Time 0.096291    
2024-02-17 12:14:34,227 - Epoch: [121][  200/  391]    Overall Loss 0.562012    Objective Loss 0.562012                                        LR 0.023500    Time 0.094359    
2024-02-17 12:14:43,231 - Epoch: [121][  300/  391]    Overall Loss 0.571322    Objective Loss 0.571322                                        LR 0.023500    Time 0.092904    
2024-02-17 12:14:51,909 - Epoch: [121][  391/  391]    Overall Loss 0.579879    Objective Loss 0.579879    Top1 83.173077    Top5 96.634615    LR 0.023500    Time 0.093466    
2024-02-17 12:14:52,071 - --- validate (epoch=121)-----------
2024-02-17 12:14:52,072 - 10000 samples (128 per mini-batch)
2024-02-17 12:14:54,713 - Epoch: [121][   79/   79]    Loss 1.297498    Top1 64.430000    Top5 89.680000    
2024-02-17 12:14:54,824 - ==> Top1: 64.430    Top5: 89.680    Loss: 1.297

2024-02-17 12:14:54,834 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:14:54,835 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:14:54,909 - 

2024-02-17 12:14:54,909 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:15:04,563 - Epoch: [122][  100/  391]    Overall Loss 0.538779    Objective Loss 0.538779                                        LR 0.023500    Time 0.096466    
2024-02-17 12:15:13,462 - Epoch: [122][  200/  391]    Overall Loss 0.547705    Objective Loss 0.547705                                        LR 0.023500    Time 0.092705    
2024-02-17 12:15:22,521 - Epoch: [122][  300/  391]    Overall Loss 0.560586    Objective Loss 0.560586                                        LR 0.023500    Time 0.091985    
2024-02-17 12:15:31,183 - Epoch: [122][  391/  391]    Overall Loss 0.568272    Objective Loss 0.568272    Top1 81.250000    Top5 97.596154    LR 0.023500    Time 0.092718    
2024-02-17 12:15:31,367 - --- validate (epoch=122)-----------
2024-02-17 12:15:31,368 - 10000 samples (128 per mini-batch)
2024-02-17 12:15:34,190 - Epoch: [122][   79/   79]    Loss 1.398728    Top1 63.080000    Top5 88.530000    
2024-02-17 12:15:34,317 - ==> Top1: 63.080    Top5: 88.530    Loss: 1.399

2024-02-17 12:15:34,335 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:15:34,336 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:15:34,411 - 

2024-02-17 12:15:34,411 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:15:44,298 - Epoch: [123][  100/  391]    Overall Loss 0.545120    Objective Loss 0.545120                                        LR 0.023500    Time 0.098797    
2024-02-17 12:15:53,744 - Epoch: [123][  200/  391]    Overall Loss 0.554063    Objective Loss 0.554063                                        LR 0.023500    Time 0.096601    
2024-02-17 12:16:02,765 - Epoch: [123][  300/  391]    Overall Loss 0.566640    Objective Loss 0.566640                                        LR 0.023500    Time 0.094457    
2024-02-17 12:16:11,422 - Epoch: [123][  391/  391]    Overall Loss 0.577591    Objective Loss 0.577591    Top1 75.961538    Top5 97.596154    LR 0.023500    Time 0.094603    
2024-02-17 12:16:11,610 - --- validate (epoch=123)-----------
2024-02-17 12:16:11,610 - 10000 samples (128 per mini-batch)
2024-02-17 12:16:14,247 - Epoch: [123][   79/   79]    Loss 1.291370    Top1 65.150000    Top5 89.640000    
2024-02-17 12:16:14,359 - ==> Top1: 65.150    Top5: 89.640    Loss: 1.291

2024-02-17 12:16:14,379 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:16:14,379 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:16:14,455 - 

2024-02-17 12:16:14,455 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:16:24,226 - Epoch: [124][  100/  391]    Overall Loss 0.551294    Objective Loss 0.551294                                        LR 0.023500    Time 0.097635    
2024-02-17 12:16:33,525 - Epoch: [124][  200/  391]    Overall Loss 0.559176    Objective Loss 0.559176                                        LR 0.023500    Time 0.095285    
2024-02-17 12:16:42,376 - Epoch: [124][  300/  391]    Overall Loss 0.567467    Objective Loss 0.567467                                        LR 0.023500    Time 0.093015    
2024-02-17 12:16:50,499 - Epoch: [124][  391/  391]    Overall Loss 0.575790    Objective Loss 0.575790    Top1 82.692308    Top5 97.596154    LR 0.023500    Time 0.092131    
2024-02-17 12:16:50,650 - --- validate (epoch=124)-----------
2024-02-17 12:16:50,651 - 10000 samples (128 per mini-batch)
2024-02-17 12:16:53,470 - Epoch: [124][   79/   79]    Loss 1.353544    Top1 64.360000    Top5 88.660000    
2024-02-17 12:16:53,642 - ==> Top1: 64.360    Top5: 88.660    Loss: 1.354

2024-02-17 12:16:53,661 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:16:53,662 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:16:53,738 - 

2024-02-17 12:16:53,738 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:17:03,661 - Epoch: [125][  100/  391]    Overall Loss 0.557675    Objective Loss 0.557675                                        LR 0.023500    Time 0.099160    
2024-02-17 12:17:12,893 - Epoch: [125][  200/  391]    Overall Loss 0.559029    Objective Loss 0.559029                                        LR 0.023500    Time 0.095713    
2024-02-17 12:17:22,171 - Epoch: [125][  300/  391]    Overall Loss 0.565728    Objective Loss 0.565728                                        LR 0.023500    Time 0.094721    
2024-02-17 12:17:30,867 - Epoch: [125][  391/  391]    Overall Loss 0.575140    Objective Loss 0.575140    Top1 86.538462    Top5 98.076923    LR 0.023500    Time 0.094905    
2024-02-17 12:17:31,075 - --- validate (epoch=125)-----------
2024-02-17 12:17:31,076 - 10000 samples (128 per mini-batch)
2024-02-17 12:17:33,711 - Epoch: [125][   79/   79]    Loss 1.319069    Top1 63.570000    Top5 89.240000    
2024-02-17 12:17:33,878 - ==> Top1: 63.570    Top5: 89.240    Loss: 1.319

2024-02-17 12:17:33,897 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:17:33,898 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:17:33,973 - 

2024-02-17 12:17:33,974 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:17:44,351 - Epoch: [126][  100/  391]    Overall Loss 0.531751    Objective Loss 0.531751                                        LR 0.023500    Time 0.103698    
2024-02-17 12:17:53,637 - Epoch: [126][  200/  391]    Overall Loss 0.548702    Objective Loss 0.548702                                        LR 0.023500    Time 0.098255    
2024-02-17 12:18:02,812 - Epoch: [126][  300/  391]    Overall Loss 0.562794    Objective Loss 0.562794                                        LR 0.023500    Time 0.096071    
2024-02-17 12:18:11,459 - Epoch: [126][  391/  391]    Overall Loss 0.566014    Objective Loss 0.566014    Top1 80.288462    Top5 98.076923    LR 0.023500    Time 0.095815    
2024-02-17 12:18:11,626 - --- validate (epoch=126)-----------
2024-02-17 12:18:11,627 - 10000 samples (128 per mini-batch)
2024-02-17 12:18:14,301 - Epoch: [126][   79/   79]    Loss 1.300004    Top1 64.620000    Top5 89.500000    
2024-02-17 12:18:14,406 - ==> Top1: 64.620    Top5: 89.500    Loss: 1.300

2024-02-17 12:18:14,429 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:18:14,430 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:18:14,505 - 

2024-02-17 12:18:14,505 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:18:24,568 - Epoch: [127][  100/  391]    Overall Loss 0.541477    Objective Loss 0.541477                                        LR 0.023500    Time 0.100553    
2024-02-17 12:18:34,075 - Epoch: [127][  200/  391]    Overall Loss 0.550580    Objective Loss 0.550580                                        LR 0.023500    Time 0.097788    
2024-02-17 12:18:43,341 - Epoch: [127][  300/  391]    Overall Loss 0.559187    Objective Loss 0.559187                                        LR 0.023500    Time 0.096063    
2024-02-17 12:18:52,033 - Epoch: [127][  391/  391]    Overall Loss 0.570268    Objective Loss 0.570268    Top1 78.846154    Top5 99.038462    LR 0.023500    Time 0.095924    
2024-02-17 12:18:52,173 - --- validate (epoch=127)-----------
2024-02-17 12:18:52,174 - 10000 samples (128 per mini-batch)
2024-02-17 12:18:54,792 - Epoch: [127][   79/   79]    Loss 1.424709    Top1 62.530000    Top5 88.140000    
2024-02-17 12:18:54,891 - ==> Top1: 62.530    Top5: 88.140    Loss: 1.425

2024-02-17 12:18:54,910 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:18:54,910 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:18:54,988 - 

2024-02-17 12:18:54,989 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:19:05,478 - Epoch: [128][  100/  391]    Overall Loss 0.533142    Objective Loss 0.533142                                        LR 0.023500    Time 0.104818    
2024-02-17 12:19:14,880 - Epoch: [128][  200/  391]    Overall Loss 0.540543    Objective Loss 0.540543                                        LR 0.023500    Time 0.099396    
2024-02-17 12:19:23,769 - Epoch: [128][  300/  391]    Overall Loss 0.556833    Objective Loss 0.556833                                        LR 0.023500    Time 0.095877    
2024-02-17 12:19:32,482 - Epoch: [128][  391/  391]    Overall Loss 0.567360    Objective Loss 0.567360    Top1 82.692308    Top5 98.076923    LR 0.023500    Time 0.095837    
2024-02-17 12:19:32,669 - --- validate (epoch=128)-----------
2024-02-17 12:19:32,670 - 10000 samples (128 per mini-batch)
2024-02-17 12:19:35,445 - Epoch: [128][   79/   79]    Loss 1.352653    Top1 63.800000    Top5 88.890000    
2024-02-17 12:19:35,562 - ==> Top1: 63.800    Top5: 88.890    Loss: 1.353

2024-02-17 12:19:35,582 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:19:35,582 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:19:35,658 - 

2024-02-17 12:19:35,659 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:19:44,565 - Epoch: [129][  100/  391]    Overall Loss 0.539323    Objective Loss 0.539323                                        LR 0.023500    Time 0.088990    
2024-02-17 12:19:53,086 - Epoch: [129][  200/  391]    Overall Loss 0.541701    Objective Loss 0.541701                                        LR 0.023500    Time 0.087081    
2024-02-17 12:20:02,312 - Epoch: [129][  300/  391]    Overall Loss 0.555666    Objective Loss 0.555666                                        LR 0.023500    Time 0.088789    
2024-02-17 12:20:11,027 - Epoch: [129][  391/  391]    Overall Loss 0.559215    Objective Loss 0.559215    Top1 81.250000    Top5 98.076923    LR 0.023500    Time 0.090403    
2024-02-17 12:20:11,147 - --- validate (epoch=129)-----------
2024-02-17 12:20:11,148 - 10000 samples (128 per mini-batch)
2024-02-17 12:20:13,791 - Epoch: [129][   79/   79]    Loss 1.362347    Top1 64.070000    Top5 89.020000    
2024-02-17 12:20:13,894 - ==> Top1: 64.070    Top5: 89.020    Loss: 1.362

2024-02-17 12:20:13,920 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:20:13,920 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:20:14,016 - 

2024-02-17 12:20:14,016 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:20:24,714 - Epoch: [130][  100/  391]    Overall Loss 0.514602    Objective Loss 0.514602                                        LR 0.023500    Time 0.106893    
2024-02-17 12:20:34,892 - Epoch: [130][  200/  391]    Overall Loss 0.527521    Objective Loss 0.527521                                        LR 0.023500    Time 0.104309    
2024-02-17 12:20:44,046 - Epoch: [130][  300/  391]    Overall Loss 0.536228    Objective Loss 0.536228                                        LR 0.023500    Time 0.100036    
2024-02-17 12:20:52,752 - Epoch: [130][  391/  391]    Overall Loss 0.550193    Objective Loss 0.550193    Top1 82.692308    Top5 97.596154    LR 0.023500    Time 0.099008    
2024-02-17 12:20:52,890 - --- validate (epoch=130)-----------
2024-02-17 12:20:52,891 - 10000 samples (128 per mini-batch)
2024-02-17 12:20:55,553 - Epoch: [130][   79/   79]    Loss 1.338975    Top1 64.160000    Top5 88.900000    
2024-02-17 12:20:55,712 - ==> Top1: 64.160    Top5: 88.900    Loss: 1.339

2024-02-17 12:20:55,732 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:20:55,733 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:20:55,809 - 

2024-02-17 12:20:55,809 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:21:05,912 - Epoch: [131][  100/  391]    Overall Loss 0.534401    Objective Loss 0.534401                                        LR 0.023500    Time 0.100955    
2024-02-17 12:21:15,536 - Epoch: [131][  200/  391]    Overall Loss 0.541284    Objective Loss 0.541284                                        LR 0.023500    Time 0.098570    
2024-02-17 12:21:24,946 - Epoch: [131][  300/  391]    Overall Loss 0.550552    Objective Loss 0.550552                                        LR 0.023500    Time 0.097063    
2024-02-17 12:21:33,616 - Epoch: [131][  391/  391]    Overall Loss 0.556768    Objective Loss 0.556768    Top1 79.807692    Top5 98.076923    LR 0.023500    Time 0.096638    
2024-02-17 12:21:33,733 - --- validate (epoch=131)-----------
2024-02-17 12:21:33,733 - 10000 samples (128 per mini-batch)
2024-02-17 12:21:36,242 - Epoch: [131][   79/   79]    Loss 1.422520    Top1 62.650000    Top5 88.430000    
2024-02-17 12:21:36,368 - ==> Top1: 62.650    Top5: 88.430    Loss: 1.423

2024-02-17 12:21:36,386 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:21:36,386 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:21:36,464 - 

2024-02-17 12:21:36,464 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:21:46,227 - Epoch: [132][  100/  391]    Overall Loss 0.524812    Objective Loss 0.524812                                        LR 0.023500    Time 0.097509    
2024-02-17 12:21:55,579 - Epoch: [132][  200/  391]    Overall Loss 0.534590    Objective Loss 0.534590                                        LR 0.023500    Time 0.095493    
2024-02-17 12:22:05,133 - Epoch: [132][  300/  391]    Overall Loss 0.547905    Objective Loss 0.547905                                        LR 0.023500    Time 0.095490    
2024-02-17 12:22:13,860 - Epoch: [132][  391/  391]    Overall Loss 0.561220    Objective Loss 0.561220    Top1 83.173077    Top5 98.076923    LR 0.023500    Time 0.095574    
2024-02-17 12:22:14,003 - --- validate (epoch=132)-----------
2024-02-17 12:22:14,004 - 10000 samples (128 per mini-batch)
2024-02-17 12:22:17,098 - Epoch: [132][   79/   79]    Loss 1.339014    Top1 64.200000    Top5 89.530000    
2024-02-17 12:22:17,314 - ==> Top1: 64.200    Top5: 89.530    Loss: 1.339

2024-02-17 12:22:17,327 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:22:17,327 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:22:17,413 - 

2024-02-17 12:22:17,413 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:22:27,351 - Epoch: [133][  100/  391]    Overall Loss 0.513847    Objective Loss 0.513847                                        LR 0.023500    Time 0.099286    
2024-02-17 12:22:36,554 - Epoch: [133][  200/  391]    Overall Loss 0.534833    Objective Loss 0.534833                                        LR 0.023500    Time 0.095631    
2024-02-17 12:22:46,214 - Epoch: [133][  300/  391]    Overall Loss 0.545445    Objective Loss 0.545445                                        LR 0.023500    Time 0.095936    
2024-02-17 12:22:54,902 - Epoch: [133][  391/  391]    Overall Loss 0.557363    Objective Loss 0.557363    Top1 85.096154    Top5 98.557692    LR 0.023500    Time 0.095818    
2024-02-17 12:22:55,102 - --- validate (epoch=133)-----------
2024-02-17 12:22:55,103 - 10000 samples (128 per mini-batch)
2024-02-17 12:22:57,752 - Epoch: [133][   79/   79]    Loss 1.387939    Top1 63.200000    Top5 88.800000    
2024-02-17 12:22:57,936 - ==> Top1: 63.200    Top5: 88.800    Loss: 1.388

2024-02-17 12:22:57,956 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:22:57,956 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:22:58,033 - 

2024-02-17 12:22:58,033 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:23:08,356 - Epoch: [134][  100/  391]    Overall Loss 0.520899    Objective Loss 0.520899                                        LR 0.023500    Time 0.103153    
2024-02-17 12:23:17,429 - Epoch: [134][  200/  391]    Overall Loss 0.534835    Objective Loss 0.534835                                        LR 0.023500    Time 0.096923    
2024-02-17 12:23:26,952 - Epoch: [134][  300/  391]    Overall Loss 0.546189    Objective Loss 0.546189                                        LR 0.023500    Time 0.096340    
2024-02-17 12:23:35,672 - Epoch: [134][  391/  391]    Overall Loss 0.559324    Objective Loss 0.559324    Top1 75.480769    Top5 97.596154    LR 0.023500    Time 0.096208    
2024-02-17 12:23:35,857 - --- validate (epoch=134)-----------
2024-02-17 12:23:35,858 - 10000 samples (128 per mini-batch)
2024-02-17 12:23:38,594 - Epoch: [134][   79/   79]    Loss 1.345968    Top1 64.770000    Top5 88.710000    
2024-02-17 12:23:38,710 - ==> Top1: 64.770    Top5: 88.710    Loss: 1.346

2024-02-17 12:23:38,731 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:23:38,731 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:23:38,814 - 

2024-02-17 12:23:38,814 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:23:48,816 - Epoch: [135][  100/  391]    Overall Loss 0.523758    Objective Loss 0.523758                                        LR 0.023500    Time 0.099943    
2024-02-17 12:23:57,936 - Epoch: [135][  200/  391]    Overall Loss 0.531264    Objective Loss 0.531264                                        LR 0.023500    Time 0.095550    
2024-02-17 12:24:07,425 - Epoch: [135][  300/  391]    Overall Loss 0.546272    Objective Loss 0.546272                                        LR 0.023500    Time 0.095310    
2024-02-17 12:24:16,160 - Epoch: [135][  391/  391]    Overall Loss 0.555146    Objective Loss 0.555146    Top1 81.730769    Top5 98.076923    LR 0.023500    Time 0.095458    
2024-02-17 12:24:16,307 - --- validate (epoch=135)-----------
2024-02-17 12:24:16,308 - 10000 samples (128 per mini-batch)
2024-02-17 12:24:19,095 - Epoch: [135][   79/   79]    Loss 1.408276    Top1 62.730000    Top5 88.480000    
2024-02-17 12:24:19,229 - ==> Top1: 62.730    Top5: 88.480    Loss: 1.408

2024-02-17 12:24:19,247 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:24:19,248 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:24:19,322 - 

2024-02-17 12:24:19,322 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:24:29,346 - Epoch: [136][  100/  391]    Overall Loss 0.515902    Objective Loss 0.515902                                        LR 0.023500    Time 0.100161    
2024-02-17 12:24:38,427 - Epoch: [136][  200/  391]    Overall Loss 0.523318    Objective Loss 0.523318                                        LR 0.023500    Time 0.095461    
2024-02-17 12:24:47,779 - Epoch: [136][  300/  391]    Overall Loss 0.537852    Objective Loss 0.537852                                        LR 0.023500    Time 0.094798    
2024-02-17 12:24:56,603 - Epoch: [136][  391/  391]    Overall Loss 0.549399    Objective Loss 0.549399    Top1 79.326923    Top5 98.076923    LR 0.023500    Time 0.095290    
2024-02-17 12:24:56,763 - --- validate (epoch=136)-----------
2024-02-17 12:24:56,764 - 10000 samples (128 per mini-batch)
2024-02-17 12:24:59,924 - Epoch: [136][   79/   79]    Loss 1.336822    Top1 63.610000    Top5 89.170000    
2024-02-17 12:25:00,041 - ==> Top1: 63.610    Top5: 89.170    Loss: 1.337

2024-02-17 12:25:00,060 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:25:00,061 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:25:00,137 - 

2024-02-17 12:25:00,138 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:25:10,232 - Epoch: [137][  100/  391]    Overall Loss 0.522268    Objective Loss 0.522268                                        LR 0.023500    Time 0.100871    
2024-02-17 12:25:19,510 - Epoch: [137][  200/  391]    Overall Loss 0.522617    Objective Loss 0.522617                                        LR 0.023500    Time 0.096798    
2024-02-17 12:25:28,825 - Epoch: [137][  300/  391]    Overall Loss 0.533800    Objective Loss 0.533800                                        LR 0.023500    Time 0.095566    
2024-02-17 12:25:37,537 - Epoch: [137][  391/  391]    Overall Loss 0.544992    Objective Loss 0.544992    Top1 81.250000    Top5 98.076923    LR 0.023500    Time 0.095594    
2024-02-17 12:25:37,677 - --- validate (epoch=137)-----------
2024-02-17 12:25:37,678 - 10000 samples (128 per mini-batch)
2024-02-17 12:25:40,259 - Epoch: [137][   79/   79]    Loss 1.417838    Top1 62.180000    Top5 88.250000    
2024-02-17 12:25:40,440 - ==> Top1: 62.180    Top5: 88.250    Loss: 1.418

2024-02-17 12:25:40,461 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:25:40,461 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:25:40,545 - 

2024-02-17 12:25:40,545 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:25:50,566 - Epoch: [138][  100/  391]    Overall Loss 0.518913    Objective Loss 0.518913                                        LR 0.023500    Time 0.100137    
2024-02-17 12:26:00,008 - Epoch: [138][  200/  391]    Overall Loss 0.531409    Objective Loss 0.531409                                        LR 0.023500    Time 0.097257    
2024-02-17 12:26:09,248 - Epoch: [138][  300/  391]    Overall Loss 0.538835    Objective Loss 0.538835                                        LR 0.023500    Time 0.095621    
2024-02-17 12:26:18,071 - Epoch: [138][  391/  391]    Overall Loss 0.549000    Objective Loss 0.549000    Top1 77.884615    Top5 96.153846    LR 0.023500    Time 0.095920    
2024-02-17 12:26:18,221 - --- validate (epoch=138)-----------
2024-02-17 12:26:18,222 - 10000 samples (128 per mini-batch)
2024-02-17 12:26:20,872 - Epoch: [138][   79/   79]    Loss 1.504057    Top1 61.290000    Top5 87.450000    
2024-02-17 12:26:20,984 - ==> Top1: 61.290    Top5: 87.450    Loss: 1.504

2024-02-17 12:26:21,003 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:26:21,004 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:26:21,080 - 

2024-02-17 12:26:21,080 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:26:31,176 - Epoch: [139][  100/  391]    Overall Loss 0.522495    Objective Loss 0.522495                                        LR 0.023500    Time 0.100877    
2024-02-17 12:26:40,511 - Epoch: [139][  200/  391]    Overall Loss 0.530554    Objective Loss 0.530554                                        LR 0.023500    Time 0.097094    
2024-02-17 12:26:49,834 - Epoch: [139][  300/  391]    Overall Loss 0.539933    Objective Loss 0.539933                                        LR 0.023500    Time 0.095789    
2024-02-17 12:26:58,559 - Epoch: [139][  391/  391]    Overall Loss 0.545849    Objective Loss 0.545849    Top1 79.326923    Top5 98.076923    LR 0.023500    Time 0.095799    
2024-02-17 12:26:58,740 - --- validate (epoch=139)-----------
2024-02-17 12:26:58,740 - 10000 samples (128 per mini-batch)
2024-02-17 12:27:01,364 - Epoch: [139][   79/   79]    Loss 1.368089    Top1 63.390000    Top5 88.880000    
2024-02-17 12:27:01,486 - ==> Top1: 63.390    Top5: 88.880    Loss: 1.368

2024-02-17 12:27:01,506 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:27:01,506 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:27:01,581 - 

2024-02-17 12:27:01,582 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:27:12,125 - Epoch: [140][  100/  391]    Overall Loss 0.506662    Objective Loss 0.506662                                        LR 0.023500    Time 0.105356    
2024-02-17 12:27:21,730 - Epoch: [140][  200/  391]    Overall Loss 0.516688    Objective Loss 0.516688                                        LR 0.023500    Time 0.100681    
2024-02-17 12:27:30,987 - Epoch: [140][  300/  391]    Overall Loss 0.526371    Objective Loss 0.526371                                        LR 0.023500    Time 0.097962    
2024-02-17 12:27:39,697 - Epoch: [140][  391/  391]    Overall Loss 0.537069    Objective Loss 0.537069    Top1 80.288462    Top5 98.557692    LR 0.023500    Time 0.097427    
2024-02-17 12:27:39,853 - --- validate (epoch=140)-----------
2024-02-17 12:27:39,854 - 10000 samples (128 per mini-batch)
2024-02-17 12:27:42,459 - Epoch: [140][   79/   79]    Loss 1.344355    Top1 63.930000    Top5 89.210000    
2024-02-17 12:27:42,577 - ==> Top1: 63.930    Top5: 89.210    Loss: 1.344

2024-02-17 12:27:42,596 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:27:42,596 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:27:42,672 - 

2024-02-17 12:27:42,672 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:27:52,495 - Epoch: [141][  100/  391]    Overall Loss 0.500247    Objective Loss 0.500247                                        LR 0.023500    Time 0.098158    
2024-02-17 12:28:02,091 - Epoch: [141][  200/  391]    Overall Loss 0.519171    Objective Loss 0.519171                                        LR 0.023500    Time 0.097033    
2024-02-17 12:28:11,606 - Epoch: [141][  300/  391]    Overall Loss 0.528672    Objective Loss 0.528672                                        LR 0.023500    Time 0.096388    
2024-02-17 12:28:19,789 - Epoch: [141][  391/  391]    Overall Loss 0.538246    Objective Loss 0.538246    Top1 81.250000    Top5 98.076923    LR 0.023500    Time 0.094873    
2024-02-17 12:28:20,019 - --- validate (epoch=141)-----------
2024-02-17 12:28:20,020 - 10000 samples (128 per mini-batch)
2024-02-17 12:28:22,640 - Epoch: [141][   79/   79]    Loss 1.428072    Top1 63.040000    Top5 88.220000    
2024-02-17 12:28:22,795 - ==> Top1: 63.040    Top5: 88.220    Loss: 1.428

2024-02-17 12:28:22,816 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:28:22,816 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:28:22,890 - 

2024-02-17 12:28:22,890 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:28:33,030 - Epoch: [142][  100/  391]    Overall Loss 0.508290    Objective Loss 0.508290                                        LR 0.023500    Time 0.101323    
2024-02-17 12:28:42,776 - Epoch: [142][  200/  391]    Overall Loss 0.517290    Objective Loss 0.517290                                        LR 0.023500    Time 0.099366    
2024-02-17 12:28:52,260 - Epoch: [142][  300/  391]    Overall Loss 0.529299    Objective Loss 0.529299                                        LR 0.023500    Time 0.097844    
2024-02-17 12:29:00,743 - Epoch: [142][  391/  391]    Overall Loss 0.539657    Objective Loss 0.539657    Top1 88.942308    Top5 99.038462    LR 0.023500    Time 0.096756    
2024-02-17 12:29:00,889 - --- validate (epoch=142)-----------
2024-02-17 12:29:00,890 - 10000 samples (128 per mini-batch)
2024-02-17 12:29:03,521 - Epoch: [142][   79/   79]    Loss 1.381608    Top1 63.870000    Top5 88.800000    
2024-02-17 12:29:03,695 - ==> Top1: 63.870    Top5: 88.800    Loss: 1.382

2024-02-17 12:29:03,715 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:29:03,716 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:29:03,791 - 

2024-02-17 12:29:03,792 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:29:13,868 - Epoch: [143][  100/  391]    Overall Loss 0.506073    Objective Loss 0.506073                                        LR 0.023500    Time 0.100694    
2024-02-17 12:29:23,394 - Epoch: [143][  200/  391]    Overall Loss 0.516722    Objective Loss 0.516722                                        LR 0.023500    Time 0.097949    
2024-02-17 12:29:32,886 - Epoch: [143][  300/  391]    Overall Loss 0.524782    Objective Loss 0.524782                                        LR 0.023500    Time 0.096924    
2024-02-17 12:29:41,414 - Epoch: [143][  391/  391]    Overall Loss 0.535591    Objective Loss 0.535591    Top1 79.326923    Top5 99.038462    LR 0.023500    Time 0.096167    
2024-02-17 12:29:41,537 - --- validate (epoch=143)-----------
2024-02-17 12:29:41,538 - 10000 samples (128 per mini-batch)
2024-02-17 12:29:44,138 - Epoch: [143][   79/   79]    Loss 1.403229    Top1 62.710000    Top5 88.530000    
2024-02-17 12:29:44,339 - ==> Top1: 62.710    Top5: 88.530    Loss: 1.403

2024-02-17 12:29:44,359 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:29:44,360 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:29:44,436 - 

2024-02-17 12:29:44,437 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:29:55,424 - Epoch: [144][  100/  391]    Overall Loss 0.511261    Objective Loss 0.511261                                        LR 0.023500    Time 0.109799    
2024-02-17 12:30:05,129 - Epoch: [144][  200/  391]    Overall Loss 0.515480    Objective Loss 0.515480                                        LR 0.023500    Time 0.103398    
2024-02-17 12:30:14,631 - Epoch: [144][  300/  391]    Overall Loss 0.523080    Objective Loss 0.523080                                        LR 0.023500    Time 0.100590    
2024-02-17 12:30:23,358 - Epoch: [144][  391/  391]    Overall Loss 0.534124    Objective Loss 0.534124    Top1 83.653846    Top5 99.519231    LR 0.023500    Time 0.099487    
2024-02-17 12:30:23,532 - --- validate (epoch=144)-----------
2024-02-17 12:30:23,534 - 10000 samples (128 per mini-batch)
2024-02-17 12:30:26,338 - Epoch: [144][   79/   79]    Loss 1.392520    Top1 63.310000    Top5 88.690000    
2024-02-17 12:30:26,461 - ==> Top1: 63.310    Top5: 88.690    Loss: 1.393

2024-02-17 12:30:26,480 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:30:26,480 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:30:26,556 - 

2024-02-17 12:30:26,557 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:30:36,522 - Epoch: [145][  100/  391]    Overall Loss 0.497500    Objective Loss 0.497500                                        LR 0.023500    Time 0.099577    
2024-02-17 12:30:46,053 - Epoch: [145][  200/  391]    Overall Loss 0.509858    Objective Loss 0.509858                                        LR 0.023500    Time 0.097419    
2024-02-17 12:30:55,471 - Epoch: [145][  300/  391]    Overall Loss 0.521146    Objective Loss 0.521146                                        LR 0.023500    Time 0.096324    
2024-02-17 12:31:04,199 - Epoch: [145][  391/  391]    Overall Loss 0.532100    Objective Loss 0.532100    Top1 86.057692    Top5 100.000000    LR 0.023500    Time 0.096218    
2024-02-17 12:31:04,385 - --- validate (epoch=145)-----------
2024-02-17 12:31:04,387 - 10000 samples (128 per mini-batch)
2024-02-17 12:31:07,144 - Epoch: [145][   79/   79]    Loss 1.369270    Top1 63.570000    Top5 88.880000    
2024-02-17 12:31:07,341 - ==> Top1: 63.570    Top5: 88.880    Loss: 1.369

2024-02-17 12:31:07,357 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:31:07,357 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:31:07,438 - 

2024-02-17 12:31:07,439 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:31:17,717 - Epoch: [146][  100/  391]    Overall Loss 0.505544    Objective Loss 0.505544                                        LR 0.023500    Time 0.102717    
2024-02-17 12:31:27,314 - Epoch: [146][  200/  391]    Overall Loss 0.508453    Objective Loss 0.508453                                        LR 0.023500    Time 0.099317    
2024-02-17 12:31:36,667 - Epoch: [146][  300/  391]    Overall Loss 0.518757    Objective Loss 0.518757                                        LR 0.023500    Time 0.097371    
2024-02-17 12:31:45,422 - Epoch: [146][  391/  391]    Overall Loss 0.527722    Objective Loss 0.527722    Top1 78.365385    Top5 95.192308    LR 0.023500    Time 0.097091    
2024-02-17 12:31:45,568 - --- validate (epoch=146)-----------
2024-02-17 12:31:45,569 - 10000 samples (128 per mini-batch)
2024-02-17 12:31:48,359 - Epoch: [146][   79/   79]    Loss 1.445637    Top1 62.150000    Top5 88.240000    
2024-02-17 12:31:48,538 - ==> Top1: 62.150    Top5: 88.240    Loss: 1.446

2024-02-17 12:31:48,557 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:31:48,557 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:31:48,632 - 

2024-02-17 12:31:48,632 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:31:58,692 - Epoch: [147][  100/  391]    Overall Loss 0.515762    Objective Loss 0.515762                                        LR 0.023500    Time 0.100525    
2024-02-17 12:32:08,395 - Epoch: [147][  200/  391]    Overall Loss 0.522330    Objective Loss 0.522330                                        LR 0.023500    Time 0.098753    
2024-02-17 12:32:17,561 - Epoch: [147][  300/  391]    Overall Loss 0.527639    Objective Loss 0.527639                                        LR 0.023500    Time 0.096376    
2024-02-17 12:32:26,113 - Epoch: [147][  391/  391]    Overall Loss 0.533106    Objective Loss 0.533106    Top1 87.500000    Top5 99.038462    LR 0.023500    Time 0.095807    
2024-02-17 12:32:26,258 - --- validate (epoch=147)-----------
2024-02-17 12:32:26,259 - 10000 samples (128 per mini-batch)
2024-02-17 12:32:28,978 - Epoch: [147][   79/   79]    Loss 1.429253    Top1 62.560000    Top5 88.260000    
2024-02-17 12:32:29,113 - ==> Top1: 62.560    Top5: 88.260    Loss: 1.429

2024-02-17 12:32:29,130 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:32:29,131 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:32:29,210 - 

2024-02-17 12:32:29,211 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:32:39,441 - Epoch: [148][  100/  391]    Overall Loss 0.483663    Objective Loss 0.483663                                        LR 0.023500    Time 0.102228    
2024-02-17 12:32:49,013 - Epoch: [148][  200/  391]    Overall Loss 0.503727    Objective Loss 0.503727                                        LR 0.023500    Time 0.098953    
2024-02-17 12:32:58,265 - Epoch: [148][  300/  391]    Overall Loss 0.519878    Objective Loss 0.519878                                        LR 0.023500    Time 0.096791    
2024-02-17 12:33:07,109 - Epoch: [148][  391/  391]    Overall Loss 0.528989    Objective Loss 0.528989    Top1 79.326923    Top5 97.115385    LR 0.023500    Time 0.096872    
2024-02-17 12:33:07,217 - --- validate (epoch=148)-----------
2024-02-17 12:33:07,217 - 10000 samples (128 per mini-batch)
2024-02-17 12:33:09,757 - Epoch: [148][   79/   79]    Loss 1.451930    Top1 62.220000    Top5 88.140000    
2024-02-17 12:33:09,867 - ==> Top1: 62.220    Top5: 88.140    Loss: 1.452

2024-02-17 12:33:09,885 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:33:09,885 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:33:09,962 - 

2024-02-17 12:33:09,962 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:33:19,959 - Epoch: [149][  100/  391]    Overall Loss 0.499607    Objective Loss 0.499607                                        LR 0.023500    Time 0.099895    
2024-02-17 12:33:29,547 - Epoch: [149][  200/  391]    Overall Loss 0.514419    Objective Loss 0.514419                                        LR 0.023500    Time 0.097865    
2024-02-17 12:33:38,535 - Epoch: [149][  300/  391]    Overall Loss 0.520507    Objective Loss 0.520507                                        LR 0.023500    Time 0.095190    
2024-02-17 12:33:46,979 - Epoch: [149][  391/  391]    Overall Loss 0.523791    Objective Loss 0.523791    Top1 77.884615    Top5 96.634615    LR 0.023500    Time 0.094619    
2024-02-17 12:33:47,163 - --- validate (epoch=149)-----------
2024-02-17 12:33:47,164 - 10000 samples (128 per mini-batch)
2024-02-17 12:33:49,840 - Epoch: [149][   79/   79]    Loss 1.443723    Top1 62.570000    Top5 88.140000    
2024-02-17 12:33:49,940 - ==> Top1: 62.570    Top5: 88.140    Loss: 1.444

2024-02-17 12:33:49,958 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:33:49,958 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:33:50,034 - 

2024-02-17 12:33:50,034 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:34:00,327 - Epoch: [150][  100/  391]    Overall Loss 0.404760    Objective Loss 0.404760                                        LR 0.005522    Time 0.102862    
2024-02-17 12:34:10,021 - Epoch: [150][  200/  391]    Overall Loss 0.394318    Objective Loss 0.394318                                        LR 0.005522    Time 0.099874    
2024-02-17 12:34:19,109 - Epoch: [150][  300/  391]    Overall Loss 0.381782    Objective Loss 0.381782                                        LR 0.005522    Time 0.096861    
2024-02-17 12:34:28,322 - Epoch: [150][  391/  391]    Overall Loss 0.376352    Objective Loss 0.376352    Top1 91.346154    Top5 99.038462    LR 0.005522    Time 0.097870    
2024-02-17 12:34:28,490 - --- validate (epoch=150)-----------
2024-02-17 12:34:28,491 - 10000 samples (128 per mini-batch)
2024-02-17 12:34:31,086 - Epoch: [150][   79/   79]    Loss 1.232871    Top1 67.010000    Top5 90.670000    
2024-02-17 12:34:31,209 - ==> Top1: 67.010    Top5: 90.670    Loss: 1.233

2024-02-17 12:34:31,228 - ==> Best [Top1: 67.010   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 150]
2024-02-17 12:34:31,228 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:34:31,319 - 

2024-02-17 12:34:31,319 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:34:41,296 - Epoch: [151][  100/  391]    Overall Loss 0.337969    Objective Loss 0.337969                                        LR 0.005522    Time 0.099695    
2024-02-17 12:34:50,830 - Epoch: [151][  200/  391]    Overall Loss 0.335457    Objective Loss 0.335457                                        LR 0.005522    Time 0.097494    
2024-02-17 12:35:00,005 - Epoch: [151][  300/  391]    Overall Loss 0.331588    Objective Loss 0.331588                                        LR 0.005522    Time 0.095565    
2024-02-17 12:35:08,556 - Epoch: [151][  391/  391]    Overall Loss 0.332760    Objective Loss 0.332760    Top1 92.307692    Top5 99.519231    LR 0.005522    Time 0.095181    
2024-02-17 12:35:08,721 - --- validate (epoch=151)-----------
2024-02-17 12:35:08,722 - 10000 samples (128 per mini-batch)
2024-02-17 12:35:11,299 - Epoch: [151][   79/   79]    Loss 1.224059    Top1 67.100000    Top5 90.580000    
2024-02-17 12:35:11,423 - ==> Top1: 67.100    Top5: 90.580    Loss: 1.224

2024-02-17 12:35:11,442 - ==> Best [Top1: 67.100   Top5: 90.580   Sparsity:0.00   Params: 1341960 on epoch: 151]
2024-02-17 12:35:11,442 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:35:11,535 - 

2024-02-17 12:35:11,536 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:35:21,815 - Epoch: [152][  100/  391]    Overall Loss 0.314529    Objective Loss 0.314529                                        LR 0.005522    Time 0.102718    
2024-02-17 12:35:31,333 - Epoch: [152][  200/  391]    Overall Loss 0.308978    Objective Loss 0.308978                                        LR 0.005522    Time 0.098926    
2024-02-17 12:35:40,549 - Epoch: [152][  300/  391]    Overall Loss 0.311261    Objective Loss 0.311261                                        LR 0.005522    Time 0.096656    
2024-02-17 12:35:48,825 - Epoch: [152][  391/  391]    Overall Loss 0.315546    Objective Loss 0.315546    Top1 85.576923    Top5 99.519231    LR 0.005522    Time 0.095315    
2024-02-17 12:35:49,000 - --- validate (epoch=152)-----------
2024-02-17 12:35:49,001 - 10000 samples (128 per mini-batch)
2024-02-17 12:35:51,551 - Epoch: [152][   79/   79]    Loss 1.244324    Top1 67.250000    Top5 90.670000    
2024-02-17 12:35:51,708 - ==> Top1: 67.250    Top5: 90.670    Loss: 1.244

2024-02-17 12:35:51,728 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:35:51,728 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:35:51,823 - 

2024-02-17 12:35:51,823 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:36:01,773 - Epoch: [153][  100/  391]    Overall Loss 0.311488    Objective Loss 0.311488                                        LR 0.005522    Time 0.099426    
2024-02-17 12:36:11,298 - Epoch: [153][  200/  391]    Overall Loss 0.308623    Objective Loss 0.308623                                        LR 0.005522    Time 0.097314    
2024-02-17 12:36:20,503 - Epoch: [153][  300/  391]    Overall Loss 0.307036    Objective Loss 0.307036                                        LR 0.005522    Time 0.095543    
2024-02-17 12:36:28,942 - Epoch: [153][  391/  391]    Overall Loss 0.307062    Objective Loss 0.307062    Top1 88.942308    Top5 100.000000    LR 0.005522    Time 0.094879    
2024-02-17 12:36:29,139 - --- validate (epoch=153)-----------
2024-02-17 12:36:29,140 - 10000 samples (128 per mini-batch)
2024-02-17 12:36:31,785 - Epoch: [153][   79/   79]    Loss 1.239815    Top1 66.950000    Top5 90.360000    
2024-02-17 12:36:31,905 - ==> Top1: 66.950    Top5: 90.360    Loss: 1.240

2024-02-17 12:36:31,924 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:36:31,924 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:36:32,000 - 

2024-02-17 12:36:32,000 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:36:42,008 - Epoch: [154][  100/  391]    Overall Loss 0.287211    Objective Loss 0.287211                                        LR 0.005522    Time 0.100002    
2024-02-17 12:36:51,356 - Epoch: [154][  200/  391]    Overall Loss 0.291827    Objective Loss 0.291827                                        LR 0.005522    Time 0.096722    
2024-02-17 12:37:00,158 - Epoch: [154][  300/  391]    Overall Loss 0.296863    Objective Loss 0.296863                                        LR 0.005522    Time 0.093806    
2024-02-17 12:37:08,552 - Epoch: [154][  391/  391]    Overall Loss 0.298064    Objective Loss 0.298064    Top1 90.865385    Top5 99.038462    LR 0.005522    Time 0.093431    
2024-02-17 12:37:08,732 - --- validate (epoch=154)-----------
2024-02-17 12:37:08,733 - 10000 samples (128 per mini-batch)
2024-02-17 12:37:11,549 - Epoch: [154][   79/   79]    Loss 1.260500    Top1 66.990000    Top5 90.430000    
2024-02-17 12:37:11,694 - ==> Top1: 66.990    Top5: 90.430    Loss: 1.261

2024-02-17 12:37:11,713 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:37:11,713 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:37:11,788 - 

2024-02-17 12:37:11,789 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:37:21,772 - Epoch: [155][  100/  391]    Overall Loss 0.279161    Objective Loss 0.279161                                        LR 0.005522    Time 0.099753    
2024-02-17 12:37:31,098 - Epoch: [155][  200/  391]    Overall Loss 0.287515    Objective Loss 0.287515                                        LR 0.005522    Time 0.096484    
2024-02-17 12:37:40,331 - Epoch: [155][  300/  391]    Overall Loss 0.289487    Objective Loss 0.289487                                        LR 0.005522    Time 0.095085    
2024-02-17 12:37:48,806 - Epoch: [155][  391/  391]    Overall Loss 0.289827    Objective Loss 0.289827    Top1 94.230769    Top5 99.519231    LR 0.005522    Time 0.094619    
2024-02-17 12:37:48,981 - --- validate (epoch=155)-----------
2024-02-17 12:37:48,982 - 10000 samples (128 per mini-batch)
2024-02-17 12:37:51,596 - Epoch: [155][   79/   79]    Loss 1.260405    Top1 66.810000    Top5 90.430000    
2024-02-17 12:37:51,779 - ==> Top1: 66.810    Top5: 90.430    Loss: 1.260

2024-02-17 12:37:51,799 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:37:51,799 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:37:51,874 - 

2024-02-17 12:37:51,874 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:38:02,047 - Epoch: [156][  100/  391]    Overall Loss 0.276131    Objective Loss 0.276131                                        LR 0.005522    Time 0.101657    
2024-02-17 12:38:11,469 - Epoch: [156][  200/  391]    Overall Loss 0.277669    Objective Loss 0.277669                                        LR 0.005522    Time 0.097914    
2024-02-17 12:38:20,209 - Epoch: [156][  300/  391]    Overall Loss 0.281614    Objective Loss 0.281614                                        LR 0.005522    Time 0.094394    
2024-02-17 12:38:28,773 - Epoch: [156][  391/  391]    Overall Loss 0.281630    Objective Loss 0.281630    Top1 91.346154    Top5 99.038462    LR 0.005522    Time 0.094316    
2024-02-17 12:38:28,940 - --- validate (epoch=156)-----------
2024-02-17 12:38:28,941 - 10000 samples (128 per mini-batch)
2024-02-17 12:38:31,527 - Epoch: [156][   79/   79]    Loss 1.252237    Top1 66.960000    Top5 90.480000    
2024-02-17 12:38:31,659 - ==> Top1: 66.960    Top5: 90.480    Loss: 1.252

2024-02-17 12:38:31,678 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:38:31,678 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:38:31,757 - 

2024-02-17 12:38:31,757 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:38:41,521 - Epoch: [157][  100/  391]    Overall Loss 0.266113    Objective Loss 0.266113                                        LR 0.005522    Time 0.097571    
2024-02-17 12:38:48,669 - Epoch: [157][  200/  391]    Overall Loss 0.273138    Objective Loss 0.273138                                        LR 0.005522    Time 0.084505    
2024-02-17 12:38:56,929 - Epoch: [157][  300/  391]    Overall Loss 0.277272    Objective Loss 0.277272                                        LR 0.005522    Time 0.083856    
2024-02-17 12:39:05,388 - Epoch: [157][  391/  391]    Overall Loss 0.279318    Objective Loss 0.279318    Top1 91.826923    Top5 99.038462    LR 0.005522    Time 0.085963    
2024-02-17 12:39:05,513 - --- validate (epoch=157)-----------
2024-02-17 12:39:05,514 - 10000 samples (128 per mini-batch)
2024-02-17 12:39:08,248 - Epoch: [157][   79/   79]    Loss 1.282982    Top1 66.430000    Top5 90.060000    
2024-02-17 12:39:08,391 - ==> Top1: 66.430    Top5: 90.060    Loss: 1.283

2024-02-17 12:39:08,407 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:39:08,408 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:39:08,490 - 

2024-02-17 12:39:08,490 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:39:18,655 - Epoch: [158][  100/  391]    Overall Loss 0.258010    Objective Loss 0.258010                                        LR 0.005522    Time 0.101575    
2024-02-17 12:39:28,144 - Epoch: [158][  200/  391]    Overall Loss 0.270554    Objective Loss 0.270554                                        LR 0.005522    Time 0.098212    
2024-02-17 12:39:37,733 - Epoch: [158][  300/  391]    Overall Loss 0.270927    Objective Loss 0.270927                                        LR 0.005522    Time 0.097421    
2024-02-17 12:39:46,279 - Epoch: [158][  391/  391]    Overall Loss 0.271884    Objective Loss 0.271884    Top1 91.826923    Top5 98.557692    LR 0.005522    Time 0.096591    
2024-02-17 12:39:46,415 - --- validate (epoch=158)-----------
2024-02-17 12:39:46,416 - 10000 samples (128 per mini-batch)
2024-02-17 12:39:49,141 - Epoch: [158][   79/   79]    Loss 1.297263    Top1 66.580000    Top5 90.440000    
2024-02-17 12:39:49,249 - ==> Top1: 66.580    Top5: 90.440    Loss: 1.297

2024-02-17 12:39:49,269 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:39:49,270 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:39:49,350 - 

2024-02-17 12:39:49,351 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:39:59,493 - Epoch: [159][  100/  391]    Overall Loss 0.262810    Objective Loss 0.262810                                        LR 0.005522    Time 0.101343    
2024-02-17 12:40:08,969 - Epoch: [159][  200/  391]    Overall Loss 0.264519    Objective Loss 0.264519                                        LR 0.005522    Time 0.098027    
2024-02-17 12:40:18,521 - Epoch: [159][  300/  391]    Overall Loss 0.265082    Objective Loss 0.265082                                        LR 0.005522    Time 0.097178    
2024-02-17 12:40:26,152 - Epoch: [159][  391/  391]    Overall Loss 0.266618    Objective Loss 0.266618    Top1 89.423077    Top5 98.076923    LR 0.005522    Time 0.094067    
2024-02-17 12:40:26,321 - --- validate (epoch=159)-----------
2024-02-17 12:40:26,321 - 10000 samples (128 per mini-batch)
2024-02-17 12:40:28,948 - Epoch: [159][   79/   79]    Loss 1.277716    Top1 66.960000    Top5 90.310000    
2024-02-17 12:40:29,150 - ==> Top1: 66.960    Top5: 90.310    Loss: 1.278

2024-02-17 12:40:29,169 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:40:29,169 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:40:29,247 - 

2024-02-17 12:40:29,248 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:40:39,491 - Epoch: [160][  100/  391]    Overall Loss 0.251683    Objective Loss 0.251683                                        LR 0.005522    Time 0.102361    
2024-02-17 12:40:49,014 - Epoch: [160][  200/  391]    Overall Loss 0.258876    Objective Loss 0.258876                                        LR 0.005522    Time 0.098774    
2024-02-17 12:40:58,372 - Epoch: [160][  300/  391]    Overall Loss 0.260912    Objective Loss 0.260912                                        LR 0.005522    Time 0.097026    
2024-02-17 12:41:06,218 - Epoch: [160][  391/  391]    Overall Loss 0.262499    Objective Loss 0.262499    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.094499    
2024-02-17 12:41:06,401 - --- validate (epoch=160)-----------
2024-02-17 12:41:06,403 - 10000 samples (128 per mini-batch)
2024-02-17 12:41:08,966 - Epoch: [160][   79/   79]    Loss 1.284113    Top1 66.650000    Top5 90.270000    
2024-02-17 12:41:09,073 - ==> Top1: 66.650    Top5: 90.270    Loss: 1.284

2024-02-17 12:41:09,094 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:41:09,094 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:41:09,169 - 

2024-02-17 12:41:09,169 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:41:19,166 - Epoch: [161][  100/  391]    Overall Loss 0.247426    Objective Loss 0.247426                                        LR 0.005522    Time 0.099893    
2024-02-17 12:41:28,613 - Epoch: [161][  200/  391]    Overall Loss 0.250927    Objective Loss 0.250927                                        LR 0.005522    Time 0.097162    
2024-02-17 12:41:38,249 - Epoch: [161][  300/  391]    Overall Loss 0.255053    Objective Loss 0.255053                                        LR 0.005522    Time 0.096880    
2024-02-17 12:41:46,747 - Epoch: [161][  391/  391]    Overall Loss 0.256470    Objective Loss 0.256470    Top1 94.711538    Top5 99.519231    LR 0.005522    Time 0.096053    
2024-02-17 12:41:46,941 - --- validate (epoch=161)-----------
2024-02-17 12:41:46,943 - 10000 samples (128 per mini-batch)
2024-02-17 12:41:49,736 - Epoch: [161][   79/   79]    Loss 1.288213    Top1 66.610000    Top5 90.280000    
2024-02-17 12:41:49,858 - ==> Top1: 66.610    Top5: 90.280    Loss: 1.288

2024-02-17 12:41:49,877 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:41:49,877 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:41:49,960 - 

2024-02-17 12:41:49,961 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:42:00,238 - Epoch: [162][  100/  391]    Overall Loss 0.245268    Objective Loss 0.245268                                        LR 0.005522    Time 0.102700    
2024-02-17 12:42:09,741 - Epoch: [162][  200/  391]    Overall Loss 0.249498    Objective Loss 0.249498                                        LR 0.005522    Time 0.098838    
2024-02-17 12:42:19,179 - Epoch: [162][  300/  391]    Overall Loss 0.251475    Objective Loss 0.251475                                        LR 0.005522    Time 0.097340    
2024-02-17 12:42:27,475 - Epoch: [162][  391/  391]    Overall Loss 0.253709    Objective Loss 0.253709    Top1 93.269231    Top5 100.000000    LR 0.005522    Time 0.095890    
2024-02-17 12:42:27,639 - --- validate (epoch=162)-----------
2024-02-17 12:42:27,640 - 10000 samples (128 per mini-batch)
2024-02-17 12:42:30,519 - Epoch: [162][   79/   79]    Loss 1.289234    Top1 66.760000    Top5 90.090000    
2024-02-17 12:42:30,670 - ==> Top1: 66.760    Top5: 90.090    Loss: 1.289

2024-02-17 12:42:30,689 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:42:30,689 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:42:30,765 - 

2024-02-17 12:42:30,766 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:42:40,677 - Epoch: [163][  100/  391]    Overall Loss 0.245569    Objective Loss 0.245569                                        LR 0.005522    Time 0.099032    
2024-02-17 12:42:50,213 - Epoch: [163][  200/  391]    Overall Loss 0.245899    Objective Loss 0.245899                                        LR 0.005522    Time 0.097174    
2024-02-17 12:42:59,755 - Epoch: [163][  300/  391]    Overall Loss 0.248700    Objective Loss 0.248700                                        LR 0.005522    Time 0.096576    
2024-02-17 12:43:08,093 - Epoch: [163][  391/  391]    Overall Loss 0.251160    Objective Loss 0.251160    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.095412    
2024-02-17 12:43:08,212 - --- validate (epoch=163)-----------
2024-02-17 12:43:08,213 - 10000 samples (128 per mini-batch)
2024-02-17 12:43:10,827 - Epoch: [163][   79/   79]    Loss 1.309039    Top1 66.980000    Top5 90.130000    
2024-02-17 12:43:10,953 - ==> Top1: 66.980    Top5: 90.130    Loss: 1.309

2024-02-17 12:43:10,972 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:43:10,972 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:43:11,047 - 

2024-02-17 12:43:11,047 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:43:20,954 - Epoch: [164][  100/  391]    Overall Loss 0.241939    Objective Loss 0.241939                                        LR 0.005522    Time 0.098997    
2024-02-17 12:43:30,437 - Epoch: [164][  200/  391]    Overall Loss 0.245077    Objective Loss 0.245077                                        LR 0.005522    Time 0.096890    
2024-02-17 12:43:39,966 - Epoch: [164][  300/  391]    Overall Loss 0.247411    Objective Loss 0.247411                                        LR 0.005522    Time 0.096341    
2024-02-17 12:43:48,222 - Epoch: [164][  391/  391]    Overall Loss 0.249987    Objective Loss 0.249987    Top1 96.153846    Top5 100.000000    LR 0.005522    Time 0.095023    
2024-02-17 12:43:48,417 - --- validate (epoch=164)-----------
2024-02-17 12:43:48,418 - 10000 samples (128 per mini-batch)
2024-02-17 12:43:51,216 - Epoch: [164][   79/   79]    Loss 1.300506    Top1 66.680000    Top5 90.150000    
2024-02-17 12:43:51,335 - ==> Top1: 66.680    Top5: 90.150    Loss: 1.301

2024-02-17 12:43:51,347 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:43:51,347 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:43:51,419 - 

2024-02-17 12:43:51,419 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:44:01,004 - Epoch: [165][  100/  391]    Overall Loss 0.230396    Objective Loss 0.230396                                        LR 0.005522    Time 0.095774    
2024-02-17 12:44:10,624 - Epoch: [165][  200/  391]    Overall Loss 0.239541    Objective Loss 0.239541                                        LR 0.005522    Time 0.095960    
2024-02-17 12:44:20,126 - Epoch: [165][  300/  391]    Overall Loss 0.242799    Objective Loss 0.242799                                        LR 0.005522    Time 0.095632    
2024-02-17 12:44:28,396 - Epoch: [165][  391/  391]    Overall Loss 0.244418    Objective Loss 0.244418    Top1 90.865385    Top5 99.519231    LR 0.005522    Time 0.094515    
2024-02-17 12:44:28,529 - --- validate (epoch=165)-----------
2024-02-17 12:44:28,530 - 10000 samples (128 per mini-batch)
2024-02-17 12:44:31,214 - Epoch: [165][   79/   79]    Loss 1.300255    Top1 66.990000    Top5 90.210000    
2024-02-17 12:44:31,354 - ==> Top1: 66.990    Top5: 90.210    Loss: 1.300

2024-02-17 12:44:31,373 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:44:31,373 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:44:31,450 - 

2024-02-17 12:44:31,450 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:44:41,836 - Epoch: [166][  100/  391]    Overall Loss 0.239461    Objective Loss 0.239461                                        LR 0.005522    Time 0.103782    
2024-02-17 12:44:51,445 - Epoch: [166][  200/  391]    Overall Loss 0.239250    Objective Loss 0.239250                                        LR 0.005522    Time 0.099910    
2024-02-17 12:45:01,098 - Epoch: [166][  300/  391]    Overall Loss 0.242436    Objective Loss 0.242436                                        LR 0.005522    Time 0.098768    
2024-02-17 12:45:09,425 - Epoch: [166][  391/  391]    Overall Loss 0.242766    Objective Loss 0.242766    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.097065    
2024-02-17 12:45:09,561 - --- validate (epoch=166)-----------
2024-02-17 12:45:09,562 - 10000 samples (128 per mini-batch)
2024-02-17 12:45:12,192 - Epoch: [166][   79/   79]    Loss 1.336991    Top1 66.440000    Top5 89.920000    
2024-02-17 12:45:12,305 - ==> Top1: 66.440    Top5: 89.920    Loss: 1.337

2024-02-17 12:45:12,324 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:45:12,325 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:45:12,404 - 

2024-02-17 12:45:12,405 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:45:22,472 - Epoch: [167][  100/  391]    Overall Loss 0.232500    Objective Loss 0.232500                                        LR 0.005522    Time 0.100599    
2024-02-17 12:45:32,060 - Epoch: [167][  200/  391]    Overall Loss 0.234312    Objective Loss 0.234312                                        LR 0.005522    Time 0.098217    
2024-02-17 12:45:41,668 - Epoch: [167][  300/  391]    Overall Loss 0.237491    Objective Loss 0.237491                                        LR 0.005522    Time 0.097486    
2024-02-17 12:45:50,091 - Epoch: [167][  391/  391]    Overall Loss 0.239577    Objective Loss 0.239577    Top1 90.865385    Top5 100.000000    LR 0.005522    Time 0.096329    
2024-02-17 12:45:50,290 - --- validate (epoch=167)-----------
2024-02-17 12:45:50,290 - 10000 samples (128 per mini-batch)
2024-02-17 12:45:53,025 - Epoch: [167][   79/   79]    Loss 1.324298    Top1 66.640000    Top5 90.110000    
2024-02-17 12:45:53,133 - ==> Top1: 66.640    Top5: 90.110    Loss: 1.324

2024-02-17 12:45:53,151 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:45:53,151 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:45:53,227 - 

2024-02-17 12:45:53,227 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:46:03,634 - Epoch: [168][  100/  391]    Overall Loss 0.222409    Objective Loss 0.222409                                        LR 0.005522    Time 0.103997    
2024-02-17 12:46:13,096 - Epoch: [168][  200/  391]    Overall Loss 0.229236    Objective Loss 0.229236                                        LR 0.005522    Time 0.099285    
2024-02-17 12:46:22,544 - Epoch: [168][  300/  391]    Overall Loss 0.233375    Objective Loss 0.233375                                        LR 0.005522    Time 0.097667    
2024-02-17 12:46:30,555 - Epoch: [168][  391/  391]    Overall Loss 0.234968    Objective Loss 0.234968    Top1 92.307692    Top5 100.000000    LR 0.005522    Time 0.095412    
2024-02-17 12:46:30,671 - --- validate (epoch=168)-----------
2024-02-17 12:46:30,671 - 10000 samples (128 per mini-batch)
2024-02-17 12:46:33,251 - Epoch: [168][   79/   79]    Loss 1.331299    Top1 66.520000    Top5 90.190000    
2024-02-17 12:46:33,370 - ==> Top1: 66.520    Top5: 90.190    Loss: 1.331

2024-02-17 12:46:33,388 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:46:33,389 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:46:33,465 - 

2024-02-17 12:46:33,466 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:46:43,530 - Epoch: [169][  100/  391]    Overall Loss 0.231639    Objective Loss 0.231639                                        LR 0.005522    Time 0.100572    
2024-02-17 12:46:53,016 - Epoch: [169][  200/  391]    Overall Loss 0.231629    Objective Loss 0.231629                                        LR 0.005522    Time 0.097692    
2024-02-17 12:47:02,190 - Epoch: [169][  300/  391]    Overall Loss 0.231205    Objective Loss 0.231205                                        LR 0.005522    Time 0.095692    
2024-02-17 12:47:10,625 - Epoch: [169][  391/  391]    Overall Loss 0.233654    Objective Loss 0.233654    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.094982    
2024-02-17 12:47:10,788 - --- validate (epoch=169)-----------
2024-02-17 12:47:10,789 - 10000 samples (128 per mini-batch)
2024-02-17 12:47:13,521 - Epoch: [169][   79/   79]    Loss 1.317482    Top1 66.820000    Top5 90.010000    
2024-02-17 12:47:13,707 - ==> Top1: 66.820    Top5: 90.010    Loss: 1.317

2024-02-17 12:47:13,726 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:47:13,727 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:47:13,803 - 

2024-02-17 12:47:13,804 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:47:24,083 - Epoch: [170][  100/  391]    Overall Loss 0.224247    Objective Loss 0.224247                                        LR 0.005522    Time 0.102668    
2024-02-17 12:47:33,637 - Epoch: [170][  200/  391]    Overall Loss 0.227131    Objective Loss 0.227131                                        LR 0.005522    Time 0.099080    
2024-02-17 12:47:43,476 - Epoch: [170][  300/  391]    Overall Loss 0.231135    Objective Loss 0.231135                                        LR 0.005522    Time 0.098836    
2024-02-17 12:47:51,874 - Epoch: [170][  391/  391]    Overall Loss 0.233916    Objective Loss 0.233916    Top1 92.307692    Top5 99.519231    LR 0.005522    Time 0.097300    
2024-02-17 12:47:52,010 - --- validate (epoch=170)-----------
2024-02-17 12:47:52,011 - 10000 samples (128 per mini-batch)
2024-02-17 12:47:54,640 - Epoch: [170][   79/   79]    Loss 1.330065    Top1 66.560000    Top5 90.030000    
2024-02-17 12:47:54,805 - ==> Top1: 66.560    Top5: 90.030    Loss: 1.330

2024-02-17 12:47:54,825 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:47:54,825 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:47:54,903 - 

2024-02-17 12:47:54,904 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:48:04,968 - Epoch: [171][  100/  391]    Overall Loss 0.220583    Objective Loss 0.220583                                        LR 0.005522    Time 0.100554    
2024-02-17 12:48:14,541 - Epoch: [171][  200/  391]    Overall Loss 0.222879    Objective Loss 0.222879                                        LR 0.005522    Time 0.098121    
2024-02-17 12:48:24,077 - Epoch: [171][  300/  391]    Overall Loss 0.225976    Objective Loss 0.225976                                        LR 0.005522    Time 0.097184    
2024-02-17 12:48:32,505 - Epoch: [171][  391/  391]    Overall Loss 0.226625    Objective Loss 0.226625    Top1 91.346154    Top5 100.000000    LR 0.005522    Time 0.096111    
2024-02-17 12:48:32,642 - --- validate (epoch=171)-----------
2024-02-17 12:48:32,643 - 10000 samples (128 per mini-batch)
2024-02-17 12:48:35,275 - Epoch: [171][   79/   79]    Loss 1.344760    Top1 66.800000    Top5 89.880000    
2024-02-17 12:48:35,475 - ==> Top1: 66.800    Top5: 89.880    Loss: 1.345

2024-02-17 12:48:35,493 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:48:35,493 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:48:35,574 - 

2024-02-17 12:48:35,574 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:48:46,021 - Epoch: [172][  100/  391]    Overall Loss 0.217787    Objective Loss 0.217787                                        LR 0.005522    Time 0.104374    
2024-02-17 12:48:55,427 - Epoch: [172][  200/  391]    Overall Loss 0.222174    Objective Loss 0.222174                                        LR 0.005522    Time 0.099196    
2024-02-17 12:49:04,852 - Epoch: [172][  300/  391]    Overall Loss 0.223789    Objective Loss 0.223789                                        LR 0.005522    Time 0.097531    
2024-02-17 12:49:13,177 - Epoch: [172][  391/  391]    Overall Loss 0.227632    Objective Loss 0.227632    Top1 90.865385    Top5 99.519231    LR 0.005522    Time 0.096112    
2024-02-17 12:49:13,336 - --- validate (epoch=172)-----------
2024-02-17 12:49:13,337 - 10000 samples (128 per mini-batch)
2024-02-17 12:49:16,305 - Epoch: [172][   79/   79]    Loss 1.352497    Top1 66.440000    Top5 89.680000    
2024-02-17 12:49:16,418 - ==> Top1: 66.440    Top5: 89.680    Loss: 1.352

2024-02-17 12:49:16,443 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:49:16,444 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:49:16,535 - 

2024-02-17 12:49:16,535 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:49:26,623 - Epoch: [173][  100/  391]    Overall Loss 0.221297    Objective Loss 0.221297                                        LR 0.005522    Time 0.100796    
2024-02-17 12:49:36,086 - Epoch: [173][  200/  391]    Overall Loss 0.223977    Objective Loss 0.223977                                        LR 0.005522    Time 0.097693    
2024-02-17 12:49:45,553 - Epoch: [173][  300/  391]    Overall Loss 0.227418    Objective Loss 0.227418                                        LR 0.005522    Time 0.096669    
2024-02-17 12:49:54,192 - Epoch: [173][  391/  391]    Overall Loss 0.229388    Objective Loss 0.229388    Top1 92.788462    Top5 100.000000    LR 0.005522    Time 0.096255    
2024-02-17 12:49:54,330 - --- validate (epoch=173)-----------
2024-02-17 12:49:54,331 - 10000 samples (128 per mini-batch)
2024-02-17 12:49:57,072 - Epoch: [173][   79/   79]    Loss 1.343826    Top1 66.600000    Top5 89.750000    
2024-02-17 12:49:57,257 - ==> Top1: 66.600    Top5: 89.750    Loss: 1.344

2024-02-17 12:49:57,278 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:49:57,278 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:49:57,354 - 

2024-02-17 12:49:57,354 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:50:07,641 - Epoch: [174][  100/  391]    Overall Loss 0.217410    Objective Loss 0.217410                                        LR 0.005522    Time 0.102794    
2024-02-17 12:50:17,147 - Epoch: [174][  200/  391]    Overall Loss 0.221184    Objective Loss 0.221184                                        LR 0.005522    Time 0.098904    
2024-02-17 12:50:26,683 - Epoch: [174][  300/  391]    Overall Loss 0.221912    Objective Loss 0.221912                                        LR 0.005522    Time 0.097707    
2024-02-17 12:50:35,137 - Epoch: [174][  391/  391]    Overall Loss 0.225677    Objective Loss 0.225677    Top1 90.865385    Top5 100.000000    LR 0.005522    Time 0.096577    
2024-02-17 12:50:35,309 - --- validate (epoch=174)-----------
2024-02-17 12:50:35,310 - 10000 samples (128 per mini-batch)
2024-02-17 12:50:38,021 - Epoch: [174][   79/   79]    Loss 1.349080    Top1 66.700000    Top5 90.000000    
2024-02-17 12:50:38,146 - ==> Top1: 66.700    Top5: 90.000    Loss: 1.349

2024-02-17 12:50:38,164 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:50:38,165 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:50:38,243 - 

2024-02-17 12:50:38,243 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:50:48,030 - Epoch: [175][  100/  391]    Overall Loss 0.200720    Objective Loss 0.200720                                        LR 0.001298    Time 0.097789    
2024-02-17 12:50:57,266 - Epoch: [175][  200/  391]    Overall Loss 0.199762    Objective Loss 0.199762                                        LR 0.001298    Time 0.095051    
2024-02-17 12:51:06,701 - Epoch: [175][  300/  391]    Overall Loss 0.198328    Objective Loss 0.198328                                        LR 0.001298    Time 0.094801    
2024-02-17 12:51:15,360 - Epoch: [175][  391/  391]    Overall Loss 0.198144    Objective Loss 0.198144    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.094872    
2024-02-17 12:51:15,519 - --- validate (epoch=175)-----------
2024-02-17 12:51:15,519 - 10000 samples (128 per mini-batch)
2024-02-17 12:51:18,273 - Epoch: [175][   79/   79]    Loss 1.320566    Top1 67.480000    Top5 90.300000    
2024-02-17 12:51:18,384 - ==> Top1: 67.480    Top5: 90.300    Loss: 1.321

2024-02-17 12:51:18,404 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:51:18,404 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:51:18,497 - 

2024-02-17 12:51:18,497 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:51:28,391 - Epoch: [176][  100/  391]    Overall Loss 0.194081    Objective Loss 0.194081                                        LR 0.001298    Time 0.098864    
2024-02-17 12:51:37,911 - Epoch: [176][  200/  391]    Overall Loss 0.188834    Objective Loss 0.188834                                        LR 0.001298    Time 0.097009    
2024-02-17 12:51:47,485 - Epoch: [176][  300/  391]    Overall Loss 0.189702    Objective Loss 0.189702                                        LR 0.001298    Time 0.096572    
2024-02-17 12:51:56,274 - Epoch: [176][  391/  391]    Overall Loss 0.191612    Objective Loss 0.191612    Top1 92.307692    Top5 100.000000    LR 0.001298    Time 0.096563    
2024-02-17 12:51:56,434 - --- validate (epoch=176)-----------
2024-02-17 12:51:56,434 - 10000 samples (128 per mini-batch)
2024-02-17 12:51:59,164 - Epoch: [176][   79/   79]    Loss 1.327038    Top1 67.030000    Top5 89.960000    
2024-02-17 12:51:59,320 - ==> Top1: 67.030    Top5: 89.960    Loss: 1.327

2024-02-17 12:51:59,339 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:51:59,339 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:51:59,424 - 

2024-02-17 12:51:59,425 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:52:09,222 - Epoch: [177][  100/  391]    Overall Loss 0.177925    Objective Loss 0.177925                                        LR 0.001298    Time 0.097892    
2024-02-17 12:52:18,841 - Epoch: [177][  200/  391]    Overall Loss 0.182974    Objective Loss 0.182974                                        LR 0.001298    Time 0.097018    
2024-02-17 12:52:28,443 - Epoch: [177][  300/  391]    Overall Loss 0.182750    Objective Loss 0.182750                                        LR 0.001298    Time 0.096671    
2024-02-17 12:52:37,169 - Epoch: [177][  391/  391]    Overall Loss 0.184228    Objective Loss 0.184228    Top1 93.750000    Top5 100.000000    LR 0.001298    Time 0.096479    
2024-02-17 12:52:37,324 - --- validate (epoch=177)-----------
2024-02-17 12:52:37,325 - 10000 samples (128 per mini-batch)
2024-02-17 12:52:40,165 - Epoch: [177][   79/   79]    Loss 1.328166    Top1 67.210000    Top5 90.120000    
2024-02-17 12:52:40,295 - ==> Top1: 67.210    Top5: 90.120    Loss: 1.328

2024-02-17 12:52:40,312 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:52:40,313 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:52:40,388 - 

2024-02-17 12:52:40,388 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:52:50,862 - Epoch: [178][  100/  391]    Overall Loss 0.176956    Objective Loss 0.176956                                        LR 0.001298    Time 0.104657    
2024-02-17 12:53:00,335 - Epoch: [178][  200/  391]    Overall Loss 0.180543    Objective Loss 0.180543                                        LR 0.001298    Time 0.099669    
2024-02-17 12:53:09,974 - Epoch: [178][  300/  391]    Overall Loss 0.182040    Objective Loss 0.182040                                        LR 0.001298    Time 0.098559    
2024-02-17 12:53:18,429 - Epoch: [178][  391/  391]    Overall Loss 0.182313    Objective Loss 0.182313    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.097234    
2024-02-17 12:53:18,607 - --- validate (epoch=178)-----------
2024-02-17 12:53:18,608 - 10000 samples (128 per mini-batch)
2024-02-17 12:53:21,670 - Epoch: [178][   79/   79]    Loss 1.330774    Top1 66.990000    Top5 90.280000    
2024-02-17 12:53:21,783 - ==> Top1: 66.990    Top5: 90.280    Loss: 1.331

2024-02-17 12:53:21,804 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:53:21,804 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:53:21,886 - 

2024-02-17 12:53:21,887 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:53:32,079 - Epoch: [179][  100/  391]    Overall Loss 0.183629    Objective Loss 0.183629                                        LR 0.001298    Time 0.101845    
2024-02-17 12:53:41,563 - Epoch: [179][  200/  391]    Overall Loss 0.180428    Objective Loss 0.180428                                        LR 0.001298    Time 0.098320    
2024-02-17 12:53:51,110 - Epoch: [179][  300/  391]    Overall Loss 0.180396    Objective Loss 0.180396                                        LR 0.001298    Time 0.097354    
2024-02-17 12:53:59,902 - Epoch: [179][  391/  391]    Overall Loss 0.180150    Objective Loss 0.180150    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.097170    
2024-02-17 12:54:00,054 - --- validate (epoch=179)-----------
2024-02-17 12:54:00,054 - 10000 samples (128 per mini-batch)
2024-02-17 12:54:03,047 - Epoch: [179][   79/   79]    Loss 1.327668    Top1 67.260000    Top5 90.160000    
2024-02-17 12:54:03,198 - ==> Top1: 67.260    Top5: 90.160    Loss: 1.328

2024-02-17 12:54:03,217 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:54:03,217 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:54:03,299 - 

2024-02-17 12:54:03,300 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:54:13,313 - Epoch: [180][  100/  391]    Overall Loss 0.178427    Objective Loss 0.178427                                        LR 0.001298    Time 0.100055    
2024-02-17 12:54:22,771 - Epoch: [180][  200/  391]    Overall Loss 0.177399    Objective Loss 0.177399                                        LR 0.001298    Time 0.097299    
2024-02-17 12:54:32,182 - Epoch: [180][  300/  391]    Overall Loss 0.179766    Objective Loss 0.179766                                        LR 0.001298    Time 0.096218    
2024-02-17 12:54:40,754 - Epoch: [180][  391/  391]    Overall Loss 0.180987    Objective Loss 0.180987    Top1 93.750000    Top5 100.000000    LR 0.001298    Time 0.095737    
2024-02-17 12:54:40,932 - --- validate (epoch=180)-----------
2024-02-17 12:54:40,932 - 10000 samples (128 per mini-batch)
2024-02-17 12:54:43,879 - Epoch: [180][   79/   79]    Loss 1.322918    Top1 67.360000    Top5 90.070000    
2024-02-17 12:54:43,983 - ==> Top1: 67.360    Top5: 90.070    Loss: 1.323

2024-02-17 12:54:44,004 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:54:44,004 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:54:44,079 - 

2024-02-17 12:54:44,079 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:54:53,304 - Epoch: [181][  100/  391]    Overall Loss 0.174672    Objective Loss 0.174672                                        LR 0.001298    Time 0.092178    
2024-02-17 12:55:01,832 - Epoch: [181][  200/  391]    Overall Loss 0.177580    Objective Loss 0.177580                                        LR 0.001298    Time 0.088705    
2024-02-17 12:55:11,217 - Epoch: [181][  300/  391]    Overall Loss 0.180572    Objective Loss 0.180572                                        LR 0.001298    Time 0.090404    
2024-02-17 12:55:19,869 - Epoch: [181][  391/  391]    Overall Loss 0.180137    Objective Loss 0.180137    Top1 97.596154    Top5 100.000000    LR 0.001298    Time 0.091480    
2024-02-17 12:55:20,048 - --- validate (epoch=181)-----------
2024-02-17 12:55:20,048 - 10000 samples (128 per mini-batch)
2024-02-17 12:55:23,151 - Epoch: [181][   79/   79]    Loss 1.326092    Top1 67.060000    Top5 89.890000    
2024-02-17 12:55:23,300 - ==> Top1: 67.060    Top5: 89.890    Loss: 1.326

2024-02-17 12:55:23,322 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:55:23,322 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:55:23,403 - 

2024-02-17 12:55:23,403 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:55:32,817 - Epoch: [182][  100/  391]    Overall Loss 0.170630    Objective Loss 0.170630                                        LR 0.001298    Time 0.094057    
2024-02-17 12:55:42,320 - Epoch: [182][  200/  391]    Overall Loss 0.172889    Objective Loss 0.172889                                        LR 0.001298    Time 0.094519    
2024-02-17 12:55:51,329 - Epoch: [182][  300/  391]    Overall Loss 0.174278    Objective Loss 0.174278                                        LR 0.001298    Time 0.093027    
2024-02-17 12:55:57,840 - Epoch: [182][  391/  391]    Overall Loss 0.176877    Objective Loss 0.176877    Top1 93.750000    Top5 99.519231    LR 0.001298    Time 0.088019    
2024-02-17 12:55:58,086 - --- validate (epoch=182)-----------
2024-02-17 12:55:58,087 - 10000 samples (128 per mini-batch)
2024-02-17 12:56:00,914 - Epoch: [182][   79/   79]    Loss 1.325496    Top1 67.070000    Top5 90.160000    
2024-02-17 12:56:01,102 - ==> Top1: 67.070    Top5: 90.160    Loss: 1.325

2024-02-17 12:56:01,124 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:56:01,124 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:56:01,205 - 

2024-02-17 12:56:01,205 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:56:11,416 - Epoch: [183][  100/  391]    Overall Loss 0.172277    Objective Loss 0.172277                                        LR 0.001298    Time 0.102035    
2024-02-17 12:56:20,869 - Epoch: [183][  200/  391]    Overall Loss 0.172480    Objective Loss 0.172480                                        LR 0.001298    Time 0.098257    
2024-02-17 12:56:30,688 - Epoch: [183][  300/  391]    Overall Loss 0.173455    Objective Loss 0.173455                                        LR 0.001298    Time 0.098218    
2024-02-17 12:56:39,394 - Epoch: [183][  391/  391]    Overall Loss 0.173278    Objective Loss 0.173278    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.097613    
2024-02-17 12:56:39,573 - --- validate (epoch=183)-----------
2024-02-17 12:56:39,574 - 10000 samples (128 per mini-batch)
2024-02-17 12:56:42,429 - Epoch: [183][   79/   79]    Loss 1.332109    Top1 67.000000    Top5 90.010000    
2024-02-17 12:56:42,551 - ==> Top1: 67.000    Top5: 90.010    Loss: 1.332

2024-02-17 12:56:42,573 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:56:42,573 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:56:42,649 - 

2024-02-17 12:56:42,649 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:56:52,511 - Epoch: [184][  100/  391]    Overall Loss 0.166013    Objective Loss 0.166013                                        LR 0.001298    Time 0.098545    
2024-02-17 12:57:02,002 - Epoch: [184][  200/  391]    Overall Loss 0.170177    Objective Loss 0.170177                                        LR 0.001298    Time 0.096702    
2024-02-17 12:57:11,434 - Epoch: [184][  300/  391]    Overall Loss 0.173037    Objective Loss 0.173037                                        LR 0.001298    Time 0.095893    
2024-02-17 12:57:20,018 - Epoch: [184][  391/  391]    Overall Loss 0.174257    Objective Loss 0.174257    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.095517    
2024-02-17 12:57:20,183 - --- validate (epoch=184)-----------
2024-02-17 12:57:20,183 - 10000 samples (128 per mini-batch)
2024-02-17 12:57:22,891 - Epoch: [184][   79/   79]    Loss 1.328481    Top1 66.740000    Top5 90.080000    
2024-02-17 12:57:23,084 - ==> Top1: 66.740    Top5: 90.080    Loss: 1.328

2024-02-17 12:57:23,104 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:57:23,104 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:57:23,180 - 

2024-02-17 12:57:23,181 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:57:32,755 - Epoch: [185][  100/  391]    Overall Loss 0.169289    Objective Loss 0.169289                                        LR 0.001298    Time 0.095671    
2024-02-17 12:57:42,282 - Epoch: [185][  200/  391]    Overall Loss 0.170546    Objective Loss 0.170546                                        LR 0.001298    Time 0.095446    
2024-02-17 12:57:52,021 - Epoch: [185][  300/  391]    Overall Loss 0.170117    Objective Loss 0.170117                                        LR 0.001298    Time 0.096077    
2024-02-17 12:58:00,692 - Epoch: [185][  391/  391]    Overall Loss 0.170657    Objective Loss 0.170657    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.095881    
2024-02-17 12:58:00,860 - --- validate (epoch=185)-----------
2024-02-17 12:58:00,860 - 10000 samples (128 per mini-batch)
2024-02-17 12:58:03,459 - Epoch: [185][   79/   79]    Loss 1.333425    Top1 67.110000    Top5 90.130000    
2024-02-17 12:58:03,587 - ==> Top1: 67.110    Top5: 90.130    Loss: 1.333

2024-02-17 12:58:03,602 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:58:03,602 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:58:03,680 - 

2024-02-17 12:58:03,681 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:58:13,534 - Epoch: [186][  100/  391]    Overall Loss 0.168003    Objective Loss 0.168003                                        LR 0.001298    Time 0.098461    
2024-02-17 12:58:22,820 - Epoch: [186][  200/  391]    Overall Loss 0.172662    Objective Loss 0.172662                                        LR 0.001298    Time 0.095634    
2024-02-17 12:58:29,912 - Epoch: [186][  300/  391]    Overall Loss 0.170748    Objective Loss 0.170748                                        LR 0.001298    Time 0.087384    
2024-02-17 12:58:38,442 - Epoch: [186][  391/  391]    Overall Loss 0.171379    Objective Loss 0.171379    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.088852    
2024-02-17 12:58:38,618 - --- validate (epoch=186)-----------
2024-02-17 12:58:38,619 - 10000 samples (128 per mini-batch)
2024-02-17 12:58:41,367 - Epoch: [186][   79/   79]    Loss 1.338057    Top1 66.920000    Top5 90.140000    
2024-02-17 12:58:41,478 - ==> Top1: 66.920    Top5: 90.140    Loss: 1.338

2024-02-17 12:58:41,497 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:58:41,497 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:58:41,573 - 

2024-02-17 12:58:41,574 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:58:51,283 - Epoch: [187][  100/  391]    Overall Loss 0.171068    Objective Loss 0.171068                                        LR 0.001298    Time 0.097019    
2024-02-17 12:59:00,780 - Epoch: [187][  200/  391]    Overall Loss 0.169351    Objective Loss 0.169351                                        LR 0.001298    Time 0.095972    
2024-02-17 12:59:10,376 - Epoch: [187][  300/  391]    Overall Loss 0.170330    Objective Loss 0.170330                                        LR 0.001298    Time 0.095949    
2024-02-17 12:59:19,000 - Epoch: [187][  391/  391]    Overall Loss 0.171265    Objective Loss 0.171265    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.095664    
2024-02-17 12:59:19,197 - --- validate (epoch=187)-----------
2024-02-17 12:59:19,197 - 10000 samples (128 per mini-batch)
2024-02-17 12:59:21,957 - Epoch: [187][   79/   79]    Loss 1.328471    Top1 66.900000    Top5 90.030000    
2024-02-17 12:59:22,074 - ==> Top1: 66.900    Top5: 90.030    Loss: 1.328

2024-02-17 12:59:22,093 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:59:22,093 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 12:59:22,171 - 

2024-02-17 12:59:22,171 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:59:32,204 - Epoch: [188][  100/  391]    Overall Loss 0.169394    Objective Loss 0.169394                                        LR 0.001298    Time 0.100249    
2024-02-17 12:59:41,811 - Epoch: [188][  200/  391]    Overall Loss 0.168713    Objective Loss 0.168713                                        LR 0.001298    Time 0.098135    
2024-02-17 12:59:51,566 - Epoch: [188][  300/  391]    Overall Loss 0.168231    Objective Loss 0.168231                                        LR 0.001298    Time 0.097923    
2024-02-17 13:00:00,279 - Epoch: [188][  391/  391]    Overall Loss 0.168887    Objective Loss 0.168887    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.097404    
2024-02-17 13:00:00,410 - --- validate (epoch=188)-----------
2024-02-17 13:00:00,410 - 10000 samples (128 per mini-batch)
2024-02-17 13:00:03,119 - Epoch: [188][   79/   79]    Loss 1.331676    Top1 66.910000    Top5 90.020000    
2024-02-17 13:00:03,250 - ==> Top1: 66.910    Top5: 90.020    Loss: 1.332

2024-02-17 13:00:03,270 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:00:03,271 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 13:00:03,346 - 

2024-02-17 13:00:03,347 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:00:13,222 - Epoch: [189][  100/  391]    Overall Loss 0.165290    Objective Loss 0.165290                                        LR 0.001298    Time 0.098679    
2024-02-17 13:00:22,705 - Epoch: [189][  200/  391]    Overall Loss 0.167899    Objective Loss 0.167899                                        LR 0.001298    Time 0.096727    
2024-02-17 13:00:32,484 - Epoch: [189][  300/  391]    Overall Loss 0.167915    Objective Loss 0.167915                                        LR 0.001298    Time 0.097068    
2024-02-17 13:00:41,526 - Epoch: [189][  391/  391]    Overall Loss 0.170486    Objective Loss 0.170486    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.097589    
2024-02-17 13:00:41,682 - --- validate (epoch=189)-----------
2024-02-17 13:00:41,683 - 10000 samples (128 per mini-batch)
2024-02-17 13:00:44,269 - Epoch: [189][   79/   79]    Loss 1.342635    Top1 66.830000    Top5 90.030000    
2024-02-17 13:00:44,441 - ==> Top1: 66.830    Top5: 90.030    Loss: 1.343

2024-02-17 13:00:44,460 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:00:44,460 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 13:00:44,537 - 

2024-02-17 13:00:44,537 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:00:54,561 - Epoch: [190][  100/  391]    Overall Loss 0.161642    Objective Loss 0.161642                                        LR 0.001298    Time 0.100160    
2024-02-17 13:01:02,337 - Epoch: [190][  200/  391]    Overall Loss 0.163029    Objective Loss 0.163029                                        LR 0.001298    Time 0.088940    
2024-02-17 13:01:11,946 - Epoch: [190][  300/  391]    Overall Loss 0.164693    Objective Loss 0.164693                                        LR 0.001298    Time 0.091309    
2024-02-17 13:01:20,637 - Epoch: [190][  391/  391]    Overall Loss 0.166364    Objective Loss 0.166364    Top1 91.826923    Top5 100.000000    LR 0.001298    Time 0.092273    
2024-02-17 13:01:20,780 - --- validate (epoch=190)-----------
2024-02-17 13:01:20,780 - 10000 samples (128 per mini-batch)
2024-02-17 13:01:23,397 - Epoch: [190][   79/   79]    Loss 1.333915    Top1 66.940000    Top5 90.200000    
2024-02-17 13:01:23,561 - ==> Top1: 66.940    Top5: 90.200    Loss: 1.334

2024-02-17 13:01:23,579 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:01:23,580 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 13:01:23,656 - 

2024-02-17 13:01:23,656 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:01:33,785 - Epoch: [191][  100/  391]    Overall Loss 0.167240    Objective Loss 0.167240                                        LR 0.001298    Time 0.101213    
2024-02-17 13:01:42,996 - Epoch: [191][  200/  391]    Overall Loss 0.165507    Objective Loss 0.165507                                        LR 0.001298    Time 0.096637    
2024-02-17 13:01:52,541 - Epoch: [191][  300/  391]    Overall Loss 0.166577    Objective Loss 0.166577                                        LR 0.001298    Time 0.096228    
2024-02-17 13:02:01,322 - Epoch: [191][  391/  391]    Overall Loss 0.167198    Objective Loss 0.167198    Top1 97.115385    Top5 99.519231    LR 0.001298    Time 0.096278    
2024-02-17 13:02:01,474 - --- validate (epoch=191)-----------
2024-02-17 13:02:01,475 - 10000 samples (128 per mini-batch)
2024-02-17 13:02:04,152 - Epoch: [191][   79/   79]    Loss 1.350309    Top1 67.060000    Top5 90.090000    
2024-02-17 13:02:04,284 - ==> Top1: 67.060    Top5: 90.090    Loss: 1.350

2024-02-17 13:02:04,304 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:02:04,304 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 13:02:04,384 - 

2024-02-17 13:02:04,384 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:02:14,404 - Epoch: [192][  100/  391]    Overall Loss 0.163152    Objective Loss 0.163152                                        LR 0.001298    Time 0.100129    
2024-02-17 13:02:23,678 - Epoch: [192][  200/  391]    Overall Loss 0.161818    Objective Loss 0.161818                                        LR 0.001298    Time 0.096410    
2024-02-17 13:02:33,170 - Epoch: [192][  300/  391]    Overall Loss 0.162984    Objective Loss 0.162984                                        LR 0.001298    Time 0.095898    
2024-02-17 13:02:41,848 - Epoch: [192][  391/  391]    Overall Loss 0.164090    Objective Loss 0.164090    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.095762    
2024-02-17 13:02:41,980 - --- validate (epoch=192)-----------
2024-02-17 13:02:41,981 - 10000 samples (128 per mini-batch)
2024-02-17 13:02:45,035 - Epoch: [192][   79/   79]    Loss 1.332412    Top1 66.850000    Top5 90.100000    
2024-02-17 13:02:45,149 - ==> Top1: 66.850    Top5: 90.100    Loss: 1.332

2024-02-17 13:02:45,167 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:02:45,168 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 13:02:45,243 - 

2024-02-17 13:02:45,244 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:02:55,295 - Epoch: [193][  100/  391]    Overall Loss 0.161500    Objective Loss 0.161500                                        LR 0.001298    Time 0.100436    
2024-02-17 13:03:04,499 - Epoch: [193][  200/  391]    Overall Loss 0.163752    Objective Loss 0.163752                                        LR 0.001298    Time 0.096215    
2024-02-17 13:03:11,769 - Epoch: [193][  300/  391]    Overall Loss 0.164758    Objective Loss 0.164758                                        LR 0.001298    Time 0.088363    
2024-02-17 13:03:19,403 - Epoch: [193][  391/  391]    Overall Loss 0.166022    Objective Loss 0.166022    Top1 94.230769    Top5 99.519231    LR 0.001298    Time 0.087311    
2024-02-17 13:03:19,521 - --- validate (epoch=193)-----------
2024-02-17 13:03:19,522 - 10000 samples (128 per mini-batch)
2024-02-17 13:03:22,320 - Epoch: [193][   79/   79]    Loss 1.348482    Top1 66.940000    Top5 90.210000    
2024-02-17 13:03:22,478 - ==> Top1: 66.940    Top5: 90.210    Loss: 1.348

2024-02-17 13:03:22,498 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:03:22,498 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 13:03:22,577 - 

2024-02-17 13:03:22,578 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:03:32,864 - Epoch: [194][  100/  391]    Overall Loss 0.159389    Objective Loss 0.159389                                        LR 0.001298    Time 0.102778    
2024-02-17 13:03:42,450 - Epoch: [194][  200/  391]    Overall Loss 0.162840    Objective Loss 0.162840                                        LR 0.001298    Time 0.099295    
2024-02-17 13:03:51,988 - Epoch: [194][  300/  391]    Overall Loss 0.164014    Objective Loss 0.164014                                        LR 0.001298    Time 0.097972    
2024-02-17 13:04:00,753 - Epoch: [194][  391/  391]    Overall Loss 0.163814    Objective Loss 0.163814    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.097577    
2024-02-17 13:04:00,900 - --- validate (epoch=194)-----------
2024-02-17 13:04:00,901 - 10000 samples (128 per mini-batch)
2024-02-17 13:04:03,608 - Epoch: [194][   79/   79]    Loss 1.345392    Top1 66.930000    Top5 90.060000    
2024-02-17 13:04:03,713 - ==> Top1: 66.930    Top5: 90.060    Loss: 1.345

2024-02-17 13:04:03,732 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:04:03,732 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 13:04:03,809 - 

2024-02-17 13:04:03,809 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:04:13,777 - Epoch: [195][  100/  391]    Overall Loss 0.162932    Objective Loss 0.162932                                        LR 0.001298    Time 0.099606    
2024-02-17 13:04:23,113 - Epoch: [195][  200/  391]    Overall Loss 0.164049    Objective Loss 0.164049                                        LR 0.001298    Time 0.096457    
2024-02-17 13:04:32,705 - Epoch: [195][  300/  391]    Overall Loss 0.164216    Objective Loss 0.164216                                        LR 0.001298    Time 0.096263    
2024-02-17 13:04:41,375 - Epoch: [195][  391/  391]    Overall Loss 0.164087    Objective Loss 0.164087    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.096021    
2024-02-17 13:04:41,538 - --- validate (epoch=195)-----------
2024-02-17 13:04:41,539 - 10000 samples (128 per mini-batch)
2024-02-17 13:04:44,142 - Epoch: [195][   79/   79]    Loss 1.341553    Top1 66.670000    Top5 90.110000    
2024-02-17 13:04:44,300 - ==> Top1: 66.670    Top5: 90.110    Loss: 1.342

2024-02-17 13:04:44,320 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:04:44,320 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 13:04:44,394 - 

2024-02-17 13:04:44,395 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:04:54,585 - Epoch: [196][  100/  391]    Overall Loss 0.164768    Objective Loss 0.164768                                        LR 0.001298    Time 0.101835    
2024-02-17 13:05:03,974 - Epoch: [196][  200/  391]    Overall Loss 0.163550    Objective Loss 0.163550                                        LR 0.001298    Time 0.097834    
2024-02-17 13:05:13,453 - Epoch: [196][  300/  391]    Overall Loss 0.163167    Objective Loss 0.163167                                        LR 0.001298    Time 0.096804    
2024-02-17 13:05:21,550 - Epoch: [196][  391/  391]    Overall Loss 0.163966    Objective Loss 0.163966    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.094970    
2024-02-17 13:05:21,691 - --- validate (epoch=196)-----------
2024-02-17 13:05:21,692 - 10000 samples (128 per mini-batch)
2024-02-17 13:05:24,544 - Epoch: [196][   79/   79]    Loss 1.365839    Top1 66.750000    Top5 89.960000    
2024-02-17 13:05:24,670 - ==> Top1: 66.750    Top5: 89.960    Loss: 1.366

2024-02-17 13:05:24,690 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:05:24,691 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 13:05:24,767 - 

2024-02-17 13:05:24,768 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:05:34,818 - Epoch: [197][  100/  391]    Overall Loss 0.160814    Objective Loss 0.160814                                        LR 0.001298    Time 0.100428    
2024-02-17 13:05:44,265 - Epoch: [197][  200/  391]    Overall Loss 0.160856    Objective Loss 0.160856                                        LR 0.001298    Time 0.097424    
2024-02-17 13:05:53,876 - Epoch: [197][  300/  391]    Overall Loss 0.162088    Objective Loss 0.162088                                        LR 0.001298    Time 0.096971    
2024-02-17 13:06:02,310 - Epoch: [197][  391/  391]    Overall Loss 0.162756    Objective Loss 0.162756    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.095960    
2024-02-17 13:06:02,459 - --- validate (epoch=197)-----------
2024-02-17 13:06:02,460 - 10000 samples (128 per mini-batch)
2024-02-17 13:06:05,138 - Epoch: [197][   79/   79]    Loss 1.361022    Top1 66.480000    Top5 90.110000    
2024-02-17 13:06:05,268 - ==> Top1: 66.480    Top5: 90.110    Loss: 1.361

2024-02-17 13:06:05,287 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:06:05,287 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 13:06:05,360 - 

2024-02-17 13:06:05,361 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:06:15,380 - Epoch: [198][  100/  391]    Overall Loss 0.158934    Objective Loss 0.158934                                        LR 0.001298    Time 0.100123    
2024-02-17 13:06:24,992 - Epoch: [198][  200/  391]    Overall Loss 0.158053    Objective Loss 0.158053                                        LR 0.001298    Time 0.098096    
2024-02-17 13:06:34,499 - Epoch: [198][  300/  391]    Overall Loss 0.161427    Objective Loss 0.161427                                        LR 0.001298    Time 0.097072    
2024-02-17 13:06:42,960 - Epoch: [198][  391/  391]    Overall Loss 0.162402    Objective Loss 0.162402    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.096109    
2024-02-17 13:06:43,095 - --- validate (epoch=198)-----------
2024-02-17 13:06:43,096 - 10000 samples (128 per mini-batch)
2024-02-17 13:06:46,111 - Epoch: [198][   79/   79]    Loss 1.353372    Top1 67.070000    Top5 90.060000    
2024-02-17 13:06:46,287 - ==> Top1: 67.070    Top5: 90.060    Loss: 1.353

2024-02-17 13:06:46,303 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:06:46,304 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 13:06:46,384 - 

2024-02-17 13:06:46,384 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:06:56,312 - Epoch: [199][  100/  391]    Overall Loss 0.161186    Objective Loss 0.161186                                        LR 0.001298    Time 0.099208    
2024-02-17 13:07:05,703 - Epoch: [199][  200/  391]    Overall Loss 0.159144    Objective Loss 0.159144                                        LR 0.001298    Time 0.096534    
2024-02-17 13:07:15,194 - Epoch: [199][  300/  391]    Overall Loss 0.159413    Objective Loss 0.159413                                        LR 0.001298    Time 0.095978    
2024-02-17 13:07:23,313 - Epoch: [199][  391/  391]    Overall Loss 0.160005    Objective Loss 0.160005    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.094396    
2024-02-17 13:07:23,465 - --- validate (epoch=199)-----------
2024-02-17 13:07:23,466 - 10000 samples (128 per mini-batch)
2024-02-17 13:07:26,123 - Epoch: [199][   79/   79]    Loss 1.355071    Top1 66.850000    Top5 90.010000    
2024-02-17 13:07:26,301 - ==> Top1: 66.850    Top5: 90.010    Loss: 1.355

2024-02-17 13:07:26,320 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:07:26,320 - Saving checkpoint to: logs/2024.02.17-105234/checkpoint.pth.tar
2024-02-17 13:07:26,394 - 

2024-02-17 13:07:26,394 - Initiating quantization aware training (QAT)...
2024-02-17 13:07:26,518 - 

2024-02-17 13:07:26,518 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:07:40,251 - Epoch: [200][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.137253    
2024-02-17 13:07:53,040 - Epoch: [200][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132549    
2024-02-17 13:08:05,562 - Epoch: [200][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130089    
2024-02-17 13:08:17,163 - Epoch: [200][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 2.884615    Top5 6.250000    LR 0.001298    Time 0.129468    
2024-02-17 13:08:17,333 - --- validate (epoch=200)-----------
2024-02-17 13:08:17,334 - 10000 samples (128 per mini-batch)
2024-02-17 13:08:23,127 - Epoch: [200][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:08:23,278 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:08:23,294 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:08:23,295 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:08:23,376 - 

2024-02-17 13:08:23,376 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:08:36,875 - Epoch: [201][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.134899    
2024-02-17 13:08:49,747 - Epoch: [201][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131781    
2024-02-17 13:09:01,980 - Epoch: [201][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128615    
2024-02-17 13:09:13,400 - Epoch: [201][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 0.961538    LR 0.001298    Time 0.127877    
2024-02-17 13:09:13,616 - --- validate (epoch=201)-----------
2024-02-17 13:09:13,617 - 10000 samples (128 per mini-batch)
2024-02-17 13:09:20,054 - Epoch: [201][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:09:20,175 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:09:20,192 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 201]
2024-02-17 13:09:20,192 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:09:20,275 - 

2024-02-17 13:09:20,275 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:09:33,527 - Epoch: [202][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132438    
2024-02-17 13:09:46,691 - Epoch: [202][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132011    
2024-02-17 13:09:59,555 - Epoch: [202][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130870    
2024-02-17 13:10:11,140 - Epoch: [202][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 3.846154    LR 0.001298    Time 0.130030    
2024-02-17 13:10:11,251 - --- validate (epoch=202)-----------
2024-02-17 13:10:11,252 - 10000 samples (128 per mini-batch)
2024-02-17 13:10:16,546 - Epoch: [202][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:10:16,634 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:10:16,841 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 202]
2024-02-17 13:10:16,841 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:10:16,919 - 

2024-02-17 13:10:16,919 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:10:30,212 - Epoch: [203][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132847    
2024-02-17 13:10:43,101 - Epoch: [203][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130841    
2024-02-17 13:10:56,423 - Epoch: [203][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131616    
2024-02-17 13:11:08,412 - Epoch: [203][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 4.326923    LR 0.001298    Time 0.131634    
2024-02-17 13:11:08,607 - --- validate (epoch=203)-----------
2024-02-17 13:11:08,608 - 10000 samples (128 per mini-batch)
2024-02-17 13:11:14,266 - Epoch: [203][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:11:14,400 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:11:14,412 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 203]
2024-02-17 13:11:14,412 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:11:14,508 - 

2024-02-17 13:11:14,509 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:11:27,700 - Epoch: [204][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131832    
2024-02-17 13:11:40,700 - Epoch: [204][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130886    
2024-02-17 13:11:52,160 - Epoch: [204][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125443    
2024-02-17 13:12:03,369 - Epoch: [204][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 5.769231    LR 0.001298    Time 0.124905    
2024-02-17 13:12:03,502 - --- validate (epoch=204)-----------
2024-02-17 13:12:03,503 - 10000 samples (128 per mini-batch)
2024-02-17 13:12:08,550 - Epoch: [204][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:12:08,651 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:12:08,667 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 204]
2024-02-17 13:12:08,667 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:12:08,731 - 

2024-02-17 13:12:08,731 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:12:21,263 - Epoch: [205][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125252    
2024-02-17 13:12:34,163 - Epoch: [205][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127098    
2024-02-17 13:12:47,063 - Epoch: [205][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127716    
2024-02-17 13:12:58,872 - Epoch: [205][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 5.769231    LR 0.001298    Time 0.128180    
2024-02-17 13:12:59,010 - --- validate (epoch=205)-----------
2024-02-17 13:12:59,011 - 10000 samples (128 per mini-batch)
2024-02-17 13:13:04,584 - Epoch: [205][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:13:04,758 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:13:04,776 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 205]
2024-02-17 13:13:04,776 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:13:04,855 - 

2024-02-17 13:13:04,855 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:13:18,412 - Epoch: [206][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135485    
2024-02-17 13:13:31,408 - Epoch: [206][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132696    
2024-02-17 13:13:44,185 - Epoch: [206][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131036    
2024-02-17 13:13:55,905 - Epoch: [206][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 2.884615    LR 0.001298    Time 0.130501    
2024-02-17 13:13:56,062 - --- validate (epoch=206)-----------
2024-02-17 13:13:56,062 - 10000 samples (128 per mini-batch)
2024-02-17 13:14:02,115 - Epoch: [206][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:14:02,265 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:14:02,284 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 206]
2024-02-17 13:14:02,285 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:14:02,365 - 

2024-02-17 13:14:02,366 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:14:15,538 - Epoch: [207][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131638    
2024-02-17 13:14:28,340 - Epoch: [207][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129804    
2024-02-17 13:14:41,341 - Epoch: [207][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129854    
2024-02-17 13:14:53,005 - Epoch: [207][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 6.250000    LR 0.001298    Time 0.129452    
2024-02-17 13:14:53,177 - --- validate (epoch=207)-----------
2024-02-17 13:14:53,178 - 10000 samples (128 per mini-batch)
2024-02-17 13:14:58,007 - Epoch: [207][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:14:58,101 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:14:58,117 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 207]
2024-02-17 13:14:58,117 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:14:58,191 - 

2024-02-17 13:14:58,191 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:15:10,974 - Epoch: [208][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127737    
2024-02-17 13:15:23,909 - Epoch: [208][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128518    
2024-02-17 13:15:36,864 - Epoch: [208][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128843    
2024-02-17 13:15:48,698 - Epoch: [208][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 3.846154    LR 0.001298    Time 0.129111    
2024-02-17 13:15:48,876 - --- validate (epoch=208)-----------
2024-02-17 13:15:48,876 - 10000 samples (128 per mini-batch)
2024-02-17 13:15:55,541 - Epoch: [208][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:15:55,684 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:15:55,705 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:15:55,705 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:15:55,788 - 

2024-02-17 13:15:55,788 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:16:09,370 - Epoch: [209][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135736    
2024-02-17 13:16:22,050 - Epoch: [209][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131244    
2024-02-17 13:16:34,963 - Epoch: [209][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130520    
2024-02-17 13:16:46,692 - Epoch: [209][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 2.403846    Top5 9.134615    LR 0.001298    Time 0.130130    
2024-02-17 13:16:46,864 - --- validate (epoch=209)-----------
2024-02-17 13:16:46,865 - 10000 samples (128 per mini-batch)
2024-02-17 13:16:52,283 - Epoch: [209][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:16:52,386 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:16:52,395 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:16:52,395 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:16:52,455 - 

2024-02-17 13:16:52,455 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:17:04,906 - Epoch: [210][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.124452    
2024-02-17 13:17:17,259 - Epoch: [210][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.123961    
2024-02-17 13:17:30,150 - Epoch: [210][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125593    
2024-02-17 13:17:41,883 - Epoch: [210][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 3.365385    LR 0.001298    Time 0.126359    
2024-02-17 13:17:42,019 - --- validate (epoch=210)-----------
2024-02-17 13:17:42,020 - 10000 samples (128 per mini-batch)
2024-02-17 13:17:47,789 - Epoch: [210][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:17:47,942 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:17:47,960 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 210]
2024-02-17 13:17:47,961 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:17:48,042 - 

2024-02-17 13:17:48,042 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:18:01,574 - Epoch: [211][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135233    
2024-02-17 13:18:14,406 - Epoch: [211][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131749    
2024-02-17 13:18:26,804 - Epoch: [211][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129143    
2024-02-17 13:18:38,840 - Epoch: [211][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 6.250000    LR 0.001298    Time 0.129855    
2024-02-17 13:18:38,977 - --- validate (epoch=211)-----------
2024-02-17 13:18:38,978 - 10000 samples (128 per mini-batch)
2024-02-17 13:18:44,917 - Epoch: [211][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:18:45,106 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:18:45,123 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 211]
2024-02-17 13:18:45,124 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:18:45,205 - 

2024-02-17 13:18:45,205 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:18:58,463 - Epoch: [212][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132497    
2024-02-17 13:19:10,659 - Epoch: [212][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127206    
2024-02-17 13:19:23,483 - Epoch: [212][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127533    
2024-02-17 13:19:35,725 - Epoch: [212][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 3.846154    LR 0.001298    Time 0.129147    
2024-02-17 13:19:35,932 - --- validate (epoch=212)-----------
2024-02-17 13:19:35,933 - 10000 samples (128 per mini-batch)
2024-02-17 13:19:40,913 - Epoch: [212][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:19:41,032 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:19:41,048 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 212]
2024-02-17 13:19:41,048 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:19:41,108 - 

2024-02-17 13:19:41,108 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:19:53,389 - Epoch: [213][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.122741    
2024-02-17 13:20:06,630 - Epoch: [213][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127548    
2024-02-17 13:20:19,490 - Epoch: [213][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127881    
2024-02-17 13:20:31,372 - Epoch: [213][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 4.326923    LR 0.001298    Time 0.128494    
2024-02-17 13:20:31,511 - --- validate (epoch=213)-----------
2024-02-17 13:20:31,512 - 10000 samples (128 per mini-batch)
2024-02-17 13:20:37,388 - Epoch: [213][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:20:37,547 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:20:37,558 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 213]
2024-02-17 13:20:37,558 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:20:37,661 - 

2024-02-17 13:20:37,662 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:20:51,437 - Epoch: [214][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.137671    
2024-02-17 13:21:03,442 - Epoch: [214][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128837    
2024-02-17 13:21:16,428 - Epoch: [214][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129159    
2024-02-17 13:21:28,174 - Epoch: [214][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 7.211538    LR 0.001298    Time 0.129129    
2024-02-17 13:21:28,354 - --- validate (epoch=214)-----------
2024-02-17 13:21:28,355 - 10000 samples (128 per mini-batch)
2024-02-17 13:21:33,437 - Epoch: [214][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:21:33,550 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:21:33,567 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 214]
2024-02-17 13:21:33,567 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:21:33,644 - 

2024-02-17 13:21:33,645 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:21:46,822 - Epoch: [215][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131697    
2024-02-17 13:22:00,033 - Epoch: [215][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131871    
2024-02-17 13:22:12,994 - Epoch: [215][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131103    
2024-02-17 13:22:24,689 - Epoch: [215][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 6.730769    LR 0.001298    Time 0.130487    
2024-02-17 13:22:24,872 - --- validate (epoch=215)-----------
2024-02-17 13:22:24,873 - 10000 samples (128 per mini-batch)
2024-02-17 13:22:30,324 - Epoch: [215][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:22:30,418 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:22:30,434 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 215]
2024-02-17 13:22:30,434 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:22:30,494 - 

2024-02-17 13:22:30,494 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:22:43,158 - Epoch: [216][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126575    
2024-02-17 13:22:55,973 - Epoch: [216][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127336    
2024-02-17 13:23:08,977 - Epoch: [216][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128220    
2024-02-17 13:23:20,553 - Epoch: [216][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 3.846154    LR 0.001298    Time 0.127972    
2024-02-17 13:23:20,667 - --- validate (epoch=216)-----------
2024-02-17 13:23:20,668 - 10000 samples (128 per mini-batch)
2024-02-17 13:23:26,487 - Epoch: [216][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:23:26,596 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:23:26,616 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 216]
2024-02-17 13:23:26,616 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:23:26,697 - 

2024-02-17 13:23:26,697 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:23:40,060 - Epoch: [217][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133556    
2024-02-17 13:23:52,882 - Epoch: [217][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130859    
2024-02-17 13:24:03,919 - Epoch: [217][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.124018    
2024-02-17 13:24:14,582 - Epoch: [217][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 2.884615    LR 0.001298    Time 0.122413    
2024-02-17 13:24:14,686 - --- validate (epoch=217)-----------
2024-02-17 13:24:14,687 - 10000 samples (128 per mini-batch)
2024-02-17 13:24:19,599 - Epoch: [217][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:24:19,716 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:24:19,732 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:24:19,732 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:24:19,806 - 

2024-02-17 13:24:19,806 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:24:31,959 - Epoch: [218][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.121452    
2024-02-17 13:24:44,800 - Epoch: [218][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.124905    
2024-02-17 13:24:58,117 - Epoch: [218][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127640    
2024-02-17 13:25:09,909 - Epoch: [218][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 4.326923    LR 0.001298    Time 0.128081    
2024-02-17 13:25:10,080 - --- validate (epoch=218)-----------
2024-02-17 13:25:10,080 - 10000 samples (128 per mini-batch)
2024-02-17 13:25:16,179 - Epoch: [218][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:25:16,337 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:25:16,356 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 218]
2024-02-17 13:25:16,357 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:25:16,440 - 

2024-02-17 13:25:16,441 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:25:29,778 - Epoch: [219][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133292    
2024-02-17 13:25:41,710 - Epoch: [219][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126283    
2024-02-17 13:25:52,918 - Epoch: [219][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.121535    
2024-02-17 13:26:02,918 - Epoch: [219][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 4.807692    LR 0.001298    Time 0.118815    
2024-02-17 13:26:03,082 - --- validate (epoch=219)-----------
2024-02-17 13:26:03,083 - 10000 samples (128 per mini-batch)
2024-02-17 13:26:09,185 - Epoch: [219][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:26:09,301 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:26:09,319 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 219]
2024-02-17 13:26:09,319 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:26:09,403 - 

2024-02-17 13:26:09,403 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:26:23,082 - Epoch: [220][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.136712    
2024-02-17 13:26:35,771 - Epoch: [220][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131775    
2024-02-17 13:26:48,644 - Epoch: [220][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130740    
2024-02-17 13:27:00,327 - Epoch: [220][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 2.884615    LR 0.001298    Time 0.130179    
2024-02-17 13:27:00,460 - --- validate (epoch=220)-----------
2024-02-17 13:27:00,462 - 10000 samples (128 per mini-batch)
2024-02-17 13:27:06,799 - Epoch: [220][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:27:06,955 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:27:06,979 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 220]
2024-02-17 13:27:06,980 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:27:07,097 - 

2024-02-17 13:27:07,098 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:27:20,869 - Epoch: [221][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.137635    
2024-02-17 13:27:34,087 - Epoch: [221][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.134876    
2024-02-17 13:27:46,900 - Epoch: [221][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132613    
2024-02-17 13:27:57,899 - Epoch: [221][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 6.730769    LR 0.001298    Time 0.129867    
2024-02-17 13:27:58,039 - --- validate (epoch=221)-----------
2024-02-17 13:27:58,039 - 10000 samples (128 per mini-batch)
2024-02-17 13:28:03,104 - Epoch: [221][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:28:03,238 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:28:03,258 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 221]
2024-02-17 13:28:03,258 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:28:03,339 - 

2024-02-17 13:28:03,340 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:28:16,904 - Epoch: [222][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135558    
2024-02-17 13:28:29,018 - Epoch: [222][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128329    
2024-02-17 13:28:40,539 - Epoch: [222][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.123940    
2024-02-17 13:28:52,171 - Epoch: [222][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.923077    Top5 8.173077    LR 0.001298    Time 0.124831    
2024-02-17 13:28:52,318 - --- validate (epoch=222)-----------
2024-02-17 13:28:52,318 - 10000 samples (128 per mini-batch)
2024-02-17 13:28:58,804 - Epoch: [222][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:28:58,939 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:28:58,956 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 222]
2024-02-17 13:28:58,956 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:28:59,045 - 

2024-02-17 13:28:59,046 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:29:11,727 - Epoch: [223][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126723    
2024-02-17 13:29:24,240 - Epoch: [223][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125900    
2024-02-17 13:29:36,823 - Epoch: [223][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125860    
2024-02-17 13:29:48,537 - Epoch: [223][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 3.846154    LR 0.001298    Time 0.126516    
2024-02-17 13:29:48,682 - --- validate (epoch=223)-----------
2024-02-17 13:29:48,683 - 10000 samples (128 per mini-batch)
2024-02-17 13:29:54,217 - Epoch: [223][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:29:54,395 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:29:54,411 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 223]
2024-02-17 13:29:54,411 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:29:54,492 - 

2024-02-17 13:29:54,492 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:30:07,810 - Epoch: [224][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133100    
2024-02-17 13:30:20,672 - Epoch: [224][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130831    
2024-02-17 13:30:32,675 - Epoch: [224][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127216    
2024-02-17 13:30:44,570 - Epoch: [224][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 6.250000    LR 0.001298    Time 0.128017    
2024-02-17 13:30:44,748 - --- validate (epoch=224)-----------
2024-02-17 13:30:44,749 - 10000 samples (128 per mini-batch)
2024-02-17 13:30:50,103 - Epoch: [224][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:30:50,208 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:30:50,222 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 224]
2024-02-17 13:30:50,223 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:30:50,296 - 

2024-02-17 13:30:50,297 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:31:03,653 - Epoch: [225][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133488    
2024-02-17 13:31:15,965 - Epoch: [225][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128278    
2024-02-17 13:31:28,904 - Epoch: [225][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128632    
2024-02-17 13:31:40,956 - Epoch: [225][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 4.326923    LR 0.001298    Time 0.129505    
2024-02-17 13:31:41,165 - --- validate (epoch=225)-----------
2024-02-17 13:31:41,166 - 10000 samples (128 per mini-batch)
2024-02-17 13:31:47,116 - Epoch: [225][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:31:47,232 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:31:47,252 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 225]
2024-02-17 13:31:47,252 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:31:47,334 - 

2024-02-17 13:31:47,335 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:32:00,964 - Epoch: [226][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.136212    
2024-02-17 13:32:13,516 - Epoch: [226][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130846    
2024-02-17 13:32:25,155 - Epoch: [226][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126010    
2024-02-17 13:32:36,934 - Epoch: [226][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 6.250000    LR 0.001298    Time 0.126797    
2024-02-17 13:32:37,076 - --- validate (epoch=226)-----------
2024-02-17 13:32:37,077 - 10000 samples (128 per mini-batch)
2024-02-17 13:32:43,063 - Epoch: [226][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:32:43,269 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:32:43,286 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 226]
2024-02-17 13:32:43,286 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:32:43,369 - 

2024-02-17 13:32:43,369 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:32:56,703 - Epoch: [227][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133259    
2024-02-17 13:33:09,632 - Epoch: [227][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131249    
2024-02-17 13:33:22,772 - Epoch: [227][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131283    
2024-02-17 13:33:34,629 - Epoch: [227][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 5.288462    LR 0.001298    Time 0.131041    
2024-02-17 13:33:34,848 - --- validate (epoch=227)-----------
2024-02-17 13:33:34,849 - 10000 samples (128 per mini-batch)
2024-02-17 13:33:40,449 - Epoch: [227][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:33:40,628 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:33:40,647 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:33:40,647 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:33:40,727 - 

2024-02-17 13:33:40,728 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:33:54,121 - Epoch: [228][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133850    
2024-02-17 13:34:06,708 - Epoch: [228][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129837    
2024-02-17 13:34:18,863 - Epoch: [228][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127060    
2024-02-17 13:34:30,513 - Epoch: [228][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 3.846154    LR 0.001298    Time 0.127271    
2024-02-17 13:34:30,702 - --- validate (epoch=228)-----------
2024-02-17 13:34:30,703 - 10000 samples (128 per mini-batch)
2024-02-17 13:34:36,777 - Epoch: [228][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:34:36,896 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:34:36,915 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 228]
2024-02-17 13:34:36,915 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:34:36,998 - 

2024-02-17 13:34:36,999 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:34:50,570 - Epoch: [229][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135596    
2024-02-17 13:35:03,849 - Epoch: [229][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.134167    
2024-02-17 13:35:17,260 - Epoch: [229][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.134129    
2024-02-17 13:35:29,043 - Epoch: [229][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 4.326923    LR 0.001298    Time 0.133035    
2024-02-17 13:35:29,191 - --- validate (epoch=229)-----------
2024-02-17 13:35:29,191 - 10000 samples (128 per mini-batch)
2024-02-17 13:35:34,907 - Epoch: [229][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:35:35,044 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:35:35,066 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 229]
2024-02-17 13:35:35,066 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:35:35,149 - 

2024-02-17 13:35:35,149 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:35:48,551 - Epoch: [230][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133927    
2024-02-17 13:36:01,113 - Epoch: [230][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129748    
2024-02-17 13:36:13,965 - Epoch: [230][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129322    
2024-02-17 13:36:25,740 - Epoch: [230][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 3.846154    LR 0.001298    Time 0.129327    
2024-02-17 13:36:25,894 - --- validate (epoch=230)-----------
2024-02-17 13:36:25,895 - 10000 samples (128 per mini-batch)
2024-02-17 13:36:32,157 - Epoch: [230][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:36:32,275 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:36:32,295 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 230]
2024-02-17 13:36:32,295 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:36:32,376 - 

2024-02-17 13:36:32,376 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:36:45,604 - Epoch: [231][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132199    
2024-02-17 13:36:57,699 - Epoch: [231][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126550    
2024-02-17 13:37:10,657 - Epoch: [231][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127546    
2024-02-17 13:37:22,630 - Epoch: [231][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 7.692308    LR 0.001298    Time 0.128470    
2024-02-17 13:37:22,767 - --- validate (epoch=231)-----------
2024-02-17 13:37:22,768 - 10000 samples (128 per mini-batch)
2024-02-17 13:37:28,321 - Epoch: [231][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:37:28,489 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:37:28,507 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 231]
2024-02-17 13:37:28,507 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:37:28,574 - 

2024-02-17 13:37:28,574 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:37:41,351 - Epoch: [232][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127697    
2024-02-17 13:37:54,296 - Epoch: [232][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128537    
2024-02-17 13:38:07,186 - Epoch: [232][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128642    
2024-02-17 13:38:19,140 - Epoch: [232][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 3.846154    LR 0.001298    Time 0.129262    
2024-02-17 13:38:19,271 - --- validate (epoch=232)-----------
2024-02-17 13:38:19,272 - 10000 samples (128 per mini-batch)
2024-02-17 13:38:25,135 - Epoch: [232][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:38:25,269 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:38:25,289 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 232]
2024-02-17 13:38:25,290 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:38:25,374 - 

2024-02-17 13:38:25,374 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:38:39,012 - Epoch: [233][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.136299    
2024-02-17 13:38:51,943 - Epoch: [233][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132778    
2024-02-17 13:39:05,101 - Epoch: [233][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132359    
2024-02-17 13:39:16,592 - Epoch: [233][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 4.807692    LR 0.001298    Time 0.130933    
2024-02-17 13:39:16,803 - --- validate (epoch=233)-----------
2024-02-17 13:39:16,804 - 10000 samples (128 per mini-batch)
2024-02-17 13:39:22,871 - Epoch: [233][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:39:23,011 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:39:23,028 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 233]
2024-02-17 13:39:23,029 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:39:23,156 - 

2024-02-17 13:39:23,157 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:39:36,701 - Epoch: [234][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135357    
2024-02-17 13:39:48,750 - Epoch: [234][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127900    
2024-02-17 13:40:01,569 - Epoch: [234][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127978    
2024-02-17 13:40:13,216 - Epoch: [234][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 4.807692    LR 0.001298    Time 0.127968    
2024-02-17 13:40:13,389 - --- validate (epoch=234)-----------
2024-02-17 13:40:13,391 - 10000 samples (128 per mini-batch)
2024-02-17 13:40:19,553 - Epoch: [234][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:40:19,714 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:40:19,733 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 234]
2024-02-17 13:40:19,734 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:40:19,822 - 

2024-02-17 13:40:19,822 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:40:33,511 - Epoch: [235][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.136806    
2024-02-17 13:40:47,014 - Epoch: [235][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135890    
2024-02-17 13:40:59,957 - Epoch: [235][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133718    
2024-02-17 13:41:11,171 - Epoch: [235][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 7.692308    LR 0.001298    Time 0.131265    
2024-02-17 13:41:11,282 - --- validate (epoch=235)-----------
2024-02-17 13:41:11,283 - 10000 samples (128 per mini-batch)
2024-02-17 13:41:17,160 - Epoch: [235][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:41:17,271 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:41:17,285 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 235]
2024-02-17 13:41:17,285 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:41:17,368 - 

2024-02-17 13:41:17,369 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:41:30,938 - Epoch: [236][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135616    
2024-02-17 13:41:43,750 - Epoch: [236][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131839    
2024-02-17 13:41:56,546 - Epoch: [236][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130527    
2024-02-17 13:42:08,264 - Epoch: [236][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 4.326923    LR 0.001298    Time 0.130107    
2024-02-17 13:42:08,441 - --- validate (epoch=236)-----------
2024-02-17 13:42:08,443 - 10000 samples (128 per mini-batch)
2024-02-17 13:42:13,638 - Epoch: [236][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:42:13,755 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:42:13,772 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 236]
2024-02-17 13:42:13,772 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:42:13,851 - 

2024-02-17 13:42:13,852 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:42:27,404 - Epoch: [237][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135444    
2024-02-17 13:42:40,366 - Epoch: [237][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132506    
2024-02-17 13:42:53,169 - Epoch: [237][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130998    
2024-02-17 13:43:04,861 - Epoch: [237][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 5.288462    LR 0.001298    Time 0.130401    
2024-02-17 13:43:05,001 - --- validate (epoch=237)-----------
2024-02-17 13:43:05,002 - 10000 samples (128 per mini-batch)
2024-02-17 13:43:10,820 - Epoch: [237][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:43:10,977 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:43:10,995 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 237]
2024-02-17 13:43:10,995 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:43:11,075 - 

2024-02-17 13:43:11,075 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:43:24,649 - Epoch: [238][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135669    
2024-02-17 13:43:37,774 - Epoch: [238][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133434    
2024-02-17 13:43:50,191 - Epoch: [238][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130329    
2024-02-17 13:44:01,513 - Epoch: [238][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 6.730769    LR 0.001298    Time 0.128941    
2024-02-17 13:44:01,610 - --- validate (epoch=238)-----------
2024-02-17 13:44:01,611 - 10000 samples (128 per mini-batch)
2024-02-17 13:44:07,807 - Epoch: [238][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:44:07,926 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:44:07,944 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 238]
2024-02-17 13:44:07,944 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:44:08,024 - 

2024-02-17 13:44:08,024 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:44:21,137 - Epoch: [239][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131048    
2024-02-17 13:44:33,862 - Epoch: [239][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129123    
2024-02-17 13:44:46,301 - Epoch: [239][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127531    
2024-02-17 13:44:57,900 - Epoch: [239][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 4.807692    LR 0.001298    Time 0.127503    
2024-02-17 13:44:58,043 - --- validate (epoch=239)-----------
2024-02-17 13:44:58,044 - 10000 samples (128 per mini-batch)
2024-02-17 13:45:03,540 - Epoch: [239][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:45:03,686 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:45:03,703 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 239]
2024-02-17 13:45:03,703 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:45:03,783 - 

2024-02-17 13:45:03,784 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:45:16,899 - Epoch: [240][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131081    
2024-02-17 13:45:29,480 - Epoch: [240][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128419    
2024-02-17 13:45:40,720 - Epoch: [240][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.123065    
2024-02-17 13:45:52,703 - Epoch: [240][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 5.288462    LR 0.001298    Time 0.125058    
2024-02-17 13:45:52,854 - --- validate (epoch=240)-----------
2024-02-17 13:45:52,855 - 10000 samples (128 per mini-batch)
2024-02-17 13:45:58,812 - Epoch: [240][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:45:58,976 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:45:58,994 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 240]
2024-02-17 13:45:58,994 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:45:59,073 - 

2024-02-17 13:45:59,074 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:46:11,705 - Epoch: [241][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126237    
2024-02-17 13:46:24,813 - Epoch: [241][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128633    
2024-02-17 13:46:37,537 - Epoch: [241][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128151    
2024-02-17 13:46:49,147 - Epoch: [241][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 6.730769    LR 0.001298    Time 0.128007    
2024-02-17 13:46:49,292 - --- validate (epoch=241)-----------
2024-02-17 13:46:49,293 - 10000 samples (128 per mini-batch)
2024-02-17 13:46:54,118 - Epoch: [241][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:46:54,240 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:46:54,257 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:46:54,257 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:46:54,319 - 

2024-02-17 13:46:54,320 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:47:06,845 - Epoch: [242][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125190    
2024-02-17 13:47:19,654 - Epoch: [242][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126612    
2024-02-17 13:47:32,060 - Epoch: [242][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125743    
2024-02-17 13:47:43,690 - Epoch: [242][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 3.365385    LR 0.001298    Time 0.126211    
2024-02-17 13:47:43,815 - --- validate (epoch=242)-----------
2024-02-17 13:47:43,816 - 10000 samples (128 per mini-batch)
2024-02-17 13:47:49,324 - Epoch: [242][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:47:49,427 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:47:49,444 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 242]
2024-02-17 13:47:49,445 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:47:49,524 - 

2024-02-17 13:47:49,525 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:48:03,228 - Epoch: [243][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.136956    
2024-02-17 13:48:15,335 - Epoch: [243][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128987    
2024-02-17 13:48:28,036 - Epoch: [243][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128313    
2024-02-17 13:48:40,110 - Epoch: [243][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 3.846154    LR 0.001298    Time 0.129316    
2024-02-17 13:48:40,259 - --- validate (epoch=243)-----------
2024-02-17 13:48:40,259 - 10000 samples (128 per mini-batch)
2024-02-17 13:48:46,082 - Epoch: [243][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:48:46,197 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:48:46,214 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 243]
2024-02-17 13:48:46,214 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:48:46,295 - 

2024-02-17 13:48:46,295 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:48:59,985 - Epoch: [244][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.136824    
2024-02-17 13:49:12,946 - Epoch: [244][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133187    
2024-02-17 13:49:25,681 - Epoch: [244][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131227    
2024-02-17 13:49:37,467 - Epoch: [244][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 3.365385    LR 0.001298    Time 0.130814    
2024-02-17 13:49:37,597 - --- validate (epoch=244)-----------
2024-02-17 13:49:37,598 - 10000 samples (128 per mini-batch)
2024-02-17 13:49:43,052 - Epoch: [244][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:49:43,197 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:49:43,214 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 244]
2024-02-17 13:49:43,215 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:49:43,297 - 

2024-02-17 13:49:43,297 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:49:56,894 - Epoch: [245][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135884    
2024-02-17 13:50:09,551 - Epoch: [245][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131202    
2024-02-17 13:50:22,222 - Epoch: [245][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129688    
2024-02-17 13:50:33,339 - Epoch: [245][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.923077    Top5 6.730769    LR 0.001298    Time 0.127925    
2024-02-17 13:50:33,487 - --- validate (epoch=245)-----------
2024-02-17 13:50:33,488 - 10000 samples (128 per mini-batch)
2024-02-17 13:50:38,976 - Epoch: [245][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:50:39,095 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:50:39,115 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 245]
2024-02-17 13:50:39,115 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:50:39,195 - 

2024-02-17 13:50:39,195 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:50:52,366 - Epoch: [246][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131629    
2024-02-17 13:51:05,330 - Epoch: [246][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130610    
2024-02-17 13:51:18,273 - Epoch: [246][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130201    
2024-02-17 13:51:29,845 - Epoch: [246][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 5.769231    LR 0.001298    Time 0.129482    
2024-02-17 13:51:29,996 - --- validate (epoch=246)-----------
2024-02-17 13:51:29,997 - 10000 samples (128 per mini-batch)
2024-02-17 13:51:36,025 - Epoch: [246][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:51:36,177 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:51:36,199 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 246]
2024-02-17 13:51:36,200 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:51:36,329 - 

2024-02-17 13:51:36,329 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:51:49,520 - Epoch: [247][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131825    
2024-02-17 13:52:01,514 - Epoch: [247][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125862    
2024-02-17 13:52:14,421 - Epoch: [247][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126914    
2024-02-17 13:52:26,082 - Epoch: [247][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 2.884615    Top5 7.211538    LR 0.001298    Time 0.127186    
2024-02-17 13:52:26,188 - --- validate (epoch=247)-----------
2024-02-17 13:52:26,189 - 10000 samples (128 per mini-batch)
2024-02-17 13:52:32,416 - Epoch: [247][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:52:32,545 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:52:32,561 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 247]
2024-02-17 13:52:32,561 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:52:32,646 - 

2024-02-17 13:52:32,646 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:52:46,289 - Epoch: [248][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.136353    
2024-02-17 13:52:58,941 - Epoch: [248][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131410    
2024-02-17 13:53:10,704 - Epoch: [248][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126799    
2024-02-17 13:53:21,823 - Epoch: [248][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 2.403846    Top5 5.288462    LR 0.001298    Time 0.125714    
2024-02-17 13:53:21,979 - --- validate (epoch=248)-----------
2024-02-17 13:53:21,980 - 10000 samples (128 per mini-batch)
2024-02-17 13:53:28,108 - Epoch: [248][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:53:28,249 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:53:28,262 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 248]
2024-02-17 13:53:28,262 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:53:28,343 - 

2024-02-17 13:53:28,343 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:53:41,751 - Epoch: [249][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133998    
2024-02-17 13:53:54,194 - Epoch: [249][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129189    
2024-02-17 13:54:06,997 - Epoch: [249][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128786    
2024-02-17 13:54:18,568 - Epoch: [249][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 2.403846    LR 0.001298    Time 0.128394    
2024-02-17 13:54:18,753 - --- validate (epoch=249)-----------
2024-02-17 13:54:18,754 - 10000 samples (128 per mini-batch)
2024-02-17 13:54:24,894 - Epoch: [249][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:54:25,002 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:54:25,023 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 249]
2024-02-17 13:54:25,023 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:54:25,107 - 

2024-02-17 13:54:25,107 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:54:38,790 - Epoch: [250][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.136754    
2024-02-17 13:54:51,048 - Epoch: [250][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129638    
2024-02-17 13:55:02,979 - Epoch: [250][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126179    
2024-02-17 13:55:14,319 - Epoch: [250][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 3.846154    LR 0.001298    Time 0.125805    
2024-02-17 13:55:14,463 - --- validate (epoch=250)-----------
2024-02-17 13:55:14,464 - 10000 samples (128 per mini-batch)
2024-02-17 13:55:20,609 - Epoch: [250][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:55:20,753 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:55:20,771 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 250]
2024-02-17 13:55:20,771 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:55:20,853 - 

2024-02-17 13:55:20,853 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:55:33,653 - Epoch: [251][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127926    
2024-02-17 13:55:45,878 - Epoch: [251][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125059    
2024-02-17 13:55:58,639 - Epoch: [251][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125891    
2024-02-17 13:56:10,643 - Epoch: [251][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 4.326923    LR 0.001298    Time 0.127282    
2024-02-17 13:56:10,798 - --- validate (epoch=251)-----------
2024-02-17 13:56:10,799 - 10000 samples (128 per mini-batch)
2024-02-17 13:56:16,905 - Epoch: [251][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:56:17,040 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:56:17,057 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 251]
2024-02-17 13:56:17,058 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:56:17,137 - 

2024-02-17 13:56:17,137 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:56:30,620 - Epoch: [252][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.134748    
2024-02-17 13:56:43,297 - Epoch: [252][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130731    
2024-02-17 13:56:56,381 - Epoch: [252][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130752    
2024-02-17 13:57:07,283 - Epoch: [252][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 5.288462    LR 0.001298    Time 0.128192    
2024-02-17 13:57:07,446 - --- validate (epoch=252)-----------
2024-02-17 13:57:07,447 - 10000 samples (128 per mini-batch)
2024-02-17 13:57:13,319 - Epoch: [252][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:57:13,443 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:57:13,462 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 252]
2024-02-17 13:57:13,463 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:57:13,543 - 

2024-02-17 13:57:13,544 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:57:26,757 - Epoch: [253][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132058    
2024-02-17 13:57:39,532 - Epoch: [253][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129875    
2024-02-17 13:57:51,891 - Epoch: [253][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127763    
2024-02-17 13:58:03,567 - Epoch: [253][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 4.807692    LR 0.001298    Time 0.127877    
2024-02-17 13:58:03,712 - --- validate (epoch=253)-----------
2024-02-17 13:58:03,713 - 10000 samples (128 per mini-batch)
2024-02-17 13:58:09,496 - Epoch: [253][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:58:09,666 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:58:09,684 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 253]
2024-02-17 13:58:09,684 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:58:09,765 - 

2024-02-17 13:58:09,765 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:58:23,772 - Epoch: [254][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.139984    
2024-02-17 13:58:35,985 - Epoch: [254][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131032    
2024-02-17 13:58:49,074 - Epoch: [254][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130966    
2024-02-17 13:59:01,027 - Epoch: [254][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 6.250000    LR 0.001298    Time 0.131043    
2024-02-17 13:59:01,137 - --- validate (epoch=254)-----------
2024-02-17 13:59:01,138 - 10000 samples (128 per mini-batch)
2024-02-17 13:59:06,589 - Epoch: [254][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 13:59:06,730 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 13:59:06,747 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 254]
2024-02-17 13:59:06,747 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 13:59:06,833 - 

2024-02-17 13:59:06,834 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:59:20,236 - Epoch: [255][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133945    
2024-02-17 13:59:33,073 - Epoch: [255][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131129    
2024-02-17 13:59:46,025 - Epoch: [255][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130576    
2024-02-17 13:59:57,972 - Epoch: [255][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 7.211538    LR 0.001298    Time 0.130730    
2024-02-17 13:59:58,117 - --- validate (epoch=255)-----------
2024-02-17 13:59:58,118 - 10000 samples (128 per mini-batch)
2024-02-17 14:00:04,028 - Epoch: [255][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:00:04,215 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:00:04,233 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 255]
2024-02-17 14:00:04,234 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:00:04,317 - 

2024-02-17 14:00:04,317 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:00:18,097 - Epoch: [256][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.137722    
2024-02-17 14:00:30,816 - Epoch: [256][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132427    
2024-02-17 14:00:43,532 - Epoch: [256][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130653    
2024-02-17 14:00:54,880 - Epoch: [256][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.923077    Top5 5.288462    LR 0.001298    Time 0.129258    
2024-02-17 14:00:54,969 - --- validate (epoch=256)-----------
2024-02-17 14:00:54,970 - 10000 samples (128 per mini-batch)
2024-02-17 14:01:00,991 - Epoch: [256][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:01:01,154 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:01:01,176 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 256]
2024-02-17 14:01:01,177 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:01:01,257 - 

2024-02-17 14:01:01,257 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:01:14,681 - Epoch: [257][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.134159    
2024-02-17 14:01:27,555 - Epoch: [257][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131426    
2024-02-17 14:01:40,531 - Epoch: [257][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130851    
2024-02-17 14:01:52,465 - Epoch: [257][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 3.846154    LR 0.001298    Time 0.130908    
2024-02-17 14:01:52,686 - --- validate (epoch=257)-----------
2024-02-17 14:01:52,687 - 10000 samples (128 per mini-batch)
2024-02-17 14:01:58,505 - Epoch: [257][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:01:58,603 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:01:58,621 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 257]
2024-02-17 14:01:58,621 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:01:58,713 - 

2024-02-17 14:01:58,714 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:02:12,098 - Epoch: [258][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133754    
2024-02-17 14:02:24,587 - Epoch: [258][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129297    
2024-02-17 14:02:37,408 - Epoch: [258][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128916    
2024-02-17 14:02:49,385 - Epoch: [258][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 4.807692    LR 0.001298    Time 0.129531    
2024-02-17 14:02:49,504 - --- validate (epoch=258)-----------
2024-02-17 14:02:49,505 - 10000 samples (128 per mini-batch)
2024-02-17 14:02:55,052 - Epoch: [258][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:02:55,263 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:02:55,495 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 258]
2024-02-17 14:02:55,496 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:02:55,611 - 

2024-02-17 14:02:55,612 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:03:09,238 - Epoch: [259][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.136175    
2024-02-17 14:03:22,186 - Epoch: [259][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132800    
2024-02-17 14:03:35,171 - Epoch: [259][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131797    
2024-02-17 14:03:47,401 - Epoch: [259][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 5.769231    LR 0.001298    Time 0.132385    
2024-02-17 14:03:47,555 - --- validate (epoch=259)-----------
2024-02-17 14:03:47,556 - 10000 samples (128 per mini-batch)
2024-02-17 14:03:53,931 - Epoch: [259][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:03:54,034 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:03:54,052 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 259]
2024-02-17 14:03:54,053 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:03:54,133 - 

2024-02-17 14:03:54,134 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:04:07,925 - Epoch: [260][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.137833    
2024-02-17 14:04:20,935 - Epoch: [260][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133938    
2024-02-17 14:04:33,725 - Epoch: [260][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131908    
2024-02-17 14:04:45,726 - Epoch: [260][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 4.807692    LR 0.001298    Time 0.131887    
2024-02-17 14:04:45,868 - --- validate (epoch=260)-----------
2024-02-17 14:04:45,868 - 10000 samples (128 per mini-batch)
2024-02-17 14:04:52,026 - Epoch: [260][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:04:52,134 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:04:52,156 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 260]
2024-02-17 14:04:52,156 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:04:52,240 - 

2024-02-17 14:04:52,241 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:05:05,588 - Epoch: [261][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133392    
2024-02-17 14:05:18,841 - Epoch: [261][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132930    
2024-02-17 14:05:31,835 - Epoch: [261][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131916    
2024-02-17 14:05:43,169 - Epoch: [261][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 6.250000    LR 0.001298    Time 0.130190    
2024-02-17 14:05:43,314 - --- validate (epoch=261)-----------
2024-02-17 14:05:43,315 - 10000 samples (128 per mini-batch)
2024-02-17 14:05:49,197 - Epoch: [261][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:05:49,330 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:05:49,344 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 261]
2024-02-17 14:05:49,345 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:05:49,425 - 

2024-02-17 14:05:49,426 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:06:02,480 - Epoch: [262][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130472    
2024-02-17 14:06:14,991 - Epoch: [262][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127763    
2024-02-17 14:06:27,145 - Epoch: [262][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125672    
2024-02-17 14:06:38,980 - Epoch: [262][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 3.365385    LR 0.001298    Time 0.126683    
2024-02-17 14:06:39,139 - --- validate (epoch=262)-----------
2024-02-17 14:06:39,140 - 10000 samples (128 per mini-batch)
2024-02-17 14:06:44,473 - Epoch: [262][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:06:44,580 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:06:44,597 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 262]
2024-02-17 14:06:44,597 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:06:44,667 - 

2024-02-17 14:06:44,667 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:06:56,346 - Epoch: [263][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.116721    
2024-02-17 14:07:09,277 - Epoch: [263][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.122995    
2024-02-17 14:07:22,191 - Epoch: [263][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125024    
2024-02-17 14:07:33,852 - Epoch: [263][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 4.326923    LR 0.001298    Time 0.125739    
2024-02-17 14:07:34,012 - --- validate (epoch=263)-----------
2024-02-17 14:07:34,013 - 10000 samples (128 per mini-batch)
2024-02-17 14:07:40,173 - Epoch: [263][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:07:40,287 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:07:40,299 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 263]
2024-02-17 14:07:40,299 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:07:40,395 - 

2024-02-17 14:07:40,395 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:07:54,036 - Epoch: [264][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.136302    
2024-02-17 14:08:07,157 - Epoch: [264][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133731    
2024-02-17 14:08:19,999 - Epoch: [264][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131942    
2024-02-17 14:08:31,301 - Epoch: [264][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 2.403846    Top5 6.250000    LR 0.001298    Time 0.130129    
2024-02-17 14:08:31,408 - --- validate (epoch=264)-----------
2024-02-17 14:08:31,408 - 10000 samples (128 per mini-batch)
2024-02-17 14:08:37,106 - Epoch: [264][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:08:37,251 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:08:37,268 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 264]
2024-02-17 14:08:37,269 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:08:37,349 - 

2024-02-17 14:08:37,349 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:08:50,886 - Epoch: [265][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135288    
2024-02-17 14:09:03,702 - Epoch: [265][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131700    
2024-02-17 14:09:16,584 - Epoch: [265][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130721    
2024-02-17 14:09:28,277 - Epoch: [265][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 4.807692    LR 0.001298    Time 0.130192    
2024-02-17 14:09:28,419 - --- validate (epoch=265)-----------
2024-02-17 14:09:28,420 - 10000 samples (128 per mini-batch)
2024-02-17 14:09:34,011 - Epoch: [265][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:09:34,170 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:09:34,187 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 265]
2024-02-17 14:09:34,188 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:09:34,272 - 

2024-02-17 14:09:34,272 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:09:47,534 - Epoch: [266][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132533    
2024-02-17 14:10:00,311 - Epoch: [266][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130129    
2024-02-17 14:10:13,101 - Epoch: [266][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129370    
2024-02-17 14:10:24,951 - Epoch: [266][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 5.769231    LR 0.001298    Time 0.129555    
2024-02-17 14:10:25,131 - --- validate (epoch=266)-----------
2024-02-17 14:10:25,132 - 10000 samples (128 per mini-batch)
2024-02-17 14:10:31,623 - Epoch: [266][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:10:31,749 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:10:31,767 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 266]
2024-02-17 14:10:31,767 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:10:31,850 - 

2024-02-17 14:10:31,850 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:10:45,584 - Epoch: [267][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.137254    
2024-02-17 14:10:58,374 - Epoch: [267][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132549    
2024-02-17 14:11:11,245 - Epoch: [267][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131252    
2024-02-17 14:11:22,228 - Epoch: [267][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 4.326923    LR 0.001298    Time 0.128784    
2024-02-17 14:11:22,374 - --- validate (epoch=267)-----------
2024-02-17 14:11:22,375 - 10000 samples (128 per mini-batch)
2024-02-17 14:11:27,976 - Epoch: [267][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:11:28,097 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:11:28,113 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 267]
2024-02-17 14:11:28,114 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:11:28,182 - 

2024-02-17 14:11:28,182 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:11:40,508 - Epoch: [268][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.123192    
2024-02-17 14:11:53,216 - Epoch: [268][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125112    
2024-02-17 14:12:05,262 - Epoch: [268][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.123543    
2024-02-17 14:12:16,111 - Epoch: [268][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 3.365385    LR 0.001298    Time 0.122527    
2024-02-17 14:12:16,252 - --- validate (epoch=268)-----------
2024-02-17 14:12:16,253 - 10000 samples (128 per mini-batch)
2024-02-17 14:12:21,470 - Epoch: [268][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:12:21,587 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:12:21,604 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 268]
2024-02-17 14:12:21,605 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:12:21,682 - 

2024-02-17 14:12:21,682 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:12:35,376 - Epoch: [269][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.136852    
2024-02-17 14:12:47,944 - Epoch: [269][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131241    
2024-02-17 14:13:00,865 - Epoch: [269][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130545    
2024-02-17 14:13:12,524 - Epoch: [269][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 5.288462    LR 0.001298    Time 0.129968    
2024-02-17 14:13:12,737 - --- validate (epoch=269)-----------
2024-02-17 14:13:12,737 - 10000 samples (128 per mini-batch)
2024-02-17 14:13:18,431 - Epoch: [269][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:13:18,554 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:13:18,564 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 269]
2024-02-17 14:13:18,564 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:13:18,641 - 

2024-02-17 14:13:18,642 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:13:32,378 - Epoch: [270][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.137286    
2024-02-17 14:13:45,095 - Epoch: [270][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132201    
2024-02-17 14:13:57,854 - Epoch: [270][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130648    
2024-02-17 14:14:09,394 - Epoch: [270][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 5.769231    LR 0.001298    Time 0.129744    
2024-02-17 14:14:09,555 - --- validate (epoch=270)-----------
2024-02-17 14:14:09,555 - 10000 samples (128 per mini-batch)
2024-02-17 14:14:14,321 - Epoch: [270][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:14:14,404 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:14:14,420 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 270]
2024-02-17 14:14:14,420 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:14:14,491 - 

2024-02-17 14:14:14,492 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:14:27,675 - Epoch: [271][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131720    
2024-02-17 14:14:40,035 - Epoch: [271][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127636    
2024-02-17 14:14:51,171 - Epoch: [271][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.122199    
2024-02-17 14:15:02,760 - Epoch: [271][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 8.653846    LR 0.001298    Time 0.123385    
2024-02-17 14:15:03,001 - --- validate (epoch=271)-----------
2024-02-17 14:15:03,002 - 10000 samples (128 per mini-batch)
2024-02-17 14:15:09,299 - Epoch: [271][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:15:09,443 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:15:09,457 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 271]
2024-02-17 14:15:09,458 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:15:09,548 - 

2024-02-17 14:15:09,549 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:15:22,824 - Epoch: [272][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132666    
2024-02-17 14:15:35,594 - Epoch: [272][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130160    
2024-02-17 14:15:48,356 - Epoch: [272][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129294    
2024-02-17 14:15:59,993 - Epoch: [272][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.923077    Top5 5.288462    LR 0.001298    Time 0.128952    
2024-02-17 14:16:00,196 - --- validate (epoch=272)-----------
2024-02-17 14:16:00,197 - 10000 samples (128 per mini-batch)
2024-02-17 14:16:06,365 - Epoch: [272][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:16:06,531 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:16:06,764 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 272]
2024-02-17 14:16:06,764 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:16:06,843 - 

2024-02-17 14:16:06,843 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:16:20,275 - Epoch: [273][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.134236    
2024-02-17 14:16:32,961 - Epoch: [273][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130520    
2024-02-17 14:16:45,481 - Epoch: [273][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128732    
2024-02-17 14:16:57,389 - Epoch: [273][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 6.730769    LR 0.001298    Time 0.129214    
2024-02-17 14:16:57,510 - --- validate (epoch=273)-----------
2024-02-17 14:16:57,511 - 10000 samples (128 per mini-batch)
2024-02-17 14:17:03,444 - Epoch: [273][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:17:03,553 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:17:03,575 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 273]
2024-02-17 14:17:03,575 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:17:03,655 - 

2024-02-17 14:17:03,656 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:17:15,301 - Epoch: [274][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.116373    
2024-02-17 14:17:28,181 - Epoch: [274][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.122562    
2024-02-17 14:17:40,615 - Epoch: [274][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.123136    
2024-02-17 14:17:52,407 - Epoch: [274][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 5.288462    LR 0.001298    Time 0.124625    
2024-02-17 14:17:52,556 - --- validate (epoch=274)-----------
2024-02-17 14:17:52,557 - 10000 samples (128 per mini-batch)
2024-02-17 14:17:58,944 - Epoch: [274][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:17:59,127 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:17:59,147 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 274]
2024-02-17 14:17:59,148 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:17:59,224 - 

2024-02-17 14:17:59,224 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:18:12,462 - Epoch: [275][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132302    
2024-02-17 14:18:25,112 - Epoch: [275][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129375    
2024-02-17 14:18:38,096 - Epoch: [275][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129512    
2024-02-17 14:18:49,580 - Epoch: [275][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 6.250000    LR 0.001298    Time 0.128729    
2024-02-17 14:18:49,682 - --- validate (epoch=275)-----------
2024-02-17 14:18:49,683 - 10000 samples (128 per mini-batch)
2024-02-17 14:18:55,146 - Epoch: [275][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:18:55,292 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:18:55,310 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 275]
2024-02-17 14:18:55,311 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:18:55,390 - 

2024-02-17 14:18:55,391 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:19:08,951 - Epoch: [276][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135522    
2024-02-17 14:19:21,036 - Epoch: [276][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128163    
2024-02-17 14:19:33,884 - Epoch: [276][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128253    
2024-02-17 14:19:45,655 - Epoch: [276][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.923077    Top5 6.250000    LR 0.001298    Time 0.128497    
2024-02-17 14:19:45,780 - --- validate (epoch=276)-----------
2024-02-17 14:19:45,781 - 10000 samples (128 per mini-batch)
2024-02-17 14:19:51,486 - Epoch: [276][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:19:51,593 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:19:51,612 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 276]
2024-02-17 14:19:51,613 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:19:51,693 - 

2024-02-17 14:19:51,694 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:20:04,981 - Epoch: [277][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132793    
2024-02-17 14:20:16,929 - Epoch: [277][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126112    
2024-02-17 14:20:29,622 - Epoch: [277][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126369    
2024-02-17 14:20:41,195 - Epoch: [277][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 1.923077    LR 0.001298    Time 0.126545    
2024-02-17 14:20:41,383 - --- validate (epoch=277)-----------
2024-02-17 14:20:41,384 - 10000 samples (128 per mini-batch)
2024-02-17 14:20:46,509 - Epoch: [277][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:20:46,621 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:20:46,639 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 277]
2024-02-17 14:20:46,639 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:20:46,715 - 

2024-02-17 14:20:46,715 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:20:59,899 - Epoch: [278][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131763    
2024-02-17 14:21:12,247 - Epoch: [278][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127595    
2024-02-17 14:21:24,279 - Epoch: [278][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125158    
2024-02-17 14:21:35,791 - Epoch: [278][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 2.884615    LR 0.001298    Time 0.125459    
2024-02-17 14:21:35,936 - --- validate (epoch=278)-----------
2024-02-17 14:21:35,937 - 10000 samples (128 per mini-batch)
2024-02-17 14:21:41,200 - Epoch: [278][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:21:41,299 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:21:41,316 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 278]
2024-02-17 14:21:41,316 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:21:41,377 - 

2024-02-17 14:21:41,377 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:21:53,751 - Epoch: [279][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.123673    
2024-02-17 14:22:06,514 - Epoch: [279][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125625    
2024-02-17 14:22:19,388 - Epoch: [279][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126647    
2024-02-17 14:22:29,121 - Epoch: [279][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 7.211538    LR 0.001298    Time 0.122055    
2024-02-17 14:22:29,302 - --- validate (epoch=279)-----------
2024-02-17 14:22:29,303 - 10000 samples (128 per mini-batch)
2024-02-17 14:22:34,718 - Epoch: [279][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:22:34,850 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:22:34,869 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 279]
2024-02-17 14:22:34,869 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:22:34,932 - 

2024-02-17 14:22:34,932 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:22:42,414 - Epoch: [280][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.074772    
2024-02-17 14:22:54,584 - Epoch: [280][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.098218    
2024-02-17 14:23:07,256 - Epoch: [280][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.107702    
2024-02-17 14:23:18,952 - Epoch: [280][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 4.326923    LR 0.001298    Time 0.112534    
2024-02-17 14:23:19,113 - --- validate (epoch=280)-----------
2024-02-17 14:23:19,115 - 10000 samples (128 per mini-batch)
2024-02-17 14:23:24,694 - Epoch: [280][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:23:24,876 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:23:24,895 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 280]
2024-02-17 14:23:24,896 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:23:24,974 - 

2024-02-17 14:23:24,975 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:23:36,388 - Epoch: [281][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.114063    
2024-02-17 14:23:49,303 - Epoch: [281][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.121581    
2024-02-17 14:24:02,377 - Epoch: [281][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.124614    
2024-02-17 14:24:13,178 - Epoch: [281][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 5.288462    LR 0.001298    Time 0.123225    
2024-02-17 14:24:13,323 - --- validate (epoch=281)-----------
2024-02-17 14:24:13,324 - 10000 samples (128 per mini-batch)
2024-02-17 14:24:18,927 - Epoch: [281][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:24:19,156 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:24:19,175 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 281]
2024-02-17 14:24:19,175 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:24:19,256 - 

2024-02-17 14:24:19,256 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:24:32,155 - Epoch: [282][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128915    
2024-02-17 14:24:44,129 - Epoch: [282][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.124304    
2024-02-17 14:24:56,479 - Epoch: [282][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.124018    
2024-02-17 14:25:08,380 - Epoch: [282][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 6.730769    LR 0.001298    Time 0.125580    
2024-02-17 14:25:08,533 - --- validate (epoch=282)-----------
2024-02-17 14:25:08,534 - 10000 samples (128 per mini-batch)
2024-02-17 14:25:14,190 - Epoch: [282][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:25:14,382 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:25:14,399 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 282]
2024-02-17 14:25:14,399 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:25:14,478 - 

2024-02-17 14:25:14,479 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:25:27,671 - Epoch: [283][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131846    
2024-02-17 14:25:40,478 - Epoch: [283][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129931    
2024-02-17 14:25:53,358 - Epoch: [283][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129535    
2024-02-17 14:26:04,889 - Epoch: [283][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 3.846154    LR 0.001298    Time 0.128867    
2024-02-17 14:26:05,095 - --- validate (epoch=283)-----------
2024-02-17 14:26:05,095 - 10000 samples (128 per mini-batch)
2024-02-17 14:26:11,247 - Epoch: [283][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:26:11,464 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:26:11,480 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 283]
2024-02-17 14:26:11,480 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:26:11,559 - 

2024-02-17 14:26:11,560 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:26:24,972 - Epoch: [284][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.134041    
2024-02-17 14:26:36,685 - Epoch: [284][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125561    
2024-02-17 14:26:49,572 - Epoch: [284][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126648    
2024-02-17 14:27:01,485 - Epoch: [284][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.923077    Top5 6.250000    LR 0.001298    Time 0.127627    
2024-02-17 14:27:01,615 - --- validate (epoch=284)-----------
2024-02-17 14:27:01,615 - 10000 samples (128 per mini-batch)
2024-02-17 14:27:08,317 - Epoch: [284][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:27:08,457 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:27:08,474 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 284]
2024-02-17 14:27:08,475 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:27:08,566 - 

2024-02-17 14:27:08,566 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:27:21,380 - Epoch: [285][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128048    
2024-02-17 14:27:33,607 - Epoch: [285][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125139    
2024-02-17 14:27:45,495 - Epoch: [285][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.123037    
2024-02-17 14:27:57,138 - Epoch: [285][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 5.769231    LR 0.001298    Time 0.124168    
2024-02-17 14:27:57,256 - --- validate (epoch=285)-----------
2024-02-17 14:27:57,257 - 10000 samples (128 per mini-batch)
2024-02-17 14:28:03,447 - Epoch: [285][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:28:03,574 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:28:03,591 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 285]
2024-02-17 14:28:03,592 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:28:03,674 - 

2024-02-17 14:28:03,675 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:28:17,429 - Epoch: [286][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.137458    
2024-02-17 14:28:30,628 - Epoch: [286][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.134699    
2024-02-17 14:28:43,260 - Epoch: [286][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131890    
2024-02-17 14:28:54,756 - Epoch: [286][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 5.769231    LR 0.001298    Time 0.130584    
2024-02-17 14:28:54,852 - --- validate (epoch=286)-----------
2024-02-17 14:28:54,853 - 10000 samples (128 per mini-batch)
2024-02-17 14:29:00,441 - Epoch: [286][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:29:00,600 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:29:00,617 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 286]
2024-02-17 14:29:00,617 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:29:00,700 - 

2024-02-17 14:29:00,700 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:29:13,927 - Epoch: [287][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132190    
2024-02-17 14:29:26,621 - Epoch: [287][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129538    
2024-02-17 14:29:39,341 - Epoch: [287][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128741    
2024-02-17 14:29:50,942 - Epoch: [287][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.923077    Top5 4.807692    LR 0.001298    Time 0.128437    
2024-02-17 14:29:51,075 - --- validate (epoch=287)-----------
2024-02-17 14:29:51,076 - 10000 samples (128 per mini-batch)
2024-02-17 14:29:57,218 - Epoch: [287][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:29:57,332 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:29:57,353 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 287]
2024-02-17 14:29:57,353 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:29:57,434 - 

2024-02-17 14:29:57,434 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:30:10,967 - Epoch: [288][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135248    
2024-02-17 14:30:23,840 - Epoch: [288][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131962    
2024-02-17 14:30:36,026 - Epoch: [288][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128578    
2024-02-17 14:30:46,557 - Epoch: [288][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 4.807692    LR 0.001298    Time 0.125577    
2024-02-17 14:30:46,723 - --- validate (epoch=288)-----------
2024-02-17 14:30:46,724 - 10000 samples (128 per mini-batch)
2024-02-17 14:30:52,442 - Epoch: [288][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:30:52,641 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:30:52,657 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 288]
2024-02-17 14:30:52,657 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:30:52,739 - 

2024-02-17 14:30:52,739 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:31:05,507 - Epoch: [289][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127598    
2024-02-17 14:31:18,593 - Epoch: [289][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129206    
2024-02-17 14:31:31,202 - Epoch: [289][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128148    
2024-02-17 14:31:42,920 - Epoch: [289][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.961538    Top5 5.769231    LR 0.001298    Time 0.128282    
2024-02-17 14:31:43,062 - --- validate (epoch=289)-----------
2024-02-17 14:31:43,063 - 10000 samples (128 per mini-batch)
2024-02-17 14:31:48,770 - Epoch: [289][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:31:48,896 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:31:48,917 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 289]
2024-02-17 14:31:48,917 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:31:48,998 - 

2024-02-17 14:31:48,999 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:32:02,293 - Epoch: [290][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132855    
2024-02-17 14:32:14,970 - Epoch: [290][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129786    
2024-02-17 14:32:27,442 - Epoch: [290][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128082    
2024-02-17 14:32:38,889 - Epoch: [290][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 3.846154    LR 0.001298    Time 0.127537    
2024-02-17 14:32:39,070 - --- validate (epoch=290)-----------
2024-02-17 14:32:39,071 - 10000 samples (128 per mini-batch)
2024-02-17 14:32:44,244 - Epoch: [290][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:32:44,352 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:32:44,369 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 290]
2024-02-17 14:32:44,369 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:32:44,444 - 

2024-02-17 14:32:44,444 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:32:57,469 - Epoch: [291][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130163    
2024-02-17 14:33:10,236 - Epoch: [291][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128892    
2024-02-17 14:33:22,920 - Epoch: [291][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128191    
2024-02-17 14:33:34,738 - Epoch: [291][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 3.846154    LR 0.001298    Time 0.128569    
2024-02-17 14:33:34,922 - --- validate (epoch=291)-----------
2024-02-17 14:33:34,923 - 10000 samples (128 per mini-batch)
2024-02-17 14:33:39,658 - Epoch: [291][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:33:39,749 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:33:39,766 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 291]
2024-02-17 14:33:39,767 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:33:39,829 - 

2024-02-17 14:33:39,829 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:33:52,747 - Epoch: [292][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129114    
2024-02-17 14:34:05,597 - Epoch: [292][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128782    
2024-02-17 14:34:18,504 - Epoch: [292][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128859    
2024-02-17 14:34:30,121 - Epoch: [292][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.923077    Top5 5.288462    LR 0.001298    Time 0.128568    
2024-02-17 14:34:30,272 - --- validate (epoch=292)-----------
2024-02-17 14:34:30,273 - 10000 samples (128 per mini-batch)
2024-02-17 14:34:36,775 - Epoch: [292][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:34:36,877 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:34:36,894 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 292]
2024-02-17 14:34:36,895 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:34:36,988 - 

2024-02-17 14:34:36,989 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:34:50,285 - Epoch: [293][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132875    
2024-02-17 14:35:03,529 - Epoch: [293][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132630    
2024-02-17 14:35:14,978 - Epoch: [293][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126568    
2024-02-17 14:35:26,166 - Epoch: [293][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.923077    Top5 8.173077    LR 0.001298    Time 0.125713    
2024-02-17 14:35:26,343 - --- validate (epoch=293)-----------
2024-02-17 14:35:26,344 - 10000 samples (128 per mini-batch)
2024-02-17 14:35:32,487 - Epoch: [293][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:35:32,612 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:35:32,632 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 293]
2024-02-17 14:35:32,632 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:35:32,713 - 

2024-02-17 14:35:32,714 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:35:46,569 - Epoch: [294][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.138472    
2024-02-17 14:35:59,495 - Epoch: [294][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.133839    
2024-02-17 14:36:12,533 - Epoch: [294][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.132670    
2024-02-17 14:36:23,126 - Epoch: [294][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 5.288462    LR 0.001298    Time 0.128872    
2024-02-17 14:36:23,249 - --- validate (epoch=294)-----------
2024-02-17 14:36:23,249 - 10000 samples (128 per mini-batch)
2024-02-17 14:36:29,009 - Epoch: [294][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:36:29,134 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:36:29,152 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 294]
2024-02-17 14:36:29,153 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:36:29,234 - 

2024-02-17 14:36:29,234 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:36:42,827 - Epoch: [295][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135844    
2024-02-17 14:36:55,470 - Epoch: [295][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131111    
2024-02-17 14:37:08,119 - Epoch: [295][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.129553    
2024-02-17 14:37:19,616 - Epoch: [295][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.480769    Top5 7.692308    LR 0.001298    Time 0.128793    
2024-02-17 14:37:19,755 - --- validate (epoch=295)-----------
2024-02-17 14:37:19,756 - 10000 samples (128 per mini-batch)
2024-02-17 14:37:25,584 - Epoch: [295][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:37:25,698 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:37:25,717 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 295]
2024-02-17 14:37:25,718 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:37:25,798 - 

2024-02-17 14:37:25,799 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:37:39,359 - Epoch: [296][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135486    
2024-02-17 14:37:52,139 - Epoch: [296][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.131616    
2024-02-17 14:38:04,929 - Epoch: [296][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.130361    
2024-02-17 14:38:16,522 - Epoch: [296][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 0.000000    Top5 4.326923    LR 0.001298    Time 0.129658    
2024-02-17 14:38:16,655 - --- validate (epoch=296)-----------
2024-02-17 14:38:16,656 - 10000 samples (128 per mini-batch)
2024-02-17 14:38:22,230 - Epoch: [296][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:38:22,352 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:38:22,369 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 296]
2024-02-17 14:38:22,370 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:38:22,454 - 

2024-02-17 14:38:22,454 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:38:35,346 - Epoch: [297][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.128843    
2024-02-17 14:38:47,817 - Epoch: [297][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126752    
2024-02-17 14:39:00,480 - Epoch: [297][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.126694    
2024-02-17 14:39:12,182 - Epoch: [297][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 7.211538    LR 0.001298    Time 0.127124    
2024-02-17 14:39:12,325 - --- validate (epoch=297)-----------
2024-02-17 14:39:12,326 - 10000 samples (128 per mini-batch)
2024-02-17 14:39:17,021 - Epoch: [297][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:39:17,173 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:39:17,190 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 297]
2024-02-17 14:39:17,191 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:39:17,253 - 

2024-02-17 14:39:17,253 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:39:29,580 - Epoch: [298][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.123196    
2024-02-17 14:39:42,292 - Epoch: [298][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125135    
2024-02-17 14:39:54,977 - Epoch: [298][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.125686    
2024-02-17 14:40:06,171 - Epoch: [298][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 5.288462    LR 0.001298    Time 0.125053    
2024-02-17 14:40:06,327 - --- validate (epoch=298)-----------
2024-02-17 14:40:06,328 - 10000 samples (128 per mini-batch)
2024-02-17 14:40:12,391 - Epoch: [298][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:40:12,524 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:40:12,535 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 298]
2024-02-17 14:40:12,535 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:40:12,629 - 

2024-02-17 14:40:12,630 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:40:26,151 - Epoch: [299][  100/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.135114    
2024-02-17 14:40:38,062 - Epoch: [299][  200/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127088    
2024-02-17 14:40:50,884 - Epoch: [299][  300/  391]    Overall Loss 4.605170    Objective Loss 4.605170                                        LR 0.001298    Time 0.127446    
2024-02-17 14:41:02,806 - Epoch: [299][  391/  391]    Overall Loss 4.605170    Objective Loss 4.605170    Top1 1.442308    Top5 6.250000    LR 0.001298    Time 0.128264    
2024-02-17 14:41:02,991 - --- validate (epoch=299)-----------
2024-02-17 14:41:02,992 - 10000 samples (128 per mini-batch)
2024-02-17 14:41:08,605 - Epoch: [299][   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:41:08,716 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:41:08,734 - ==> Best [Top1: 1.000   Top5: 5.000   Sparsity:0.00   Params: 1341960 on epoch: 299]
2024-02-17 14:41:08,734 - Saving checkpoint to: logs/2024.02.17-105234/qat_checkpoint.pth.tar
2024-02-17 14:41:08,814 - --- test ---------------------
2024-02-17 14:41:08,814 - 10000 samples (128 per mini-batch)
2024-02-17 14:41:14,243 - Test: [   79/   79]    Loss 4.605170    Top1 1.000000    Top5 5.000000    
2024-02-17 14:41:14,377 - ==> Top1: 1.000    Top5: 5.000    Loss: 4.605

2024-02-17 14:41:14,391 - 
2024-02-17 14:41:14,392 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.02.17-105234/2024.02.17-105234.log
