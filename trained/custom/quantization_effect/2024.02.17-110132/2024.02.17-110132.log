2024-02-17 11:01:32,970 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.02.17-110132/2024.02.17-110132.log
2024-02-17 11:01:36,326 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2024-02-17 11:01:36,326 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2024-02-17 11:01:37,834 - Dataset sizes:
	training=50000
	validation=10000
	test=10000
2024-02-17 11:01:37,835 - Reading compression schedule from: policies/schedule-cifar100-effnet2.yaml
2024-02-17 11:01:37,852 - 

2024-02-17 11:01:37,853 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:01:45,920 - Epoch: [0][  100/  500]    Overall Loss 4.275056    Objective Loss 4.275056                                        LR 0.001000    Time 0.080620    
2024-02-17 11:01:52,977 - Epoch: [0][  200/  500]    Overall Loss 4.121238    Objective Loss 4.121238                                        LR 0.001000    Time 0.075570    
2024-02-17 11:01:59,796 - Epoch: [0][  300/  500]    Overall Loss 4.013989    Objective Loss 4.013989                                        LR 0.001000    Time 0.073097    
2024-02-17 11:02:07,004 - Epoch: [0][  400/  500]    Overall Loss 3.929156    Objective Loss 3.929156                                        LR 0.001000    Time 0.072828    
2024-02-17 11:02:14,172 - Epoch: [0][  500/  500]    Overall Loss 3.866253    Objective Loss 3.866253    Top1 11.500000    Top5 46.000000    LR 0.001000    Time 0.072588    
2024-02-17 11:02:14,263 - --- validate (epoch=0)-----------
2024-02-17 11:02:14,263 - 10000 samples (100 per mini-batch)
2024-02-17 11:02:17,639 - Epoch: [0][  100/  100]    Loss 6.616838    Top1 1.510000    Top5 6.210000    
2024-02-17 11:02:17,782 - ==> Top1: 1.510    Top5: 6.210    Loss: 6.617

2024-02-17 11:02:17,794 - ==> Best [Top1: 1.510   Top5: 6.210   Sparsity:0.00   Params: 753952 on epoch: 0]
2024-02-17 11:02:17,794 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:02:17,860 - 

2024-02-17 11:02:17,860 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:02:25,283 - Epoch: [1][  100/  500]    Overall Loss 3.500466    Objective Loss 3.500466                                        LR 0.001000    Time 0.074171    
2024-02-17 11:02:32,229 - Epoch: [1][  200/  500]    Overall Loss 3.457511    Objective Loss 3.457511                                        LR 0.001000    Time 0.071792    
2024-02-17 11:02:39,323 - Epoch: [1][  300/  500]    Overall Loss 3.418027    Objective Loss 3.418027                                        LR 0.001000    Time 0.071494    
2024-02-17 11:02:46,475 - Epoch: [1][  400/  500]    Overall Loss 3.380266    Objective Loss 3.380266                                        LR 0.001000    Time 0.071491    
2024-02-17 11:02:53,519 - Epoch: [1][  500/  500]    Overall Loss 3.341529    Objective Loss 3.341529    Top1 16.000000    Top5 45.000000    LR 0.001000    Time 0.071272    
2024-02-17 11:02:53,678 - --- validate (epoch=1)-----------
2024-02-17 11:02:53,679 - 10000 samples (100 per mini-batch)
2024-02-17 11:02:56,772 - Epoch: [1][  100/  100]    Loss 3.431657    Top1 17.000000    Top5 44.840000    
2024-02-17 11:02:56,947 - ==> Top1: 17.000    Top5: 44.840    Loss: 3.432

2024-02-17 11:02:56,958 - ==> Best [Top1: 17.000   Top5: 44.840   Sparsity:0.00   Params: 753952 on epoch: 1]
2024-02-17 11:02:56,958 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:02:57,036 - 

2024-02-17 11:02:57,036 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:03:04,872 - Epoch: [2][  100/  500]    Overall Loss 3.099394    Objective Loss 3.099394                                        LR 0.001000    Time 0.078299    
2024-02-17 11:03:11,994 - Epoch: [2][  200/  500]    Overall Loss 3.083873    Objective Loss 3.083873                                        LR 0.001000    Time 0.074740    
2024-02-17 11:03:19,074 - Epoch: [2][  300/  500]    Overall Loss 3.067328    Objective Loss 3.067328                                        LR 0.001000    Time 0.073412    
2024-02-17 11:03:26,223 - Epoch: [2][  400/  500]    Overall Loss 3.046143    Objective Loss 3.046143                                        LR 0.001000    Time 0.072920    
2024-02-17 11:03:33,382 - Epoch: [2][  500/  500]    Overall Loss 3.019589    Objective Loss 3.019589    Top1 23.000000    Top5 60.000000    LR 0.001000    Time 0.072644    
2024-02-17 11:03:33,490 - --- validate (epoch=2)-----------
2024-02-17 11:03:33,491 - 10000 samples (100 per mini-batch)
2024-02-17 11:03:36,542 - Epoch: [2][  100/  100]    Loss 3.225702    Top1 22.180000    Top5 51.550000    
2024-02-17 11:03:36,657 - ==> Top1: 22.180    Top5: 51.550    Loss: 3.226

2024-02-17 11:03:36,667 - ==> Best [Top1: 22.180   Top5: 51.550   Sparsity:0.00   Params: 753952 on epoch: 2]
2024-02-17 11:03:36,667 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:03:36,754 - 

2024-02-17 11:03:36,754 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:03:44,607 - Epoch: [3][  100/  500]    Overall Loss 2.865246    Objective Loss 2.865246                                        LR 0.001000    Time 0.078467    
2024-02-17 11:03:51,993 - Epoch: [3][  200/  500]    Overall Loss 2.844952    Objective Loss 2.844952                                        LR 0.001000    Time 0.076135    
2024-02-17 11:03:59,190 - Epoch: [3][  300/  500]    Overall Loss 2.833951    Objective Loss 2.833951                                        LR 0.001000    Time 0.074734    
2024-02-17 11:04:06,442 - Epoch: [3][  400/  500]    Overall Loss 2.819812    Objective Loss 2.819812                                        LR 0.001000    Time 0.074169    
2024-02-17 11:04:13,693 - Epoch: [3][  500/  500]    Overall Loss 2.805881    Objective Loss 2.805881    Top1 25.000000    Top5 62.000000    LR 0.001000    Time 0.073826    
2024-02-17 11:04:13,800 - --- validate (epoch=3)-----------
2024-02-17 11:04:13,803 - 10000 samples (100 per mini-batch)
2024-02-17 11:04:16,834 - Epoch: [3][  100/  100]    Loss 3.079837    Top1 24.790000    Top5 56.140000    
2024-02-17 11:04:16,945 - ==> Top1: 24.790    Top5: 56.140    Loss: 3.080

2024-02-17 11:04:16,957 - ==> Best [Top1: 24.790   Top5: 56.140   Sparsity:0.00   Params: 753952 on epoch: 3]
2024-02-17 11:04:16,958 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:04:17,039 - 

2024-02-17 11:04:17,039 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:04:24,739 - Epoch: [4][  100/  500]    Overall Loss 2.697732    Objective Loss 2.697732                                        LR 0.001000    Time 0.076933    
2024-02-17 11:04:31,989 - Epoch: [4][  200/  500]    Overall Loss 2.681969    Objective Loss 2.681969                                        LR 0.001000    Time 0.074692    
2024-02-17 11:04:39,083 - Epoch: [4][  300/  500]    Overall Loss 2.668564    Objective Loss 2.668564                                        LR 0.001000    Time 0.073429    
2024-02-17 11:04:46,117 - Epoch: [4][  400/  500]    Overall Loss 2.648206    Objective Loss 2.648206                                        LR 0.001000    Time 0.072645    
2024-02-17 11:04:53,002 - Epoch: [4][  500/  500]    Overall Loss 2.628857    Objective Loss 2.628857    Top1 30.000000    Top5 64.500000    LR 0.001000    Time 0.071879    
2024-02-17 11:04:53,125 - --- validate (epoch=4)-----------
2024-02-17 11:04:53,126 - 10000 samples (100 per mini-batch)
2024-02-17 11:04:56,626 - Epoch: [4][  100/  100]    Loss 2.708755    Top1 30.410000    Top5 63.190000    
2024-02-17 11:04:56,755 - ==> Top1: 30.410    Top5: 63.190    Loss: 2.709

2024-02-17 11:04:56,765 - ==> Best [Top1: 30.410   Top5: 63.190   Sparsity:0.00   Params: 753952 on epoch: 4]
2024-02-17 11:04:56,766 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:04:56,842 - 

2024-02-17 11:04:56,843 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:05:04,444 - Epoch: [5][  100/  500]    Overall Loss 2.506355    Objective Loss 2.506355                                        LR 0.001000    Time 0.075957    
2024-02-17 11:05:11,569 - Epoch: [5][  200/  500]    Overall Loss 2.506461    Objective Loss 2.506461                                        LR 0.001000    Time 0.073579    
2024-02-17 11:05:18,634 - Epoch: [5][  300/  500]    Overall Loss 2.498337    Objective Loss 2.498337                                        LR 0.001000    Time 0.072587    
2024-02-17 11:05:25,767 - Epoch: [5][  400/  500]    Overall Loss 2.493585    Objective Loss 2.493585                                        LR 0.001000    Time 0.072263    
2024-02-17 11:05:32,926 - Epoch: [5][  500/  500]    Overall Loss 2.480782    Objective Loss 2.480782    Top1 38.000000    Top5 68.500000    LR 0.001000    Time 0.072120    
2024-02-17 11:05:33,049 - --- validate (epoch=5)-----------
2024-02-17 11:05:33,050 - 10000 samples (100 per mini-batch)
2024-02-17 11:05:36,117 - Epoch: [5][  100/  100]    Loss 2.529645    Top1 34.250000    Top5 67.130000    
2024-02-17 11:05:36,221 - ==> Top1: 34.250    Top5: 67.130    Loss: 2.530

2024-02-17 11:05:36,233 - ==> Best [Top1: 34.250   Top5: 67.130   Sparsity:0.00   Params: 753952 on epoch: 5]
2024-02-17 11:05:36,234 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:05:36,313 - 

2024-02-17 11:05:36,314 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:05:44,009 - Epoch: [6][  100/  500]    Overall Loss 2.366514    Objective Loss 2.366514                                        LR 0.001000    Time 0.076892    
2024-02-17 11:05:51,296 - Epoch: [6][  200/  500]    Overall Loss 2.383490    Objective Loss 2.383490                                        LR 0.001000    Time 0.074861    
2024-02-17 11:05:58,595 - Epoch: [6][  300/  500]    Overall Loss 2.374303    Objective Loss 2.374303                                        LR 0.001000    Time 0.074221    
2024-02-17 11:06:05,756 - Epoch: [6][  400/  500]    Overall Loss 2.364162    Objective Loss 2.364162                                        LR 0.001000    Time 0.073558    
2024-02-17 11:06:12,911 - Epoch: [6][  500/  500]    Overall Loss 2.355942    Objective Loss 2.355942    Top1 34.500000    Top5 65.000000    LR 0.001000    Time 0.073146    
2024-02-17 11:06:13,079 - --- validate (epoch=6)-----------
2024-02-17 11:06:13,080 - 10000 samples (100 per mini-batch)
2024-02-17 11:06:16,275 - Epoch: [6][  100/  100]    Loss 2.720212    Top1 32.220000    Top5 64.090000    
2024-02-17 11:06:16,442 - ==> Top1: 32.220    Top5: 64.090    Loss: 2.720

2024-02-17 11:06:16,453 - ==> Best [Top1: 34.250   Top5: 67.130   Sparsity:0.00   Params: 753952 on epoch: 5]
2024-02-17 11:06:16,453 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:06:16,515 - 

2024-02-17 11:06:16,515 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:06:24,483 - Epoch: [7][  100/  500]    Overall Loss 2.270916    Objective Loss 2.270916                                        LR 0.001000    Time 0.079605    
2024-02-17 11:06:31,683 - Epoch: [7][  200/  500]    Overall Loss 2.274110    Objective Loss 2.274110                                        LR 0.001000    Time 0.075775    
2024-02-17 11:06:38,868 - Epoch: [7][  300/  500]    Overall Loss 2.270804    Objective Loss 2.270804                                        LR 0.001000    Time 0.074453    
2024-02-17 11:06:46,012 - Epoch: [7][  400/  500]    Overall Loss 2.270610    Objective Loss 2.270610                                        LR 0.001000    Time 0.073689    
2024-02-17 11:06:53,132 - Epoch: [7][  500/  500]    Overall Loss 2.261209    Objective Loss 2.261209    Top1 35.000000    Top5 70.500000    LR 0.001000    Time 0.073184    
2024-02-17 11:06:53,261 - --- validate (epoch=7)-----------
2024-02-17 11:06:53,268 - 10000 samples (100 per mini-batch)
2024-02-17 11:06:56,187 - Epoch: [7][  100/  100]    Loss 2.439252    Top1 37.610000    Top5 69.580000    
2024-02-17 11:06:56,357 - ==> Top1: 37.610    Top5: 69.580    Loss: 2.439

2024-02-17 11:06:56,369 - ==> Best [Top1: 37.610   Top5: 69.580   Sparsity:0.00   Params: 753952 on epoch: 7]
2024-02-17 11:06:56,370 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:06:56,449 - 

2024-02-17 11:06:56,450 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:07:04,483 - Epoch: [8][  100/  500]    Overall Loss 2.151179    Objective Loss 2.151179                                        LR 0.001000    Time 0.080273    
2024-02-17 11:07:11,601 - Epoch: [8][  200/  500]    Overall Loss 2.173651    Objective Loss 2.173651                                        LR 0.001000    Time 0.075702    
2024-02-17 11:07:18,725 - Epoch: [8][  300/  500]    Overall Loss 2.162682    Objective Loss 2.162682                                        LR 0.001000    Time 0.074203    
2024-02-17 11:07:25,953 - Epoch: [8][  400/  500]    Overall Loss 2.168870    Objective Loss 2.168870                                        LR 0.001000    Time 0.073709    
2024-02-17 11:07:33,202 - Epoch: [8][  500/  500]    Overall Loss 2.163665    Objective Loss 2.163665    Top1 41.000000    Top5 75.500000    LR 0.001000    Time 0.073457    
2024-02-17 11:07:33,369 - --- validate (epoch=8)-----------
2024-02-17 11:07:33,370 - 10000 samples (100 per mini-batch)
2024-02-17 11:07:36,394 - Epoch: [8][  100/  100]    Loss 2.393116    Top1 38.040000    Top5 70.480000    
2024-02-17 11:07:36,499 - ==> Top1: 38.040    Top5: 70.480    Loss: 2.393

2024-02-17 11:07:36,510 - ==> Best [Top1: 38.040   Top5: 70.480   Sparsity:0.00   Params: 753952 on epoch: 8]
2024-02-17 11:07:36,510 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:07:36,589 - 

2024-02-17 11:07:36,590 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:07:44,306 - Epoch: [9][  100/  500]    Overall Loss 2.059027    Objective Loss 2.059027                                        LR 0.001000    Time 0.077106    
2024-02-17 11:07:51,417 - Epoch: [9][  200/  500]    Overall Loss 2.077117    Objective Loss 2.077117                                        LR 0.001000    Time 0.074086    
2024-02-17 11:07:58,465 - Epoch: [9][  300/  500]    Overall Loss 2.086387    Objective Loss 2.086387                                        LR 0.001000    Time 0.072871    
2024-02-17 11:08:05,532 - Epoch: [9][  400/  500]    Overall Loss 2.082988    Objective Loss 2.082988                                        LR 0.001000    Time 0.072310    
2024-02-17 11:08:12,614 - Epoch: [9][  500/  500]    Overall Loss 2.083254    Objective Loss 2.083254    Top1 47.000000    Top5 78.000000    LR 0.001000    Time 0.072003    
2024-02-17 11:08:12,804 - --- validate (epoch=9)-----------
2024-02-17 11:08:12,805 - 10000 samples (100 per mini-batch)
2024-02-17 11:08:15,696 - Epoch: [9][  100/  100]    Loss 2.423922    Top1 39.470000    Top5 70.630000    
2024-02-17 11:08:15,823 - ==> Top1: 39.470    Top5: 70.630    Loss: 2.424

2024-02-17 11:08:15,833 - ==> Best [Top1: 39.470   Top5: 70.630   Sparsity:0.00   Params: 753952 on epoch: 9]
2024-02-17 11:08:15,833 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:08:15,909 - 

2024-02-17 11:08:15,909 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:08:23,486 - Epoch: [10][  100/  500]    Overall Loss 2.034494    Objective Loss 2.034494                                        LR 0.001000    Time 0.075714    
2024-02-17 11:08:30,654 - Epoch: [10][  200/  500]    Overall Loss 2.030089    Objective Loss 2.030089                                        LR 0.001000    Time 0.073672    
2024-02-17 11:08:37,912 - Epoch: [10][  300/  500]    Overall Loss 2.026545    Objective Loss 2.026545                                        LR 0.001000    Time 0.073295    
2024-02-17 11:08:45,167 - Epoch: [10][  400/  500]    Overall Loss 2.018472    Objective Loss 2.018472                                        LR 0.001000    Time 0.073096    
2024-02-17 11:08:52,311 - Epoch: [10][  500/  500]    Overall Loss 2.021649    Objective Loss 2.021649    Top1 43.000000    Top5 74.500000    LR 0.001000    Time 0.072757    
2024-02-17 11:08:52,455 - --- validate (epoch=10)-----------
2024-02-17 11:08:52,455 - 10000 samples (100 per mini-batch)
2024-02-17 11:08:55,674 - Epoch: [10][  100/  100]    Loss 2.272644    Top1 40.380000    Top5 72.820000    
2024-02-17 11:08:55,820 - ==> Top1: 40.380    Top5: 72.820    Loss: 2.273

2024-02-17 11:08:55,826 - ==> Best [Top1: 40.380   Top5: 72.820   Sparsity:0.00   Params: 753952 on epoch: 10]
2024-02-17 11:08:55,827 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:08:55,908 - 

2024-02-17 11:08:55,908 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:09:03,553 - Epoch: [11][  100/  500]    Overall Loss 1.962647    Objective Loss 1.962647                                        LR 0.001000    Time 0.076393    
2024-02-17 11:09:10,679 - Epoch: [11][  200/  500]    Overall Loss 1.956996    Objective Loss 1.956996                                        LR 0.001000    Time 0.073808    
2024-02-17 11:09:17,860 - Epoch: [11][  300/  500]    Overall Loss 1.955405    Objective Loss 1.955405                                        LR 0.001000    Time 0.073129    
2024-02-17 11:09:24,965 - Epoch: [11][  400/  500]    Overall Loss 1.954840    Objective Loss 1.954840                                        LR 0.001000    Time 0.072597    
2024-02-17 11:09:32,027 - Epoch: [11][  500/  500]    Overall Loss 1.954866    Objective Loss 1.954866    Top1 53.000000    Top5 78.500000    LR 0.001000    Time 0.072193    
2024-02-17 11:09:32,144 - --- validate (epoch=11)-----------
2024-02-17 11:09:32,145 - 10000 samples (100 per mini-batch)
2024-02-17 11:09:35,052 - Epoch: [11][  100/  100]    Loss 2.191544    Top1 42.860000    Top5 74.630000    
2024-02-17 11:09:35,211 - ==> Top1: 42.860    Top5: 74.630    Loss: 2.192

2024-02-17 11:09:35,221 - ==> Best [Top1: 42.860   Top5: 74.630   Sparsity:0.00   Params: 753952 on epoch: 11]
2024-02-17 11:09:35,222 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:09:35,296 - 

2024-02-17 11:09:35,297 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:09:43,190 - Epoch: [12][  100/  500]    Overall Loss 1.905880    Objective Loss 1.905880                                        LR 0.001000    Time 0.078877    
2024-02-17 11:09:50,302 - Epoch: [12][  200/  500]    Overall Loss 1.911123    Objective Loss 1.911123                                        LR 0.001000    Time 0.074974    
2024-02-17 11:09:57,482 - Epoch: [12][  300/  500]    Overall Loss 1.905539    Objective Loss 1.905539                                        LR 0.001000    Time 0.073902    
2024-02-17 11:10:04,601 - Epoch: [12][  400/  500]    Overall Loss 1.901324    Objective Loss 1.901324                                        LR 0.001000    Time 0.073214    
2024-02-17 11:10:11,614 - Epoch: [12][  500/  500]    Overall Loss 1.901744    Objective Loss 1.901744    Top1 49.000000    Top5 80.000000    LR 0.001000    Time 0.072588    
2024-02-17 11:10:11,751 - --- validate (epoch=12)-----------
2024-02-17 11:10:11,752 - 10000 samples (100 per mini-batch)
2024-02-17 11:10:14,635 - Epoch: [12][  100/  100]    Loss 2.092381    Top1 45.170000    Top5 76.320000    
2024-02-17 11:10:14,758 - ==> Top1: 45.170    Top5: 76.320    Loss: 2.092

2024-02-17 11:10:14,768 - ==> Best [Top1: 45.170   Top5: 76.320   Sparsity:0.00   Params: 753952 on epoch: 12]
2024-02-17 11:10:14,769 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:10:14,847 - 

2024-02-17 11:10:14,848 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:10:22,568 - Epoch: [13][  100/  500]    Overall Loss 1.821593    Objective Loss 1.821593                                        LR 0.001000    Time 0.077144    
2024-02-17 11:10:29,757 - Epoch: [13][  200/  500]    Overall Loss 1.839496    Objective Loss 1.839496                                        LR 0.001000    Time 0.074496    
2024-02-17 11:10:36,796 - Epoch: [13][  300/  500]    Overall Loss 1.848061    Objective Loss 1.848061                                        LR 0.001000    Time 0.073113    
2024-02-17 11:10:43,885 - Epoch: [13][  400/  500]    Overall Loss 1.847638    Objective Loss 1.847638                                        LR 0.001000    Time 0.072547    
2024-02-17 11:10:50,920 - Epoch: [13][  500/  500]    Overall Loss 1.848399    Objective Loss 1.848399    Top1 49.000000    Top5 80.000000    LR 0.001000    Time 0.072099    
2024-02-17 11:10:51,041 - --- validate (epoch=13)-----------
2024-02-17 11:10:51,041 - 10000 samples (100 per mini-batch)
2024-02-17 11:10:54,030 - Epoch: [13][  100/  100]    Loss 2.009546    Top1 46.370000    Top5 77.500000    
2024-02-17 11:10:54,148 - ==> Top1: 46.370    Top5: 77.500    Loss: 2.010

2024-02-17 11:10:54,158 - ==> Best [Top1: 46.370   Top5: 77.500   Sparsity:0.00   Params: 753952 on epoch: 13]
2024-02-17 11:10:54,159 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:10:54,236 - 

2024-02-17 11:10:54,237 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:11:01,949 - Epoch: [14][  100/  500]    Overall Loss 1.775488    Objective Loss 1.775488                                        LR 0.001000    Time 0.077053    
2024-02-17 11:11:09,048 - Epoch: [14][  200/  500]    Overall Loss 1.784257    Objective Loss 1.784257                                        LR 0.001000    Time 0.073999    
2024-02-17 11:11:16,070 - Epoch: [14][  300/  500]    Overall Loss 1.796584    Objective Loss 1.796584                                        LR 0.001000    Time 0.072725    
2024-02-17 11:11:23,130 - Epoch: [14][  400/  500]    Overall Loss 1.795666    Objective Loss 1.795666                                        LR 0.001000    Time 0.072183    
2024-02-17 11:11:30,273 - Epoch: [14][  500/  500]    Overall Loss 1.796371    Objective Loss 1.796371    Top1 48.000000    Top5 74.500000    LR 0.001000    Time 0.072024    
2024-02-17 11:11:30,419 - --- validate (epoch=14)-----------
2024-02-17 11:11:30,420 - 10000 samples (100 per mini-batch)
2024-02-17 11:11:33,601 - Epoch: [14][  100/  100]    Loss 2.156361    Top1 43.530000    Top5 76.080000    
2024-02-17 11:11:33,760 - ==> Top1: 43.530    Top5: 76.080    Loss: 2.156

2024-02-17 11:11:33,771 - ==> Best [Top1: 46.370   Top5: 77.500   Sparsity:0.00   Params: 753952 on epoch: 13]
2024-02-17 11:11:33,771 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:11:33,847 - 

2024-02-17 11:11:33,847 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:11:41,526 - Epoch: [15][  100/  500]    Overall Loss 1.753944    Objective Loss 1.753944                                        LR 0.001000    Time 0.076726    
2024-02-17 11:11:48,639 - Epoch: [15][  200/  500]    Overall Loss 1.765189    Objective Loss 1.765189                                        LR 0.001000    Time 0.073909    
2024-02-17 11:11:55,765 - Epoch: [15][  300/  500]    Overall Loss 1.756644    Objective Loss 1.756644                                        LR 0.001000    Time 0.073010    
2024-02-17 11:12:02,844 - Epoch: [15][  400/  500]    Overall Loss 1.757607    Objective Loss 1.757607                                        LR 0.001000    Time 0.072445    
2024-02-17 11:12:10,120 - Epoch: [15][  500/  500]    Overall Loss 1.759161    Objective Loss 1.759161    Top1 54.000000    Top5 80.000000    LR 0.001000    Time 0.072499    
2024-02-17 11:12:10,310 - --- validate (epoch=15)-----------
2024-02-17 11:12:10,311 - 10000 samples (100 per mini-batch)
2024-02-17 11:12:13,194 - Epoch: [15][  100/  100]    Loss 2.198651    Top1 44.390000    Top5 74.910000    
2024-02-17 11:12:13,337 - ==> Top1: 44.390    Top5: 74.910    Loss: 2.199

2024-02-17 11:12:13,348 - ==> Best [Top1: 46.370   Top5: 77.500   Sparsity:0.00   Params: 753952 on epoch: 13]
2024-02-17 11:12:13,349 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:12:13,411 - 

2024-02-17 11:12:13,411 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:12:21,347 - Epoch: [16][  100/  500]    Overall Loss 1.711494    Objective Loss 1.711494                                        LR 0.001000    Time 0.079303    
2024-02-17 11:12:28,509 - Epoch: [16][  200/  500]    Overall Loss 1.725115    Objective Loss 1.725115                                        LR 0.001000    Time 0.075441    
2024-02-17 11:12:35,613 - Epoch: [16][  300/  500]    Overall Loss 1.719435    Objective Loss 1.719435                                        LR 0.001000    Time 0.073961    
2024-02-17 11:12:42,779 - Epoch: [16][  400/  500]    Overall Loss 1.718150    Objective Loss 1.718150                                        LR 0.001000    Time 0.073374    
2024-02-17 11:12:50,030 - Epoch: [16][  500/  500]    Overall Loss 1.720831    Objective Loss 1.720831    Top1 49.500000    Top5 82.500000    LR 0.001000    Time 0.073191    
2024-02-17 11:12:50,211 - --- validate (epoch=16)-----------
2024-02-17 11:12:50,211 - 10000 samples (100 per mini-batch)
2024-02-17 11:12:53,181 - Epoch: [16][  100/  100]    Loss 2.099510    Top1 45.580000    Top5 77.060000    
2024-02-17 11:12:53,279 - ==> Top1: 45.580    Top5: 77.060    Loss: 2.100

2024-02-17 11:12:53,291 - ==> Best [Top1: 46.370   Top5: 77.500   Sparsity:0.00   Params: 753952 on epoch: 13]
2024-02-17 11:12:53,292 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:12:53,354 - 

2024-02-17 11:12:53,354 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:13:00,967 - Epoch: [17][  100/  500]    Overall Loss 1.660831    Objective Loss 1.660831                                        LR 0.001000    Time 0.076073    
2024-02-17 11:13:08,200 - Epoch: [17][  200/  500]    Overall Loss 1.676588    Objective Loss 1.676588                                        LR 0.001000    Time 0.074181    
2024-02-17 11:13:15,562 - Epoch: [17][  300/  500]    Overall Loss 1.675820    Objective Loss 1.675820                                        LR 0.001000    Time 0.073979    
2024-02-17 11:13:22,716 - Epoch: [17][  400/  500]    Overall Loss 1.679714    Objective Loss 1.679714                                        LR 0.001000    Time 0.073356    
2024-02-17 11:13:29,980 - Epoch: [17][  500/  500]    Overall Loss 1.685002    Objective Loss 1.685002    Top1 53.500000    Top5 84.500000    LR 0.001000    Time 0.073205    
2024-02-17 11:13:30,162 - --- validate (epoch=17)-----------
2024-02-17 11:13:30,163 - 10000 samples (100 per mini-batch)
2024-02-17 11:13:33,121 - Epoch: [17][  100/  100]    Loss 2.105406    Top1 45.680000    Top5 77.190000    
2024-02-17 11:13:33,254 - ==> Top1: 45.680    Top5: 77.190    Loss: 2.105

2024-02-17 11:13:33,265 - ==> Best [Top1: 46.370   Top5: 77.500   Sparsity:0.00   Params: 753952 on epoch: 13]
2024-02-17 11:13:33,265 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:13:33,328 - 

2024-02-17 11:13:33,329 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:13:40,878 - Epoch: [18][  100/  500]    Overall Loss 1.621041    Objective Loss 1.621041                                        LR 0.001000    Time 0.075436    
2024-02-17 11:13:48,230 - Epoch: [18][  200/  500]    Overall Loss 1.613933    Objective Loss 1.613933                                        LR 0.001000    Time 0.074453    
2024-02-17 11:13:55,399 - Epoch: [18][  300/  500]    Overall Loss 1.632973    Objective Loss 1.632973                                        LR 0.001000    Time 0.073521    
2024-02-17 11:14:02,533 - Epoch: [18][  400/  500]    Overall Loss 1.638953    Objective Loss 1.638953                                        LR 0.001000    Time 0.072964    
2024-02-17 11:14:09,708 - Epoch: [18][  500/  500]    Overall Loss 1.642707    Objective Loss 1.642707    Top1 50.500000    Top5 84.000000    LR 0.001000    Time 0.072713    
2024-02-17 11:14:09,868 - --- validate (epoch=18)-----------
2024-02-17 11:14:09,869 - 10000 samples (100 per mini-batch)
2024-02-17 11:14:13,006 - Epoch: [18][  100/  100]    Loss 1.904714    Top1 49.260000    Top5 79.860000    
2024-02-17 11:14:13,114 - ==> Top1: 49.260    Top5: 79.860    Loss: 1.905

2024-02-17 11:14:13,120 - ==> Best [Top1: 49.260   Top5: 79.860   Sparsity:0.00   Params: 753952 on epoch: 18]
2024-02-17 11:14:13,120 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:14:13,193 - 

2024-02-17 11:14:13,193 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:14:20,838 - Epoch: [19][  100/  500]    Overall Loss 1.607954    Objective Loss 1.607954                                        LR 0.001000    Time 0.076385    
2024-02-17 11:14:27,989 - Epoch: [19][  200/  500]    Overall Loss 1.585529    Objective Loss 1.585529                                        LR 0.001000    Time 0.073927    
2024-02-17 11:14:35,096 - Epoch: [19][  300/  500]    Overall Loss 1.596851    Objective Loss 1.596851                                        LR 0.001000    Time 0.072959    
2024-02-17 11:14:42,300 - Epoch: [19][  400/  500]    Overall Loss 1.601803    Objective Loss 1.601803                                        LR 0.001000    Time 0.072717    
2024-02-17 11:14:49,442 - Epoch: [19][  500/  500]    Overall Loss 1.603009    Objective Loss 1.603009    Top1 52.500000    Top5 87.000000    LR 0.001000    Time 0.072450    
2024-02-17 11:14:49,582 - --- validate (epoch=19)-----------
2024-02-17 11:14:49,583 - 10000 samples (100 per mini-batch)
2024-02-17 11:14:52,470 - Epoch: [19][  100/  100]    Loss 1.988589    Top1 48.330000    Top5 79.250000    
2024-02-17 11:14:52,573 - ==> Top1: 48.330    Top5: 79.250    Loss: 1.989

2024-02-17 11:14:52,580 - ==> Best [Top1: 49.260   Top5: 79.860   Sparsity:0.00   Params: 753952 on epoch: 18]
2024-02-17 11:14:52,580 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:14:52,639 - 

2024-02-17 11:14:52,639 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:15:00,670 - Epoch: [20][  100/  500]    Overall Loss 1.564594    Objective Loss 1.564594                                        LR 0.001000    Time 0.080246    
2024-02-17 11:15:07,793 - Epoch: [20][  200/  500]    Overall Loss 1.562131    Objective Loss 1.562131                                        LR 0.001000    Time 0.075716    
2024-02-17 11:15:14,877 - Epoch: [20][  300/  500]    Overall Loss 1.562905    Objective Loss 1.562905                                        LR 0.001000    Time 0.074076    
2024-02-17 11:15:21,992 - Epoch: [20][  400/  500]    Overall Loss 1.571484    Objective Loss 1.571484                                        LR 0.001000    Time 0.073334    
2024-02-17 11:15:29,304 - Epoch: [20][  500/  500]    Overall Loss 1.575743    Objective Loss 1.575743    Top1 57.000000    Top5 87.500000    LR 0.001000    Time 0.073282    
2024-02-17 11:15:29,431 - --- validate (epoch=20)-----------
2024-02-17 11:15:29,431 - 10000 samples (100 per mini-batch)
2024-02-17 11:15:32,486 - Epoch: [20][  100/  100]    Loss 1.934092    Top1 49.540000    Top5 80.070000    
2024-02-17 11:15:32,604 - ==> Top1: 49.540    Top5: 80.070    Loss: 1.934

2024-02-17 11:15:32,614 - ==> Best [Top1: 49.540   Top5: 80.070   Sparsity:0.00   Params: 753952 on epoch: 20]
2024-02-17 11:15:32,615 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:15:32,697 - 

2024-02-17 11:15:32,698 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:15:40,388 - Epoch: [21][  100/  500]    Overall Loss 1.538992    Objective Loss 1.538992                                        LR 0.001000    Time 0.076831    
2024-02-17 11:15:47,451 - Epoch: [21][  200/  500]    Overall Loss 1.546913    Objective Loss 1.546913                                        LR 0.001000    Time 0.073705    
2024-02-17 11:15:54,571 - Epoch: [21][  300/  500]    Overall Loss 1.546955    Objective Loss 1.546955                                        LR 0.001000    Time 0.072858    
2024-02-17 11:16:01,721 - Epoch: [21][  400/  500]    Overall Loss 1.549520    Objective Loss 1.549520                                        LR 0.001000    Time 0.072506    
2024-02-17 11:16:09,000 - Epoch: [21][  500/  500]    Overall Loss 1.548122    Objective Loss 1.548122    Top1 51.500000    Top5 83.000000    LR 0.001000    Time 0.072555    
2024-02-17 11:16:09,127 - --- validate (epoch=21)-----------
2024-02-17 11:16:09,128 - 10000 samples (100 per mini-batch)
2024-02-17 11:16:12,120 - Epoch: [21][  100/  100]    Loss 2.062266    Top1 47.660000    Top5 77.650000    
2024-02-17 11:16:12,221 - ==> Top1: 47.660    Top5: 77.650    Loss: 2.062

2024-02-17 11:16:12,231 - ==> Best [Top1: 49.540   Top5: 80.070   Sparsity:0.00   Params: 753952 on epoch: 20]
2024-02-17 11:16:12,232 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:16:12,294 - 

2024-02-17 11:16:12,295 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:16:20,173 - Epoch: [22][  100/  500]    Overall Loss 1.496551    Objective Loss 1.496551                                        LR 0.001000    Time 0.078729    
2024-02-17 11:16:27,379 - Epoch: [22][  200/  500]    Overall Loss 1.495316    Objective Loss 1.495316                                        LR 0.001000    Time 0.075372    
2024-02-17 11:16:34,488 - Epoch: [22][  300/  500]    Overall Loss 1.502963    Objective Loss 1.502963                                        LR 0.001000    Time 0.073928    
2024-02-17 11:16:41,730 - Epoch: [22][  400/  500]    Overall Loss 1.511468    Objective Loss 1.511468                                        LR 0.001000    Time 0.073540    
2024-02-17 11:16:48,905 - Epoch: [22][  500/  500]    Overall Loss 1.516580    Objective Loss 1.516580    Top1 61.000000    Top5 85.500000    LR 0.001000    Time 0.073174    
2024-02-17 11:16:49,036 - --- validate (epoch=22)-----------
2024-02-17 11:16:49,037 - 10000 samples (100 per mini-batch)
2024-02-17 11:16:52,048 - Epoch: [22][  100/  100]    Loss 2.009924    Top1 49.130000    Top5 79.100000    
2024-02-17 11:16:52,173 - ==> Top1: 49.130    Top5: 79.100    Loss: 2.010

2024-02-17 11:16:52,183 - ==> Best [Top1: 49.540   Top5: 80.070   Sparsity:0.00   Params: 753952 on epoch: 20]
2024-02-17 11:16:52,184 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:16:52,244 - 

2024-02-17 11:16:52,244 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:16:59,944 - Epoch: [23][  100/  500]    Overall Loss 1.491574    Objective Loss 1.491574                                        LR 0.001000    Time 0.076939    
2024-02-17 11:17:07,156 - Epoch: [23][  200/  500]    Overall Loss 1.483728    Objective Loss 1.483728                                        LR 0.001000    Time 0.074502    
2024-02-17 11:17:14,212 - Epoch: [23][  300/  500]    Overall Loss 1.485817    Objective Loss 1.485817                                        LR 0.001000    Time 0.073174    
2024-02-17 11:17:21,339 - Epoch: [23][  400/  500]    Overall Loss 1.494041    Objective Loss 1.494041                                        LR 0.001000    Time 0.072688    
2024-02-17 11:17:28,529 - Epoch: [23][  500/  500]    Overall Loss 1.495390    Objective Loss 1.495390    Top1 51.500000    Top5 80.000000    LR 0.001000    Time 0.072522    
2024-02-17 11:17:28,677 - --- validate (epoch=23)-----------
2024-02-17 11:17:28,678 - 10000 samples (100 per mini-batch)
2024-02-17 11:17:31,730 - Epoch: [23][  100/  100]    Loss 1.921122    Top1 49.480000    Top5 80.280000    
2024-02-17 11:17:31,827 - ==> Top1: 49.480    Top5: 80.280    Loss: 1.921

2024-02-17 11:17:31,840 - ==> Best [Top1: 49.540   Top5: 80.070   Sparsity:0.00   Params: 753952 on epoch: 20]
2024-02-17 11:17:31,840 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:17:31,904 - 

2024-02-17 11:17:31,904 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:17:39,802 - Epoch: [24][  100/  500]    Overall Loss 1.432018    Objective Loss 1.432018                                        LR 0.001000    Time 0.078920    
2024-02-17 11:17:46,847 - Epoch: [24][  200/  500]    Overall Loss 1.439184    Objective Loss 1.439184                                        LR 0.001000    Time 0.074668    
2024-02-17 11:17:53,979 - Epoch: [24][  300/  500]    Overall Loss 1.446274    Objective Loss 1.446274                                        LR 0.001000    Time 0.073536    
2024-02-17 11:18:01,054 - Epoch: [24][  400/  500]    Overall Loss 1.455156    Objective Loss 1.455156                                        LR 0.001000    Time 0.072828    
2024-02-17 11:18:08,361 - Epoch: [24][  500/  500]    Overall Loss 1.462438    Objective Loss 1.462438    Top1 54.500000    Top5 88.000000    LR 0.001000    Time 0.072868    
2024-02-17 11:18:08,479 - --- validate (epoch=24)-----------
2024-02-17 11:18:08,480 - 10000 samples (100 per mini-batch)
2024-02-17 11:18:11,381 - Epoch: [24][  100/  100]    Loss 2.288580    Top1 45.180000    Top5 76.230000    
2024-02-17 11:18:11,540 - ==> Top1: 45.180    Top5: 76.230    Loss: 2.289

2024-02-17 11:18:11,548 - ==> Best [Top1: 49.540   Top5: 80.070   Sparsity:0.00   Params: 753952 on epoch: 20]
2024-02-17 11:18:11,548 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:18:11,608 - 

2024-02-17 11:18:11,609 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:18:19,012 - Epoch: [25][  100/  500]    Overall Loss 1.423045    Objective Loss 1.423045                                        LR 0.001000    Time 0.073964    
2024-02-17 11:18:26,145 - Epoch: [25][  200/  500]    Overall Loss 1.432119    Objective Loss 1.432119                                        LR 0.001000    Time 0.072625    
2024-02-17 11:18:33,248 - Epoch: [25][  300/  500]    Overall Loss 1.435971    Objective Loss 1.435971                                        LR 0.001000    Time 0.072080    
2024-02-17 11:18:40,458 - Epoch: [25][  400/  500]    Overall Loss 1.439155    Objective Loss 1.439155                                        LR 0.001000    Time 0.072073    
2024-02-17 11:18:47,804 - Epoch: [25][  500/  500]    Overall Loss 1.446353    Objective Loss 1.446353    Top1 59.000000    Top5 88.000000    LR 0.001000    Time 0.072343    
2024-02-17 11:18:47,920 - --- validate (epoch=25)-----------
2024-02-17 11:18:47,920 - 10000 samples (100 per mini-batch)
2024-02-17 11:18:50,980 - Epoch: [25][  100/  100]    Loss 1.934736    Top1 50.320000    Top5 80.460000    
2024-02-17 11:18:51,096 - ==> Top1: 50.320    Top5: 80.460    Loss: 1.935

2024-02-17 11:18:51,102 - ==> Best [Top1: 50.320   Top5: 80.460   Sparsity:0.00   Params: 753952 on epoch: 25]
2024-02-17 11:18:51,102 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:18:51,180 - 

2024-02-17 11:18:51,180 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:18:58,998 - Epoch: [26][  100/  500]    Overall Loss 1.370702    Objective Loss 1.370702                                        LR 0.001000    Time 0.078129    
2024-02-17 11:19:06,142 - Epoch: [26][  200/  500]    Overall Loss 1.381659    Objective Loss 1.381659                                        LR 0.001000    Time 0.074760    
2024-02-17 11:19:13,172 - Epoch: [26][  300/  500]    Overall Loss 1.401317    Objective Loss 1.401317                                        LR 0.001000    Time 0.073258    
2024-02-17 11:19:20,180 - Epoch: [26][  400/  500]    Overall Loss 1.416794    Objective Loss 1.416794                                        LR 0.001000    Time 0.072454    
2024-02-17 11:19:27,474 - Epoch: [26][  500/  500]    Overall Loss 1.416493    Objective Loss 1.416493    Top1 64.500000    Top5 84.500000    LR 0.001000    Time 0.072542    
2024-02-17 11:19:27,610 - --- validate (epoch=26)-----------
2024-02-17 11:19:27,610 - 10000 samples (100 per mini-batch)
2024-02-17 11:19:30,553 - Epoch: [26][  100/  100]    Loss 1.971634    Top1 49.450000    Top5 79.280000    
2024-02-17 11:19:30,668 - ==> Top1: 49.450    Top5: 79.280    Loss: 1.972

2024-02-17 11:19:30,675 - ==> Best [Top1: 50.320   Top5: 80.460   Sparsity:0.00   Params: 753952 on epoch: 25]
2024-02-17 11:19:30,675 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:19:30,735 - 

2024-02-17 11:19:30,736 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:19:38,351 - Epoch: [27][  100/  500]    Overall Loss 1.348677    Objective Loss 1.348677                                        LR 0.001000    Time 0.076100    
2024-02-17 11:19:45,484 - Epoch: [27][  200/  500]    Overall Loss 1.366492    Objective Loss 1.366492                                        LR 0.001000    Time 0.073690    
2024-02-17 11:19:52,544 - Epoch: [27][  300/  500]    Overall Loss 1.378561    Objective Loss 1.378561                                        LR 0.001000    Time 0.072645    
2024-02-17 11:19:59,715 - Epoch: [27][  400/  500]    Overall Loss 1.381536    Objective Loss 1.381536                                        LR 0.001000    Time 0.072400    
2024-02-17 11:20:06,923 - Epoch: [27][  500/  500]    Overall Loss 1.394964    Objective Loss 1.394964    Top1 60.500000    Top5 88.000000    LR 0.001000    Time 0.072326    
2024-02-17 11:20:07,049 - --- validate (epoch=27)-----------
2024-02-17 11:20:07,050 - 10000 samples (100 per mini-batch)
2024-02-17 11:20:09,908 - Epoch: [27][  100/  100]    Loss 2.050875    Top1 49.450000    Top5 78.590000    
2024-02-17 11:20:10,015 - ==> Top1: 49.450    Top5: 78.590    Loss: 2.051

2024-02-17 11:20:10,026 - ==> Best [Top1: 50.320   Top5: 80.460   Sparsity:0.00   Params: 753952 on epoch: 25]
2024-02-17 11:20:10,026 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:20:10,087 - 

2024-02-17 11:20:10,088 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:20:17,691 - Epoch: [28][  100/  500]    Overall Loss 1.328535    Objective Loss 1.328535                                        LR 0.001000    Time 0.075980    
2024-02-17 11:20:24,792 - Epoch: [28][  200/  500]    Overall Loss 1.350845    Objective Loss 1.350845                                        LR 0.001000    Time 0.073473    
2024-02-17 11:20:31,855 - Epoch: [28][  300/  500]    Overall Loss 1.362088    Objective Loss 1.362088                                        LR 0.001000    Time 0.072509    
2024-02-17 11:20:38,914 - Epoch: [28][  400/  500]    Overall Loss 1.363851    Objective Loss 1.363851                                        LR 0.001000    Time 0.072020    
2024-02-17 11:20:45,955 - Epoch: [28][  500/  500]    Overall Loss 1.367499    Objective Loss 1.367499    Top1 62.000000    Top5 87.500000    LR 0.001000    Time 0.071690    
2024-02-17 11:20:46,077 - --- validate (epoch=28)-----------
2024-02-17 11:20:46,078 - 10000 samples (100 per mini-batch)
2024-02-17 11:20:49,252 - Epoch: [28][  100/  100]    Loss 1.873124    Top1 51.380000    Top5 81.140000    
2024-02-17 11:20:49,363 - ==> Top1: 51.380    Top5: 81.140    Loss: 1.873

2024-02-17 11:20:49,374 - ==> Best [Top1: 51.380   Top5: 81.140   Sparsity:0.00   Params: 753952 on epoch: 28]
2024-02-17 11:20:49,375 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:20:49,451 - 

2024-02-17 11:20:49,451 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:20:57,070 - Epoch: [29][  100/  500]    Overall Loss 1.325078    Objective Loss 1.325078                                        LR 0.001000    Time 0.076131    
2024-02-17 11:21:04,182 - Epoch: [29][  200/  500]    Overall Loss 1.337117    Objective Loss 1.337117                                        LR 0.001000    Time 0.073603    
2024-02-17 11:21:11,280 - Epoch: [29][  300/  500]    Overall Loss 1.347000    Objective Loss 1.347000                                        LR 0.001000    Time 0.072714    
2024-02-17 11:21:18,382 - Epoch: [29][  400/  500]    Overall Loss 1.351536    Objective Loss 1.351536                                        LR 0.001000    Time 0.072279    
2024-02-17 11:21:25,625 - Epoch: [29][  500/  500]    Overall Loss 1.354046    Objective Loss 1.354046    Top1 63.000000    Top5 88.500000    LR 0.001000    Time 0.072299    
2024-02-17 11:21:25,755 - --- validate (epoch=29)-----------
2024-02-17 11:21:25,755 - 10000 samples (100 per mini-batch)
2024-02-17 11:21:28,769 - Epoch: [29][  100/  100]    Loss 1.797290    Top1 53.410000    Top5 81.720000    
2024-02-17 11:21:28,878 - ==> Top1: 53.410    Top5: 81.720    Loss: 1.797

2024-02-17 11:21:28,888 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:21:28,889 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:21:28,966 - 

2024-02-17 11:21:28,966 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:21:36,905 - Epoch: [30][  100/  500]    Overall Loss 1.287730    Objective Loss 1.287730                                        LR 0.001000    Time 0.079328    
2024-02-17 11:21:44,074 - Epoch: [30][  200/  500]    Overall Loss 1.299301    Objective Loss 1.299301                                        LR 0.001000    Time 0.075489    
2024-02-17 11:21:51,089 - Epoch: [30][  300/  500]    Overall Loss 1.313185    Objective Loss 1.313185                                        LR 0.001000    Time 0.073692    
2024-02-17 11:21:58,284 - Epoch: [30][  400/  500]    Overall Loss 1.322461    Objective Loss 1.322461                                        LR 0.001000    Time 0.073245    
2024-02-17 11:22:05,564 - Epoch: [30][  500/  500]    Overall Loss 1.331671    Objective Loss 1.331671    Top1 67.000000    Top5 89.000000    LR 0.001000    Time 0.073148    
2024-02-17 11:22:05,698 - --- validate (epoch=30)-----------
2024-02-17 11:22:05,699 - 10000 samples (100 per mini-batch)
2024-02-17 11:22:08,723 - Epoch: [30][  100/  100]    Loss 1.824264    Top1 52.090000    Top5 80.960000    
2024-02-17 11:22:08,852 - ==> Top1: 52.090    Top5: 80.960    Loss: 1.824

2024-02-17 11:22:08,862 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:22:08,863 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:22:08,925 - 

2024-02-17 11:22:08,925 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:22:16,506 - Epoch: [31][  100/  500]    Overall Loss 1.285199    Objective Loss 1.285199                                        LR 0.001000    Time 0.075745    
2024-02-17 11:22:23,844 - Epoch: [31][  200/  500]    Overall Loss 1.281403    Objective Loss 1.281403                                        LR 0.001000    Time 0.074540    
2024-02-17 11:22:30,989 - Epoch: [31][  300/  500]    Overall Loss 1.289112    Objective Loss 1.289112                                        LR 0.001000    Time 0.073495    
2024-02-17 11:22:37,988 - Epoch: [31][  400/  500]    Overall Loss 1.297153    Objective Loss 1.297153                                        LR 0.001000    Time 0.072608    
2024-02-17 11:22:44,924 - Epoch: [31][  500/  500]    Overall Loss 1.304195    Objective Loss 1.304195    Top1 65.000000    Top5 88.000000    LR 0.001000    Time 0.071952    
2024-02-17 11:22:45,059 - --- validate (epoch=31)-----------
2024-02-17 11:22:45,060 - 10000 samples (100 per mini-batch)
2024-02-17 11:22:48,151 - Epoch: [31][  100/  100]    Loss 1.831615    Top1 52.490000    Top5 81.970000    
2024-02-17 11:22:48,253 - ==> Top1: 52.490    Top5: 81.970    Loss: 1.832

2024-02-17 11:22:48,262 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:22:48,263 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:22:48,328 - 

2024-02-17 11:22:48,329 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:22:56,147 - Epoch: [32][  100/  500]    Overall Loss 1.256892    Objective Loss 1.256892                                        LR 0.001000    Time 0.078133    
2024-02-17 11:23:03,402 - Epoch: [32][  200/  500]    Overall Loss 1.268618    Objective Loss 1.268618                                        LR 0.001000    Time 0.075314    
2024-02-17 11:23:10,438 - Epoch: [32][  300/  500]    Overall Loss 1.278748    Objective Loss 1.278748                                        LR 0.001000    Time 0.073650    
2024-02-17 11:23:17,596 - Epoch: [32][  400/  500]    Overall Loss 1.282109    Objective Loss 1.282109                                        LR 0.001000    Time 0.073120    
2024-02-17 11:23:24,969 - Epoch: [32][  500/  500]    Overall Loss 1.286492    Objective Loss 1.286492    Top1 62.000000    Top5 92.000000    LR 0.001000    Time 0.073233    
2024-02-17 11:23:25,134 - --- validate (epoch=32)-----------
2024-02-17 11:23:25,135 - 10000 samples (100 per mini-batch)
2024-02-17 11:23:28,335 - Epoch: [32][  100/  100]    Loss 1.907953    Top1 51.330000    Top5 81.220000    
2024-02-17 11:23:28,437 - ==> Top1: 51.330    Top5: 81.220    Loss: 1.908

2024-02-17 11:23:28,444 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:23:28,444 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:23:28,525 - 

2024-02-17 11:23:28,526 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:23:36,309 - Epoch: [33][  100/  500]    Overall Loss 1.234660    Objective Loss 1.234660                                        LR 0.001000    Time 0.077773    
2024-02-17 11:23:43,407 - Epoch: [33][  200/  500]    Overall Loss 1.252759    Objective Loss 1.252759                                        LR 0.001000    Time 0.074357    
2024-02-17 11:23:50,519 - Epoch: [33][  300/  500]    Overall Loss 1.261966    Objective Loss 1.261966                                        LR 0.001000    Time 0.073258    
2024-02-17 11:23:57,676 - Epoch: [33][  400/  500]    Overall Loss 1.266817    Objective Loss 1.266817                                        LR 0.001000    Time 0.072824    
2024-02-17 11:24:04,805 - Epoch: [33][  500/  500]    Overall Loss 1.273002    Objective Loss 1.273002    Top1 59.500000    Top5 85.000000    LR 0.001000    Time 0.072508    
2024-02-17 11:24:04,922 - --- validate (epoch=33)-----------
2024-02-17 11:24:04,923 - 10000 samples (100 per mini-batch)
2024-02-17 11:24:07,914 - Epoch: [33][  100/  100]    Loss 1.876940    Top1 51.540000    Top5 81.710000    
2024-02-17 11:24:08,047 - ==> Top1: 51.540    Top5: 81.710    Loss: 1.877

2024-02-17 11:24:08,058 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:24:08,058 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:24:08,120 - 

2024-02-17 11:24:08,121 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:24:16,102 - Epoch: [34][  100/  500]    Overall Loss 1.218805    Objective Loss 1.218805                                        LR 0.001000    Time 0.079752    
2024-02-17 11:24:23,268 - Epoch: [34][  200/  500]    Overall Loss 1.223326    Objective Loss 1.223326                                        LR 0.001000    Time 0.075686    
2024-02-17 11:24:30,349 - Epoch: [34][  300/  500]    Overall Loss 1.228984    Objective Loss 1.228984                                        LR 0.001000    Time 0.074046    
2024-02-17 11:24:37,609 - Epoch: [34][  400/  500]    Overall Loss 1.236273    Objective Loss 1.236273                                        LR 0.001000    Time 0.073674    
2024-02-17 11:24:44,816 - Epoch: [34][  500/  500]    Overall Loss 1.248150    Objective Loss 1.248150    Top1 64.000000    Top5 90.500000    LR 0.001000    Time 0.073344    
2024-02-17 11:24:44,989 - --- validate (epoch=34)-----------
2024-02-17 11:24:44,990 - 10000 samples (100 per mini-batch)
2024-02-17 11:24:47,958 - Epoch: [34][  100/  100]    Loss 1.937653    Top1 52.230000    Top5 80.870000    
2024-02-17 11:24:48,081 - ==> Top1: 52.230    Top5: 80.870    Loss: 1.938

2024-02-17 11:24:48,092 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:24:48,092 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:24:48,153 - 

2024-02-17 11:24:48,153 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:24:55,713 - Epoch: [35][  100/  500]    Overall Loss 1.183362    Objective Loss 1.183362                                        LR 0.001000    Time 0.075537    
2024-02-17 11:25:02,999 - Epoch: [35][  200/  500]    Overall Loss 1.197224    Objective Loss 1.197224                                        LR 0.001000    Time 0.074178    
2024-02-17 11:25:10,324 - Epoch: [35][  300/  500]    Overall Loss 1.205798    Objective Loss 1.205798                                        LR 0.001000    Time 0.073854    
2024-02-17 11:25:17,523 - Epoch: [35][  400/  500]    Overall Loss 1.225171    Objective Loss 1.225171                                        LR 0.001000    Time 0.073376    
2024-02-17 11:25:24,727 - Epoch: [35][  500/  500]    Overall Loss 1.231215    Objective Loss 1.231215    Top1 63.000000    Top5 90.000000    LR 0.001000    Time 0.073102    
2024-02-17 11:25:24,863 - --- validate (epoch=35)-----------
2024-02-17 11:25:24,863 - 10000 samples (100 per mini-batch)
2024-02-17 11:25:27,803 - Epoch: [35][  100/  100]    Loss 1.796121    Top1 53.010000    Top5 82.540000    
2024-02-17 11:25:27,924 - ==> Top1: 53.010    Top5: 82.540    Loss: 1.796

2024-02-17 11:25:27,935 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:25:27,936 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:25:27,998 - 

2024-02-17 11:25:27,998 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:25:35,354 - Epoch: [36][  100/  500]    Overall Loss 1.157656    Objective Loss 1.157656                                        LR 0.001000    Time 0.073506    
2024-02-17 11:25:42,586 - Epoch: [36][  200/  500]    Overall Loss 1.183921    Objective Loss 1.183921                                        LR 0.001000    Time 0.072890    
2024-02-17 11:25:49,781 - Epoch: [36][  300/  500]    Overall Loss 1.203144    Objective Loss 1.203144                                        LR 0.001000    Time 0.072560    
2024-02-17 11:25:57,020 - Epoch: [36][  400/  500]    Overall Loss 1.207459    Objective Loss 1.207459                                        LR 0.001000    Time 0.072507    
2024-02-17 11:26:04,240 - Epoch: [36][  500/  500]    Overall Loss 1.211898    Objective Loss 1.211898    Top1 68.000000    Top5 90.500000    LR 0.001000    Time 0.072437    
2024-02-17 11:26:04,417 - --- validate (epoch=36)-----------
2024-02-17 11:26:04,418 - 10000 samples (100 per mini-batch)
2024-02-17 11:26:07,301 - Epoch: [36][  100/  100]    Loss 1.850014    Top1 52.670000    Top5 81.390000    
2024-02-17 11:26:07,415 - ==> Top1: 52.670    Top5: 81.390    Loss: 1.850

2024-02-17 11:26:07,421 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:26:07,422 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:26:07,694 - 

2024-02-17 11:26:07,695 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:26:15,174 - Epoch: [37][  100/  500]    Overall Loss 1.168624    Objective Loss 1.168624                                        LR 0.001000    Time 0.074742    
2024-02-17 11:26:22,247 - Epoch: [37][  200/  500]    Overall Loss 1.180387    Objective Loss 1.180387                                        LR 0.001000    Time 0.072714    
2024-02-17 11:26:29,394 - Epoch: [37][  300/  500]    Overall Loss 1.177521    Objective Loss 1.177521                                        LR 0.001000    Time 0.072283    
2024-02-17 11:26:36,621 - Epoch: [37][  400/  500]    Overall Loss 1.183165    Objective Loss 1.183165                                        LR 0.001000    Time 0.072269    
2024-02-17 11:26:43,756 - Epoch: [37][  500/  500]    Overall Loss 1.195524    Objective Loss 1.195524    Top1 57.000000    Top5 86.000000    LR 0.001000    Time 0.072076    
2024-02-17 11:26:43,913 - --- validate (epoch=37)-----------
2024-02-17 11:26:43,914 - 10000 samples (100 per mini-batch)
2024-02-17 11:26:46,753 - Epoch: [37][  100/  100]    Loss 1.863734    Top1 52.960000    Top5 81.110000    
2024-02-17 11:26:46,953 - ==> Top1: 52.960    Top5: 81.110    Loss: 1.864

2024-02-17 11:26:46,964 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:26:46,964 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:26:47,028 - 

2024-02-17 11:26:47,028 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:26:54,639 - Epoch: [38][  100/  500]    Overall Loss 1.126354    Objective Loss 1.126354                                        LR 0.001000    Time 0.076053    
2024-02-17 11:27:01,771 - Epoch: [38][  200/  500]    Overall Loss 1.148542    Objective Loss 1.148542                                        LR 0.001000    Time 0.073664    
2024-02-17 11:27:08,867 - Epoch: [38][  300/  500]    Overall Loss 1.159107    Objective Loss 1.159107                                        LR 0.001000    Time 0.072746    
2024-02-17 11:27:15,955 - Epoch: [38][  400/  500]    Overall Loss 1.166049    Objective Loss 1.166049                                        LR 0.001000    Time 0.072268    
2024-02-17 11:27:23,230 - Epoch: [38][  500/  500]    Overall Loss 1.173457    Objective Loss 1.173457    Top1 60.000000    Top5 89.500000    LR 0.001000    Time 0.072354    
2024-02-17 11:27:23,370 - --- validate (epoch=38)-----------
2024-02-17 11:27:23,371 - 10000 samples (100 per mini-batch)
2024-02-17 11:27:26,488 - Epoch: [38][  100/  100]    Loss 1.933706    Top1 51.900000    Top5 81.390000    
2024-02-17 11:27:26,650 - ==> Top1: 51.900    Top5: 81.390    Loss: 1.934

2024-02-17 11:27:26,660 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:27:26,660 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:27:26,722 - 

2024-02-17 11:27:26,723 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:27:34,403 - Epoch: [39][  100/  500]    Overall Loss 1.136036    Objective Loss 1.136036                                        LR 0.001000    Time 0.076751    
2024-02-17 11:27:41,627 - Epoch: [39][  200/  500]    Overall Loss 1.146980    Objective Loss 1.146980                                        LR 0.001000    Time 0.074470    
2024-02-17 11:27:48,704 - Epoch: [39][  300/  500]    Overall Loss 1.155615    Objective Loss 1.155615                                        LR 0.001000    Time 0.073224    
2024-02-17 11:27:55,957 - Epoch: [39][  400/  500]    Overall Loss 1.160542    Objective Loss 1.160542                                        LR 0.001000    Time 0.073037    
2024-02-17 11:28:02,984 - Epoch: [39][  500/  500]    Overall Loss 1.164606    Objective Loss 1.164606    Top1 64.500000    Top5 92.500000    LR 0.001000    Time 0.072474    
2024-02-17 11:28:03,122 - --- validate (epoch=39)-----------
2024-02-17 11:28:03,123 - 10000 samples (100 per mini-batch)
2024-02-17 11:28:06,010 - Epoch: [39][  100/  100]    Loss 1.959881    Top1 52.020000    Top5 80.500000    
2024-02-17 11:28:06,141 - ==> Top1: 52.020    Top5: 80.500    Loss: 1.960

2024-02-17 11:28:06,152 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:28:06,152 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:28:06,215 - 

2024-02-17 11:28:06,216 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:28:14,092 - Epoch: [40][  100/  500]    Overall Loss 1.099555    Objective Loss 1.099555                                        LR 0.001000    Time 0.078705    
2024-02-17 11:28:21,386 - Epoch: [40][  200/  500]    Overall Loss 1.107798    Objective Loss 1.107798                                        LR 0.001000    Time 0.075799    
2024-02-17 11:28:28,449 - Epoch: [40][  300/  500]    Overall Loss 1.119186    Objective Loss 1.119186                                        LR 0.001000    Time 0.074063    
2024-02-17 11:28:35,767 - Epoch: [40][  400/  500]    Overall Loss 1.135157    Objective Loss 1.135157                                        LR 0.001000    Time 0.073831    
2024-02-17 11:28:42,949 - Epoch: [40][  500/  500]    Overall Loss 1.139456    Objective Loss 1.139456    Top1 67.500000    Top5 91.000000    LR 0.001000    Time 0.073420    
2024-02-17 11:28:43,073 - --- validate (epoch=40)-----------
2024-02-17 11:28:43,074 - 10000 samples (100 per mini-batch)
2024-02-17 11:28:46,032 - Epoch: [40][  100/  100]    Loss 1.839705    Top1 53.910000    Top5 82.340000    
2024-02-17 11:28:46,158 - ==> Top1: 53.910    Top5: 82.340    Loss: 1.840

2024-02-17 11:28:46,169 - ==> Best [Top1: 53.910   Top5: 82.340   Sparsity:0.00   Params: 753952 on epoch: 40]
2024-02-17 11:28:46,170 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:28:46,246 - 

2024-02-17 11:28:46,246 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:28:53,864 - Epoch: [41][  100/  500]    Overall Loss 1.098113    Objective Loss 1.098113                                        LR 0.001000    Time 0.076122    
2024-02-17 11:29:00,983 - Epoch: [41][  200/  500]    Overall Loss 1.099937    Objective Loss 1.099937                                        LR 0.001000    Time 0.073634    
2024-02-17 11:29:08,140 - Epoch: [41][  300/  500]    Overall Loss 1.105861    Objective Loss 1.105861                                        LR 0.001000    Time 0.072931    
2024-02-17 11:29:15,313 - Epoch: [41][  400/  500]    Overall Loss 1.114760    Objective Loss 1.114760                                        LR 0.001000    Time 0.072622    
2024-02-17 11:29:22,703 - Epoch: [41][  500/  500]    Overall Loss 1.122951    Objective Loss 1.122951    Top1 68.000000    Top5 91.500000    LR 0.001000    Time 0.072868    
2024-02-17 11:29:22,839 - --- validate (epoch=41)-----------
2024-02-17 11:29:22,840 - 10000 samples (100 per mini-batch)
2024-02-17 11:29:25,805 - Epoch: [41][  100/  100]    Loss 1.782460    Top1 54.690000    Top5 82.720000    
2024-02-17 11:29:25,902 - ==> Top1: 54.690    Top5: 82.720    Loss: 1.782

2024-02-17 11:29:25,913 - ==> Best [Top1: 54.690   Top5: 82.720   Sparsity:0.00   Params: 753952 on epoch: 41]
2024-02-17 11:29:25,913 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:29:25,999 - 

2024-02-17 11:29:25,999 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:29:33,942 - Epoch: [42][  100/  500]    Overall Loss 1.089304    Objective Loss 1.089304                                        LR 0.001000    Time 0.079373    
2024-02-17 11:29:41,133 - Epoch: [42][  200/  500]    Overall Loss 1.088567    Objective Loss 1.088567                                        LR 0.001000    Time 0.075617    
2024-02-17 11:29:48,329 - Epoch: [42][  300/  500]    Overall Loss 1.090409    Objective Loss 1.090409                                        LR 0.001000    Time 0.074383    
2024-02-17 11:29:55,583 - Epoch: [42][  400/  500]    Overall Loss 1.103778    Objective Loss 1.103778                                        LR 0.001000    Time 0.073910    
2024-02-17 11:30:02,784 - Epoch: [42][  500/  500]    Overall Loss 1.108860    Objective Loss 1.108860    Top1 71.500000    Top5 90.000000    LR 0.001000    Time 0.073522    
2024-02-17 11:30:02,898 - --- validate (epoch=42)-----------
2024-02-17 11:30:02,899 - 10000 samples (100 per mini-batch)
2024-02-17 11:30:05,768 - Epoch: [42][  100/  100]    Loss 1.801879    Top1 54.520000    Top5 82.590000    
2024-02-17 11:30:05,889 - ==> Top1: 54.520    Top5: 82.590    Loss: 1.802

2024-02-17 11:30:05,901 - ==> Best [Top1: 54.690   Top5: 82.720   Sparsity:0.00   Params: 753952 on epoch: 41]
2024-02-17 11:30:05,902 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:30:05,964 - 

2024-02-17 11:30:05,964 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:30:13,537 - Epoch: [43][  100/  500]    Overall Loss 1.059019    Objective Loss 1.059019                                        LR 0.001000    Time 0.075670    
2024-02-17 11:30:20,651 - Epoch: [43][  200/  500]    Overall Loss 1.070833    Objective Loss 1.070833                                        LR 0.001000    Time 0.073384    
2024-02-17 11:30:27,735 - Epoch: [43][  300/  500]    Overall Loss 1.076827    Objective Loss 1.076827                                        LR 0.001000    Time 0.072521    
2024-02-17 11:30:34,725 - Epoch: [43][  400/  500]    Overall Loss 1.089423    Objective Loss 1.089423                                        LR 0.001000    Time 0.071856    
2024-02-17 11:30:42,003 - Epoch: [43][  500/  500]    Overall Loss 1.100200    Objective Loss 1.100200    Top1 63.000000    Top5 89.000000    LR 0.001000    Time 0.072032    
2024-02-17 11:30:42,169 - --- validate (epoch=43)-----------
2024-02-17 11:30:42,169 - 10000 samples (100 per mini-batch)
2024-02-17 11:30:45,109 - Epoch: [43][  100/  100]    Loss 1.852823    Top1 53.770000    Top5 83.190000    
2024-02-17 11:30:45,208 - ==> Top1: 53.770    Top5: 83.190    Loss: 1.853

2024-02-17 11:30:45,218 - ==> Best [Top1: 54.690   Top5: 82.720   Sparsity:0.00   Params: 753952 on epoch: 41]
2024-02-17 11:30:45,219 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:30:45,279 - 

2024-02-17 11:30:45,280 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:30:52,889 - Epoch: [44][  100/  500]    Overall Loss 1.047904    Objective Loss 1.047904                                        LR 0.001000    Time 0.076031    
2024-02-17 11:30:59,881 - Epoch: [44][  200/  500]    Overall Loss 1.062250    Objective Loss 1.062250                                        LR 0.001000    Time 0.072957    
2024-02-17 11:31:07,076 - Epoch: [44][  300/  500]    Overall Loss 1.065268    Objective Loss 1.065268                                        LR 0.001000    Time 0.072605    
2024-02-17 11:31:14,288 - Epoch: [44][  400/  500]    Overall Loss 1.066647    Objective Loss 1.066647                                        LR 0.001000    Time 0.072472    
2024-02-17 11:31:21,557 - Epoch: [44][  500/  500]    Overall Loss 1.078985    Objective Loss 1.078985    Top1 67.000000    Top5 87.500000    LR 0.001000    Time 0.072506    
2024-02-17 11:31:21,665 - --- validate (epoch=44)-----------
2024-02-17 11:31:21,666 - 10000 samples (100 per mini-batch)
2024-02-17 11:31:24,831 - Epoch: [44][  100/  100]    Loss 1.821990    Top1 54.740000    Top5 82.280000    
2024-02-17 11:31:24,929 - ==> Top1: 54.740    Top5: 82.280    Loss: 1.822

2024-02-17 11:31:24,940 - ==> Best [Top1: 54.740   Top5: 82.280   Sparsity:0.00   Params: 753952 on epoch: 44]
2024-02-17 11:31:24,940 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:31:25,017 - 

2024-02-17 11:31:25,017 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:31:32,543 - Epoch: [45][  100/  500]    Overall Loss 1.003887    Objective Loss 1.003887                                        LR 0.001000    Time 0.075206    
2024-02-17 11:31:39,670 - Epoch: [45][  200/  500]    Overall Loss 1.031504    Objective Loss 1.031504                                        LR 0.001000    Time 0.073214    
2024-02-17 11:31:46,854 - Epoch: [45][  300/  500]    Overall Loss 1.046354    Objective Loss 1.046354                                        LR 0.001000    Time 0.072742    
2024-02-17 11:31:53,962 - Epoch: [45][  400/  500]    Overall Loss 1.056035    Objective Loss 1.056035                                        LR 0.001000    Time 0.072316    
2024-02-17 11:32:01,278 - Epoch: [45][  500/  500]    Overall Loss 1.068227    Objective Loss 1.068227    Top1 66.000000    Top5 89.000000    LR 0.001000    Time 0.072474    
2024-02-17 11:32:01,415 - --- validate (epoch=45)-----------
2024-02-17 11:32:01,416 - 10000 samples (100 per mini-batch)
2024-02-17 11:32:04,285 - Epoch: [45][  100/  100]    Loss 1.828801    Top1 54.110000    Top5 82.480000    
2024-02-17 11:32:04,463 - ==> Top1: 54.110    Top5: 82.480    Loss: 1.829

2024-02-17 11:32:04,473 - ==> Best [Top1: 54.740   Top5: 82.280   Sparsity:0.00   Params: 753952 on epoch: 44]
2024-02-17 11:32:04,474 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:32:04,535 - 

2024-02-17 11:32:04,535 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:32:12,328 - Epoch: [46][  100/  500]    Overall Loss 1.009476    Objective Loss 1.009476                                        LR 0.001000    Time 0.077874    
2024-02-17 11:32:19,545 - Epoch: [46][  200/  500]    Overall Loss 1.027685    Objective Loss 1.027685                                        LR 0.001000    Time 0.074996    
2024-02-17 11:32:26,792 - Epoch: [46][  300/  500]    Overall Loss 1.036520    Objective Loss 1.036520                                        LR 0.001000    Time 0.074142    
2024-02-17 11:32:33,847 - Epoch: [46][  400/  500]    Overall Loss 1.042552    Objective Loss 1.042552                                        LR 0.001000    Time 0.073233    
2024-02-17 11:32:41,079 - Epoch: [46][  500/  500]    Overall Loss 1.052531    Objective Loss 1.052531    Top1 73.000000    Top5 96.000000    LR 0.001000    Time 0.073043    
2024-02-17 11:32:41,198 - --- validate (epoch=46)-----------
2024-02-17 11:32:41,199 - 10000 samples (100 per mini-batch)
2024-02-17 11:32:44,080 - Epoch: [46][  100/  100]    Loss 1.830720    Top1 53.730000    Top5 82.630000    
2024-02-17 11:32:44,178 - ==> Top1: 53.730    Top5: 82.630    Loss: 1.831

2024-02-17 11:32:44,190 - ==> Best [Top1: 54.740   Top5: 82.280   Sparsity:0.00   Params: 753952 on epoch: 44]
2024-02-17 11:32:44,191 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:32:44,253 - 

2024-02-17 11:32:44,253 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:32:51,778 - Epoch: [47][  100/  500]    Overall Loss 0.999660    Objective Loss 0.999660                                        LR 0.001000    Time 0.075185    
2024-02-17 11:32:58,930 - Epoch: [47][  200/  500]    Overall Loss 1.003852    Objective Loss 1.003852                                        LR 0.001000    Time 0.073335    
2024-02-17 11:33:05,919 - Epoch: [47][  300/  500]    Overall Loss 1.014563    Objective Loss 1.014563                                        LR 0.001000    Time 0.072172    
2024-02-17 11:33:12,988 - Epoch: [47][  400/  500]    Overall Loss 1.020945    Objective Loss 1.020945                                        LR 0.001000    Time 0.071791    
2024-02-17 11:33:20,200 - Epoch: [47][  500/  500]    Overall Loss 1.030677    Objective Loss 1.030677    Top1 70.000000    Top5 95.000000    LR 0.001000    Time 0.071848    
2024-02-17 11:33:20,318 - --- validate (epoch=47)-----------
2024-02-17 11:33:20,318 - 10000 samples (100 per mini-batch)
2024-02-17 11:33:23,216 - Epoch: [47][  100/  100]    Loss 1.896644    Top1 54.310000    Top5 82.350000    
2024-02-17 11:33:23,350 - ==> Top1: 54.310    Top5: 82.350    Loss: 1.897

2024-02-17 11:33:23,361 - ==> Best [Top1: 54.740   Top5: 82.280   Sparsity:0.00   Params: 753952 on epoch: 44]
2024-02-17 11:33:23,361 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:33:23,422 - 

2024-02-17 11:33:23,422 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:33:30,923 - Epoch: [48][  100/  500]    Overall Loss 1.005949    Objective Loss 1.005949                                        LR 0.001000    Time 0.074953    
2024-02-17 11:33:38,075 - Epoch: [48][  200/  500]    Overall Loss 1.002851    Objective Loss 1.002851                                        LR 0.001000    Time 0.073214    
2024-02-17 11:33:45,337 - Epoch: [48][  300/  500]    Overall Loss 1.013319    Objective Loss 1.013319                                        LR 0.001000    Time 0.073003    
2024-02-17 11:33:52,489 - Epoch: [48][  400/  500]    Overall Loss 1.015516    Objective Loss 1.015516                                        LR 0.001000    Time 0.072622    
2024-02-17 11:33:59,449 - Epoch: [48][  500/  500]    Overall Loss 1.024054    Objective Loss 1.024054    Top1 69.000000    Top5 93.000000    LR 0.001000    Time 0.072010    
2024-02-17 11:33:59,595 - --- validate (epoch=48)-----------
2024-02-17 11:33:59,596 - 10000 samples (100 per mini-batch)
2024-02-17 11:34:02,658 - Epoch: [48][  100/  100]    Loss 1.873272    Top1 54.180000    Top5 82.180000    
2024-02-17 11:34:02,773 - ==> Top1: 54.180    Top5: 82.180    Loss: 1.873

2024-02-17 11:34:02,780 - ==> Best [Top1: 54.740   Top5: 82.280   Sparsity:0.00   Params: 753952 on epoch: 44]
2024-02-17 11:34:02,780 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:34:02,843 - 

2024-02-17 11:34:02,844 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:34:10,323 - Epoch: [49][  100/  500]    Overall Loss 0.991793    Objective Loss 0.991793                                        LR 0.001000    Time 0.074735    
2024-02-17 11:34:17,413 - Epoch: [49][  200/  500]    Overall Loss 0.981546    Objective Loss 0.981546                                        LR 0.001000    Time 0.072800    
2024-02-17 11:34:24,696 - Epoch: [49][  300/  500]    Overall Loss 0.990645    Objective Loss 0.990645                                        LR 0.001000    Time 0.072796    
2024-02-17 11:34:31,848 - Epoch: [49][  400/  500]    Overall Loss 1.000108    Objective Loss 1.000108                                        LR 0.001000    Time 0.072467    
2024-02-17 11:34:39,041 - Epoch: [49][  500/  500]    Overall Loss 1.004350    Objective Loss 1.004350    Top1 66.000000    Top5 91.000000    LR 0.001000    Time 0.072348    
2024-02-17 11:34:39,192 - --- validate (epoch=49)-----------
2024-02-17 11:34:39,193 - 10000 samples (100 per mini-batch)
2024-02-17 11:34:42,154 - Epoch: [49][  100/  100]    Loss 1.923390    Top1 53.300000    Top5 82.380000    
2024-02-17 11:34:42,289 - ==> Top1: 53.300    Top5: 82.380    Loss: 1.923

2024-02-17 11:34:42,299 - ==> Best [Top1: 54.740   Top5: 82.280   Sparsity:0.00   Params: 753952 on epoch: 44]
2024-02-17 11:34:42,299 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:34:42,362 - 

2024-02-17 11:34:42,362 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:34:50,268 - Epoch: [50][  100/  500]    Overall Loss 0.891832    Objective Loss 0.891832                                        LR 0.000500    Time 0.079002    
2024-02-17 11:34:57,431 - Epoch: [50][  200/  500]    Overall Loss 0.875344    Objective Loss 0.875344                                        LR 0.000500    Time 0.075292    
2024-02-17 11:35:04,588 - Epoch: [50][  300/  500]    Overall Loss 0.871916    Objective Loss 0.871916                                        LR 0.000500    Time 0.074036    
2024-02-17 11:35:11,718 - Epoch: [50][  400/  500]    Overall Loss 0.872488    Objective Loss 0.872488                                        LR 0.000500    Time 0.073341    
2024-02-17 11:35:19,065 - Epoch: [50][  500/  500]    Overall Loss 0.877727    Objective Loss 0.877727    Top1 69.000000    Top5 92.500000    LR 0.000500    Time 0.073357    
2024-02-17 11:35:19,196 - --- validate (epoch=50)-----------
2024-02-17 11:35:19,197 - 10000 samples (100 per mini-batch)
2024-02-17 11:35:22,121 - Epoch: [50][  100/  100]    Loss 1.586217    Top1 58.580000    Top5 85.530000    
2024-02-17 11:35:22,225 - ==> Top1: 58.580    Top5: 85.530    Loss: 1.586

2024-02-17 11:35:22,233 - ==> Best [Top1: 58.580   Top5: 85.530   Sparsity:0.00   Params: 753952 on epoch: 50]
2024-02-17 11:35:22,234 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:35:22,330 - 

2024-02-17 11:35:22,331 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:35:29,929 - Epoch: [51][  100/  500]    Overall Loss 0.807896    Objective Loss 0.807896                                        LR 0.000500    Time 0.075901    
2024-02-17 11:35:36,939 - Epoch: [51][  200/  500]    Overall Loss 0.833882    Objective Loss 0.833882                                        LR 0.000500    Time 0.072981    
2024-02-17 11:35:44,120 - Epoch: [51][  300/  500]    Overall Loss 0.841966    Objective Loss 0.841966                                        LR 0.000500    Time 0.072576    
2024-02-17 11:35:51,274 - Epoch: [51][  400/  500]    Overall Loss 0.849738    Objective Loss 0.849738                                        LR 0.000500    Time 0.072307    
2024-02-17 11:35:58,386 - Epoch: [51][  500/  500]    Overall Loss 0.849891    Objective Loss 0.849891    Top1 72.000000    Top5 95.500000    LR 0.000500    Time 0.072060    
2024-02-17 11:35:58,562 - --- validate (epoch=51)-----------
2024-02-17 11:35:58,563 - 10000 samples (100 per mini-batch)
2024-02-17 11:36:01,447 - Epoch: [51][  100/  100]    Loss 1.623510    Top1 58.210000    Top5 85.200000    
2024-02-17 11:36:01,630 - ==> Top1: 58.210    Top5: 85.200    Loss: 1.624

2024-02-17 11:36:01,641 - ==> Best [Top1: 58.580   Top5: 85.530   Sparsity:0.00   Params: 753952 on epoch: 50]
2024-02-17 11:36:01,641 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:36:01,704 - 

2024-02-17 11:36:01,705 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:36:09,519 - Epoch: [52][  100/  500]    Overall Loss 0.800980    Objective Loss 0.800980                                        LR 0.000500    Time 0.078089    
2024-02-17 11:36:16,766 - Epoch: [52][  200/  500]    Overall Loss 0.809246    Objective Loss 0.809246                                        LR 0.000500    Time 0.075252    
2024-02-17 11:36:23,973 - Epoch: [52][  300/  500]    Overall Loss 0.815269    Objective Loss 0.815269                                        LR 0.000500    Time 0.074178    
2024-02-17 11:36:31,091 - Epoch: [52][  400/  500]    Overall Loss 0.828829    Objective Loss 0.828829                                        LR 0.000500    Time 0.073417    
2024-02-17 11:36:38,300 - Epoch: [52][  500/  500]    Overall Loss 0.833809    Objective Loss 0.833809    Top1 72.000000    Top5 93.000000    LR 0.000500    Time 0.073143    
2024-02-17 11:36:38,458 - --- validate (epoch=52)-----------
2024-02-17 11:36:38,459 - 10000 samples (100 per mini-batch)
2024-02-17 11:36:41,389 - Epoch: [52][  100/  100]    Loss 1.649348    Top1 58.060000    Top5 85.130000    
2024-02-17 11:36:41,491 - ==> Top1: 58.060    Top5: 85.130    Loss: 1.649

2024-02-17 11:36:41,502 - ==> Best [Top1: 58.580   Top5: 85.530   Sparsity:0.00   Params: 753952 on epoch: 50]
2024-02-17 11:36:41,502 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:36:41,565 - 

2024-02-17 11:36:41,565 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:36:49,053 - Epoch: [53][  100/  500]    Overall Loss 0.789713    Objective Loss 0.789713                                        LR 0.000500    Time 0.074827    
2024-02-17 11:36:56,240 - Epoch: [53][  200/  500]    Overall Loss 0.799589    Objective Loss 0.799589                                        LR 0.000500    Time 0.073323    
2024-02-17 11:37:03,368 - Epoch: [53][  300/  500]    Overall Loss 0.810559    Objective Loss 0.810559                                        LR 0.000500    Time 0.072628    
2024-02-17 11:37:10,465 - Epoch: [53][  400/  500]    Overall Loss 0.815939    Objective Loss 0.815939                                        LR 0.000500    Time 0.072204    
2024-02-17 11:37:17,832 - Epoch: [53][  500/  500]    Overall Loss 0.820874    Objective Loss 0.820874    Top1 70.000000    Top5 95.000000    LR 0.000500    Time 0.072487    
2024-02-17 11:37:18,001 - --- validate (epoch=53)-----------
2024-02-17 11:37:18,001 - 10000 samples (100 per mini-batch)
2024-02-17 11:37:20,935 - Epoch: [53][  100/  100]    Loss 1.603811    Top1 58.960000    Top5 85.370000    
2024-02-17 11:37:21,055 - ==> Top1: 58.960    Top5: 85.370    Loss: 1.604

2024-02-17 11:37:21,066 - ==> Best [Top1: 58.960   Top5: 85.370   Sparsity:0.00   Params: 753952 on epoch: 53]
2024-02-17 11:37:21,066 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:37:21,154 - 

2024-02-17 11:37:21,154 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:37:28,957 - Epoch: [54][  100/  500]    Overall Loss 0.796024    Objective Loss 0.796024                                        LR 0.000500    Time 0.077963    
2024-02-17 11:37:36,233 - Epoch: [54][  200/  500]    Overall Loss 0.797079    Objective Loss 0.797079                                        LR 0.000500    Time 0.075337    
2024-02-17 11:37:43,391 - Epoch: [54][  300/  500]    Overall Loss 0.806079    Objective Loss 0.806079                                        LR 0.000500    Time 0.074069    
2024-02-17 11:37:50,438 - Epoch: [54][  400/  500]    Overall Loss 0.813309    Objective Loss 0.813309                                        LR 0.000500    Time 0.073160    
2024-02-17 11:37:57,475 - Epoch: [54][  500/  500]    Overall Loss 0.818176    Objective Loss 0.818176    Top1 76.500000    Top5 96.000000    LR 0.000500    Time 0.072594    
2024-02-17 11:37:57,585 - --- validate (epoch=54)-----------
2024-02-17 11:37:57,585 - 10000 samples (100 per mini-batch)
2024-02-17 11:38:00,495 - Epoch: [54][  100/  100]    Loss 1.654874    Top1 58.370000    Top5 84.970000    
2024-02-17 11:38:00,615 - ==> Top1: 58.370    Top5: 84.970    Loss: 1.655

2024-02-17 11:38:00,627 - ==> Best [Top1: 58.960   Top5: 85.370   Sparsity:0.00   Params: 753952 on epoch: 53]
2024-02-17 11:38:00,627 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:38:00,691 - 

2024-02-17 11:38:00,692 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:38:08,236 - Epoch: [55][  100/  500]    Overall Loss 0.769146    Objective Loss 0.769146                                        LR 0.000500    Time 0.075385    
2024-02-17 11:38:15,452 - Epoch: [55][  200/  500]    Overall Loss 0.780171    Objective Loss 0.780171                                        LR 0.000500    Time 0.073748    
2024-02-17 11:38:22,659 - Epoch: [55][  300/  500]    Overall Loss 0.789821    Objective Loss 0.789821                                        LR 0.000500    Time 0.073177    
2024-02-17 11:38:29,823 - Epoch: [55][  400/  500]    Overall Loss 0.797652    Objective Loss 0.797652                                        LR 0.000500    Time 0.072781    
2024-02-17 11:38:36,997 - Epoch: [55][  500/  500]    Overall Loss 0.802727    Objective Loss 0.802727    Top1 80.000000    Top5 95.500000    LR 0.000500    Time 0.072564    
2024-02-17 11:38:37,187 - --- validate (epoch=55)-----------
2024-02-17 11:38:37,188 - 10000 samples (100 per mini-batch)
2024-02-17 11:38:40,096 - Epoch: [55][  100/  100]    Loss 1.577737    Top1 59.690000    Top5 85.980000    
2024-02-17 11:38:40,203 - ==> Top1: 59.690    Top5: 85.980    Loss: 1.578

2024-02-17 11:38:40,214 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:38:40,215 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:38:40,291 - 

2024-02-17 11:38:40,291 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:38:48,174 - Epoch: [56][  100/  500]    Overall Loss 0.767662    Objective Loss 0.767662                                        LR 0.000500    Time 0.078778    
2024-02-17 11:38:55,288 - Epoch: [56][  200/  500]    Overall Loss 0.775037    Objective Loss 0.775037                                        LR 0.000500    Time 0.074937    
2024-02-17 11:39:02,345 - Epoch: [56][  300/  500]    Overall Loss 0.782996    Objective Loss 0.782996                                        LR 0.000500    Time 0.073465    
2024-02-17 11:39:09,454 - Epoch: [56][  400/  500]    Overall Loss 0.793193    Objective Loss 0.793193                                        LR 0.000500    Time 0.072862    
2024-02-17 11:39:16,617 - Epoch: [56][  500/  500]    Overall Loss 0.798635    Objective Loss 0.798635    Top1 76.500000    Top5 95.500000    LR 0.000500    Time 0.072608    
2024-02-17 11:39:16,758 - --- validate (epoch=56)-----------
2024-02-17 11:39:16,758 - 10000 samples (100 per mini-batch)
2024-02-17 11:39:19,754 - Epoch: [56][  100/  100]    Loss 1.677374    Top1 57.960000    Top5 85.060000    
2024-02-17 11:39:19,865 - ==> Top1: 57.960    Top5: 85.060    Loss: 1.677

2024-02-17 11:39:19,876 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:39:19,876 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:39:19,938 - 

2024-02-17 11:39:19,938 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:39:27,521 - Epoch: [57][  100/  500]    Overall Loss 0.748854    Objective Loss 0.748854                                        LR 0.000500    Time 0.075772    
2024-02-17 11:39:34,627 - Epoch: [57][  200/  500]    Overall Loss 0.757948    Objective Loss 0.757948                                        LR 0.000500    Time 0.073392    
2024-02-17 11:39:41,720 - Epoch: [57][  300/  500]    Overall Loss 0.768813    Objective Loss 0.768813                                        LR 0.000500    Time 0.072555    
2024-02-17 11:39:48,765 - Epoch: [57][  400/  500]    Overall Loss 0.777358    Objective Loss 0.777358                                        LR 0.000500    Time 0.072019    
2024-02-17 11:39:55,980 - Epoch: [57][  500/  500]    Overall Loss 0.782456    Objective Loss 0.782456    Top1 78.000000    Top5 95.000000    LR 0.000500    Time 0.072037    
2024-02-17 11:39:56,128 - --- validate (epoch=57)-----------
2024-02-17 11:39:56,129 - 10000 samples (100 per mini-batch)
2024-02-17 11:39:59,117 - Epoch: [57][  100/  100]    Loss 1.637951    Top1 58.710000    Top5 85.710000    
2024-02-17 11:39:59,237 - ==> Top1: 58.710    Top5: 85.710    Loss: 1.638

2024-02-17 11:39:59,248 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:39:59,248 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:39:59,311 - 

2024-02-17 11:39:59,312 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:40:06,905 - Epoch: [58][  100/  500]    Overall Loss 0.757408    Objective Loss 0.757408                                        LR 0.000500    Time 0.075872    
2024-02-17 11:40:14,134 - Epoch: [58][  200/  500]    Overall Loss 0.752679    Objective Loss 0.752679                                        LR 0.000500    Time 0.074060    
2024-02-17 11:40:21,310 - Epoch: [58][  300/  500]    Overall Loss 0.761102    Objective Loss 0.761102                                        LR 0.000500    Time 0.073281    
2024-02-17 11:40:28,576 - Epoch: [58][  400/  500]    Overall Loss 0.769629    Objective Loss 0.769629                                        LR 0.000500    Time 0.073112    
2024-02-17 11:40:35,736 - Epoch: [58][  500/  500]    Overall Loss 0.776744    Objective Loss 0.776744    Top1 75.500000    Top5 94.500000    LR 0.000500    Time 0.072801    
2024-02-17 11:40:35,858 - --- validate (epoch=58)-----------
2024-02-17 11:40:35,859 - 10000 samples (100 per mini-batch)
2024-02-17 11:40:39,200 - Epoch: [58][  100/  100]    Loss 1.693725    Top1 58.040000    Top5 84.660000    
2024-02-17 11:40:39,324 - ==> Top1: 58.040    Top5: 84.660    Loss: 1.694

2024-02-17 11:40:39,334 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:40:39,335 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:40:39,395 - 

2024-02-17 11:40:39,395 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:40:47,050 - Epoch: [59][  100/  500]    Overall Loss 0.725762    Objective Loss 0.725762                                        LR 0.000500    Time 0.076492    
2024-02-17 11:40:54,175 - Epoch: [59][  200/  500]    Overall Loss 0.742909    Objective Loss 0.742909                                        LR 0.000500    Time 0.073849    
2024-02-17 11:41:01,305 - Epoch: [59][  300/  500]    Overall Loss 0.745777    Objective Loss 0.745777                                        LR 0.000500    Time 0.072983    
2024-02-17 11:41:08,423 - Epoch: [59][  400/  500]    Overall Loss 0.756595    Objective Loss 0.756595                                        LR 0.000500    Time 0.072521    
2024-02-17 11:41:15,793 - Epoch: [59][  500/  500]    Overall Loss 0.762398    Objective Loss 0.762398    Top1 74.500000    Top5 96.000000    LR 0.000500    Time 0.072748    
2024-02-17 11:41:15,915 - --- validate (epoch=59)-----------
2024-02-17 11:41:15,916 - 10000 samples (100 per mini-batch)
2024-02-17 11:41:18,965 - Epoch: [59][  100/  100]    Loss 1.658248    Top1 58.860000    Top5 85.150000    
2024-02-17 11:41:19,077 - ==> Top1: 58.860    Top5: 85.150    Loss: 1.658

2024-02-17 11:41:19,088 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:41:19,089 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:41:19,151 - 

2024-02-17 11:41:19,151 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:41:27,184 - Epoch: [60][  100/  500]    Overall Loss 0.717924    Objective Loss 0.717924                                        LR 0.000500    Time 0.080268    
2024-02-17 11:41:34,516 - Epoch: [60][  200/  500]    Overall Loss 0.740746    Objective Loss 0.740746                                        LR 0.000500    Time 0.076776    
2024-02-17 11:41:41,759 - Epoch: [60][  300/  500]    Overall Loss 0.746455    Objective Loss 0.746455                                        LR 0.000500    Time 0.075313    
2024-02-17 11:41:48,975 - Epoch: [60][  400/  500]    Overall Loss 0.755625    Objective Loss 0.755625                                        LR 0.000500    Time 0.074514    
2024-02-17 11:41:56,172 - Epoch: [60][  500/  500]    Overall Loss 0.761782    Objective Loss 0.761782    Top1 77.000000    Top5 96.000000    LR 0.000500    Time 0.073995    
2024-02-17 11:41:56,290 - --- validate (epoch=60)-----------
2024-02-17 11:41:56,291 - 10000 samples (100 per mini-batch)
2024-02-17 11:41:59,493 - Epoch: [60][  100/  100]    Loss 1.683339    Top1 57.680000    Top5 85.160000    
2024-02-17 11:41:59,611 - ==> Top1: 57.680    Top5: 85.160    Loss: 1.683

2024-02-17 11:41:59,626 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:41:59,626 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:41:59,691 - 

2024-02-17 11:41:59,692 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:42:07,464 - Epoch: [61][  100/  500]    Overall Loss 0.723786    Objective Loss 0.723786                                        LR 0.000500    Time 0.077662    
2024-02-17 11:42:14,619 - Epoch: [61][  200/  500]    Overall Loss 0.725941    Objective Loss 0.725941                                        LR 0.000500    Time 0.074585    
2024-02-17 11:42:21,681 - Epoch: [61][  300/  500]    Overall Loss 0.736931    Objective Loss 0.736931                                        LR 0.000500    Time 0.073248    
2024-02-17 11:42:28,775 - Epoch: [61][  400/  500]    Overall Loss 0.743888    Objective Loss 0.743888                                        LR 0.000500    Time 0.072661    
2024-02-17 11:42:36,093 - Epoch: [61][  500/  500]    Overall Loss 0.750233    Objective Loss 0.750233    Top1 76.500000    Top5 97.000000    LR 0.000500    Time 0.072754    
2024-02-17 11:42:36,217 - --- validate (epoch=61)-----------
2024-02-17 11:42:36,218 - 10000 samples (100 per mini-batch)
2024-02-17 11:42:39,190 - Epoch: [61][  100/  100]    Loss 1.703892    Top1 58.030000    Top5 84.660000    
2024-02-17 11:42:39,298 - ==> Top1: 58.030    Top5: 84.660    Loss: 1.704

2024-02-17 11:42:39,310 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:42:39,311 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:42:39,372 - 

2024-02-17 11:42:39,373 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:42:47,074 - Epoch: [62][  100/  500]    Overall Loss 0.716316    Objective Loss 0.716316                                        LR 0.000500    Time 0.076942    
2024-02-17 11:42:54,091 - Epoch: [62][  200/  500]    Overall Loss 0.725154    Objective Loss 0.725154                                        LR 0.000500    Time 0.073535    
2024-02-17 11:43:01,110 - Epoch: [62][  300/  500]    Overall Loss 0.726598    Objective Loss 0.726598                                        LR 0.000500    Time 0.072406    
2024-02-17 11:43:08,275 - Epoch: [62][  400/  500]    Overall Loss 0.732765    Objective Loss 0.732765                                        LR 0.000500    Time 0.072207    
2024-02-17 11:43:15,579 - Epoch: [62][  500/  500]    Overall Loss 0.735718    Objective Loss 0.735718    Top1 77.500000    Top5 97.000000    LR 0.000500    Time 0.072363    
2024-02-17 11:43:15,706 - --- validate (epoch=62)-----------
2024-02-17 11:43:15,706 - 10000 samples (100 per mini-batch)
2024-02-17 11:43:18,892 - Epoch: [62][  100/  100]    Loss 1.695187    Top1 58.080000    Top5 84.730000    
2024-02-17 11:43:19,046 - ==> Top1: 58.080    Top5: 84.730    Loss: 1.695

2024-02-17 11:43:19,057 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:43:19,057 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:43:19,116 - 

2024-02-17 11:43:19,117 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:43:26,879 - Epoch: [63][  100/  500]    Overall Loss 0.708055    Objective Loss 0.708055                                        LR 0.000500    Time 0.077567    
2024-02-17 11:43:34,029 - Epoch: [63][  200/  500]    Overall Loss 0.707810    Objective Loss 0.707810                                        LR 0.000500    Time 0.074510    
2024-02-17 11:43:41,173 - Epoch: [63][  300/  500]    Overall Loss 0.714700    Objective Loss 0.714700                                        LR 0.000500    Time 0.073472    
2024-02-17 11:43:48,267 - Epoch: [63][  400/  500]    Overall Loss 0.722657    Objective Loss 0.722657                                        LR 0.000500    Time 0.072827    
2024-02-17 11:43:55,403 - Epoch: [63][  500/  500]    Overall Loss 0.728273    Objective Loss 0.728273    Top1 78.000000    Top5 94.500000    LR 0.000500    Time 0.072526    
2024-02-17 11:43:55,583 - --- validate (epoch=63)-----------
2024-02-17 11:43:55,584 - 10000 samples (100 per mini-batch)
2024-02-17 11:43:58,531 - Epoch: [63][  100/  100]    Loss 1.712738    Top1 58.530000    Top5 85.230000    
2024-02-17 11:43:58,642 - ==> Top1: 58.530    Top5: 85.230    Loss: 1.713

2024-02-17 11:43:58,653 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:43:58,654 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:43:58,717 - 

2024-02-17 11:43:58,717 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:44:06,336 - Epoch: [64][  100/  500]    Overall Loss 0.699875    Objective Loss 0.699875                                        LR 0.000500    Time 0.076135    
2024-02-17 11:44:13,353 - Epoch: [64][  200/  500]    Overall Loss 0.711025    Objective Loss 0.711025                                        LR 0.000500    Time 0.073128    
2024-02-17 11:44:20,390 - Epoch: [64][  300/  500]    Overall Loss 0.711866    Objective Loss 0.711866                                        LR 0.000500    Time 0.072194    
2024-02-17 11:44:27,453 - Epoch: [64][  400/  500]    Overall Loss 0.718434    Objective Loss 0.718434                                        LR 0.000500    Time 0.071793    
2024-02-17 11:44:34,548 - Epoch: [64][  500/  500]    Overall Loss 0.721861    Objective Loss 0.721861    Top1 80.000000    Top5 97.500000    LR 0.000500    Time 0.071617    
2024-02-17 11:44:34,677 - --- validate (epoch=64)-----------
2024-02-17 11:44:34,678 - 10000 samples (100 per mini-batch)
2024-02-17 11:44:37,907 - Epoch: [64][  100/  100]    Loss 1.672986    Top1 58.580000    Top5 85.230000    
2024-02-17 11:44:38,011 - ==> Top1: 58.580    Top5: 85.230    Loss: 1.673

2024-02-17 11:44:38,021 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:44:38,021 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:44:38,083 - 

2024-02-17 11:44:38,083 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:44:45,646 - Epoch: [65][  100/  500]    Overall Loss 0.685307    Objective Loss 0.685307                                        LR 0.000500    Time 0.075570    
2024-02-17 11:44:52,732 - Epoch: [65][  200/  500]    Overall Loss 0.691732    Objective Loss 0.691732                                        LR 0.000500    Time 0.073192    
2024-02-17 11:44:59,885 - Epoch: [65][  300/  500]    Overall Loss 0.702046    Objective Loss 0.702046                                        LR 0.000500    Time 0.072625    
2024-02-17 11:45:07,025 - Epoch: [65][  400/  500]    Overall Loss 0.707181    Objective Loss 0.707181                                        LR 0.000500    Time 0.072308    
2024-02-17 11:45:14,160 - Epoch: [65][  500/  500]    Overall Loss 0.714602    Objective Loss 0.714602    Top1 77.000000    Top5 96.000000    LR 0.000500    Time 0.072107    
2024-02-17 11:45:14,347 - --- validate (epoch=65)-----------
2024-02-17 11:45:14,348 - 10000 samples (100 per mini-batch)
2024-02-17 11:45:17,539 - Epoch: [65][  100/  100]    Loss 1.658163    Top1 58.720000    Top5 85.510000    
2024-02-17 11:45:17,648 - ==> Top1: 58.720    Top5: 85.510    Loss: 1.658

2024-02-17 11:45:17,659 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:45:17,660 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:45:17,722 - 

2024-02-17 11:45:17,722 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:45:25,627 - Epoch: [66][  100/  500]    Overall Loss 0.674366    Objective Loss 0.674366                                        LR 0.000500    Time 0.078993    
2024-02-17 11:45:32,831 - Epoch: [66][  200/  500]    Overall Loss 0.684975    Objective Loss 0.684975                                        LR 0.000500    Time 0.075497    
2024-02-17 11:45:40,102 - Epoch: [66][  300/  500]    Overall Loss 0.694046    Objective Loss 0.694046                                        LR 0.000500    Time 0.074553    
2024-02-17 11:45:47,288 - Epoch: [66][  400/  500]    Overall Loss 0.699602    Objective Loss 0.699602                                        LR 0.000500    Time 0.073868    
2024-02-17 11:45:54,469 - Epoch: [66][  500/  500]    Overall Loss 0.704723    Objective Loss 0.704723    Top1 74.000000    Top5 93.000000    LR 0.000500    Time 0.073447    
2024-02-17 11:45:54,588 - --- validate (epoch=66)-----------
2024-02-17 11:45:54,590 - 10000 samples (100 per mini-batch)
2024-02-17 11:45:57,874 - Epoch: [66][  100/  100]    Loss 1.779810    Top1 57.320000    Top5 84.590000    
2024-02-17 11:45:57,976 - ==> Top1: 57.320    Top5: 84.590    Loss: 1.780

2024-02-17 11:45:57,995 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:45:57,996 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:45:58,066 - 

2024-02-17 11:45:58,066 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:46:05,689 - Epoch: [67][  100/  500]    Overall Loss 0.667879    Objective Loss 0.667879                                        LR 0.000500    Time 0.076172    
2024-02-17 11:46:12,775 - Epoch: [67][  200/  500]    Overall Loss 0.670391    Objective Loss 0.670391                                        LR 0.000500    Time 0.073491    
2024-02-17 11:46:19,884 - Epoch: [67][  300/  500]    Overall Loss 0.682634    Objective Loss 0.682634                                        LR 0.000500    Time 0.072676    
2024-02-17 11:46:27,038 - Epoch: [67][  400/  500]    Overall Loss 0.693407    Objective Loss 0.693407                                        LR 0.000500    Time 0.072382    
2024-02-17 11:46:34,272 - Epoch: [67][  500/  500]    Overall Loss 0.701100    Objective Loss 0.701100    Top1 82.000000    Top5 97.500000    LR 0.000500    Time 0.072365    
2024-02-17 11:46:34,395 - --- validate (epoch=67)-----------
2024-02-17 11:46:34,396 - 10000 samples (100 per mini-batch)
2024-02-17 11:46:37,592 - Epoch: [67][  100/  100]    Loss 1.624452    Top1 59.700000    Top5 85.970000    
2024-02-17 11:46:37,742 - ==> Top1: 59.700    Top5: 85.970    Loss: 1.624

2024-02-17 11:46:37,751 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:46:37,751 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:46:37,845 - 

2024-02-17 11:46:37,846 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:46:45,907 - Epoch: [68][  100/  500]    Overall Loss 0.666309    Objective Loss 0.666309                                        LR 0.000500    Time 0.080544    
2024-02-17 11:46:52,985 - Epoch: [68][  200/  500]    Overall Loss 0.671433    Objective Loss 0.671433                                        LR 0.000500    Time 0.075642    
2024-02-17 11:47:00,031 - Epoch: [68][  300/  500]    Overall Loss 0.681416    Objective Loss 0.681416                                        LR 0.000500    Time 0.073899    
2024-02-17 11:47:07,183 - Epoch: [68][  400/  500]    Overall Loss 0.681536    Objective Loss 0.681536                                        LR 0.000500    Time 0.073293    
2024-02-17 11:47:14,420 - Epoch: [68][  500/  500]    Overall Loss 0.689015    Objective Loss 0.689015    Top1 81.000000    Top5 96.500000    LR 0.000500    Time 0.073099    
2024-02-17 11:47:14,540 - --- validate (epoch=68)-----------
2024-02-17 11:47:14,541 - 10000 samples (100 per mini-batch)
2024-02-17 11:47:17,597 - Epoch: [68][  100/  100]    Loss 1.762042    Top1 57.330000    Top5 84.180000    
2024-02-17 11:47:17,710 - ==> Top1: 57.330    Top5: 84.180    Loss: 1.762

2024-02-17 11:47:17,724 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:47:17,725 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:47:17,791 - 

2024-02-17 11:47:17,792 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:47:25,454 - Epoch: [69][  100/  500]    Overall Loss 0.670748    Objective Loss 0.670748                                        LR 0.000500    Time 0.076544    
2024-02-17 11:47:32,686 - Epoch: [69][  200/  500]    Overall Loss 0.678400    Objective Loss 0.678400                                        LR 0.000500    Time 0.074408    
2024-02-17 11:47:39,778 - Epoch: [69][  300/  500]    Overall Loss 0.680186    Objective Loss 0.680186                                        LR 0.000500    Time 0.073232    
2024-02-17 11:47:46,945 - Epoch: [69][  400/  500]    Overall Loss 0.683780    Objective Loss 0.683780                                        LR 0.000500    Time 0.072830    
2024-02-17 11:47:54,247 - Epoch: [69][  500/  500]    Overall Loss 0.688127    Objective Loss 0.688127    Top1 75.000000    Top5 95.500000    LR 0.000500    Time 0.072859    
2024-02-17 11:47:54,424 - --- validate (epoch=69)-----------
2024-02-17 11:47:54,424 - 10000 samples (100 per mini-batch)
2024-02-17 11:47:57,426 - Epoch: [69][  100/  100]    Loss 1.698582    Top1 58.650000    Top5 85.260000    
2024-02-17 11:47:57,580 - ==> Top1: 58.650    Top5: 85.260    Loss: 1.699

2024-02-17 11:47:57,590 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:47:57,591 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:47:57,653 - 

2024-02-17 11:47:57,653 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:48:05,353 - Epoch: [70][  100/  500]    Overall Loss 0.654303    Objective Loss 0.654303                                        LR 0.000500    Time 0.076927    
2024-02-17 11:48:12,587 - Epoch: [70][  200/  500]    Overall Loss 0.663836    Objective Loss 0.663836                                        LR 0.000500    Time 0.074609    
2024-02-17 11:48:19,808 - Epoch: [70][  300/  500]    Overall Loss 0.670636    Objective Loss 0.670636                                        LR 0.000500    Time 0.073795    
2024-02-17 11:48:26,933 - Epoch: [70][  400/  500]    Overall Loss 0.675985    Objective Loss 0.675985                                        LR 0.000500    Time 0.073147    
2024-02-17 11:48:34,178 - Epoch: [70][  500/  500]    Overall Loss 0.682836    Objective Loss 0.682836    Top1 80.000000    Top5 96.500000    LR 0.000500    Time 0.072998    
2024-02-17 11:48:34,311 - --- validate (epoch=70)-----------
2024-02-17 11:48:34,312 - 10000 samples (100 per mini-batch)
2024-02-17 11:48:37,458 - Epoch: [70][  100/  100]    Loss 1.732679    Top1 58.640000    Top5 84.830000    
2024-02-17 11:48:37,582 - ==> Top1: 58.640    Top5: 84.830    Loss: 1.733

2024-02-17 11:48:37,591 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:48:37,591 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:48:37,655 - 

2024-02-17 11:48:37,655 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:48:45,280 - Epoch: [71][  100/  500]    Overall Loss 0.640584    Objective Loss 0.640584                                        LR 0.000500    Time 0.076192    
2024-02-17 11:48:52,481 - Epoch: [71][  200/  500]    Overall Loss 0.644814    Objective Loss 0.644814                                        LR 0.000500    Time 0.074080    
2024-02-17 11:48:59,578 - Epoch: [71][  300/  500]    Overall Loss 0.658395    Objective Loss 0.658395                                        LR 0.000500    Time 0.073028    
2024-02-17 11:49:06,713 - Epoch: [71][  400/  500]    Overall Loss 0.664975    Objective Loss 0.664975                                        LR 0.000500    Time 0.072596    
2024-02-17 11:49:13,987 - Epoch: [71][  500/  500]    Overall Loss 0.673418    Objective Loss 0.673418    Top1 76.500000    Top5 97.500000    LR 0.000500    Time 0.072616    
2024-02-17 11:49:14,110 - --- validate (epoch=71)-----------
2024-02-17 11:49:14,111 - 10000 samples (100 per mini-batch)
2024-02-17 11:49:16,964 - Epoch: [71][  100/  100]    Loss 1.694343    Top1 59.110000    Top5 85.550000    
2024-02-17 11:49:17,076 - ==> Top1: 59.110    Top5: 85.550    Loss: 1.694

2024-02-17 11:49:17,087 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:49:17,088 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:49:17,149 - 

2024-02-17 11:49:17,149 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:49:25,164 - Epoch: [72][  100/  500]    Overall Loss 0.634068    Objective Loss 0.634068                                        LR 0.000500    Time 0.080085    
2024-02-17 11:49:32,437 - Epoch: [72][  200/  500]    Overall Loss 0.639392    Objective Loss 0.639392                                        LR 0.000500    Time 0.076387    
2024-02-17 11:49:39,560 - Epoch: [72][  300/  500]    Overall Loss 0.649119    Objective Loss 0.649119                                        LR 0.000500    Time 0.074652    
2024-02-17 11:49:46,673 - Epoch: [72][  400/  500]    Overall Loss 0.660243    Objective Loss 0.660243                                        LR 0.000500    Time 0.073760    
2024-02-17 11:49:53,920 - Epoch: [72][  500/  500]    Overall Loss 0.662036    Objective Loss 0.662036    Top1 75.500000    Top5 97.000000    LR 0.000500    Time 0.073492    
2024-02-17 11:49:54,060 - --- validate (epoch=72)-----------
2024-02-17 11:49:54,061 - 10000 samples (100 per mini-batch)
2024-02-17 11:49:57,101 - Epoch: [72][  100/  100]    Loss 1.789423    Top1 58.120000    Top5 84.880000    
2024-02-17 11:49:57,217 - ==> Top1: 58.120    Top5: 84.880    Loss: 1.789

2024-02-17 11:49:57,226 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:49:57,226 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:49:57,291 - 

2024-02-17 11:49:57,291 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:50:04,947 - Epoch: [73][  100/  500]    Overall Loss 0.635874    Objective Loss 0.635874                                        LR 0.000500    Time 0.076505    
2024-02-17 11:50:12,098 - Epoch: [73][  200/  500]    Overall Loss 0.634746    Objective Loss 0.634746                                        LR 0.000500    Time 0.073983    
2024-02-17 11:50:19,243 - Epoch: [73][  300/  500]    Overall Loss 0.643160    Objective Loss 0.643160                                        LR 0.000500    Time 0.073124    
2024-02-17 11:50:26,487 - Epoch: [73][  400/  500]    Overall Loss 0.652762    Objective Loss 0.652762                                        LR 0.000500    Time 0.072942    
2024-02-17 11:50:33,761 - Epoch: [73][  500/  500]    Overall Loss 0.657868    Objective Loss 0.657868    Top1 81.000000    Top5 97.000000    LR 0.000500    Time 0.072893    
2024-02-17 11:50:33,890 - --- validate (epoch=73)-----------
2024-02-17 11:50:33,891 - 10000 samples (100 per mini-batch)
2024-02-17 11:50:36,764 - Epoch: [73][  100/  100]    Loss 1.707734    Top1 58.790000    Top5 85.400000    
2024-02-17 11:50:36,854 - ==> Top1: 58.790    Top5: 85.400    Loss: 1.708

2024-02-17 11:50:36,866 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:50:36,867 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:50:36,929 - 

2024-02-17 11:50:36,929 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:50:44,718 - Epoch: [74][  100/  500]    Overall Loss 0.609792    Objective Loss 0.609792                                        LR 0.000500    Time 0.077827    
2024-02-17 11:50:51,476 - Epoch: [74][  200/  500]    Overall Loss 0.622190    Objective Loss 0.622190                                        LR 0.000500    Time 0.072689    
2024-02-17 11:50:58,351 - Epoch: [74][  300/  500]    Overall Loss 0.634530    Objective Loss 0.634530                                        LR 0.000500    Time 0.071364    
2024-02-17 11:51:05,501 - Epoch: [74][  400/  500]    Overall Loss 0.644153    Objective Loss 0.644153                                        LR 0.000500    Time 0.071387    
2024-02-17 11:51:12,703 - Epoch: [74][  500/  500]    Overall Loss 0.649697    Objective Loss 0.649697    Top1 84.000000    Top5 98.000000    LR 0.000500    Time 0.071504    
2024-02-17 11:51:12,813 - --- validate (epoch=74)-----------
2024-02-17 11:51:12,814 - 10000 samples (100 per mini-batch)
2024-02-17 11:51:15,812 - Epoch: [74][  100/  100]    Loss 1.735290    Top1 58.450000    Top5 85.140000    
2024-02-17 11:51:15,922 - ==> Top1: 58.450    Top5: 85.140    Loss: 1.735

2024-02-17 11:51:15,930 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:51:15,930 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:51:15,989 - 

2024-02-17 11:51:15,990 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:51:23,688 - Epoch: [75][  100/  500]    Overall Loss 0.620486    Objective Loss 0.620486                                        LR 0.000500    Time 0.076923    
2024-02-17 11:51:30,835 - Epoch: [75][  200/  500]    Overall Loss 0.633876    Objective Loss 0.633876                                        LR 0.000500    Time 0.074175    
2024-02-17 11:51:38,044 - Epoch: [75][  300/  500]    Overall Loss 0.637861    Objective Loss 0.637861                                        LR 0.000500    Time 0.073465    
2024-02-17 11:51:44,911 - Epoch: [75][  400/  500]    Overall Loss 0.641115    Objective Loss 0.641115                                        LR 0.000500    Time 0.072256    
2024-02-17 11:51:51,996 - Epoch: [75][  500/  500]    Overall Loss 0.644184    Objective Loss 0.644184    Top1 79.500000    Top5 97.000000    LR 0.000500    Time 0.071966    
2024-02-17 11:51:52,115 - --- validate (epoch=75)-----------
2024-02-17 11:51:52,115 - 10000 samples (100 per mini-batch)
2024-02-17 11:51:55,139 - Epoch: [75][  100/  100]    Loss 1.705996    Top1 59.100000    Top5 85.270000    
2024-02-17 11:51:55,287 - ==> Top1: 59.100    Top5: 85.270    Loss: 1.706

2024-02-17 11:51:55,298 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:51:55,299 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:51:55,360 - 

2024-02-17 11:51:55,360 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:52:03,215 - Epoch: [76][  100/  500]    Overall Loss 0.610907    Objective Loss 0.610907                                        LR 0.000500    Time 0.078494    
2024-02-17 11:52:10,287 - Epoch: [76][  200/  500]    Overall Loss 0.624727    Objective Loss 0.624727                                        LR 0.000500    Time 0.074582    
2024-02-17 11:52:17,382 - Epoch: [76][  300/  500]    Overall Loss 0.624320    Objective Loss 0.624320                                        LR 0.000500    Time 0.073360    
2024-02-17 11:52:24,612 - Epoch: [76][  400/  500]    Overall Loss 0.631084    Objective Loss 0.631084                                        LR 0.000500    Time 0.073083    
2024-02-17 11:52:31,974 - Epoch: [76][  500/  500]    Overall Loss 0.636151    Objective Loss 0.636151    Top1 77.500000    Top5 98.500000    LR 0.000500    Time 0.073182    
2024-02-17 11:52:32,096 - --- validate (epoch=76)-----------
2024-02-17 11:52:32,096 - 10000 samples (100 per mini-batch)
2024-02-17 11:52:35,115 - Epoch: [76][  100/  100]    Loss 1.739712    Top1 58.250000    Top5 85.250000    
2024-02-17 11:52:35,232 - ==> Top1: 58.250    Top5: 85.250    Loss: 1.740

2024-02-17 11:52:35,243 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:52:35,243 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:52:35,306 - 

2024-02-17 11:52:35,307 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:52:43,044 - Epoch: [77][  100/  500]    Overall Loss 0.597206    Objective Loss 0.597206                                        LR 0.000500    Time 0.077298    
2024-02-17 11:52:50,071 - Epoch: [77][  200/  500]    Overall Loss 0.601952    Objective Loss 0.601952                                        LR 0.000500    Time 0.073759    
2024-02-17 11:52:57,277 - Epoch: [77][  300/  500]    Overall Loss 0.609414    Objective Loss 0.609414                                        LR 0.000500    Time 0.073178    
2024-02-17 11:53:04,410 - Epoch: [77][  400/  500]    Overall Loss 0.621360    Objective Loss 0.621360                                        LR 0.000500    Time 0.072705    
2024-02-17 11:53:11,603 - Epoch: [77][  500/  500]    Overall Loss 0.630023    Objective Loss 0.630023    Top1 84.000000    Top5 99.000000    LR 0.000500    Time 0.072541    
2024-02-17 11:53:11,797 - --- validate (epoch=77)-----------
2024-02-17 11:53:11,798 - 10000 samples (100 per mini-batch)
2024-02-17 11:53:14,906 - Epoch: [77][  100/  100]    Loss 1.715761    Top1 58.450000    Top5 85.800000    
2024-02-17 11:53:15,037 - ==> Top1: 58.450    Top5: 85.800    Loss: 1.716

2024-02-17 11:53:15,054 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:53:15,054 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:53:15,119 - 

2024-02-17 11:53:15,119 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:53:22,807 - Epoch: [78][  100/  500]    Overall Loss 0.602155    Objective Loss 0.602155                                        LR 0.000500    Time 0.076819    
2024-02-17 11:53:29,951 - Epoch: [78][  200/  500]    Overall Loss 0.619924    Objective Loss 0.619924                                        LR 0.000500    Time 0.074108    
2024-02-17 11:53:37,163 - Epoch: [78][  300/  500]    Overall Loss 0.618334    Objective Loss 0.618334                                        LR 0.000500    Time 0.073430    
2024-02-17 11:53:44,241 - Epoch: [78][  400/  500]    Overall Loss 0.623308    Objective Loss 0.623308                                        LR 0.000500    Time 0.072757    
2024-02-17 11:53:51,370 - Epoch: [78][  500/  500]    Overall Loss 0.626740    Objective Loss 0.626740    Top1 73.500000    Top5 97.500000    LR 0.000500    Time 0.072455    
2024-02-17 11:53:51,512 - --- validate (epoch=78)-----------
2024-02-17 11:53:51,513 - 10000 samples (100 per mini-batch)
2024-02-17 11:53:54,452 - Epoch: [78][  100/  100]    Loss 1.753041    Top1 57.960000    Top5 85.190000    
2024-02-17 11:53:54,625 - ==> Top1: 57.960    Top5: 85.190    Loss: 1.753

2024-02-17 11:53:54,636 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:53:54,637 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:53:54,701 - 

2024-02-17 11:53:54,701 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:54:02,451 - Epoch: [79][  100/  500]    Overall Loss 0.591097    Objective Loss 0.591097                                        LR 0.000500    Time 0.077442    
2024-02-17 11:54:09,659 - Epoch: [79][  200/  500]    Overall Loss 0.602335    Objective Loss 0.602335                                        LR 0.000500    Time 0.074736    
2024-02-17 11:54:16,743 - Epoch: [79][  300/  500]    Overall Loss 0.608733    Objective Loss 0.608733                                        LR 0.000500    Time 0.073424    
2024-02-17 11:54:23,746 - Epoch: [79][  400/  500]    Overall Loss 0.613700    Objective Loss 0.613700                                        LR 0.000500    Time 0.072567    
2024-02-17 11:54:30,989 - Epoch: [79][  500/  500]    Overall Loss 0.619532    Objective Loss 0.619532    Top1 76.000000    Top5 97.000000    LR 0.000500    Time 0.072531    
2024-02-17 11:54:31,149 - --- validate (epoch=79)-----------
2024-02-17 11:54:31,150 - 10000 samples (100 per mini-batch)
2024-02-17 11:54:34,204 - Epoch: [79][  100/  100]    Loss 1.748150    Top1 58.340000    Top5 85.160000    
2024-02-17 11:54:34,334 - ==> Top1: 58.340    Top5: 85.160    Loss: 1.748

2024-02-17 11:54:34,345 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:54:34,345 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:54:34,408 - 

2024-02-17 11:54:34,408 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:54:42,372 - Epoch: [80][  100/  500]    Overall Loss 0.586069    Objective Loss 0.586069                                        LR 0.000500    Time 0.079580    
2024-02-17 11:54:49,613 - Epoch: [80][  200/  500]    Overall Loss 0.584702    Objective Loss 0.584702                                        LR 0.000500    Time 0.075972    
2024-02-17 11:54:56,738 - Epoch: [80][  300/  500]    Overall Loss 0.597921    Objective Loss 0.597921                                        LR 0.000500    Time 0.074384    
2024-02-17 11:55:03,916 - Epoch: [80][  400/  500]    Overall Loss 0.604512    Objective Loss 0.604512                                        LR 0.000500    Time 0.073721    
2024-02-17 11:55:11,282 - Epoch: [80][  500/  500]    Overall Loss 0.610524    Objective Loss 0.610524    Top1 83.500000    Top5 97.500000    LR 0.000500    Time 0.073701    
2024-02-17 11:55:11,407 - --- validate (epoch=80)-----------
2024-02-17 11:55:11,408 - 10000 samples (100 per mini-batch)
2024-02-17 11:55:14,386 - Epoch: [80][  100/  100]    Loss 1.771340    Top1 58.120000    Top5 84.990000    
2024-02-17 11:55:14,520 - ==> Top1: 58.120    Top5: 84.990    Loss: 1.771

2024-02-17 11:55:14,534 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:55:14,534 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:55:14,597 - 

2024-02-17 11:55:14,597 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:55:22,202 - Epoch: [81][  100/  500]    Overall Loss 0.581604    Objective Loss 0.581604                                        LR 0.000500    Time 0.075963    
2024-02-17 11:55:29,392 - Epoch: [81][  200/  500]    Overall Loss 0.582514    Objective Loss 0.582514                                        LR 0.000500    Time 0.073913    
2024-02-17 11:55:36,566 - Epoch: [81][  300/  500]    Overall Loss 0.598248    Objective Loss 0.598248                                        LR 0.000500    Time 0.073173    
2024-02-17 11:55:43,689 - Epoch: [81][  400/  500]    Overall Loss 0.606046    Objective Loss 0.606046                                        LR 0.000500    Time 0.072676    
2024-02-17 11:55:50,981 - Epoch: [81][  500/  500]    Overall Loss 0.609403    Objective Loss 0.609403    Top1 82.000000    Top5 97.000000    LR 0.000500    Time 0.072716    
2024-02-17 11:55:51,177 - --- validate (epoch=81)-----------
2024-02-17 11:55:51,177 - 10000 samples (100 per mini-batch)
2024-02-17 11:55:54,329 - Epoch: [81][  100/  100]    Loss 1.777510    Top1 58.730000    Top5 84.850000    
2024-02-17 11:55:54,448 - ==> Top1: 58.730    Top5: 84.850    Loss: 1.778

2024-02-17 11:55:54,459 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:55:54,459 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:55:54,521 - 

2024-02-17 11:55:54,522 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:56:02,187 - Epoch: [82][  100/  500]    Overall Loss 0.580655    Objective Loss 0.580655                                        LR 0.000500    Time 0.076598    
2024-02-17 11:56:09,420 - Epoch: [82][  200/  500]    Overall Loss 0.586616    Objective Loss 0.586616                                        LR 0.000500    Time 0.074439    
2024-02-17 11:56:16,761 - Epoch: [82][  300/  500]    Overall Loss 0.591215    Objective Loss 0.591215                                        LR 0.000500    Time 0.074080    
2024-02-17 11:56:23,826 - Epoch: [82][  400/  500]    Overall Loss 0.591351    Objective Loss 0.591351                                        LR 0.000500    Time 0.073210    
2024-02-17 11:56:30,953 - Epoch: [82][  500/  500]    Overall Loss 0.597625    Objective Loss 0.597625    Top1 83.500000    Top5 99.000000    LR 0.000500    Time 0.072814    
2024-02-17 11:56:31,102 - --- validate (epoch=82)-----------
2024-02-17 11:56:31,103 - 10000 samples (100 per mini-batch)
2024-02-17 11:56:34,208 - Epoch: [82][  100/  100]    Loss 1.825459    Top1 57.690000    Top5 84.730000    
2024-02-17 11:56:34,314 - ==> Top1: 57.690    Top5: 84.730    Loss: 1.825

2024-02-17 11:56:34,553 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:56:34,553 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:56:34,609 - 

2024-02-17 11:56:34,610 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:56:42,207 - Epoch: [83][  100/  500]    Overall Loss 0.563809    Objective Loss 0.563809                                        LR 0.000500    Time 0.075919    
2024-02-17 11:56:49,390 - Epoch: [83][  200/  500]    Overall Loss 0.573816    Objective Loss 0.573816                                        LR 0.000500    Time 0.073850    
2024-02-17 11:56:56,607 - Epoch: [83][  300/  500]    Overall Loss 0.576394    Objective Loss 0.576394                                        LR 0.000500    Time 0.073275    
2024-02-17 11:57:03,694 - Epoch: [83][  400/  500]    Overall Loss 0.585076    Objective Loss 0.585076                                        LR 0.000500    Time 0.072662    
2024-02-17 11:57:10,909 - Epoch: [83][  500/  500]    Overall Loss 0.590422    Objective Loss 0.590422    Top1 83.000000    Top5 97.000000    LR 0.000500    Time 0.072551    
2024-02-17 11:57:11,030 - --- validate (epoch=83)-----------
2024-02-17 11:57:11,031 - 10000 samples (100 per mini-batch)
2024-02-17 11:57:13,960 - Epoch: [83][  100/  100]    Loss 1.793144    Top1 58.300000    Top5 85.080000    
2024-02-17 11:57:14,071 - ==> Top1: 58.300    Top5: 85.080    Loss: 1.793

2024-02-17 11:57:14,081 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:57:14,081 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:57:14,147 - 

2024-02-17 11:57:14,148 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:57:21,721 - Epoch: [84][  100/  500]    Overall Loss 0.537897    Objective Loss 0.537897                                        LR 0.000500    Time 0.075672    
2024-02-17 11:57:29,000 - Epoch: [84][  200/  500]    Overall Loss 0.562149    Objective Loss 0.562149                                        LR 0.000500    Time 0.074212    
2024-02-17 11:57:36,210 - Epoch: [84][  300/  500]    Overall Loss 0.570473    Objective Loss 0.570473                                        LR 0.000500    Time 0.073491    
2024-02-17 11:57:43,252 - Epoch: [84][  400/  500]    Overall Loss 0.577448    Objective Loss 0.577448                                        LR 0.000500    Time 0.072712    
2024-02-17 11:57:50,404 - Epoch: [84][  500/  500]    Overall Loss 0.587891    Objective Loss 0.587891    Top1 81.000000    Top5 97.000000    LR 0.000500    Time 0.072465    
2024-02-17 11:57:50,510 - --- validate (epoch=84)-----------
2024-02-17 11:57:50,511 - 10000 samples (100 per mini-batch)
2024-02-17 11:57:53,579 - Epoch: [84][  100/  100]    Loss 1.739641    Top1 58.970000    Top5 85.100000    
2024-02-17 11:57:53,687 - ==> Top1: 58.970    Top5: 85.100    Loss: 1.740

2024-02-17 11:57:53,701 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:57:53,702 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:57:53,761 - 

2024-02-17 11:57:53,762 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:58:01,393 - Epoch: [85][  100/  500]    Overall Loss 0.554587    Objective Loss 0.554587                                        LR 0.000500    Time 0.076258    
2024-02-17 11:58:08,531 - Epoch: [85][  200/  500]    Overall Loss 0.572589    Objective Loss 0.572589                                        LR 0.000500    Time 0.073796    
2024-02-17 11:58:15,760 - Epoch: [85][  300/  500]    Overall Loss 0.576759    Objective Loss 0.576759                                        LR 0.000500    Time 0.073279    
2024-02-17 11:58:22,806 - Epoch: [85][  400/  500]    Overall Loss 0.583177    Objective Loss 0.583177                                        LR 0.000500    Time 0.072564    
2024-02-17 11:58:29,995 - Epoch: [85][  500/  500]    Overall Loss 0.589292    Objective Loss 0.589292    Top1 81.000000    Top5 97.000000    LR 0.000500    Time 0.072420    
2024-02-17 11:58:30,143 - --- validate (epoch=85)-----------
2024-02-17 11:58:30,144 - 10000 samples (100 per mini-batch)
2024-02-17 11:58:33,047 - Epoch: [85][  100/  100]    Loss 1.771237    Top1 58.690000    Top5 85.650000    
2024-02-17 11:58:33,144 - ==> Top1: 58.690    Top5: 85.650    Loss: 1.771

2024-02-17 11:58:33,155 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:58:33,156 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:58:33,218 - 

2024-02-17 11:58:33,218 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:58:41,104 - Epoch: [86][  100/  500]    Overall Loss 0.539606    Objective Loss 0.539606                                        LR 0.000500    Time 0.078798    
2024-02-17 11:58:48,004 - Epoch: [86][  200/  500]    Overall Loss 0.546295    Objective Loss 0.546295                                        LR 0.000500    Time 0.073879    
2024-02-17 11:58:55,295 - Epoch: [86][  300/  500]    Overall Loss 0.558972    Objective Loss 0.558972                                        LR 0.000500    Time 0.073542    
2024-02-17 11:59:02,472 - Epoch: [86][  400/  500]    Overall Loss 0.567789    Objective Loss 0.567789                                        LR 0.000500    Time 0.073088    
2024-02-17 11:59:09,652 - Epoch: [86][  500/  500]    Overall Loss 0.574149    Objective Loss 0.574149    Top1 82.000000    Top5 97.000000    LR 0.000500    Time 0.072823    
2024-02-17 11:59:09,794 - --- validate (epoch=86)-----------
2024-02-17 11:59:09,794 - 10000 samples (100 per mini-batch)
2024-02-17 11:59:12,760 - Epoch: [86][  100/  100]    Loss 1.823585    Top1 57.930000    Top5 84.860000    
2024-02-17 11:59:12,890 - ==> Top1: 57.930    Top5: 84.860    Loss: 1.824

2024-02-17 11:59:12,907 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:59:12,908 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:59:12,977 - 

2024-02-17 11:59:12,977 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:59:20,648 - Epoch: [87][  100/  500]    Overall Loss 0.556764    Objective Loss 0.556764                                        LR 0.000500    Time 0.076652    
2024-02-17 11:59:27,625 - Epoch: [87][  200/  500]    Overall Loss 0.549502    Objective Loss 0.549502                                        LR 0.000500    Time 0.073191    
2024-02-17 11:59:34,725 - Epoch: [87][  300/  500]    Overall Loss 0.558405    Objective Loss 0.558405                                        LR 0.000500    Time 0.072446    
2024-02-17 11:59:41,807 - Epoch: [87][  400/  500]    Overall Loss 0.566278    Objective Loss 0.566278                                        LR 0.000500    Time 0.072030    
2024-02-17 11:59:49,005 - Epoch: [87][  500/  500]    Overall Loss 0.571798    Objective Loss 0.571798    Top1 79.000000    Top5 97.500000    LR 0.000500    Time 0.072011    
2024-02-17 11:59:49,119 - --- validate (epoch=87)-----------
2024-02-17 11:59:49,120 - 10000 samples (100 per mini-batch)
2024-02-17 11:59:52,054 - Epoch: [87][  100/  100]    Loss 1.827984    Top1 58.240000    Top5 84.840000    
2024-02-17 11:59:52,150 - ==> Top1: 58.240    Top5: 84.840    Loss: 1.828

2024-02-17 11:59:52,161 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:59:52,162 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 11:59:52,225 - 

2024-02-17 11:59:52,226 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:00:00,064 - Epoch: [88][  100/  500]    Overall Loss 0.547444    Objective Loss 0.547444                                        LR 0.000500    Time 0.078326    
2024-02-17 12:00:07,170 - Epoch: [88][  200/  500]    Overall Loss 0.558846    Objective Loss 0.558846                                        LR 0.000500    Time 0.074671    
2024-02-17 12:00:14,349 - Epoch: [88][  300/  500]    Overall Loss 0.563672    Objective Loss 0.563672                                        LR 0.000500    Time 0.073698    
2024-02-17 12:00:21,594 - Epoch: [88][  400/  500]    Overall Loss 0.568608    Objective Loss 0.568608                                        LR 0.000500    Time 0.073376    
2024-02-17 12:00:28,880 - Epoch: [88][  500/  500]    Overall Loss 0.572333    Objective Loss 0.572333    Top1 87.000000    Top5 98.000000    LR 0.000500    Time 0.073264    
2024-02-17 12:00:28,992 - --- validate (epoch=88)-----------
2024-02-17 12:00:28,992 - 10000 samples (100 per mini-batch)
2024-02-17 12:00:31,887 - Epoch: [88][  100/  100]    Loss 1.751668    Top1 58.800000    Top5 85.800000    
2024-02-17 12:00:31,996 - ==> Top1: 58.800    Top5: 85.800    Loss: 1.752

2024-02-17 12:00:32,007 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:00:32,007 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:00:32,068 - 

2024-02-17 12:00:32,068 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:00:39,653 - Epoch: [89][  100/  500]    Overall Loss 0.537153    Objective Loss 0.537153                                        LR 0.000500    Time 0.075793    
2024-02-17 12:00:46,844 - Epoch: [89][  200/  500]    Overall Loss 0.542063    Objective Loss 0.542063                                        LR 0.000500    Time 0.073831    
2024-02-17 12:00:54,114 - Epoch: [89][  300/  500]    Overall Loss 0.545037    Objective Loss 0.545037                                        LR 0.000500    Time 0.073437    
2024-02-17 12:01:01,415 - Epoch: [89][  400/  500]    Overall Loss 0.550915    Objective Loss 0.550915                                        LR 0.000500    Time 0.073320    
2024-02-17 12:01:08,790 - Epoch: [89][  500/  500]    Overall Loss 0.556556    Objective Loss 0.556556    Top1 87.000000    Top5 97.000000    LR 0.000500    Time 0.073398    
2024-02-17 12:01:08,946 - --- validate (epoch=89)-----------
2024-02-17 12:01:08,947 - 10000 samples (100 per mini-batch)
2024-02-17 12:01:11,827 - Epoch: [89][  100/  100]    Loss 1.848719    Top1 57.640000    Top5 84.930000    
2024-02-17 12:01:11,999 - ==> Top1: 57.640    Top5: 84.930    Loss: 1.849

2024-02-17 12:01:12,009 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:01:12,010 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:01:12,070 - 

2024-02-17 12:01:12,070 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:01:19,877 - Epoch: [90][  100/  500]    Overall Loss 0.529408    Objective Loss 0.529408                                        LR 0.000500    Time 0.078021    
2024-02-17 12:01:26,924 - Epoch: [90][  200/  500]    Overall Loss 0.539266    Objective Loss 0.539266                                        LR 0.000500    Time 0.074221    
2024-02-17 12:01:34,240 - Epoch: [90][  300/  500]    Overall Loss 0.540605    Objective Loss 0.540605                                        LR 0.000500    Time 0.073852    
2024-02-17 12:01:41,348 - Epoch: [90][  400/  500]    Overall Loss 0.546338    Objective Loss 0.546338                                        LR 0.000500    Time 0.073148    
2024-02-17 12:01:48,582 - Epoch: [90][  500/  500]    Overall Loss 0.556061    Objective Loss 0.556061    Top1 83.000000    Top5 97.500000    LR 0.000500    Time 0.072979    
2024-02-17 12:01:48,700 - --- validate (epoch=90)-----------
2024-02-17 12:01:48,701 - 10000 samples (100 per mini-batch)
2024-02-17 12:01:51,611 - Epoch: [90][  100/  100]    Loss 1.862603    Top1 57.940000    Top5 84.360000    
2024-02-17 12:01:51,718 - ==> Top1: 57.940    Top5: 84.360    Loss: 1.863

2024-02-17 12:01:51,730 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:01:51,730 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:01:51,793 - 

2024-02-17 12:01:51,793 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:01:59,352 - Epoch: [91][  100/  500]    Overall Loss 0.518050    Objective Loss 0.518050                                        LR 0.000500    Time 0.075530    
2024-02-17 12:02:06,431 - Epoch: [91][  200/  500]    Overall Loss 0.528958    Objective Loss 0.528958                                        LR 0.000500    Time 0.073131    
2024-02-17 12:02:13,602 - Epoch: [91][  300/  500]    Overall Loss 0.539271    Objective Loss 0.539271                                        LR 0.000500    Time 0.072644    
2024-02-17 12:02:20,550 - Epoch: [91][  400/  500]    Overall Loss 0.544243    Objective Loss 0.544243                                        LR 0.000500    Time 0.071840    
2024-02-17 12:02:27,801 - Epoch: [91][  500/  500]    Overall Loss 0.549738    Objective Loss 0.549738    Top1 87.000000    Top5 99.000000    LR 0.000500    Time 0.071965    
2024-02-17 12:02:27,927 - --- validate (epoch=91)-----------
2024-02-17 12:02:27,927 - 10000 samples (100 per mini-batch)
2024-02-17 12:02:30,847 - Epoch: [91][  100/  100]    Loss 1.888735    Top1 57.300000    Top5 84.190000    
2024-02-17 12:02:30,956 - ==> Top1: 57.300    Top5: 84.190    Loss: 1.889

2024-02-17 12:02:30,972 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:02:30,973 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:02:31,033 - 

2024-02-17 12:02:31,033 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:02:38,944 - Epoch: [92][  100/  500]    Overall Loss 0.512143    Objective Loss 0.512143                                        LR 0.000500    Time 0.079054    
2024-02-17 12:02:46,258 - Epoch: [92][  200/  500]    Overall Loss 0.524299    Objective Loss 0.524299                                        LR 0.000500    Time 0.076072    
2024-02-17 12:02:53,441 - Epoch: [92][  300/  500]    Overall Loss 0.532035    Objective Loss 0.532035                                        LR 0.000500    Time 0.074643    
2024-02-17 12:03:00,523 - Epoch: [92][  400/  500]    Overall Loss 0.539652    Objective Loss 0.539652                                        LR 0.000500    Time 0.073677    
2024-02-17 12:03:07,709 - Epoch: [92][  500/  500]    Overall Loss 0.544782    Objective Loss 0.544782    Top1 77.500000    Top5 96.500000    LR 0.000500    Time 0.073305    
2024-02-17 12:03:07,844 - --- validate (epoch=92)-----------
2024-02-17 12:03:07,845 - 10000 samples (100 per mini-batch)
2024-02-17 12:03:10,751 - Epoch: [92][  100/  100]    Loss 1.756985    Top1 59.200000    Top5 85.740000    
2024-02-17 12:03:10,894 - ==> Top1: 59.200    Top5: 85.740    Loss: 1.757

2024-02-17 12:03:10,905 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:03:10,905 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:03:10,969 - 

2024-02-17 12:03:10,969 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:03:18,543 - Epoch: [93][  100/  500]    Overall Loss 0.506487    Objective Loss 0.506487                                        LR 0.000500    Time 0.075679    
2024-02-17 12:03:25,631 - Epoch: [93][  200/  500]    Overall Loss 0.517290    Objective Loss 0.517290                                        LR 0.000500    Time 0.073256    
2024-02-17 12:03:32,833 - Epoch: [93][  300/  500]    Overall Loss 0.525091    Objective Loss 0.525091                                        LR 0.000500    Time 0.072830    
2024-02-17 12:03:39,839 - Epoch: [93][  400/  500]    Overall Loss 0.529492    Objective Loss 0.529492                                        LR 0.000500    Time 0.072126    
2024-02-17 12:03:47,070 - Epoch: [93][  500/  500]    Overall Loss 0.537910    Objective Loss 0.537910    Top1 83.000000    Top5 99.000000    LR 0.000500    Time 0.072153    
2024-02-17 12:03:47,207 - --- validate (epoch=93)-----------
2024-02-17 12:03:47,208 - 10000 samples (100 per mini-batch)
2024-02-17 12:03:50,210 - Epoch: [93][  100/  100]    Loss 1.832554    Top1 58.650000    Top5 84.650000    
2024-02-17 12:03:50,362 - ==> Top1: 58.650    Top5: 84.650    Loss: 1.833

2024-02-17 12:03:50,378 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:03:50,379 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:03:50,440 - 

2024-02-17 12:03:50,440 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:03:58,335 - Epoch: [94][  100/  500]    Overall Loss 0.500161    Objective Loss 0.500161                                        LR 0.000500    Time 0.078869    
2024-02-17 12:04:05,488 - Epoch: [94][  200/  500]    Overall Loss 0.509390    Objective Loss 0.509390                                        LR 0.000500    Time 0.075178    
2024-02-17 12:04:12,695 - Epoch: [94][  300/  500]    Overall Loss 0.520344    Objective Loss 0.520344                                        LR 0.000500    Time 0.074127    
2024-02-17 12:04:19,650 - Epoch: [94][  400/  500]    Overall Loss 0.527340    Objective Loss 0.527340                                        LR 0.000500    Time 0.072975    
2024-02-17 12:04:26,763 - Epoch: [94][  500/  500]    Overall Loss 0.534484    Objective Loss 0.534484    Top1 84.000000    Top5 98.000000    LR 0.000500    Time 0.072597    
2024-02-17 12:04:26,874 - --- validate (epoch=94)-----------
2024-02-17 12:04:26,875 - 10000 samples (100 per mini-batch)
2024-02-17 12:04:29,525 - Epoch: [94][  100/  100]    Loss 1.857067    Top1 57.960000    Top5 84.550000    
2024-02-17 12:04:29,631 - ==> Top1: 57.960    Top5: 84.550    Loss: 1.857

2024-02-17 12:04:29,643 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:04:29,644 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:04:29,709 - 

2024-02-17 12:04:29,710 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:04:37,297 - Epoch: [95][  100/  500]    Overall Loss 0.505093    Objective Loss 0.505093                                        LR 0.000500    Time 0.075817    
2024-02-17 12:04:44,552 - Epoch: [95][  200/  500]    Overall Loss 0.510239    Objective Loss 0.510239                                        LR 0.000500    Time 0.074161    
2024-02-17 12:04:51,590 - Epoch: [95][  300/  500]    Overall Loss 0.512875    Objective Loss 0.512875                                        LR 0.000500    Time 0.072886    
2024-02-17 12:04:58,754 - Epoch: [95][  400/  500]    Overall Loss 0.522760    Objective Loss 0.522760                                        LR 0.000500    Time 0.072564    
2024-02-17 12:05:06,026 - Epoch: [95][  500/  500]    Overall Loss 0.532060    Objective Loss 0.532060    Top1 84.000000    Top5 97.000000    LR 0.000500    Time 0.072587    
2024-02-17 12:05:06,211 - --- validate (epoch=95)-----------
2024-02-17 12:05:06,212 - 10000 samples (100 per mini-batch)
2024-02-17 12:05:09,075 - Epoch: [95][  100/  100]    Loss 1.794046    Top1 58.590000    Top5 85.450000    
2024-02-17 12:05:09,173 - ==> Top1: 58.590    Top5: 85.450    Loss: 1.794

2024-02-17 12:05:09,184 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:05:09,185 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:05:09,245 - 

2024-02-17 12:05:09,246 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:05:17,063 - Epoch: [96][  100/  500]    Overall Loss 0.506437    Objective Loss 0.506437                                        LR 0.000500    Time 0.078119    
2024-02-17 12:05:24,177 - Epoch: [96][  200/  500]    Overall Loss 0.510144    Objective Loss 0.510144                                        LR 0.000500    Time 0.074607    
2024-02-17 12:05:31,370 - Epoch: [96][  300/  500]    Overall Loss 0.513762    Objective Loss 0.513762                                        LR 0.000500    Time 0.073699    
2024-02-17 12:05:38,466 - Epoch: [96][  400/  500]    Overall Loss 0.521659    Objective Loss 0.521659                                        LR 0.000500    Time 0.073003    
2024-02-17 12:05:45,697 - Epoch: [96][  500/  500]    Overall Loss 0.523922    Objective Loss 0.523922    Top1 86.000000    Top5 98.500000    LR 0.000500    Time 0.072855    
2024-02-17 12:05:45,829 - --- validate (epoch=96)-----------
2024-02-17 12:05:45,830 - 10000 samples (100 per mini-batch)
2024-02-17 12:05:48,699 - Epoch: [96][  100/  100]    Loss 1.863796    Top1 57.850000    Top5 84.610000    
2024-02-17 12:05:48,794 - ==> Top1: 57.850    Top5: 84.610    Loss: 1.864

2024-02-17 12:05:48,805 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:05:48,806 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:05:48,868 - 

2024-02-17 12:05:48,869 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:05:56,384 - Epoch: [97][  100/  500]    Overall Loss 0.492927    Objective Loss 0.492927                                        LR 0.000500    Time 0.075097    
2024-02-17 12:06:03,467 - Epoch: [97][  200/  500]    Overall Loss 0.510134    Objective Loss 0.510134                                        LR 0.000500    Time 0.072937    
2024-02-17 12:06:10,599 - Epoch: [97][  300/  500]    Overall Loss 0.511359    Objective Loss 0.511359                                        LR 0.000500    Time 0.072383    
2024-02-17 12:06:17,710 - Epoch: [97][  400/  500]    Overall Loss 0.517971    Objective Loss 0.517971                                        LR 0.000500    Time 0.072054    
2024-02-17 12:06:24,924 - Epoch: [97][  500/  500]    Overall Loss 0.522544    Objective Loss 0.522544    Top1 80.000000    Top5 96.500000    LR 0.000500    Time 0.072063    
2024-02-17 12:06:25,065 - --- validate (epoch=97)-----------
2024-02-17 12:06:25,066 - 10000 samples (100 per mini-batch)
2024-02-17 12:06:27,954 - Epoch: [97][  100/  100]    Loss 1.799315    Top1 58.500000    Top5 85.210000    
2024-02-17 12:06:28,069 - ==> Top1: 58.500    Top5: 85.210    Loss: 1.799

2024-02-17 12:06:28,079 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:06:28,079 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:06:28,134 - 

2024-02-17 12:06:28,134 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:06:35,725 - Epoch: [98][  100/  500]    Overall Loss 0.492416    Objective Loss 0.492416                                        LR 0.000500    Time 0.075853    
2024-02-17 12:06:43,083 - Epoch: [98][  200/  500]    Overall Loss 0.509689    Objective Loss 0.509689                                        LR 0.000500    Time 0.074695    
2024-02-17 12:06:50,223 - Epoch: [98][  300/  500]    Overall Loss 0.514820    Objective Loss 0.514820                                        LR 0.000500    Time 0.073581    
2024-02-17 12:06:57,361 - Epoch: [98][  400/  500]    Overall Loss 0.519473    Objective Loss 0.519473                                        LR 0.000500    Time 0.073018    
2024-02-17 12:07:04,479 - Epoch: [98][  500/  500]    Overall Loss 0.520509    Objective Loss 0.520509    Top1 78.500000    Top5 96.000000    LR 0.000500    Time 0.072643    
2024-02-17 12:07:04,674 - --- validate (epoch=98)-----------
2024-02-17 12:07:04,674 - 10000 samples (100 per mini-batch)
2024-02-17 12:07:07,848 - Epoch: [98][  100/  100]    Loss 1.888811    Top1 57.210000    Top5 84.670000    
2024-02-17 12:07:07,967 - ==> Top1: 57.210    Top5: 84.670    Loss: 1.889

2024-02-17 12:07:07,977 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:07:07,977 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:07:08,038 - 

2024-02-17 12:07:08,039 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:07:15,617 - Epoch: [99][  100/  500]    Overall Loss 0.491538    Objective Loss 0.491538                                        LR 0.000500    Time 0.075728    
2024-02-17 12:07:22,698 - Epoch: [99][  200/  500]    Overall Loss 0.495809    Objective Loss 0.495809                                        LR 0.000500    Time 0.073250    
2024-02-17 12:07:29,756 - Epoch: [99][  300/  500]    Overall Loss 0.503789    Objective Loss 0.503789                                        LR 0.000500    Time 0.072345    
2024-02-17 12:07:36,808 - Epoch: [99][  400/  500]    Overall Loss 0.509748    Objective Loss 0.509748                                        LR 0.000500    Time 0.071876    
2024-02-17 12:07:43,968 - Epoch: [99][  500/  500]    Overall Loss 0.509678    Objective Loss 0.509678    Top1 83.000000    Top5 98.000000    LR 0.000500    Time 0.071813    
2024-02-17 12:07:44,090 - --- validate (epoch=99)-----------
2024-02-17 12:07:44,091 - 10000 samples (100 per mini-batch)
2024-02-17 12:07:47,058 - Epoch: [99][  100/  100]    Loss 1.846241    Top1 58.420000    Top5 85.290000    
2024-02-17 12:07:47,228 - ==> Top1: 58.420    Top5: 85.290    Loss: 1.846

2024-02-17 12:07:47,240 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:07:47,241 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:07:47,317 - 

2024-02-17 12:07:47,317 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:07:54,900 - Epoch: [100][  100/  500]    Overall Loss 0.451988    Objective Loss 0.451988                                        LR 0.000250    Time 0.075772    
2024-02-17 12:08:02,212 - Epoch: [100][  200/  500]    Overall Loss 0.452809    Objective Loss 0.452809                                        LR 0.000250    Time 0.074425    
2024-02-17 12:08:09,370 - Epoch: [100][  300/  500]    Overall Loss 0.448110    Objective Loss 0.448110                                        LR 0.000250    Time 0.073459    
2024-02-17 12:08:16,574 - Epoch: [100][  400/  500]    Overall Loss 0.446328    Objective Loss 0.446328                                        LR 0.000250    Time 0.073094    
2024-02-17 12:08:23,829 - Epoch: [100][  500/  500]    Overall Loss 0.444304    Objective Loss 0.444304    Top1 86.000000    Top5 99.500000    LR 0.000250    Time 0.072976    
2024-02-17 12:08:23,938 - --- validate (epoch=100)-----------
2024-02-17 12:08:23,939 - 10000 samples (100 per mini-batch)
2024-02-17 12:08:27,048 - Epoch: [100][  100/  100]    Loss 1.728898    Top1 59.900000    Top5 85.980000    
2024-02-17 12:08:27,159 - ==> Top1: 59.900    Top5: 85.980    Loss: 1.729

2024-02-17 12:08:27,170 - ==> Best [Top1: 59.900   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 100]
2024-02-17 12:08:27,170 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:08:27,245 - 

2024-02-17 12:08:27,246 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:08:34,833 - Epoch: [101][  100/  500]    Overall Loss 0.418033    Objective Loss 0.418033                                        LR 0.000250    Time 0.075819    
2024-02-17 12:08:42,026 - Epoch: [101][  200/  500]    Overall Loss 0.421524    Objective Loss 0.421524                                        LR 0.000250    Time 0.073852    
2024-02-17 12:08:49,112 - Epoch: [101][  300/  500]    Overall Loss 0.426896    Objective Loss 0.426896                                        LR 0.000250    Time 0.072842    
2024-02-17 12:08:56,262 - Epoch: [101][  400/  500]    Overall Loss 0.427725    Objective Loss 0.427725                                        LR 0.000250    Time 0.072494    
2024-02-17 12:09:03,613 - Epoch: [101][  500/  500]    Overall Loss 0.429468    Objective Loss 0.429468    Top1 86.000000    Top5 98.500000    LR 0.000250    Time 0.072689    
2024-02-17 12:09:03,714 - --- validate (epoch=101)-----------
2024-02-17 12:09:03,715 - 10000 samples (100 per mini-batch)
2024-02-17 12:09:06,661 - Epoch: [101][  100/  100]    Loss 1.757559    Top1 59.600000    Top5 86.150000    
2024-02-17 12:09:06,843 - ==> Top1: 59.600    Top5: 86.150    Loss: 1.758

2024-02-17 12:09:06,851 - ==> Best [Top1: 59.900   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 100]
2024-02-17 12:09:06,851 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:09:06,924 - 

2024-02-17 12:09:06,924 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:09:14,688 - Epoch: [102][  100/  500]    Overall Loss 0.416229    Objective Loss 0.416229                                        LR 0.000250    Time 0.077584    
2024-02-17 12:09:21,889 - Epoch: [102][  200/  500]    Overall Loss 0.413444    Objective Loss 0.413444                                        LR 0.000250    Time 0.074774    
2024-02-17 12:09:29,027 - Epoch: [102][  300/  500]    Overall Loss 0.420871    Objective Loss 0.420871                                        LR 0.000250    Time 0.073627    
2024-02-17 12:09:36,138 - Epoch: [102][  400/  500]    Overall Loss 0.427168    Objective Loss 0.427168                                        LR 0.000250    Time 0.072987    
2024-02-17 12:09:43,265 - Epoch: [102][  500/  500]    Overall Loss 0.427152    Objective Loss 0.427152    Top1 89.000000    Top5 99.500000    LR 0.000250    Time 0.072635    
2024-02-17 12:09:43,370 - --- validate (epoch=102)-----------
2024-02-17 12:09:43,371 - 10000 samples (100 per mini-batch)
2024-02-17 12:09:46,159 - Epoch: [102][  100/  100]    Loss 1.739574    Top1 60.070000    Top5 86.210000    
2024-02-17 12:09:46,266 - ==> Top1: 60.070    Top5: 86.210    Loss: 1.740

2024-02-17 12:09:46,277 - ==> Best [Top1: 60.070   Top5: 86.210   Sparsity:0.00   Params: 753952 on epoch: 102]
2024-02-17 12:09:46,278 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:09:46,354 - 

2024-02-17 12:09:46,354 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:09:53,968 - Epoch: [103][  100/  500]    Overall Loss 0.403438    Objective Loss 0.403438                                        LR 0.000250    Time 0.076078    
2024-02-17 12:10:01,235 - Epoch: [103][  200/  500]    Overall Loss 0.401079    Objective Loss 0.401079                                        LR 0.000250    Time 0.074355    
2024-02-17 12:10:08,277 - Epoch: [103][  300/  500]    Overall Loss 0.409565    Objective Loss 0.409565                                        LR 0.000250    Time 0.073026    
2024-02-17 12:10:15,378 - Epoch: [103][  400/  500]    Overall Loss 0.415376    Objective Loss 0.415376                                        LR 0.000250    Time 0.072513    
2024-02-17 12:10:22,499 - Epoch: [103][  500/  500]    Overall Loss 0.418406    Objective Loss 0.418406    Top1 89.000000    Top5 99.500000    LR 0.000250    Time 0.072243    
2024-02-17 12:10:22,620 - --- validate (epoch=103)-----------
2024-02-17 12:10:22,621 - 10000 samples (100 per mini-batch)
2024-02-17 12:10:25,623 - Epoch: [103][  100/  100]    Loss 1.754008    Top1 59.800000    Top5 85.760000    
2024-02-17 12:10:25,768 - ==> Top1: 59.800    Top5: 85.760    Loss: 1.754

2024-02-17 12:10:25,779 - ==> Best [Top1: 60.070   Top5: 86.210   Sparsity:0.00   Params: 753952 on epoch: 102]
2024-02-17 12:10:25,780 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:10:25,841 - 

2024-02-17 12:10:25,841 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:10:33,830 - Epoch: [104][  100/  500]    Overall Loss 0.394743    Objective Loss 0.394743                                        LR 0.000250    Time 0.079832    
2024-02-17 12:10:41,196 - Epoch: [104][  200/  500]    Overall Loss 0.400930    Objective Loss 0.400930                                        LR 0.000250    Time 0.076722    
2024-02-17 12:10:48,293 - Epoch: [104][  300/  500]    Overall Loss 0.403892    Objective Loss 0.403892                                        LR 0.000250    Time 0.074787    
2024-02-17 12:10:55,463 - Epoch: [104][  400/  500]    Overall Loss 0.407532    Objective Loss 0.407532                                        LR 0.000250    Time 0.074004    
2024-02-17 12:11:02,718 - Epoch: [104][  500/  500]    Overall Loss 0.412013    Objective Loss 0.412013    Top1 87.000000    Top5 99.000000    LR 0.000250    Time 0.073706    
2024-02-17 12:11:02,864 - --- validate (epoch=104)-----------
2024-02-17 12:11:02,864 - 10000 samples (100 per mini-batch)
2024-02-17 12:11:06,132 - Epoch: [104][  100/  100]    Loss 1.762275    Top1 60.200000    Top5 86.020000    
2024-02-17 12:11:06,297 - ==> Top1: 60.200    Top5: 86.020    Loss: 1.762

2024-02-17 12:11:06,307 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:11:06,308 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:11:06,382 - 

2024-02-17 12:11:06,383 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:11:13,910 - Epoch: [105][  100/  500]    Overall Loss 0.393309    Objective Loss 0.393309                                        LR 0.000250    Time 0.075213    
2024-02-17 12:11:21,170 - Epoch: [105][  200/  500]    Overall Loss 0.400789    Objective Loss 0.400789                                        LR 0.000250    Time 0.073886    
2024-02-17 12:11:28,521 - Epoch: [105][  300/  500]    Overall Loss 0.402713    Objective Loss 0.402713                                        LR 0.000250    Time 0.073747    
2024-02-17 12:11:35,662 - Epoch: [105][  400/  500]    Overall Loss 0.410494    Objective Loss 0.410494                                        LR 0.000250    Time 0.073151    
2024-02-17 12:11:42,940 - Epoch: [105][  500/  500]    Overall Loss 0.411551    Objective Loss 0.411551    Top1 88.500000    Top5 99.500000    LR 0.000250    Time 0.073067    
2024-02-17 12:11:43,060 - --- validate (epoch=105)-----------
2024-02-17 12:11:43,060 - 10000 samples (100 per mini-batch)
2024-02-17 12:11:46,336 - Epoch: [105][  100/  100]    Loss 1.748009    Top1 59.960000    Top5 86.130000    
2024-02-17 12:11:46,507 - ==> Top1: 59.960    Top5: 86.130    Loss: 1.748

2024-02-17 12:11:46,516 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:11:46,516 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:11:46,580 - 

2024-02-17 12:11:46,581 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:11:54,446 - Epoch: [106][  100/  500]    Overall Loss 0.394131    Objective Loss 0.394131                                        LR 0.000250    Time 0.078595    
2024-02-17 12:12:01,702 - Epoch: [106][  200/  500]    Overall Loss 0.396050    Objective Loss 0.396050                                        LR 0.000250    Time 0.075553    
2024-02-17 12:12:08,762 - Epoch: [106][  300/  500]    Overall Loss 0.398229    Objective Loss 0.398229                                        LR 0.000250    Time 0.073886    
2024-02-17 12:12:15,842 - Epoch: [106][  400/  500]    Overall Loss 0.399904    Objective Loss 0.399904                                        LR 0.000250    Time 0.073104    
2024-02-17 12:12:22,984 - Epoch: [106][  500/  500]    Overall Loss 0.403155    Objective Loss 0.403155    Top1 86.500000    Top5 99.500000    LR 0.000250    Time 0.072758    
2024-02-17 12:12:23,122 - --- validate (epoch=106)-----------
2024-02-17 12:12:23,123 - 10000 samples (100 per mini-batch)
2024-02-17 12:12:26,348 - Epoch: [106][  100/  100]    Loss 1.801233    Top1 59.730000    Top5 85.940000    
2024-02-17 12:12:26,444 - ==> Top1: 59.730    Top5: 85.940    Loss: 1.801

2024-02-17 12:12:26,451 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:12:26,452 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:12:26,507 - 

2024-02-17 12:12:26,507 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:12:34,133 - Epoch: [107][  100/  500]    Overall Loss 0.401932    Objective Loss 0.401932                                        LR 0.000250    Time 0.076199    
2024-02-17 12:12:41,248 - Epoch: [107][  200/  500]    Overall Loss 0.399534    Objective Loss 0.399534                                        LR 0.000250    Time 0.073653    
2024-02-17 12:12:48,317 - Epoch: [107][  300/  500]    Overall Loss 0.402269    Objective Loss 0.402269                                        LR 0.000250    Time 0.072648    
2024-02-17 12:12:55,399 - Epoch: [107][  400/  500]    Overall Loss 0.400546    Objective Loss 0.400546                                        LR 0.000250    Time 0.072180    
2024-02-17 12:13:02,206 - Epoch: [107][  500/  500]    Overall Loss 0.405310    Objective Loss 0.405310    Top1 87.000000    Top5 100.000000    LR 0.000250    Time 0.071351    
2024-02-17 12:13:02,308 - --- validate (epoch=107)-----------
2024-02-17 12:13:02,309 - 10000 samples (100 per mini-batch)
2024-02-17 12:13:05,564 - Epoch: [107][  100/  100]    Loss 1.785273    Top1 59.360000    Top5 86.040000    
2024-02-17 12:13:05,716 - ==> Top1: 59.360    Top5: 86.040    Loss: 1.785

2024-02-17 12:13:05,728 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:13:05,729 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:13:05,794 - 

2024-02-17 12:13:05,794 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:13:13,451 - Epoch: [108][  100/  500]    Overall Loss 0.384186    Objective Loss 0.384186                                        LR 0.000250    Time 0.076510    
2024-02-17 12:13:20,701 - Epoch: [108][  200/  500]    Overall Loss 0.387768    Objective Loss 0.387768                                        LR 0.000250    Time 0.074480    
2024-02-17 12:13:27,813 - Epoch: [108][  300/  500]    Overall Loss 0.390102    Objective Loss 0.390102                                        LR 0.000250    Time 0.073345    
2024-02-17 12:13:34,942 - Epoch: [108][  400/  500]    Overall Loss 0.397599    Objective Loss 0.397599                                        LR 0.000250    Time 0.072820    
2024-02-17 12:13:42,073 - Epoch: [108][  500/  500]    Overall Loss 0.399284    Objective Loss 0.399284    Top1 87.000000    Top5 98.500000    LR 0.000250    Time 0.072509    
2024-02-17 12:13:42,188 - --- validate (epoch=108)-----------
2024-02-17 12:13:42,189 - 10000 samples (100 per mini-batch)
2024-02-17 12:13:45,487 - Epoch: [108][  100/  100]    Loss 1.784492    Top1 60.080000    Top5 85.660000    
2024-02-17 12:13:45,590 - ==> Top1: 60.080    Top5: 85.660    Loss: 1.784

2024-02-17 12:13:45,603 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:13:45,604 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:13:45,667 - 

2024-02-17 12:13:45,667 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:13:53,302 - Epoch: [109][  100/  500]    Overall Loss 0.383582    Objective Loss 0.383582                                        LR 0.000250    Time 0.076287    
2024-02-17 12:14:00,444 - Epoch: [109][  200/  500]    Overall Loss 0.395178    Objective Loss 0.395178                                        LR 0.000250    Time 0.073827    
2024-02-17 12:14:07,579 - Epoch: [109][  300/  500]    Overall Loss 0.394470    Objective Loss 0.394470                                        LR 0.000250    Time 0.072988    
2024-02-17 12:14:14,675 - Epoch: [109][  400/  500]    Overall Loss 0.396336    Objective Loss 0.396336                                        LR 0.000250    Time 0.072470    
2024-02-17 12:14:21,785 - Epoch: [109][  500/  500]    Overall Loss 0.397936    Objective Loss 0.397936    Top1 92.500000    Top5 99.500000    LR 0.000250    Time 0.072186    
2024-02-17 12:14:21,925 - --- validate (epoch=109)-----------
2024-02-17 12:14:21,925 - 10000 samples (100 per mini-batch)
2024-02-17 12:14:24,932 - Epoch: [109][  100/  100]    Loss 1.760970    Top1 60.100000    Top5 86.090000    
2024-02-17 12:14:25,130 - ==> Top1: 60.100    Top5: 86.090    Loss: 1.761

2024-02-17 12:14:25,140 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:14:25,141 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:14:25,204 - 

2024-02-17 12:14:25,204 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:14:32,893 - Epoch: [110][  100/  500]    Overall Loss 0.375902    Objective Loss 0.375902                                        LR 0.000250    Time 0.076835    
2024-02-17 12:14:39,825 - Epoch: [110][  200/  500]    Overall Loss 0.385555    Objective Loss 0.385555                                        LR 0.000250    Time 0.073061    
2024-02-17 12:14:46,959 - Epoch: [110][  300/  500]    Overall Loss 0.391136    Objective Loss 0.391136                                        LR 0.000250    Time 0.072471    
2024-02-17 12:14:54,100 - Epoch: [110][  400/  500]    Overall Loss 0.393006    Objective Loss 0.393006                                        LR 0.000250    Time 0.072193    
2024-02-17 12:15:01,243 - Epoch: [110][  500/  500]    Overall Loss 0.395539    Objective Loss 0.395539    Top1 88.000000    Top5 98.500000    LR 0.000250    Time 0.072031    
2024-02-17 12:15:01,397 - --- validate (epoch=110)-----------
2024-02-17 12:15:01,398 - 10000 samples (100 per mini-batch)
2024-02-17 12:15:04,521 - Epoch: [110][  100/  100]    Loss 1.768408    Top1 59.650000    Top5 85.850000    
2024-02-17 12:15:04,627 - ==> Top1: 59.650    Top5: 85.850    Loss: 1.768

2024-02-17 12:15:04,637 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:15:04,637 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:15:04,694 - 

2024-02-17 12:15:04,695 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:15:12,339 - Epoch: [111][  100/  500]    Overall Loss 0.376204    Objective Loss 0.376204                                        LR 0.000250    Time 0.076393    
2024-02-17 12:15:19,530 - Epoch: [111][  200/  500]    Overall Loss 0.375309    Objective Loss 0.375309                                        LR 0.000250    Time 0.074129    
2024-02-17 12:15:26,697 - Epoch: [111][  300/  500]    Overall Loss 0.379431    Objective Loss 0.379431                                        LR 0.000250    Time 0.073293    
2024-02-17 12:15:33,871 - Epoch: [111][  400/  500]    Overall Loss 0.384439    Objective Loss 0.384439                                        LR 0.000250    Time 0.072893    
2024-02-17 12:15:40,978 - Epoch: [111][  500/  500]    Overall Loss 0.384868    Objective Loss 0.384868    Top1 91.500000    Top5 99.500000    LR 0.000250    Time 0.072521    
2024-02-17 12:15:41,107 - --- validate (epoch=111)-----------
2024-02-17 12:15:41,108 - 10000 samples (100 per mini-batch)
2024-02-17 12:15:44,234 - Epoch: [111][  100/  100]    Loss 1.789759    Top1 59.550000    Top5 85.860000    
2024-02-17 12:15:44,343 - ==> Top1: 59.550    Top5: 85.860    Loss: 1.790

2024-02-17 12:15:44,354 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:15:44,354 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:15:44,416 - 

2024-02-17 12:15:44,416 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:15:52,407 - Epoch: [112][  100/  500]    Overall Loss 0.372916    Objective Loss 0.372916                                        LR 0.000250    Time 0.079840    
2024-02-17 12:15:59,571 - Epoch: [112][  200/  500]    Overall Loss 0.377291    Objective Loss 0.377291                                        LR 0.000250    Time 0.075718    
2024-02-17 12:16:06,759 - Epoch: [112][  300/  500]    Overall Loss 0.377563    Objective Loss 0.377563                                        LR 0.000250    Time 0.074422    
2024-02-17 12:16:13,959 - Epoch: [112][  400/  500]    Overall Loss 0.383397    Objective Loss 0.383397                                        LR 0.000250    Time 0.073807    
2024-02-17 12:16:21,153 - Epoch: [112][  500/  500]    Overall Loss 0.386634    Objective Loss 0.386634    Top1 85.500000    Top5 99.500000    LR 0.000250    Time 0.073424    
2024-02-17 12:16:21,270 - --- validate (epoch=112)-----------
2024-02-17 12:16:21,271 - 10000 samples (100 per mini-batch)
2024-02-17 12:16:24,304 - Epoch: [112][  100/  100]    Loss 1.808364    Top1 59.860000    Top5 86.170000    
2024-02-17 12:16:24,403 - ==> Top1: 59.860    Top5: 86.170    Loss: 1.808

2024-02-17 12:16:24,419 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:16:24,419 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:16:24,484 - 

2024-02-17 12:16:24,484 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:16:32,172 - Epoch: [113][  100/  500]    Overall Loss 0.379976    Objective Loss 0.379976                                        LR 0.000250    Time 0.076823    
2024-02-17 12:16:39,288 - Epoch: [113][  200/  500]    Overall Loss 0.376639    Objective Loss 0.376639                                        LR 0.000250    Time 0.073965    
2024-02-17 12:16:46,366 - Epoch: [113][  300/  500]    Overall Loss 0.377383    Objective Loss 0.377383                                        LR 0.000250    Time 0.072889    
2024-02-17 12:16:53,418 - Epoch: [113][  400/  500]    Overall Loss 0.382346    Objective Loss 0.382346                                        LR 0.000250    Time 0.072288    
2024-02-17 12:17:00,572 - Epoch: [113][  500/  500]    Overall Loss 0.384519    Objective Loss 0.384519    Top1 86.000000    Top5 99.500000    LR 0.000250    Time 0.072129    
2024-02-17 12:17:00,784 - --- validate (epoch=113)-----------
2024-02-17 12:17:00,785 - 10000 samples (100 per mini-batch)
2024-02-17 12:17:03,826 - Epoch: [113][  100/  100]    Loss 1.799925    Top1 59.570000    Top5 85.930000    
2024-02-17 12:17:03,955 - ==> Top1: 59.570    Top5: 85.930    Loss: 1.800

2024-02-17 12:17:03,965 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:17:03,965 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:17:04,025 - 

2024-02-17 12:17:04,025 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:17:11,759 - Epoch: [114][  100/  500]    Overall Loss 0.362519    Objective Loss 0.362519                                        LR 0.000250    Time 0.077281    
2024-02-17 12:17:18,861 - Epoch: [114][  200/  500]    Overall Loss 0.368233    Objective Loss 0.368233                                        LR 0.000250    Time 0.074129    
2024-02-17 12:17:26,017 - Epoch: [114][  300/  500]    Overall Loss 0.368697    Objective Loss 0.368697                                        LR 0.000250    Time 0.073260    
2024-02-17 12:17:33,152 - Epoch: [114][  400/  500]    Overall Loss 0.373663    Objective Loss 0.373663                                        LR 0.000250    Time 0.072772    
2024-02-17 12:17:40,082 - Epoch: [114][  500/  500]    Overall Loss 0.380259    Objective Loss 0.380259    Top1 90.000000    Top5 99.000000    LR 0.000250    Time 0.072070    
2024-02-17 12:17:40,219 - --- validate (epoch=114)-----------
2024-02-17 12:17:40,220 - 10000 samples (100 per mini-batch)
2024-02-17 12:17:43,091 - Epoch: [114][  100/  100]    Loss 1.824426    Top1 59.310000    Top5 85.840000    
2024-02-17 12:17:43,193 - ==> Top1: 59.310    Top5: 85.840    Loss: 1.824

2024-02-17 12:17:43,204 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:17:43,204 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:17:43,266 - 

2024-02-17 12:17:43,267 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:17:50,884 - Epoch: [115][  100/  500]    Overall Loss 0.372828    Objective Loss 0.372828                                        LR 0.000250    Time 0.076120    
2024-02-17 12:17:57,824 - Epoch: [115][  200/  500]    Overall Loss 0.373907    Objective Loss 0.373907                                        LR 0.000250    Time 0.072739    
2024-02-17 12:18:04,952 - Epoch: [115][  300/  500]    Overall Loss 0.372180    Objective Loss 0.372180                                        LR 0.000250    Time 0.072239    
2024-02-17 12:18:12,061 - Epoch: [115][  400/  500]    Overall Loss 0.377547    Objective Loss 0.377547                                        LR 0.000250    Time 0.071941    
2024-02-17 12:18:19,301 - Epoch: [115][  500/  500]    Overall Loss 0.378982    Objective Loss 0.378982    Top1 85.000000    Top5 98.500000    LR 0.000250    Time 0.072024    
2024-02-17 12:18:19,437 - --- validate (epoch=115)-----------
2024-02-17 12:18:19,438 - 10000 samples (100 per mini-batch)
2024-02-17 12:18:22,296 - Epoch: [115][  100/  100]    Loss 1.803950    Top1 59.830000    Top5 85.820000    
2024-02-17 12:18:22,504 - ==> Top1: 59.830    Top5: 85.820    Loss: 1.804

2024-02-17 12:18:22,515 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:18:22,515 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:18:22,585 - 

2024-02-17 12:18:22,585 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:18:30,248 - Epoch: [116][  100/  500]    Overall Loss 0.370018    Objective Loss 0.370018                                        LR 0.000250    Time 0.076568    
2024-02-17 12:18:37,462 - Epoch: [116][  200/  500]    Overall Loss 0.365850    Objective Loss 0.365850                                        LR 0.000250    Time 0.074332    
2024-02-17 12:18:44,682 - Epoch: [116][  300/  500]    Overall Loss 0.368996    Objective Loss 0.368996                                        LR 0.000250    Time 0.073608    
2024-02-17 12:18:51,861 - Epoch: [116][  400/  500]    Overall Loss 0.370000    Objective Loss 0.370000                                        LR 0.000250    Time 0.073141    
2024-02-17 12:18:59,103 - Epoch: [116][  500/  500]    Overall Loss 0.374056    Objective Loss 0.374056    Top1 85.000000    Top5 99.500000    LR 0.000250    Time 0.072989    
2024-02-17 12:18:59,245 - --- validate (epoch=116)-----------
2024-02-17 12:18:59,246 - 10000 samples (100 per mini-batch)
2024-02-17 12:19:02,117 - Epoch: [116][  100/  100]    Loss 1.815954    Top1 59.720000    Top5 85.730000    
2024-02-17 12:19:02,214 - ==> Top1: 59.720    Top5: 85.730    Loss: 1.816

2024-02-17 12:19:02,450 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:19:02,451 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:19:02,504 - 

2024-02-17 12:19:02,504 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:19:10,144 - Epoch: [117][  100/  500]    Overall Loss 0.357760    Objective Loss 0.357760                                        LR 0.000250    Time 0.076344    
2024-02-17 12:19:17,199 - Epoch: [117][  200/  500]    Overall Loss 0.356955    Objective Loss 0.356955                                        LR 0.000250    Time 0.073425    
2024-02-17 12:19:24,272 - Epoch: [117][  300/  500]    Overall Loss 0.364442    Objective Loss 0.364442                                        LR 0.000250    Time 0.072514    
2024-02-17 12:19:30,996 - Epoch: [117][  400/  500]    Overall Loss 0.370243    Objective Loss 0.370243                                        LR 0.000250    Time 0.071187    
2024-02-17 12:19:37,914 - Epoch: [117][  500/  500]    Overall Loss 0.372194    Objective Loss 0.372194    Top1 89.500000    Top5 99.500000    LR 0.000250    Time 0.070778    
2024-02-17 12:19:38,101 - --- validate (epoch=117)-----------
2024-02-17 12:19:38,102 - 10000 samples (100 per mini-batch)
2024-02-17 12:19:40,966 - Epoch: [117][  100/  100]    Loss 1.813385    Top1 60.020000    Top5 85.900000    
2024-02-17 12:19:41,083 - ==> Top1: 60.020    Top5: 85.900    Loss: 1.813

2024-02-17 12:19:41,098 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:19:41,098 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:19:41,164 - 

2024-02-17 12:19:41,164 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:19:48,862 - Epoch: [118][  100/  500]    Overall Loss 0.357653    Objective Loss 0.357653                                        LR 0.000250    Time 0.076924    
2024-02-17 12:19:55,934 - Epoch: [118][  200/  500]    Overall Loss 0.353822    Objective Loss 0.353822                                        LR 0.000250    Time 0.073799    
2024-02-17 12:20:03,266 - Epoch: [118][  300/  500]    Overall Loss 0.354413    Objective Loss 0.354413                                        LR 0.000250    Time 0.073626    
2024-02-17 12:20:10,392 - Epoch: [118][  400/  500]    Overall Loss 0.362473    Objective Loss 0.362473                                        LR 0.000250    Time 0.073023    
2024-02-17 12:20:17,607 - Epoch: [118][  500/  500]    Overall Loss 0.368500    Objective Loss 0.368500    Top1 89.000000    Top5 99.500000    LR 0.000250    Time 0.072839    
2024-02-17 12:20:17,718 - --- validate (epoch=118)-----------
2024-02-17 12:20:17,719 - 10000 samples (100 per mini-batch)
2024-02-17 12:20:20,843 - Epoch: [118][  100/  100]    Loss 1.849383    Top1 59.390000    Top5 85.570000    
2024-02-17 12:20:20,950 - ==> Top1: 59.390    Top5: 85.570    Loss: 1.849

2024-02-17 12:20:20,962 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:20:20,962 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:20:21,023 - 

2024-02-17 12:20:21,023 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:20:28,558 - Epoch: [119][  100/  500]    Overall Loss 0.345665    Objective Loss 0.345665                                        LR 0.000250    Time 0.075289    
2024-02-17 12:20:35,878 - Epoch: [119][  200/  500]    Overall Loss 0.359003    Objective Loss 0.359003                                        LR 0.000250    Time 0.074223    
2024-02-17 12:20:43,020 - Epoch: [119][  300/  500]    Overall Loss 0.360961    Objective Loss 0.360961                                        LR 0.000250    Time 0.073274    
2024-02-17 12:20:50,192 - Epoch: [119][  400/  500]    Overall Loss 0.365260    Objective Loss 0.365260                                        LR 0.000250    Time 0.072873    
2024-02-17 12:20:57,334 - Epoch: [119][  500/  500]    Overall Loss 0.368261    Objective Loss 0.368261    Top1 88.000000    Top5 99.500000    LR 0.000250    Time 0.072575    
2024-02-17 12:20:57,511 - --- validate (epoch=119)-----------
2024-02-17 12:20:57,512 - 10000 samples (100 per mini-batch)
2024-02-17 12:21:00,390 - Epoch: [119][  100/  100]    Loss 1.855645    Top1 59.090000    Top5 85.700000    
2024-02-17 12:21:00,496 - ==> Top1: 59.090    Top5: 85.700    Loss: 1.856

2024-02-17 12:21:00,508 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:21:00,508 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:21:00,570 - 

2024-02-17 12:21:00,570 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:21:08,152 - Epoch: [120][  100/  500]    Overall Loss 0.339139    Objective Loss 0.339139                                        LR 0.000250    Time 0.075762    
2024-02-17 12:21:15,347 - Epoch: [120][  200/  500]    Overall Loss 0.346002    Objective Loss 0.346002                                        LR 0.000250    Time 0.073834    
2024-02-17 12:21:22,524 - Epoch: [120][  300/  500]    Overall Loss 0.356069    Objective Loss 0.356069                                        LR 0.000250    Time 0.073130    
2024-02-17 12:21:29,671 - Epoch: [120][  400/  500]    Overall Loss 0.359483    Objective Loss 0.359483                                        LR 0.000250    Time 0.072705    
2024-02-17 12:21:36,809 - Epoch: [120][  500/  500]    Overall Loss 0.365972    Objective Loss 0.365972    Top1 89.000000    Top5 98.500000    LR 0.000250    Time 0.072431    
2024-02-17 12:21:36,982 - --- validate (epoch=120)-----------
2024-02-17 12:21:36,983 - 10000 samples (100 per mini-batch)
2024-02-17 12:21:40,084 - Epoch: [120][  100/  100]    Loss 1.849578    Top1 59.150000    Top5 85.530000    
2024-02-17 12:21:40,251 - ==> Top1: 59.150    Top5: 85.530    Loss: 1.850

2024-02-17 12:21:40,257 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:21:40,258 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:21:40,315 - 

2024-02-17 12:21:40,315 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:21:47,944 - Epoch: [121][  100/  500]    Overall Loss 0.353543    Objective Loss 0.353543                                        LR 0.000250    Time 0.076229    
2024-02-17 12:21:55,168 - Epoch: [121][  200/  500]    Overall Loss 0.355225    Objective Loss 0.355225                                        LR 0.000250    Time 0.074212    
2024-02-17 12:22:02,388 - Epoch: [121][  300/  500]    Overall Loss 0.357259    Objective Loss 0.357259                                        LR 0.000250    Time 0.073525    
2024-02-17 12:22:09,645 - Epoch: [121][  400/  500]    Overall Loss 0.362525    Objective Loss 0.362525                                        LR 0.000250    Time 0.073277    
2024-02-17 12:22:16,951 - Epoch: [121][  500/  500]    Overall Loss 0.365523    Objective Loss 0.365523    Top1 92.500000    Top5 99.500000    LR 0.000250    Time 0.073224    
2024-02-17 12:22:17,092 - --- validate (epoch=121)-----------
2024-02-17 12:22:17,093 - 10000 samples (100 per mini-batch)
2024-02-17 12:22:19,992 - Epoch: [121][  100/  100]    Loss 1.824821    Top1 59.830000    Top5 86.040000    
2024-02-17 12:22:20,178 - ==> Top1: 59.830    Top5: 86.040    Loss: 1.825

2024-02-17 12:22:20,189 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:22:20,189 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:22:20,249 - 

2024-02-17 12:22:20,249 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:22:28,034 - Epoch: [122][  100/  500]    Overall Loss 0.342752    Objective Loss 0.342752                                        LR 0.000250    Time 0.077792    
2024-02-17 12:22:35,109 - Epoch: [122][  200/  500]    Overall Loss 0.351260    Objective Loss 0.351260                                        LR 0.000250    Time 0.074247    
2024-02-17 12:22:42,278 - Epoch: [122][  300/  500]    Overall Loss 0.356273    Objective Loss 0.356273                                        LR 0.000250    Time 0.073379    
2024-02-17 12:22:49,484 - Epoch: [122][  400/  500]    Overall Loss 0.360148    Objective Loss 0.360148                                        LR 0.000250    Time 0.073039    
2024-02-17 12:22:56,693 - Epoch: [122][  500/  500]    Overall Loss 0.361480    Objective Loss 0.361480    Top1 86.500000    Top5 99.500000    LR 0.000250    Time 0.072841    
2024-02-17 12:22:56,815 - --- validate (epoch=122)-----------
2024-02-17 12:22:56,816 - 10000 samples (100 per mini-batch)
2024-02-17 12:22:59,825 - Epoch: [122][  100/  100]    Loss 1.836520    Top1 59.600000    Top5 85.500000    
2024-02-17 12:22:59,931 - ==> Top1: 59.600    Top5: 85.500    Loss: 1.837

2024-02-17 12:22:59,942 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:22:59,942 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:23:00,003 - 

2024-02-17 12:23:00,003 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:23:07,666 - Epoch: [123][  100/  500]    Overall Loss 0.335022    Objective Loss 0.335022                                        LR 0.000250    Time 0.076580    
2024-02-17 12:23:14,846 - Epoch: [123][  200/  500]    Overall Loss 0.342234    Objective Loss 0.342234                                        LR 0.000250    Time 0.074164    
2024-02-17 12:23:22,005 - Epoch: [123][  300/  500]    Overall Loss 0.348723    Objective Loss 0.348723                                        LR 0.000250    Time 0.073294    
2024-02-17 12:23:29,219 - Epoch: [123][  400/  500]    Overall Loss 0.351100    Objective Loss 0.351100                                        LR 0.000250    Time 0.072992    
2024-02-17 12:23:36,462 - Epoch: [123][  500/  500]    Overall Loss 0.353314    Objective Loss 0.353314    Top1 84.500000    Top5 100.000000    LR 0.000250    Time 0.072870    
2024-02-17 12:23:36,584 - --- validate (epoch=123)-----------
2024-02-17 12:23:36,585 - 10000 samples (100 per mini-batch)
2024-02-17 12:23:39,644 - Epoch: [123][  100/  100]    Loss 1.858697    Top1 59.060000    Top5 85.740000    
2024-02-17 12:23:39,747 - ==> Top1: 59.060    Top5: 85.740    Loss: 1.859

2024-02-17 12:23:39,758 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:23:39,758 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:23:39,818 - 

2024-02-17 12:23:39,818 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:23:47,568 - Epoch: [124][  100/  500]    Overall Loss 0.339974    Objective Loss 0.339974                                        LR 0.000250    Time 0.077441    
2024-02-17 12:23:54,647 - Epoch: [124][  200/  500]    Overall Loss 0.345771    Objective Loss 0.345771                                        LR 0.000250    Time 0.074096    
2024-02-17 12:24:01,448 - Epoch: [124][  300/  500]    Overall Loss 0.345579    Objective Loss 0.345579                                        LR 0.000250    Time 0.072055    
2024-02-17 12:24:08,579 - Epoch: [124][  400/  500]    Overall Loss 0.349166    Objective Loss 0.349166                                        LR 0.000250    Time 0.071859    
2024-02-17 12:24:15,700 - Epoch: [124][  500/  500]    Overall Loss 0.353267    Objective Loss 0.353267    Top1 87.000000    Top5 99.000000    LR 0.000250    Time 0.071719    
2024-02-17 12:24:15,887 - --- validate (epoch=124)-----------
2024-02-17 12:24:15,887 - 10000 samples (100 per mini-batch)
2024-02-17 12:24:19,051 - Epoch: [124][  100/  100]    Loss 1.852374    Top1 59.910000    Top5 85.740000    
2024-02-17 12:24:19,167 - ==> Top1: 59.910    Top5: 85.740    Loss: 1.852

2024-02-17 12:24:19,179 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:24:19,180 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:24:19,242 - 

2024-02-17 12:24:19,243 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:24:26,933 - Epoch: [125][  100/  500]    Overall Loss 0.342641    Objective Loss 0.342641                                        LR 0.000250    Time 0.076844    
2024-02-17 12:24:34,217 - Epoch: [125][  200/  500]    Overall Loss 0.339652    Objective Loss 0.339652                                        LR 0.000250    Time 0.074821    
2024-02-17 12:24:41,238 - Epoch: [125][  300/  500]    Overall Loss 0.342186    Objective Loss 0.342186                                        LR 0.000250    Time 0.073267    
2024-02-17 12:24:48,393 - Epoch: [125][  400/  500]    Overall Loss 0.347361    Objective Loss 0.347361                                        LR 0.000250    Time 0.072828    
2024-02-17 12:24:55,604 - Epoch: [125][  500/  500]    Overall Loss 0.349484    Objective Loss 0.349484    Top1 92.500000    Top5 100.000000    LR 0.000250    Time 0.072676    
2024-02-17 12:24:55,774 - --- validate (epoch=125)-----------
2024-02-17 12:24:55,775 - 10000 samples (100 per mini-batch)
2024-02-17 12:24:58,919 - Epoch: [125][  100/  100]    Loss 1.864347    Top1 59.370000    Top5 85.530000    
2024-02-17 12:24:59,092 - ==> Top1: 59.370    Top5: 85.530    Loss: 1.864

2024-02-17 12:24:59,101 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:24:59,102 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:24:59,169 - 

2024-02-17 12:24:59,169 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:25:07,049 - Epoch: [126][  100/  500]    Overall Loss 0.328410    Objective Loss 0.328410                                        LR 0.000250    Time 0.078735    
2024-02-17 12:25:14,250 - Epoch: [126][  200/  500]    Overall Loss 0.342120    Objective Loss 0.342120                                        LR 0.000250    Time 0.075348    
2024-02-17 12:25:21,194 - Epoch: [126][  300/  500]    Overall Loss 0.346524    Objective Loss 0.346524                                        LR 0.000250    Time 0.073364    
2024-02-17 12:25:28,118 - Epoch: [126][  400/  500]    Overall Loss 0.350320    Objective Loss 0.350320                                        LR 0.000250    Time 0.072323    
2024-02-17 12:25:35,274 - Epoch: [126][  500/  500]    Overall Loss 0.349495    Objective Loss 0.349495    Top1 89.500000    Top5 98.500000    LR 0.000250    Time 0.072163    
2024-02-17 12:25:35,396 - --- validate (epoch=126)-----------
2024-02-17 12:25:35,397 - 10000 samples (100 per mini-batch)
2024-02-17 12:25:38,329 - Epoch: [126][  100/  100]    Loss 1.874391    Top1 58.960000    Top5 85.670000    
2024-02-17 12:25:38,434 - ==> Top1: 58.960    Top5: 85.670    Loss: 1.874

2024-02-17 12:25:38,440 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:25:38,440 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:25:38,499 - 

2024-02-17 12:25:38,499 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:25:46,058 - Epoch: [127][  100/  500]    Overall Loss 0.337505    Objective Loss 0.337505                                        LR 0.000250    Time 0.075533    
2024-02-17 12:25:53,277 - Epoch: [127][  200/  500]    Overall Loss 0.341469    Objective Loss 0.341469                                        LR 0.000250    Time 0.073837    
2024-02-17 12:26:00,463 - Epoch: [127][  300/  500]    Overall Loss 0.342730    Objective Loss 0.342730                                        LR 0.000250    Time 0.073165    
2024-02-17 12:26:07,539 - Epoch: [127][  400/  500]    Overall Loss 0.347414    Objective Loss 0.347414                                        LR 0.000250    Time 0.072555    
2024-02-17 12:26:14,619 - Epoch: [127][  500/  500]    Overall Loss 0.350394    Objective Loss 0.350394    Top1 87.500000    Top5 99.000000    LR 0.000250    Time 0.072193    
2024-02-17 12:26:14,744 - --- validate (epoch=127)-----------
2024-02-17 12:26:14,745 - 10000 samples (100 per mini-batch)
2024-02-17 12:26:17,687 - Epoch: [127][  100/  100]    Loss 1.868177    Top1 59.330000    Top5 85.890000    
2024-02-17 12:26:17,810 - ==> Top1: 59.330    Top5: 85.890    Loss: 1.868

2024-02-17 12:26:17,821 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:26:17,821 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:26:17,884 - 

2024-02-17 12:26:17,884 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:26:25,711 - Epoch: [128][  100/  500]    Overall Loss 0.321977    Objective Loss 0.321977                                        LR 0.000250    Time 0.078216    
2024-02-17 12:26:32,710 - Epoch: [128][  200/  500]    Overall Loss 0.328395    Objective Loss 0.328395                                        LR 0.000250    Time 0.074079    
2024-02-17 12:26:39,823 - Epoch: [128][  300/  500]    Overall Loss 0.335899    Objective Loss 0.335899                                        LR 0.000250    Time 0.073082    
2024-02-17 12:26:46,889 - Epoch: [128][  400/  500]    Overall Loss 0.340811    Objective Loss 0.340811                                        LR 0.000250    Time 0.072468    
2024-02-17 12:26:54,172 - Epoch: [128][  500/  500]    Overall Loss 0.344051    Objective Loss 0.344051    Top1 90.000000    Top5 100.000000    LR 0.000250    Time 0.072530    
2024-02-17 12:26:54,316 - --- validate (epoch=128)-----------
2024-02-17 12:26:54,317 - 10000 samples (100 per mini-batch)
2024-02-17 12:26:57,245 - Epoch: [128][  100/  100]    Loss 1.862338    Top1 59.780000    Top5 86.150000    
2024-02-17 12:26:57,361 - ==> Top1: 59.780    Top5: 86.150    Loss: 1.862

2024-02-17 12:26:57,371 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:26:57,371 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:26:57,433 - 

2024-02-17 12:26:57,434 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:27:05,107 - Epoch: [129][  100/  500]    Overall Loss 0.327128    Objective Loss 0.327128                                        LR 0.000250    Time 0.076677    
2024-02-17 12:27:12,351 - Epoch: [129][  200/  500]    Overall Loss 0.330816    Objective Loss 0.330816                                        LR 0.000250    Time 0.074534    
2024-02-17 12:27:19,519 - Epoch: [129][  300/  500]    Overall Loss 0.334286    Objective Loss 0.334286                                        LR 0.000250    Time 0.073568    
2024-02-17 12:27:26,568 - Epoch: [129][  400/  500]    Overall Loss 0.336386    Objective Loss 0.336386                                        LR 0.000250    Time 0.072788    
2024-02-17 12:27:33,810 - Epoch: [129][  500/  500]    Overall Loss 0.339072    Objective Loss 0.339072    Top1 89.500000    Top5 100.000000    LR 0.000250    Time 0.072706    
2024-02-17 12:27:33,957 - --- validate (epoch=129)-----------
2024-02-17 12:27:33,958 - 10000 samples (100 per mini-batch)
2024-02-17 12:27:36,906 - Epoch: [129][  100/  100]    Loss 1.913295    Top1 59.320000    Top5 85.110000    
2024-02-17 12:27:37,048 - ==> Top1: 59.320    Top5: 85.110    Loss: 1.913

2024-02-17 12:27:37,059 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:27:37,059 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:27:37,115 - 

2024-02-17 12:27:37,115 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:27:44,900 - Epoch: [130][  100/  500]    Overall Loss 0.334636    Objective Loss 0.334636                                        LR 0.000250    Time 0.077799    
2024-02-17 12:27:52,084 - Epoch: [130][  200/  500]    Overall Loss 0.330480    Objective Loss 0.330480                                        LR 0.000250    Time 0.074796    
2024-02-17 12:27:59,267 - Epoch: [130][  300/  500]    Overall Loss 0.327833    Objective Loss 0.327833                                        LR 0.000250    Time 0.073792    
2024-02-17 12:28:06,374 - Epoch: [130][  400/  500]    Overall Loss 0.331335    Objective Loss 0.331335                                        LR 0.000250    Time 0.073102    
2024-02-17 12:28:13,589 - Epoch: [130][  500/  500]    Overall Loss 0.335437    Objective Loss 0.335437    Top1 90.000000    Top5 98.500000    LR 0.000250    Time 0.072901    
2024-02-17 12:28:13,763 - --- validate (epoch=130)-----------
2024-02-17 12:28:13,764 - 10000 samples (100 per mini-batch)
2024-02-17 12:28:16,672 - Epoch: [130][  100/  100]    Loss 1.907367    Top1 59.130000    Top5 85.190000    
2024-02-17 12:28:16,774 - ==> Top1: 59.130    Top5: 85.190    Loss: 1.907

2024-02-17 12:28:16,787 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:28:16,787 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:28:16,848 - 

2024-02-17 12:28:16,849 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:28:24,412 - Epoch: [131][  100/  500]    Overall Loss 0.317695    Objective Loss 0.317695                                        LR 0.000250    Time 0.075584    
2024-02-17 12:28:31,642 - Epoch: [131][  200/  500]    Overall Loss 0.322430    Objective Loss 0.322430                                        LR 0.000250    Time 0.073919    
2024-02-17 12:28:38,809 - Epoch: [131][  300/  500]    Overall Loss 0.333403    Objective Loss 0.333403                                        LR 0.000250    Time 0.073151    
2024-02-17 12:28:45,941 - Epoch: [131][  400/  500]    Overall Loss 0.334155    Objective Loss 0.334155                                        LR 0.000250    Time 0.072683    
2024-02-17 12:28:53,189 - Epoch: [131][  500/  500]    Overall Loss 0.337027    Objective Loss 0.337027    Top1 92.500000    Top5 99.500000    LR 0.000250    Time 0.072635    
2024-02-17 12:28:53,326 - --- validate (epoch=131)-----------
2024-02-17 12:28:53,327 - 10000 samples (100 per mini-batch)
2024-02-17 12:28:56,435 - Epoch: [131][  100/  100]    Loss 1.899934    Top1 59.270000    Top5 85.610000    
2024-02-17 12:28:56,531 - ==> Top1: 59.270    Top5: 85.610    Loss: 1.900

2024-02-17 12:28:56,541 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:28:56,542 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:28:56,605 - 

2024-02-17 12:28:56,605 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:29:04,586 - Epoch: [132][  100/  500]    Overall Loss 0.324536    Objective Loss 0.324536                                        LR 0.000250    Time 0.079750    
2024-02-17 12:29:11,746 - Epoch: [132][  200/  500]    Overall Loss 0.326878    Objective Loss 0.326878                                        LR 0.000250    Time 0.075647    
2024-02-17 12:29:18,869 - Epoch: [132][  300/  500]    Overall Loss 0.329249    Objective Loss 0.329249                                        LR 0.000250    Time 0.074161    
2024-02-17 12:29:26,063 - Epoch: [132][  400/  500]    Overall Loss 0.333414    Objective Loss 0.333414                                        LR 0.000250    Time 0.073595    
2024-02-17 12:29:33,197 - Epoch: [132][  500/  500]    Overall Loss 0.336415    Objective Loss 0.336415    Top1 88.000000    Top5 99.500000    LR 0.000250    Time 0.073136    
2024-02-17 12:29:33,320 - --- validate (epoch=132)-----------
2024-02-17 12:29:33,321 - 10000 samples (100 per mini-batch)
2024-02-17 12:29:36,334 - Epoch: [132][  100/  100]    Loss 1.892687    Top1 59.240000    Top5 85.570000    
2024-02-17 12:29:36,448 - ==> Top1: 59.240    Top5: 85.570    Loss: 1.893

2024-02-17 12:29:36,460 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:29:36,460 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:29:36,523 - 

2024-02-17 12:29:36,523 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:29:44,162 - Epoch: [133][  100/  500]    Overall Loss 0.314647    Objective Loss 0.314647                                        LR 0.000250    Time 0.076336    
2024-02-17 12:29:51,398 - Epoch: [133][  200/  500]    Overall Loss 0.318626    Objective Loss 0.318626                                        LR 0.000250    Time 0.074324    
2024-02-17 12:29:58,503 - Epoch: [133][  300/  500]    Overall Loss 0.325949    Objective Loss 0.325949                                        LR 0.000250    Time 0.073218    
2024-02-17 12:30:05,560 - Epoch: [133][  400/  500]    Overall Loss 0.330286    Objective Loss 0.330286                                        LR 0.000250    Time 0.072547    
2024-02-17 12:30:12,765 - Epoch: [133][  500/  500]    Overall Loss 0.332519    Objective Loss 0.332519    Top1 92.500000    Top5 100.000000    LR 0.000250    Time 0.072439    
2024-02-17 12:30:12,883 - --- validate (epoch=133)-----------
2024-02-17 12:30:12,884 - 10000 samples (100 per mini-batch)
2024-02-17 12:30:15,728 - Epoch: [133][  100/  100]    Loss 1.894338    Top1 59.280000    Top5 85.410000    
2024-02-17 12:30:15,849 - ==> Top1: 59.280    Top5: 85.410    Loss: 1.894

2024-02-17 12:30:15,860 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:30:15,861 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:30:15,922 - 

2024-02-17 12:30:15,922 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:30:23,663 - Epoch: [134][  100/  500]    Overall Loss 0.323956    Objective Loss 0.323956                                        LR 0.000250    Time 0.077350    
2024-02-17 12:30:30,570 - Epoch: [134][  200/  500]    Overall Loss 0.318200    Objective Loss 0.318200                                        LR 0.000250    Time 0.073191    
2024-02-17 12:30:37,412 - Epoch: [134][  300/  500]    Overall Loss 0.323282    Objective Loss 0.323282                                        LR 0.000250    Time 0.071588    
2024-02-17 12:30:44,566 - Epoch: [134][  400/  500]    Overall Loss 0.326481    Objective Loss 0.326481                                        LR 0.000250    Time 0.071565    
2024-02-17 12:30:51,747 - Epoch: [134][  500/  500]    Overall Loss 0.330671    Objective Loss 0.330671    Top1 89.500000    Top5 99.000000    LR 0.000250    Time 0.071605    
2024-02-17 12:30:51,873 - --- validate (epoch=134)-----------
2024-02-17 12:30:51,874 - 10000 samples (100 per mini-batch)
2024-02-17 12:30:55,094 - Epoch: [134][  100/  100]    Loss 1.921497    Top1 59.110000    Top5 85.390000    
2024-02-17 12:30:55,245 - ==> Top1: 59.110    Top5: 85.390    Loss: 1.921

2024-02-17 12:30:55,257 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:30:55,257 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:30:55,319 - 

2024-02-17 12:30:55,320 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:31:03,006 - Epoch: [135][  100/  500]    Overall Loss 0.319096    Objective Loss 0.319096                                        LR 0.000250    Time 0.076799    
2024-02-17 12:31:10,144 - Epoch: [135][  200/  500]    Overall Loss 0.320444    Objective Loss 0.320444                                        LR 0.000250    Time 0.074065    
2024-02-17 12:31:17,377 - Epoch: [135][  300/  500]    Overall Loss 0.323427    Objective Loss 0.323427                                        LR 0.000250    Time 0.073473    
2024-02-17 12:31:24,463 - Epoch: [135][  400/  500]    Overall Loss 0.329200    Objective Loss 0.329200                                        LR 0.000250    Time 0.072809    
2024-02-17 12:31:31,590 - Epoch: [135][  500/  500]    Overall Loss 0.330982    Objective Loss 0.330982    Top1 90.000000    Top5 99.500000    LR 0.000250    Time 0.072494    
2024-02-17 12:31:31,700 - --- validate (epoch=135)-----------
2024-02-17 12:31:31,701 - 10000 samples (100 per mini-batch)
2024-02-17 12:31:34,574 - Epoch: [135][  100/  100]    Loss 1.874001    Top1 59.550000    Top5 85.790000    
2024-02-17 12:31:34,665 - ==> Top1: 59.550    Top5: 85.790    Loss: 1.874

2024-02-17 12:31:34,672 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:31:34,672 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:31:34,754 - 

2024-02-17 12:31:34,754 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:31:42,618 - Epoch: [136][  100/  500]    Overall Loss 0.312774    Objective Loss 0.312774                                        LR 0.000250    Time 0.078582    
2024-02-17 12:31:49,753 - Epoch: [136][  200/  500]    Overall Loss 0.318205    Objective Loss 0.318205                                        LR 0.000250    Time 0.074946    
2024-02-17 12:31:56,911 - Epoch: [136][  300/  500]    Overall Loss 0.322399    Objective Loss 0.322399                                        LR 0.000250    Time 0.073804    
2024-02-17 12:32:04,162 - Epoch: [136][  400/  500]    Overall Loss 0.329251    Objective Loss 0.329251                                        LR 0.000250    Time 0.073470    
2024-02-17 12:32:11,352 - Epoch: [136][  500/  500]    Overall Loss 0.331625    Objective Loss 0.331625    Top1 91.000000    Top5 99.500000    LR 0.000250    Time 0.073146    
2024-02-17 12:32:11,525 - --- validate (epoch=136)-----------
2024-02-17 12:32:11,526 - 10000 samples (100 per mini-batch)
2024-02-17 12:32:14,450 - Epoch: [136][  100/  100]    Loss 1.928612    Top1 58.880000    Top5 85.260000    
2024-02-17 12:32:14,582 - ==> Top1: 58.880    Top5: 85.260    Loss: 1.929

2024-02-17 12:32:14,593 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:32:14,593 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:32:14,654 - 

2024-02-17 12:32:14,654 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:32:22,297 - Epoch: [137][  100/  500]    Overall Loss 0.320526    Objective Loss 0.320526                                        LR 0.000250    Time 0.076373    
2024-02-17 12:32:29,475 - Epoch: [137][  200/  500]    Overall Loss 0.319600    Objective Loss 0.319600                                        LR 0.000250    Time 0.074051    
2024-02-17 12:32:36,482 - Epoch: [137][  300/  500]    Overall Loss 0.321798    Objective Loss 0.321798                                        LR 0.000250    Time 0.072710    
2024-02-17 12:32:43,620 - Epoch: [137][  400/  500]    Overall Loss 0.324250    Objective Loss 0.324250                                        LR 0.000250    Time 0.072366    
2024-02-17 12:32:50,837 - Epoch: [137][  500/  500]    Overall Loss 0.326408    Objective Loss 0.326408    Top1 92.500000    Top5 99.500000    LR 0.000250    Time 0.072317    
2024-02-17 12:32:50,962 - --- validate (epoch=137)-----------
2024-02-17 12:32:50,963 - 10000 samples (100 per mini-batch)
2024-02-17 12:32:53,924 - Epoch: [137][  100/  100]    Loss 1.924970    Top1 59.600000    Top5 85.320000    
2024-02-17 12:32:54,031 - ==> Top1: 59.600    Top5: 85.320    Loss: 1.925

2024-02-17 12:32:54,042 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:32:54,043 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:32:54,103 - 

2024-02-17 12:32:54,104 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:33:01,949 - Epoch: [138][  100/  500]    Overall Loss 0.312813    Objective Loss 0.312813                                        LR 0.000250    Time 0.078395    
2024-02-17 12:33:09,224 - Epoch: [138][  200/  500]    Overall Loss 0.319293    Objective Loss 0.319293                                        LR 0.000250    Time 0.075552    
2024-02-17 12:33:16,446 - Epoch: [138][  300/  500]    Overall Loss 0.317928    Objective Loss 0.317928                                        LR 0.000250    Time 0.074426    
2024-02-17 12:33:23,595 - Epoch: [138][  400/  500]    Overall Loss 0.321298    Objective Loss 0.321298                                        LR 0.000250    Time 0.073680    
2024-02-17 12:33:30,797 - Epoch: [138][  500/  500]    Overall Loss 0.325568    Objective Loss 0.325568    Top1 89.500000    Top5 99.500000    LR 0.000250    Time 0.073339    
2024-02-17 12:33:30,934 - --- validate (epoch=138)-----------
2024-02-17 12:33:30,935 - 10000 samples (100 per mini-batch)
2024-02-17 12:33:33,984 - Epoch: [138][  100/  100]    Loss 1.897192    Top1 59.600000    Top5 85.370000    
2024-02-17 12:33:34,104 - ==> Top1: 59.600    Top5: 85.370    Loss: 1.897

2024-02-17 12:33:34,115 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:33:34,116 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:33:34,179 - 

2024-02-17 12:33:34,179 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:33:41,784 - Epoch: [139][  100/  500]    Overall Loss 0.313691    Objective Loss 0.313691                                        LR 0.000250    Time 0.075986    
2024-02-17 12:33:48,987 - Epoch: [139][  200/  500]    Overall Loss 0.316112    Objective Loss 0.316112                                        LR 0.000250    Time 0.073984    
2024-02-17 12:33:56,097 - Epoch: [139][  300/  500]    Overall Loss 0.321306    Objective Loss 0.321306                                        LR 0.000250    Time 0.073009    
2024-02-17 12:34:03,362 - Epoch: [139][  400/  500]    Overall Loss 0.322199    Objective Loss 0.322199                                        LR 0.000250    Time 0.072906    
2024-02-17 12:34:10,662 - Epoch: [139][  500/  500]    Overall Loss 0.323036    Objective Loss 0.323036    Top1 88.500000    Top5 99.000000    LR 0.000250    Time 0.072918    
2024-02-17 12:34:10,765 - --- validate (epoch=139)-----------
2024-02-17 12:34:10,765 - 10000 samples (100 per mini-batch)
2024-02-17 12:34:14,004 - Epoch: [139][  100/  100]    Loss 1.915504    Top1 59.400000    Top5 85.500000    
2024-02-17 12:34:14,106 - ==> Top1: 59.400    Top5: 85.500    Loss: 1.916

2024-02-17 12:34:14,118 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:34:14,119 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:34:14,180 - 

2024-02-17 12:34:14,180 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:34:22,159 - Epoch: [140][  100/  500]    Overall Loss 0.308380    Objective Loss 0.308380                                        LR 0.000250    Time 0.079732    
2024-02-17 12:34:29,311 - Epoch: [140][  200/  500]    Overall Loss 0.316878    Objective Loss 0.316878                                        LR 0.000250    Time 0.075600    
2024-02-17 12:34:36,453 - Epoch: [140][  300/  500]    Overall Loss 0.317922    Objective Loss 0.317922                                        LR 0.000250    Time 0.074191    
2024-02-17 12:34:43,696 - Epoch: [140][  400/  500]    Overall Loss 0.319950    Objective Loss 0.319950                                        LR 0.000250    Time 0.073739    
2024-02-17 12:34:50,944 - Epoch: [140][  500/  500]    Overall Loss 0.322267    Objective Loss 0.322267    Top1 92.000000    Top5 99.500000    LR 0.000250    Time 0.073479    
2024-02-17 12:34:51,083 - --- validate (epoch=140)-----------
2024-02-17 12:34:51,084 - 10000 samples (100 per mini-batch)
2024-02-17 12:34:54,187 - Epoch: [140][  100/  100]    Loss 1.888656    Top1 59.530000    Top5 85.440000    
2024-02-17 12:34:54,290 - ==> Top1: 59.530    Top5: 85.440    Loss: 1.889

2024-02-17 12:34:54,302 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:34:54,302 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:34:54,366 - 

2024-02-17 12:34:54,366 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:35:02,185 - Epoch: [141][  100/  500]    Overall Loss 0.303841    Objective Loss 0.303841                                        LR 0.000250    Time 0.078128    
2024-02-17 12:35:09,280 - Epoch: [141][  200/  500]    Overall Loss 0.305617    Objective Loss 0.305617                                        LR 0.000250    Time 0.074519    
2024-02-17 12:35:16,482 - Epoch: [141][  300/  500]    Overall Loss 0.307391    Objective Loss 0.307391                                        LR 0.000250    Time 0.073672    
2024-02-17 12:35:23,664 - Epoch: [141][  400/  500]    Overall Loss 0.311601    Objective Loss 0.311601                                        LR 0.000250    Time 0.073196    
2024-02-17 12:35:30,882 - Epoch: [141][  500/  500]    Overall Loss 0.314979    Objective Loss 0.314979    Top1 92.000000    Top5 99.500000    LR 0.000250    Time 0.072985    
2024-02-17 12:35:31,073 - --- validate (epoch=141)-----------
2024-02-17 12:35:31,074 - 10000 samples (100 per mini-batch)
2024-02-17 12:35:34,073 - Epoch: [141][  100/  100]    Loss 1.918699    Top1 59.460000    Top5 85.740000    
2024-02-17 12:35:34,176 - ==> Top1: 59.460    Top5: 85.740    Loss: 1.919

2024-02-17 12:35:34,186 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:35:34,187 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:35:34,252 - 

2024-02-17 12:35:34,252 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:35:41,908 - Epoch: [142][  100/  500]    Overall Loss 0.301498    Objective Loss 0.301498                                        LR 0.000250    Time 0.076505    
2024-02-17 12:35:48,971 - Epoch: [142][  200/  500]    Overall Loss 0.307584    Objective Loss 0.307584                                        LR 0.000250    Time 0.073546    
2024-02-17 12:35:56,704 - Epoch: [142][  300/  500]    Overall Loss 0.312257    Objective Loss 0.312257                                        LR 0.000250    Time 0.074790    
2024-02-17 12:36:03,986 - Epoch: [142][  400/  500]    Overall Loss 0.315166    Objective Loss 0.315166                                        LR 0.000250    Time 0.074287    
2024-02-17 12:36:11,082 - Epoch: [142][  500/  500]    Overall Loss 0.318074    Objective Loss 0.318074    Top1 90.000000    Top5 100.000000    LR 0.000250    Time 0.073614    
2024-02-17 12:36:11,260 - --- validate (epoch=142)-----------
2024-02-17 12:36:11,261 - 10000 samples (100 per mini-batch)
2024-02-17 12:36:14,384 - Epoch: [142][  100/  100]    Loss 1.891356    Top1 59.630000    Top5 85.490000    
2024-02-17 12:36:14,490 - ==> Top1: 59.630    Top5: 85.490    Loss: 1.891

2024-02-17 12:36:14,502 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:36:14,502 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:36:14,564 - 

2024-02-17 12:36:14,565 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:36:22,134 - Epoch: [143][  100/  500]    Overall Loss 0.305017    Objective Loss 0.305017                                        LR 0.000250    Time 0.075639    
2024-02-17 12:36:29,365 - Epoch: [143][  200/  500]    Overall Loss 0.305078    Objective Loss 0.305078                                        LR 0.000250    Time 0.073955    
2024-02-17 12:36:36,510 - Epoch: [143][  300/  500]    Overall Loss 0.305630    Objective Loss 0.305630                                        LR 0.000250    Time 0.073103    
2024-02-17 12:36:43,583 - Epoch: [143][  400/  500]    Overall Loss 0.308593    Objective Loss 0.308593                                        LR 0.000250    Time 0.072501    
2024-02-17 12:36:50,768 - Epoch: [143][  500/  500]    Overall Loss 0.310301    Objective Loss 0.310301    Top1 92.000000    Top5 99.500000    LR 0.000250    Time 0.072361    
2024-02-17 12:36:50,875 - --- validate (epoch=143)-----------
2024-02-17 12:36:50,876 - 10000 samples (100 per mini-batch)
2024-02-17 12:36:53,844 - Epoch: [143][  100/  100]    Loss 1.907204    Top1 60.080000    Top5 85.530000    
2024-02-17 12:36:53,961 - ==> Top1: 60.080    Top5: 85.530    Loss: 1.907

2024-02-17 12:36:53,972 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:36:53,972 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:36:54,033 - 

2024-02-17 12:36:54,033 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:37:01,820 - Epoch: [144][  100/  500]    Overall Loss 0.296074    Objective Loss 0.296074                                        LR 0.000250    Time 0.077818    
2024-02-17 12:37:08,898 - Epoch: [144][  200/  500]    Overall Loss 0.292306    Objective Loss 0.292306                                        LR 0.000250    Time 0.074277    
2024-02-17 12:37:16,158 - Epoch: [144][  300/  500]    Overall Loss 0.299635    Objective Loss 0.299635                                        LR 0.000250    Time 0.073703    
2024-02-17 12:37:23,343 - Epoch: [144][  400/  500]    Overall Loss 0.302514    Objective Loss 0.302514                                        LR 0.000250    Time 0.073230    
2024-02-17 12:37:30,416 - Epoch: [144][  500/  500]    Overall Loss 0.306938    Objective Loss 0.306938    Top1 91.000000    Top5 100.000000    LR 0.000250    Time 0.072720    
2024-02-17 12:37:30,576 - --- validate (epoch=144)-----------
2024-02-17 12:37:30,577 - 10000 samples (100 per mini-batch)
2024-02-17 12:37:33,602 - Epoch: [144][  100/  100]    Loss 1.907192    Top1 59.280000    Top5 85.520000    
2024-02-17 12:37:33,726 - ==> Top1: 59.280    Top5: 85.520    Loss: 1.907

2024-02-17 12:37:33,741 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:37:33,741 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:37:33,806 - 

2024-02-17 12:37:33,807 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:37:41,550 - Epoch: [145][  100/  500]    Overall Loss 0.282330    Objective Loss 0.282330                                        LR 0.000250    Time 0.077349    
2024-02-17 12:37:48,750 - Epoch: [145][  200/  500]    Overall Loss 0.292444    Objective Loss 0.292444                                        LR 0.000250    Time 0.074647    
2024-02-17 12:37:55,909 - Epoch: [145][  300/  500]    Overall Loss 0.299170    Objective Loss 0.299170                                        LR 0.000250    Time 0.073616    
2024-02-17 12:38:03,082 - Epoch: [145][  400/  500]    Overall Loss 0.304370    Objective Loss 0.304370                                        LR 0.000250    Time 0.073133    
2024-02-17 12:38:10,190 - Epoch: [145][  500/  500]    Overall Loss 0.308219    Objective Loss 0.308219    Top1 92.500000    Top5 98.500000    LR 0.000250    Time 0.072714    
2024-02-17 12:38:10,308 - --- validate (epoch=145)-----------
2024-02-17 12:38:10,309 - 10000 samples (100 per mini-batch)
2024-02-17 12:38:13,295 - Epoch: [145][  100/  100]    Loss 1.923322    Top1 59.140000    Top5 85.370000    
2024-02-17 12:38:13,411 - ==> Top1: 59.140    Top5: 85.370    Loss: 1.923

2024-02-17 12:38:13,422 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:38:13,423 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:38:13,491 - 

2024-02-17 12:38:13,491 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:38:21,561 - Epoch: [146][  100/  500]    Overall Loss 0.295699    Objective Loss 0.295699                                        LR 0.000250    Time 0.080641    
2024-02-17 12:38:28,760 - Epoch: [146][  200/  500]    Overall Loss 0.296993    Objective Loss 0.296993                                        LR 0.000250    Time 0.076293    
2024-02-17 12:38:35,935 - Epoch: [146][  300/  500]    Overall Loss 0.299033    Objective Loss 0.299033                                        LR 0.000250    Time 0.074764    
2024-02-17 12:38:43,147 - Epoch: [146][  400/  500]    Overall Loss 0.301379    Objective Loss 0.301379                                        LR 0.000250    Time 0.074093    
2024-02-17 12:38:50,447 - Epoch: [146][  500/  500]    Overall Loss 0.305143    Objective Loss 0.305143    Top1 88.500000    Top5 99.500000    LR 0.000250    Time 0.073864    
2024-02-17 12:38:50,628 - --- validate (epoch=146)-----------
2024-02-17 12:38:50,629 - 10000 samples (100 per mini-batch)
2024-02-17 12:38:53,527 - Epoch: [146][  100/  100]    Loss 1.971627    Top1 58.810000    Top5 85.000000    
2024-02-17 12:38:53,650 - ==> Top1: 58.810    Top5: 85.000    Loss: 1.972

2024-02-17 12:38:53,658 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:38:53,659 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:38:53,740 - 

2024-02-17 12:38:53,740 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:39:01,579 - Epoch: [147][  100/  500]    Overall Loss 0.309138    Objective Loss 0.309138                                        LR 0.000250    Time 0.078330    
2024-02-17 12:39:08,705 - Epoch: [147][  200/  500]    Overall Loss 0.306801    Objective Loss 0.306801                                        LR 0.000250    Time 0.074776    
2024-02-17 12:39:15,888 - Epoch: [147][  300/  500]    Overall Loss 0.305706    Objective Loss 0.305706                                        LR 0.000250    Time 0.073778    
2024-02-17 12:39:22,750 - Epoch: [147][  400/  500]    Overall Loss 0.307134    Objective Loss 0.307134                                        LR 0.000250    Time 0.072479    
2024-02-17 12:39:29,494 - Epoch: [147][  500/  500]    Overall Loss 0.307280    Objective Loss 0.307280    Top1 94.000000    Top5 99.500000    LR 0.000250    Time 0.071466    
2024-02-17 12:39:29,670 - --- validate (epoch=147)-----------
2024-02-17 12:39:29,670 - 10000 samples (100 per mini-batch)
2024-02-17 12:39:32,585 - Epoch: [147][  100/  100]    Loss 1.915154    Top1 59.680000    Top5 85.160000    
2024-02-17 12:39:32,683 - ==> Top1: 59.680    Top5: 85.160    Loss: 1.915

2024-02-17 12:39:32,695 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:39:32,695 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:39:32,757 - 

2024-02-17 12:39:32,757 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:39:40,466 - Epoch: [148][  100/  500]    Overall Loss 0.302657    Objective Loss 0.302657                                        LR 0.000250    Time 0.077037    
2024-02-17 12:39:47,554 - Epoch: [148][  200/  500]    Overall Loss 0.302141    Objective Loss 0.302141                                        LR 0.000250    Time 0.073936    
2024-02-17 12:39:54,378 - Epoch: [148][  300/  500]    Overall Loss 0.303867    Objective Loss 0.303867                                        LR 0.000250    Time 0.072027    
2024-02-17 12:40:01,233 - Epoch: [148][  400/  500]    Overall Loss 0.304256    Objective Loss 0.304256                                        LR 0.000250    Time 0.071148    
2024-02-17 12:40:08,124 - Epoch: [148][  500/  500]    Overall Loss 0.306868    Objective Loss 0.306868    Top1 88.000000    Top5 100.000000    LR 0.000250    Time 0.070695    
2024-02-17 12:40:08,310 - --- validate (epoch=148)-----------
2024-02-17 12:40:08,312 - 10000 samples (100 per mini-batch)
2024-02-17 12:40:11,225 - Epoch: [148][  100/  100]    Loss 1.888129    Top1 60.240000    Top5 85.950000    
2024-02-17 12:40:11,339 - ==> Top1: 60.240    Top5: 85.950    Loss: 1.888

2024-02-17 12:40:11,349 - ==> Best [Top1: 60.240   Top5: 85.950   Sparsity:0.00   Params: 753952 on epoch: 148]
2024-02-17 12:40:11,350 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:40:11,434 - 

2024-02-17 12:40:11,435 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:40:19,006 - Epoch: [149][  100/  500]    Overall Loss 0.288324    Objective Loss 0.288324                                        LR 0.000250    Time 0.075656    
2024-02-17 12:40:26,188 - Epoch: [149][  200/  500]    Overall Loss 0.292148    Objective Loss 0.292148                                        LR 0.000250    Time 0.073718    
2024-02-17 12:40:33,375 - Epoch: [149][  300/  500]    Overall Loss 0.299755    Objective Loss 0.299755                                        LR 0.000250    Time 0.073085    
2024-02-17 12:40:40,654 - Epoch: [149][  400/  500]    Overall Loss 0.303344    Objective Loss 0.303344                                        LR 0.000250    Time 0.073000    
2024-02-17 12:40:47,764 - Epoch: [149][  500/  500]    Overall Loss 0.304277    Objective Loss 0.304277    Top1 90.000000    Top5 99.000000    LR 0.000250    Time 0.072612    
2024-02-17 12:40:47,929 - --- validate (epoch=149)-----------
2024-02-17 12:40:47,930 - 10000 samples (100 per mini-batch)
2024-02-17 12:40:50,794 - Epoch: [149][  100/  100]    Loss 1.935919    Top1 59.500000    Top5 85.440000    
2024-02-17 12:40:50,926 - ==> Top1: 59.500    Top5: 85.440    Loss: 1.936

2024-02-17 12:40:50,938 - ==> Best [Top1: 60.240   Top5: 85.950   Sparsity:0.00   Params: 753952 on epoch: 148]
2024-02-17 12:40:50,939 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:40:51,001 - 

2024-02-17 12:40:51,001 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:40:58,905 - Epoch: [150][  100/  500]    Overall Loss 0.273165    Objective Loss 0.273165                                        LR 0.000125    Time 0.078985    
2024-02-17 12:41:06,258 - Epoch: [150][  200/  500]    Overall Loss 0.268597    Objective Loss 0.268597                                        LR 0.000125    Time 0.076235    
2024-02-17 12:41:13,593 - Epoch: [150][  300/  500]    Overall Loss 0.272199    Objective Loss 0.272199                                        LR 0.000125    Time 0.075259    
2024-02-17 12:41:20,815 - Epoch: [150][  400/  500]    Overall Loss 0.273386    Objective Loss 0.273386                                        LR 0.000125    Time 0.074487    
2024-02-17 12:41:27,934 - Epoch: [150][  500/  500]    Overall Loss 0.273813    Objective Loss 0.273813    Top1 90.500000    Top5 99.500000    LR 0.000125    Time 0.073819    
2024-02-17 12:41:28,081 - --- validate (epoch=150)-----------
2024-02-17 12:41:28,082 - 10000 samples (100 per mini-batch)
2024-02-17 12:41:30,978 - Epoch: [150][  100/  100]    Loss 1.883507    Top1 60.170000    Top5 85.790000    
2024-02-17 12:41:31,130 - ==> Top1: 60.170    Top5: 85.790    Loss: 1.884

2024-02-17 12:41:31,142 - ==> Best [Top1: 60.240   Top5: 85.950   Sparsity:0.00   Params: 753952 on epoch: 148]
2024-02-17 12:41:31,143 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:41:31,206 - 

2024-02-17 12:41:31,206 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:41:38,852 - Epoch: [151][  100/  500]    Overall Loss 0.265116    Objective Loss 0.265116                                        LR 0.000125    Time 0.076403    
2024-02-17 12:41:45,904 - Epoch: [151][  200/  500]    Overall Loss 0.262215    Objective Loss 0.262215                                        LR 0.000125    Time 0.073438    
2024-02-17 12:41:53,080 - Epoch: [151][  300/  500]    Overall Loss 0.262135    Objective Loss 0.262135                                        LR 0.000125    Time 0.072865    
2024-02-17 12:42:00,256 - Epoch: [151][  400/  500]    Overall Loss 0.263139    Objective Loss 0.263139                                        LR 0.000125    Time 0.072577    
2024-02-17 12:42:07,341 - Epoch: [151][  500/  500]    Overall Loss 0.265369    Objective Loss 0.265369    Top1 94.500000    Top5 100.000000    LR 0.000125    Time 0.072223    
2024-02-17 12:42:07,456 - --- validate (epoch=151)-----------
2024-02-17 12:42:07,457 - 10000 samples (100 per mini-batch)
2024-02-17 12:42:10,305 - Epoch: [151][  100/  100]    Loss 1.877078    Top1 60.580000    Top5 85.860000    
2024-02-17 12:42:10,410 - ==> Top1: 60.580    Top5: 85.860    Loss: 1.877

2024-02-17 12:42:10,416 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:42:10,416 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:42:10,490 - 

2024-02-17 12:42:10,491 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:42:18,285 - Epoch: [152][  100/  500]    Overall Loss 0.258555    Objective Loss 0.258555                                        LR 0.000125    Time 0.077891    
2024-02-17 12:42:25,407 - Epoch: [152][  200/  500]    Overall Loss 0.261813    Objective Loss 0.261813                                        LR 0.000125    Time 0.074525    
2024-02-17 12:42:32,572 - Epoch: [152][  300/  500]    Overall Loss 0.261460    Objective Loss 0.261460                                        LR 0.000125    Time 0.073550    
2024-02-17 12:42:39,758 - Epoch: [152][  400/  500]    Overall Loss 0.264246    Objective Loss 0.264246                                        LR 0.000125    Time 0.073112    
2024-02-17 12:42:46,843 - Epoch: [152][  500/  500]    Overall Loss 0.264172    Objective Loss 0.264172    Top1 95.000000    Top5 100.000000    LR 0.000125    Time 0.072652    
2024-02-17 12:42:47,018 - --- validate (epoch=152)-----------
2024-02-17 12:42:47,019 - 10000 samples (100 per mini-batch)
2024-02-17 12:42:49,843 - Epoch: [152][  100/  100]    Loss 1.897309    Top1 60.000000    Top5 86.000000    
2024-02-17 12:42:49,960 - ==> Top1: 60.000    Top5: 86.000    Loss: 1.897

2024-02-17 12:42:49,969 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:42:49,970 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:42:50,023 - 

2024-02-17 12:42:50,024 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:42:57,569 - Epoch: [153][  100/  500]    Overall Loss 0.254941    Objective Loss 0.254941                                        LR 0.000125    Time 0.075394    
2024-02-17 12:43:04,663 - Epoch: [153][  200/  500]    Overall Loss 0.253205    Objective Loss 0.253205                                        LR 0.000125    Time 0.073147    
2024-02-17 12:43:11,765 - Epoch: [153][  300/  500]    Overall Loss 0.257455    Objective Loss 0.257455                                        LR 0.000125    Time 0.072423    
2024-02-17 12:43:18,928 - Epoch: [153][  400/  500]    Overall Loss 0.258693    Objective Loss 0.258693                                        LR 0.000125    Time 0.072215    
2024-02-17 12:43:26,012 - Epoch: [153][  500/  500]    Overall Loss 0.261254    Objective Loss 0.261254    Top1 95.500000    Top5 100.000000    LR 0.000125    Time 0.071929    
2024-02-17 12:43:26,151 - --- validate (epoch=153)-----------
2024-02-17 12:43:26,151 - 10000 samples (100 per mini-batch)
2024-02-17 12:43:29,027 - Epoch: [153][  100/  100]    Loss 1.893290    Top1 60.220000    Top5 85.750000    
2024-02-17 12:43:29,126 - ==> Top1: 60.220    Top5: 85.750    Loss: 1.893

2024-02-17 12:43:29,137 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:43:29,137 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:43:29,197 - 

2024-02-17 12:43:29,197 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:43:36,760 - Epoch: [154][  100/  500]    Overall Loss 0.248995    Objective Loss 0.248995                                        LR 0.000125    Time 0.075573    
2024-02-17 12:43:44,000 - Epoch: [154][  200/  500]    Overall Loss 0.251812    Objective Loss 0.251812                                        LR 0.000125    Time 0.073964    
2024-02-17 12:43:51,248 - Epoch: [154][  300/  500]    Overall Loss 0.256797    Objective Loss 0.256797                                        LR 0.000125    Time 0.073454    
2024-02-17 12:43:58,644 - Epoch: [154][  400/  500]    Overall Loss 0.259046    Objective Loss 0.259046                                        LR 0.000125    Time 0.073569    
2024-02-17 12:44:05,724 - Epoch: [154][  500/  500]    Overall Loss 0.261252    Objective Loss 0.261252    Top1 89.500000    Top5 100.000000    LR 0.000125    Time 0.073006    
2024-02-17 12:44:05,895 - --- validate (epoch=154)-----------
2024-02-17 12:44:05,895 - 10000 samples (100 per mini-batch)
2024-02-17 12:44:09,014 - Epoch: [154][  100/  100]    Loss 1.909400    Top1 59.950000    Top5 85.850000    
2024-02-17 12:44:09,128 - ==> Top1: 59.950    Top5: 85.850    Loss: 1.909

2024-02-17 12:44:09,138 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:44:09,138 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:44:09,198 - 

2024-02-17 12:44:09,198 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:44:16,728 - Epoch: [155][  100/  500]    Overall Loss 0.246360    Objective Loss 0.246360                                        LR 0.000125    Time 0.075238    
2024-02-17 12:44:23,894 - Epoch: [155][  200/  500]    Overall Loss 0.251740    Objective Loss 0.251740                                        LR 0.000125    Time 0.073431    
2024-02-17 12:44:31,083 - Epoch: [155][  300/  500]    Overall Loss 0.255473    Objective Loss 0.255473                                        LR 0.000125    Time 0.072903    
2024-02-17 12:44:38,122 - Epoch: [155][  400/  500]    Overall Loss 0.254105    Objective Loss 0.254105                                        LR 0.000125    Time 0.072263    
2024-02-17 12:44:45,200 - Epoch: [155][  500/  500]    Overall Loss 0.255617    Objective Loss 0.255617    Top1 95.500000    Top5 100.000000    LR 0.000125    Time 0.071957    
2024-02-17 12:44:45,383 - --- validate (epoch=155)-----------
2024-02-17 12:44:45,384 - 10000 samples (100 per mini-batch)
2024-02-17 12:44:48,287 - Epoch: [155][  100/  100]    Loss 1.907299    Top1 59.790000    Top5 85.910000    
2024-02-17 12:44:48,411 - ==> Top1: 59.790    Top5: 85.910    Loss: 1.907

2024-02-17 12:44:48,423 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:44:48,423 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:44:48,486 - 

2024-02-17 12:44:48,486 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:44:55,939 - Epoch: [156][  100/  500]    Overall Loss 0.252096    Objective Loss 0.252096                                        LR 0.000125    Time 0.074479    
2024-02-17 12:45:02,758 - Epoch: [156][  200/  500]    Overall Loss 0.250345    Objective Loss 0.250345                                        LR 0.000125    Time 0.071317    
2024-02-17 12:45:09,794 - Epoch: [156][  300/  500]    Overall Loss 0.254333    Objective Loss 0.254333                                        LR 0.000125    Time 0.070984    
2024-02-17 12:45:16,996 - Epoch: [156][  400/  500]    Overall Loss 0.256664    Objective Loss 0.256664                                        LR 0.000125    Time 0.071232    
2024-02-17 12:45:24,383 - Epoch: [156][  500/  500]    Overall Loss 0.256761    Objective Loss 0.256761    Top1 95.000000    Top5 100.000000    LR 0.000125    Time 0.071750    
2024-02-17 12:45:24,551 - --- validate (epoch=156)-----------
2024-02-17 12:45:24,552 - 10000 samples (100 per mini-batch)
2024-02-17 12:45:27,598 - Epoch: [156][  100/  100]    Loss 1.902428    Top1 60.500000    Top5 85.740000    
2024-02-17 12:45:27,699 - ==> Top1: 60.500    Top5: 85.740    Loss: 1.902

2024-02-17 12:45:27,713 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:45:27,713 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:45:27,778 - 

2024-02-17 12:45:27,779 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:45:35,378 - Epoch: [157][  100/  500]    Overall Loss 0.245394    Objective Loss 0.245394                                        LR 0.000125    Time 0.075917    
2024-02-17 12:45:42,579 - Epoch: [157][  200/  500]    Overall Loss 0.251643    Objective Loss 0.251643                                        LR 0.000125    Time 0.073941    
2024-02-17 12:45:49,620 - Epoch: [157][  300/  500]    Overall Loss 0.251384    Objective Loss 0.251384                                        LR 0.000125    Time 0.072751    
2024-02-17 12:45:56,902 - Epoch: [157][  400/  500]    Overall Loss 0.255264    Objective Loss 0.255264                                        LR 0.000125    Time 0.072757    
2024-02-17 12:46:03,995 - Epoch: [157][  500/  500]    Overall Loss 0.256547    Objective Loss 0.256547    Top1 95.500000    Top5 99.500000    LR 0.000125    Time 0.072383    
2024-02-17 12:46:04,162 - --- validate (epoch=157)-----------
2024-02-17 12:46:04,163 - 10000 samples (100 per mini-batch)
2024-02-17 12:46:07,037 - Epoch: [157][  100/  100]    Loss 1.920516    Top1 59.920000    Top5 85.730000    
2024-02-17 12:46:07,133 - ==> Top1: 59.920    Top5: 85.730    Loss: 1.921

2024-02-17 12:46:07,140 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:46:07,140 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:46:07,196 - 

2024-02-17 12:46:07,197 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:46:14,763 - Epoch: [158][  100/  500]    Overall Loss 0.243929    Objective Loss 0.243929                                        LR 0.000125    Time 0.075586    
2024-02-17 12:46:21,916 - Epoch: [158][  200/  500]    Overall Loss 0.251566    Objective Loss 0.251566                                        LR 0.000125    Time 0.073535    
2024-02-17 12:46:29,015 - Epoch: [158][  300/  500]    Overall Loss 0.253808    Objective Loss 0.253808                                        LR 0.000125    Time 0.072672    
2024-02-17 12:46:36,103 - Epoch: [158][  400/  500]    Overall Loss 0.257415    Objective Loss 0.257415                                        LR 0.000125    Time 0.072214    
2024-02-17 12:46:43,172 - Epoch: [158][  500/  500]    Overall Loss 0.258460    Objective Loss 0.258460    Top1 89.500000    Top5 100.000000    LR 0.000125    Time 0.071900    
2024-02-17 12:46:43,352 - --- validate (epoch=158)-----------
2024-02-17 12:46:43,353 - 10000 samples (100 per mini-batch)
2024-02-17 12:46:46,581 - Epoch: [158][  100/  100]    Loss 1.905086    Top1 60.190000    Top5 85.770000    
2024-02-17 12:46:46,684 - ==> Top1: 60.190    Top5: 85.770    Loss: 1.905

2024-02-17 12:46:46,695 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:46:46,695 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:46:46,757 - 

2024-02-17 12:46:46,758 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:46:54,252 - Epoch: [159][  100/  500]    Overall Loss 0.240422    Objective Loss 0.240422                                        LR 0.000125    Time 0.074886    
2024-02-17 12:47:01,286 - Epoch: [159][  200/  500]    Overall Loss 0.247656    Objective Loss 0.247656                                        LR 0.000125    Time 0.072591    
2024-02-17 12:47:08,455 - Epoch: [159][  300/  500]    Overall Loss 0.253079    Objective Loss 0.253079                                        LR 0.000125    Time 0.072276    
2024-02-17 12:47:15,778 - Epoch: [159][  400/  500]    Overall Loss 0.255299    Objective Loss 0.255299                                        LR 0.000125    Time 0.072504    
2024-02-17 12:47:22,959 - Epoch: [159][  500/  500]    Overall Loss 0.253614    Objective Loss 0.253614    Top1 92.500000    Top5 99.500000    LR 0.000125    Time 0.072356    
2024-02-17 12:47:23,144 - --- validate (epoch=159)-----------
2024-02-17 12:47:23,145 - 10000 samples (100 per mini-batch)
2024-02-17 12:47:26,031 - Epoch: [159][  100/  100]    Loss 1.922087    Top1 59.890000    Top5 85.770000    
2024-02-17 12:47:26,166 - ==> Top1: 59.890    Top5: 85.770    Loss: 1.922

2024-02-17 12:47:26,172 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:47:26,172 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:47:26,240 - 

2024-02-17 12:47:26,240 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:47:33,967 - Epoch: [160][  100/  500]    Overall Loss 0.250435    Objective Loss 0.250435                                        LR 0.000125    Time 0.077208    
2024-02-17 12:47:41,233 - Epoch: [160][  200/  500]    Overall Loss 0.255699    Objective Loss 0.255699                                        LR 0.000125    Time 0.074912    
2024-02-17 12:47:48,324 - Epoch: [160][  300/  500]    Overall Loss 0.255090    Objective Loss 0.255090                                        LR 0.000125    Time 0.073563    
2024-02-17 12:47:55,535 - Epoch: [160][  400/  500]    Overall Loss 0.253042    Objective Loss 0.253042                                        LR 0.000125    Time 0.073189    
2024-02-17 12:48:02,715 - Epoch: [160][  500/  500]    Overall Loss 0.252557    Objective Loss 0.252557    Top1 94.500000    Top5 100.000000    LR 0.000125    Time 0.072903    
2024-02-17 12:48:02,898 - --- validate (epoch=160)-----------
2024-02-17 12:48:02,899 - 10000 samples (100 per mini-batch)
2024-02-17 12:48:05,872 - Epoch: [160][  100/  100]    Loss 1.924850    Top1 60.170000    Top5 85.660000    
2024-02-17 12:48:06,015 - ==> Top1: 60.170    Top5: 85.660    Loss: 1.925

2024-02-17 12:48:06,027 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:48:06,027 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:48:06,089 - 

2024-02-17 12:48:06,090 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:48:13,697 - Epoch: [161][  100/  500]    Overall Loss 0.242642    Objective Loss 0.242642                                        LR 0.000125    Time 0.076018    
2024-02-17 12:48:20,715 - Epoch: [161][  200/  500]    Overall Loss 0.249146    Objective Loss 0.249146                                        LR 0.000125    Time 0.073080    
2024-02-17 12:48:27,872 - Epoch: [161][  300/  500]    Overall Loss 0.249139    Objective Loss 0.249139                                        LR 0.000125    Time 0.072561    
2024-02-17 12:48:35,121 - Epoch: [161][  400/  500]    Overall Loss 0.250134    Objective Loss 0.250134                                        LR 0.000125    Time 0.072532    
2024-02-17 12:48:42,439 - Epoch: [161][  500/  500]    Overall Loss 0.251034    Objective Loss 0.251034    Top1 93.500000    Top5 100.000000    LR 0.000125    Time 0.072652    
2024-02-17 12:48:42,551 - --- validate (epoch=161)-----------
2024-02-17 12:48:42,552 - 10000 samples (100 per mini-batch)
2024-02-17 12:48:45,461 - Epoch: [161][  100/  100]    Loss 1.905843    Top1 60.080000    Top5 85.760000    
2024-02-17 12:48:45,561 - ==> Top1: 60.080    Top5: 85.760    Loss: 1.906

2024-02-17 12:48:45,572 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:48:45,572 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:48:45,659 - 

2024-02-17 12:48:45,659 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:48:53,494 - Epoch: [162][  100/  500]    Overall Loss 0.234019    Objective Loss 0.234019                                        LR 0.000125    Time 0.078288    
2024-02-17 12:49:00,623 - Epoch: [162][  200/  500]    Overall Loss 0.244368    Objective Loss 0.244368                                        LR 0.000125    Time 0.074767    
2024-02-17 12:49:07,897 - Epoch: [162][  300/  500]    Overall Loss 0.245240    Objective Loss 0.245240                                        LR 0.000125    Time 0.074077    
2024-02-17 12:49:15,136 - Epoch: [162][  400/  500]    Overall Loss 0.245359    Objective Loss 0.245359                                        LR 0.000125    Time 0.073644    
2024-02-17 12:49:22,221 - Epoch: [162][  500/  500]    Overall Loss 0.247341    Objective Loss 0.247341    Top1 91.000000    Top5 99.500000    LR 0.000125    Time 0.073078    
2024-02-17 12:49:22,386 - --- validate (epoch=162)-----------
2024-02-17 12:49:22,387 - 10000 samples (100 per mini-batch)
2024-02-17 12:49:25,280 - Epoch: [162][  100/  100]    Loss 1.919250    Top1 60.410000    Top5 85.600000    
2024-02-17 12:49:25,390 - ==> Top1: 60.410    Top5: 85.600    Loss: 1.919

2024-02-17 12:49:25,401 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:49:25,402 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:49:25,463 - 

2024-02-17 12:49:25,463 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:49:33,087 - Epoch: [163][  100/  500]    Overall Loss 0.238383    Objective Loss 0.238383                                        LR 0.000125    Time 0.076185    
2024-02-17 12:49:40,211 - Epoch: [163][  200/  500]    Overall Loss 0.246432    Objective Loss 0.246432                                        LR 0.000125    Time 0.073687    
2024-02-17 12:49:47,331 - Epoch: [163][  300/  500]    Overall Loss 0.249270    Objective Loss 0.249270                                        LR 0.000125    Time 0.072845    
2024-02-17 12:49:54,531 - Epoch: [163][  400/  500]    Overall Loss 0.248447    Objective Loss 0.248447                                        LR 0.000125    Time 0.072622    
2024-02-17 12:50:01,729 - Epoch: [163][  500/  500]    Overall Loss 0.248890    Objective Loss 0.248890    Top1 93.500000    Top5 99.500000    LR 0.000125    Time 0.072485    
2024-02-17 12:50:01,873 - --- validate (epoch=163)-----------
2024-02-17 12:50:01,873 - 10000 samples (100 per mini-batch)
2024-02-17 12:50:04,802 - Epoch: [163][  100/  100]    Loss 1.931699    Top1 59.970000    Top5 85.960000    
2024-02-17 12:50:04,932 - ==> Top1: 59.970    Top5: 85.960    Loss: 1.932

2024-02-17 12:50:04,944 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:50:04,944 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:50:05,007 - 

2024-02-17 12:50:05,007 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:50:12,934 - Epoch: [164][  100/  500]    Overall Loss 0.241553    Objective Loss 0.241553                                        LR 0.000125    Time 0.079214    
2024-02-17 12:50:19,979 - Epoch: [164][  200/  500]    Overall Loss 0.247464    Objective Loss 0.247464                                        LR 0.000125    Time 0.074809    
2024-02-17 12:50:27,056 - Epoch: [164][  300/  500]    Overall Loss 0.247544    Objective Loss 0.247544                                        LR 0.000125    Time 0.073449    
2024-02-17 12:50:34,182 - Epoch: [164][  400/  500]    Overall Loss 0.249057    Objective Loss 0.249057                                        LR 0.000125    Time 0.072891    
2024-02-17 12:50:41,327 - Epoch: [164][  500/  500]    Overall Loss 0.249866    Objective Loss 0.249866    Top1 95.000000    Top5 100.000000    LR 0.000125    Time 0.072594    
2024-02-17 12:50:41,453 - --- validate (epoch=164)-----------
2024-02-17 12:50:41,455 - 10000 samples (100 per mini-batch)
2024-02-17 12:50:44,555 - Epoch: [164][  100/  100]    Loss 1.929833    Top1 59.980000    Top5 85.800000    
2024-02-17 12:50:44,654 - ==> Top1: 59.980    Top5: 85.800    Loss: 1.930

2024-02-17 12:50:44,666 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:50:44,666 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:50:44,743 - 

2024-02-17 12:50:44,744 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:50:52,449 - Epoch: [165][  100/  500]    Overall Loss 0.238838    Objective Loss 0.238838                                        LR 0.000125    Time 0.076987    
2024-02-17 12:50:59,586 - Epoch: [165][  200/  500]    Overall Loss 0.240462    Objective Loss 0.240462                                        LR 0.000125    Time 0.074153    
2024-02-17 12:51:06,706 - Epoch: [165][  300/  500]    Overall Loss 0.243240    Objective Loss 0.243240                                        LR 0.000125    Time 0.073153    
2024-02-17 12:51:13,883 - Epoch: [165][  400/  500]    Overall Loss 0.244188    Objective Loss 0.244188                                        LR 0.000125    Time 0.072796    
2024-02-17 12:51:21,083 - Epoch: [165][  500/  500]    Overall Loss 0.244595    Objective Loss 0.244595    Top1 92.500000    Top5 100.000000    LR 0.000125    Time 0.072629    
2024-02-17 12:51:21,266 - --- validate (epoch=165)-----------
2024-02-17 12:51:21,266 - 10000 samples (100 per mini-batch)
2024-02-17 12:51:24,406 - Epoch: [165][  100/  100]    Loss 1.920215    Top1 60.150000    Top5 85.790000    
2024-02-17 12:51:24,502 - ==> Top1: 60.150    Top5: 85.790    Loss: 1.920

2024-02-17 12:51:24,513 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:51:24,513 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:51:24,574 - 

2024-02-17 12:51:24,575 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:51:32,176 - Epoch: [166][  100/  500]    Overall Loss 0.245896    Objective Loss 0.245896                                        LR 0.000125    Time 0.075952    
2024-02-17 12:51:39,351 - Epoch: [166][  200/  500]    Overall Loss 0.247981    Objective Loss 0.247981                                        LR 0.000125    Time 0.073829    
2024-02-17 12:51:46,467 - Epoch: [166][  300/  500]    Overall Loss 0.246354    Objective Loss 0.246354                                        LR 0.000125    Time 0.072923    
2024-02-17 12:51:53,693 - Epoch: [166][  400/  500]    Overall Loss 0.246137    Objective Loss 0.246137                                        LR 0.000125    Time 0.072745    
2024-02-17 12:52:00,842 - Epoch: [166][  500/  500]    Overall Loss 0.246829    Objective Loss 0.246829    Top1 91.000000    Top5 100.000000    LR 0.000125    Time 0.072486    
2024-02-17 12:52:01,039 - --- validate (epoch=166)-----------
2024-02-17 12:52:01,040 - 10000 samples (100 per mini-batch)
2024-02-17 12:52:04,344 - Epoch: [166][  100/  100]    Loss 1.935571    Top1 59.970000    Top5 85.610000    
2024-02-17 12:52:04,450 - ==> Top1: 59.970    Top5: 85.610    Loss: 1.936

2024-02-17 12:52:04,462 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:52:04,462 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:52:04,526 - 

2024-02-17 12:52:04,527 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:52:11,742 - Epoch: [167][  100/  500]    Overall Loss 0.236638    Objective Loss 0.236638                                        LR 0.000125    Time 0.072092    
2024-02-17 12:52:18,452 - Epoch: [167][  200/  500]    Overall Loss 0.237886    Objective Loss 0.237886                                        LR 0.000125    Time 0.069577    
2024-02-17 12:52:25,175 - Epoch: [167][  300/  500]    Overall Loss 0.240551    Objective Loss 0.240551                                        LR 0.000125    Time 0.068785    
2024-02-17 12:52:32,210 - Epoch: [167][  400/  500]    Overall Loss 0.241208    Objective Loss 0.241208                                        LR 0.000125    Time 0.069168    
2024-02-17 12:52:39,421 - Epoch: [167][  500/  500]    Overall Loss 0.244225    Objective Loss 0.244225    Top1 88.500000    Top5 100.000000    LR 0.000125    Time 0.069747    
2024-02-17 12:52:39,618 - --- validate (epoch=167)-----------
2024-02-17 12:52:39,619 - 10000 samples (100 per mini-batch)
2024-02-17 12:52:42,565 - Epoch: [167][  100/  100]    Loss 1.930874    Top1 60.170000    Top5 85.860000    
2024-02-17 12:52:42,662 - ==> Top1: 60.170    Top5: 85.860    Loss: 1.931

2024-02-17 12:52:42,673 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:52:42,673 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:52:42,735 - 

2024-02-17 12:52:42,735 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:52:50,355 - Epoch: [168][  100/  500]    Overall Loss 0.232428    Objective Loss 0.232428                                        LR 0.000125    Time 0.076140    
2024-02-17 12:52:57,513 - Epoch: [168][  200/  500]    Overall Loss 0.237562    Objective Loss 0.237562                                        LR 0.000125    Time 0.073832    
2024-02-17 12:53:04,614 - Epoch: [168][  300/  500]    Overall Loss 0.240998    Objective Loss 0.240998                                        LR 0.000125    Time 0.072878    
2024-02-17 12:53:11,814 - Epoch: [168][  400/  500]    Overall Loss 0.243139    Objective Loss 0.243139                                        LR 0.000125    Time 0.072647    
2024-02-17 12:53:18,918 - Epoch: [168][  500/  500]    Overall Loss 0.242613    Objective Loss 0.242613    Top1 94.000000    Top5 99.500000    LR 0.000125    Time 0.072317    
2024-02-17 12:53:19,040 - --- validate (epoch=168)-----------
2024-02-17 12:53:19,041 - 10000 samples (100 per mini-batch)
2024-02-17 12:53:22,423 - Epoch: [168][  100/  100]    Loss 1.940044    Top1 60.110000    Top5 85.750000    
2024-02-17 12:53:22,530 - ==> Top1: 60.110    Top5: 85.750    Loss: 1.940

2024-02-17 12:53:22,536 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:53:22,536 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:53:22,609 - 

2024-02-17 12:53:22,609 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:53:30,407 - Epoch: [169][  100/  500]    Overall Loss 0.233130    Objective Loss 0.233130                                        LR 0.000125    Time 0.077920    
2024-02-17 12:53:37,511 - Epoch: [169][  200/  500]    Overall Loss 0.239286    Objective Loss 0.239286                                        LR 0.000125    Time 0.074458    
2024-02-17 12:53:44,587 - Epoch: [169][  300/  500]    Overall Loss 0.239623    Objective Loss 0.239623                                        LR 0.000125    Time 0.073214    
2024-02-17 12:53:51,812 - Epoch: [169][  400/  500]    Overall Loss 0.240834    Objective Loss 0.240834                                        LR 0.000125    Time 0.072962    
2024-02-17 12:53:59,062 - Epoch: [169][  500/  500]    Overall Loss 0.241750    Objective Loss 0.241750    Top1 90.500000    Top5 100.000000    LR 0.000125    Time 0.072859    
2024-02-17 12:53:59,241 - --- validate (epoch=169)-----------
2024-02-17 12:53:59,242 - 10000 samples (100 per mini-batch)
2024-02-17 12:54:02,559 - Epoch: [169][  100/  100]    Loss 1.938254    Top1 60.110000    Top5 85.580000    
2024-02-17 12:54:02,665 - ==> Top1: 60.110    Top5: 85.580    Loss: 1.938

2024-02-17 12:54:02,677 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:54:02,677 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:54:02,740 - 

2024-02-17 12:54:02,740 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:54:10,764 - Epoch: [170][  100/  500]    Overall Loss 0.229916    Objective Loss 0.229916                                        LR 0.000125    Time 0.080175    
2024-02-17 12:54:17,900 - Epoch: [170][  200/  500]    Overall Loss 0.236218    Objective Loss 0.236218                                        LR 0.000125    Time 0.075747    
2024-02-17 12:54:24,944 - Epoch: [170][  300/  500]    Overall Loss 0.237610    Objective Loss 0.237610                                        LR 0.000125    Time 0.073965    
2024-02-17 12:54:32,220 - Epoch: [170][  400/  500]    Overall Loss 0.239643    Objective Loss 0.239643                                        LR 0.000125    Time 0.073653    
2024-02-17 12:54:39,355 - Epoch: [170][  500/  500]    Overall Loss 0.242543    Objective Loss 0.242543    Top1 94.000000    Top5 100.000000    LR 0.000125    Time 0.073184    
2024-02-17 12:54:39,477 - --- validate (epoch=170)-----------
2024-02-17 12:54:39,478 - 10000 samples (100 per mini-batch)
2024-02-17 12:54:42,770 - Epoch: [170][  100/  100]    Loss 1.934659    Top1 60.060000    Top5 85.940000    
2024-02-17 12:54:42,876 - ==> Top1: 60.060    Top5: 85.940    Loss: 1.935

2024-02-17 12:54:42,882 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:54:42,882 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:54:42,945 - 

2024-02-17 12:54:42,945 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:54:50,514 - Epoch: [171][  100/  500]    Overall Loss 0.222846    Objective Loss 0.222846                                        LR 0.000125    Time 0.075630    
2024-02-17 12:54:57,621 - Epoch: [171][  200/  500]    Overall Loss 0.231142    Objective Loss 0.231142                                        LR 0.000125    Time 0.073331    
2024-02-17 12:55:04,759 - Epoch: [171][  300/  500]    Overall Loss 0.233896    Objective Loss 0.233896                                        LR 0.000125    Time 0.072665    
2024-02-17 12:55:11,881 - Epoch: [171][  400/  500]    Overall Loss 0.235802    Objective Loss 0.235802                                        LR 0.000125    Time 0.072292    
2024-02-17 12:55:19,021 - Epoch: [171][  500/  500]    Overall Loss 0.236756    Objective Loss 0.236756    Top1 92.000000    Top5 99.500000    LR 0.000125    Time 0.072105    
2024-02-17 12:55:19,114 - --- validate (epoch=171)-----------
2024-02-17 12:55:19,115 - 10000 samples (100 per mini-batch)
2024-02-17 12:55:22,355 - Epoch: [171][  100/  100]    Loss 1.950507    Top1 59.930000    Top5 85.490000    
2024-02-17 12:55:22,464 - ==> Top1: 59.930    Top5: 85.490    Loss: 1.951

2024-02-17 12:55:22,472 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:55:22,472 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:55:22,535 - 

2024-02-17 12:55:22,536 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:55:30,297 - Epoch: [172][  100/  500]    Overall Loss 0.233213    Objective Loss 0.233213                                        LR 0.000125    Time 0.077554    
2024-02-17 12:55:37,364 - Epoch: [172][  200/  500]    Overall Loss 0.233640    Objective Loss 0.233640                                        LR 0.000125    Time 0.074088    
2024-02-17 12:55:44,487 - Epoch: [172][  300/  500]    Overall Loss 0.234297    Objective Loss 0.234297                                        LR 0.000125    Time 0.073123    
2024-02-17 12:55:51,760 - Epoch: [172][  400/  500]    Overall Loss 0.237797    Objective Loss 0.237797                                        LR 0.000125    Time 0.073013    
2024-02-17 12:55:58,880 - Epoch: [172][  500/  500]    Overall Loss 0.237228    Objective Loss 0.237228    Top1 94.500000    Top5 100.000000    LR 0.000125    Time 0.072642    
2024-02-17 12:55:59,009 - --- validate (epoch=172)-----------
2024-02-17 12:55:59,009 - 10000 samples (100 per mini-batch)
2024-02-17 12:56:02,451 - Epoch: [172][  100/  100]    Loss 1.936042    Top1 59.900000    Top5 85.730000    
2024-02-17 12:56:02,553 - ==> Top1: 59.900    Top5: 85.730    Loss: 1.936

2024-02-17 12:56:02,562 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:56:02,563 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:56:02,626 - 

2024-02-17 12:56:02,627 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:56:10,295 - Epoch: [173][  100/  500]    Overall Loss 0.222833    Objective Loss 0.222833                                        LR 0.000125    Time 0.076627    
2024-02-17 12:56:17,632 - Epoch: [173][  200/  500]    Overall Loss 0.226368    Objective Loss 0.226368                                        LR 0.000125    Time 0.074974    
2024-02-17 12:56:24,791 - Epoch: [173][  300/  500]    Overall Loss 0.233803    Objective Loss 0.233803                                        LR 0.000125    Time 0.073831    
2024-02-17 12:56:31,745 - Epoch: [173][  400/  500]    Overall Loss 0.236299    Objective Loss 0.236299                                        LR 0.000125    Time 0.072749    
2024-02-17 12:56:38,493 - Epoch: [173][  500/  500]    Overall Loss 0.237052    Objective Loss 0.237052    Top1 94.000000    Top5 100.000000    LR 0.000125    Time 0.071689    
2024-02-17 12:56:38,646 - --- validate (epoch=173)-----------
2024-02-17 12:56:38,646 - 10000 samples (100 per mini-batch)
2024-02-17 12:56:41,784 - Epoch: [173][  100/  100]    Loss 1.957766    Top1 59.780000    Top5 85.580000    
2024-02-17 12:56:41,933 - ==> Top1: 59.780    Top5: 85.580    Loss: 1.958

2024-02-17 12:56:41,950 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:56:41,950 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:56:42,010 - 

2024-02-17 12:56:42,010 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:56:49,909 - Epoch: [174][  100/  500]    Overall Loss 0.225888    Objective Loss 0.225888                                        LR 0.000125    Time 0.078936    
2024-02-17 12:56:57,029 - Epoch: [174][  200/  500]    Overall Loss 0.232191    Objective Loss 0.232191                                        LR 0.000125    Time 0.075045    
2024-02-17 12:57:04,152 - Epoch: [174][  300/  500]    Overall Loss 0.234775    Objective Loss 0.234775                                        LR 0.000125    Time 0.073761    
2024-02-17 12:57:11,257 - Epoch: [174][  400/  500]    Overall Loss 0.236206    Objective Loss 0.236206                                        LR 0.000125    Time 0.073070    
2024-02-17 12:57:18,394 - Epoch: [174][  500/  500]    Overall Loss 0.237734    Objective Loss 0.237734    Top1 92.500000    Top5 99.500000    LR 0.000125    Time 0.072722    
2024-02-17 12:57:18,565 - --- validate (epoch=174)-----------
2024-02-17 12:57:18,566 - 10000 samples (100 per mini-batch)
2024-02-17 12:57:21,620 - Epoch: [174][  100/  100]    Loss 1.943706    Top1 59.710000    Top5 85.770000    
2024-02-17 12:57:21,714 - ==> Top1: 59.710    Top5: 85.770    Loss: 1.944

2024-02-17 12:57:21,725 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:57:21,726 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:57:21,793 - 

2024-02-17 12:57:21,794 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:57:29,409 - Epoch: [175][  100/  500]    Overall Loss 0.225993    Objective Loss 0.225993                                        LR 0.000125    Time 0.076098    
2024-02-17 12:57:36,608 - Epoch: [175][  200/  500]    Overall Loss 0.233519    Objective Loss 0.233519                                        LR 0.000125    Time 0.074014    
2024-02-17 12:57:43,943 - Epoch: [175][  300/  500]    Overall Loss 0.234766    Objective Loss 0.234766                                        LR 0.000125    Time 0.073778    
2024-02-17 12:57:51,167 - Epoch: [175][  400/  500]    Overall Loss 0.235085    Objective Loss 0.235085                                        LR 0.000125    Time 0.073382    
2024-02-17 12:57:58,331 - Epoch: [175][  500/  500]    Overall Loss 0.236514    Objective Loss 0.236514    Top1 94.000000    Top5 100.000000    LR 0.000125    Time 0.073023    
2024-02-17 12:57:58,466 - --- validate (epoch=175)-----------
2024-02-17 12:57:58,467 - 10000 samples (100 per mini-batch)
2024-02-17 12:58:01,443 - Epoch: [175][  100/  100]    Loss 1.950114    Top1 59.730000    Top5 85.760000    
2024-02-17 12:58:01,560 - ==> Top1: 59.730    Top5: 85.760    Loss: 1.950

2024-02-17 12:58:01,572 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:58:01,572 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:58:01,633 - 

2024-02-17 12:58:01,633 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:58:09,432 - Epoch: [176][  100/  500]    Overall Loss 0.237072    Objective Loss 0.237072                                        LR 0.000125    Time 0.077933    
2024-02-17 12:58:16,579 - Epoch: [176][  200/  500]    Overall Loss 0.234331    Objective Loss 0.234331                                        LR 0.000125    Time 0.074681    
2024-02-17 12:58:23,768 - Epoch: [176][  300/  500]    Overall Loss 0.234554    Objective Loss 0.234554                                        LR 0.000125    Time 0.073736    
2024-02-17 12:58:30,936 - Epoch: [176][  400/  500]    Overall Loss 0.234573    Objective Loss 0.234573                                        LR 0.000125    Time 0.073211    
2024-02-17 12:58:37,985 - Epoch: [176][  500/  500]    Overall Loss 0.234733    Objective Loss 0.234733    Top1 96.500000    Top5 100.000000    LR 0.000125    Time 0.072658    
2024-02-17 12:58:38,114 - --- validate (epoch=176)-----------
2024-02-17 12:58:38,115 - 10000 samples (100 per mini-batch)
2024-02-17 12:58:41,233 - Epoch: [176][  100/  100]    Loss 1.951452    Top1 59.990000    Top5 85.820000    
2024-02-17 12:58:41,332 - ==> Top1: 59.990    Top5: 85.820    Loss: 1.951

2024-02-17 12:58:41,340 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:58:41,340 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:58:41,409 - 

2024-02-17 12:58:41,410 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:58:49,007 - Epoch: [177][  100/  500]    Overall Loss 0.229778    Objective Loss 0.229778                                        LR 0.000125    Time 0.075919    
2024-02-17 12:58:56,059 - Epoch: [177][  200/  500]    Overall Loss 0.233496    Objective Loss 0.233496                                        LR 0.000125    Time 0.073197    
2024-02-17 12:59:03,173 - Epoch: [177][  300/  500]    Overall Loss 0.234882    Objective Loss 0.234882                                        LR 0.000125    Time 0.072497    
2024-02-17 12:59:10,403 - Epoch: [177][  400/  500]    Overall Loss 0.234475    Objective Loss 0.234475                                        LR 0.000125    Time 0.072437    
2024-02-17 12:59:17,580 - Epoch: [177][  500/  500]    Overall Loss 0.233947    Objective Loss 0.233947    Top1 97.000000    Top5 100.000000    LR 0.000125    Time 0.072295    
2024-02-17 12:59:17,715 - --- validate (epoch=177)-----------
2024-02-17 12:59:17,716 - 10000 samples (100 per mini-batch)
2024-02-17 12:59:20,689 - Epoch: [177][  100/  100]    Loss 1.948968    Top1 59.860000    Top5 85.470000    
2024-02-17 12:59:20,817 - ==> Top1: 59.860    Top5: 85.470    Loss: 1.949

2024-02-17 12:59:20,823 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:59:20,824 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 12:59:20,879 - 

2024-02-17 12:59:20,880 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:59:28,768 - Epoch: [178][  100/  500]    Overall Loss 0.229757    Objective Loss 0.229757                                        LR 0.000125    Time 0.078827    
2024-02-17 12:59:35,932 - Epoch: [178][  200/  500]    Overall Loss 0.227889    Objective Loss 0.227889                                        LR 0.000125    Time 0.075209    
2024-02-17 12:59:43,109 - Epoch: [178][  300/  500]    Overall Loss 0.233380    Objective Loss 0.233380                                        LR 0.000125    Time 0.074050    
2024-02-17 12:59:50,206 - Epoch: [178][  400/  500]    Overall Loss 0.233113    Objective Loss 0.233113                                        LR 0.000125    Time 0.073268    
2024-02-17 12:59:57,693 - Epoch: [178][  500/  500]    Overall Loss 0.232737    Objective Loss 0.232737    Top1 91.000000    Top5 100.000000    LR 0.000125    Time 0.073580    
2024-02-17 12:59:57,838 - --- validate (epoch=178)-----------
2024-02-17 12:59:57,839 - 10000 samples (100 per mini-batch)
2024-02-17 13:00:00,754 - Epoch: [178][  100/  100]    Loss 1.957101    Top1 60.500000    Top5 85.740000    
2024-02-17 13:00:00,888 - ==> Top1: 60.500    Top5: 85.740    Loss: 1.957

2024-02-17 13:00:00,896 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:00:00,896 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:00:00,951 - 

2024-02-17 13:00:00,951 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:00:08,605 - Epoch: [179][  100/  500]    Overall Loss 0.225508    Objective Loss 0.225508                                        LR 0.000125    Time 0.076480    
2024-02-17 13:00:15,642 - Epoch: [179][  200/  500]    Overall Loss 0.226666    Objective Loss 0.226666                                        LR 0.000125    Time 0.073404    
2024-02-17 13:00:22,659 - Epoch: [179][  300/  500]    Overall Loss 0.228379    Objective Loss 0.228379                                        LR 0.000125    Time 0.072314    
2024-02-17 13:00:29,798 - Epoch: [179][  400/  500]    Overall Loss 0.229011    Objective Loss 0.229011                                        LR 0.000125    Time 0.072071    
2024-02-17 13:00:36,987 - Epoch: [179][  500/  500]    Overall Loss 0.229008    Objective Loss 0.229008    Top1 94.000000    Top5 100.000000    LR 0.000125    Time 0.072027    
2024-02-17 13:00:37,098 - --- validate (epoch=179)-----------
2024-02-17 13:00:37,099 - 10000 samples (100 per mini-batch)
2024-02-17 13:00:39,974 - Epoch: [179][  100/  100]    Loss 1.946505    Top1 60.290000    Top5 85.820000    
2024-02-17 13:00:40,137 - ==> Top1: 60.290    Top5: 85.820    Loss: 1.947

2024-02-17 13:00:40,149 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:00:40,149 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:00:40,211 - 

2024-02-17 13:00:40,211 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:00:48,252 - Epoch: [180][  100/  500]    Overall Loss 0.226473    Objective Loss 0.226473                                        LR 0.000125    Time 0.080353    
2024-02-17 13:00:55,343 - Epoch: [180][  200/  500]    Overall Loss 0.226837    Objective Loss 0.226837                                        LR 0.000125    Time 0.075611    
2024-02-17 13:01:03,349 - Epoch: [180][  300/  500]    Overall Loss 0.230913    Objective Loss 0.230913                                        LR 0.000125    Time 0.077078    
2024-02-17 13:01:10,530 - Epoch: [180][  400/  500]    Overall Loss 0.231669    Objective Loss 0.231669                                        LR 0.000125    Time 0.075751    
2024-02-17 13:01:17,709 - Epoch: [180][  500/  500]    Overall Loss 0.230858    Objective Loss 0.230858    Top1 91.000000    Top5 100.000000    LR 0.000125    Time 0.074949    
2024-02-17 13:01:17,891 - --- validate (epoch=180)-----------
2024-02-17 13:01:17,891 - 10000 samples (100 per mini-batch)
2024-02-17 13:01:20,898 - Epoch: [180][  100/  100]    Loss 1.952863    Top1 60.010000    Top5 85.820000    
2024-02-17 13:01:21,015 - ==> Top1: 60.010    Top5: 85.820    Loss: 1.953

2024-02-17 13:01:21,025 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:01:21,026 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:01:21,081 - 

2024-02-17 13:01:21,082 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:01:28,758 - Epoch: [181][  100/  500]    Overall Loss 0.221967    Objective Loss 0.221967                                        LR 0.000125    Time 0.076703    
2024-02-17 13:01:35,836 - Epoch: [181][  200/  500]    Overall Loss 0.225882    Objective Loss 0.225882                                        LR 0.000125    Time 0.073721    
2024-02-17 13:01:43,054 - Epoch: [181][  300/  500]    Overall Loss 0.229935    Objective Loss 0.229935                                        LR 0.000125    Time 0.073191    
2024-02-17 13:01:50,187 - Epoch: [181][  400/  500]    Overall Loss 0.229989    Objective Loss 0.229989                                        LR 0.000125    Time 0.072717    
2024-02-17 13:01:57,656 - Epoch: [181][  500/  500]    Overall Loss 0.229479    Objective Loss 0.229479    Top1 94.500000    Top5 100.000000    LR 0.000125    Time 0.073104    
2024-02-17 13:01:57,774 - --- validate (epoch=181)-----------
2024-02-17 13:01:57,775 - 10000 samples (100 per mini-batch)
2024-02-17 13:02:00,807 - Epoch: [181][  100/  100]    Loss 1.967669    Top1 59.920000    Top5 85.520000    
2024-02-17 13:02:00,966 - ==> Top1: 59.920    Top5: 85.520    Loss: 1.968

2024-02-17 13:02:00,977 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:02:00,978 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:02:01,039 - 

2024-02-17 13:02:01,040 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:02:08,669 - Epoch: [182][  100/  500]    Overall Loss 0.228324    Objective Loss 0.228324                                        LR 0.000125    Time 0.076240    
2024-02-17 13:02:15,719 - Epoch: [182][  200/  500]    Overall Loss 0.222645    Objective Loss 0.222645                                        LR 0.000125    Time 0.073349    
2024-02-17 13:02:22,738 - Epoch: [182][  300/  500]    Overall Loss 0.223732    Objective Loss 0.223732                                        LR 0.000125    Time 0.072282    
2024-02-17 13:02:29,915 - Epoch: [182][  400/  500]    Overall Loss 0.227632    Objective Loss 0.227632                                        LR 0.000125    Time 0.072142    
2024-02-17 13:02:36,866 - Epoch: [182][  500/  500]    Overall Loss 0.229066    Objective Loss 0.229066    Top1 92.500000    Top5 99.500000    LR 0.000125    Time 0.071608    
2024-02-17 13:02:37,031 - --- validate (epoch=182)-----------
2024-02-17 13:02:37,031 - 10000 samples (100 per mini-batch)
2024-02-17 13:02:40,164 - Epoch: [182][  100/  100]    Loss 1.974157    Top1 59.970000    Top5 85.750000    
2024-02-17 13:02:40,294 - ==> Top1: 59.970    Top5: 85.750    Loss: 1.974

2024-02-17 13:02:40,307 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:02:40,307 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:02:40,368 - 

2024-02-17 13:02:40,368 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:02:48,006 - Epoch: [183][  100/  500]    Overall Loss 0.215227    Objective Loss 0.215227                                        LR 0.000125    Time 0.076314    
2024-02-17 13:02:55,289 - Epoch: [183][  200/  500]    Overall Loss 0.222679    Objective Loss 0.222679                                        LR 0.000125    Time 0.074554    
2024-02-17 13:03:02,416 - Epoch: [183][  300/  500]    Overall Loss 0.226788    Objective Loss 0.226788                                        LR 0.000125    Time 0.073444    
2024-02-17 13:03:09,544 - Epoch: [183][  400/  500]    Overall Loss 0.227187    Objective Loss 0.227187                                        LR 0.000125    Time 0.072892    
2024-02-17 13:03:16,675 - Epoch: [183][  500/  500]    Overall Loss 0.227088    Objective Loss 0.227088    Top1 94.000000    Top5 99.000000    LR 0.000125    Time 0.072568    
2024-02-17 13:03:16,803 - --- validate (epoch=183)-----------
2024-02-17 13:03:16,804 - 10000 samples (100 per mini-batch)
2024-02-17 13:03:19,664 - Epoch: [183][  100/  100]    Loss 1.957040    Top1 59.830000    Top5 85.790000    
2024-02-17 13:03:19,770 - ==> Top1: 59.830    Top5: 85.790    Loss: 1.957

2024-02-17 13:03:19,782 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:03:19,782 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:03:19,842 - 

2024-02-17 13:03:19,842 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:03:27,812 - Epoch: [184][  100/  500]    Overall Loss 0.219644    Objective Loss 0.219644                                        LR 0.000125    Time 0.079641    
2024-02-17 13:03:34,916 - Epoch: [184][  200/  500]    Overall Loss 0.224112    Objective Loss 0.224112                                        LR 0.000125    Time 0.075320    
2024-02-17 13:03:42,083 - Epoch: [184][  300/  500]    Overall Loss 0.224402    Objective Loss 0.224402                                        LR 0.000125    Time 0.074088    
2024-02-17 13:03:49,289 - Epoch: [184][  400/  500]    Overall Loss 0.227515    Objective Loss 0.227515                                        LR 0.000125    Time 0.073570    
2024-02-17 13:03:56,626 - Epoch: [184][  500/  500]    Overall Loss 0.229596    Objective Loss 0.229596    Top1 93.500000    Top5 99.500000    LR 0.000125    Time 0.073521    
2024-02-17 13:03:56,744 - --- validate (epoch=184)-----------
2024-02-17 13:03:56,745 - 10000 samples (100 per mini-batch)
2024-02-17 13:03:59,757 - Epoch: [184][  100/  100]    Loss 1.966017    Top1 60.000000    Top5 85.630000    
2024-02-17 13:03:59,855 - ==> Top1: 60.000    Top5: 85.630    Loss: 1.966

2024-02-17 13:03:59,865 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:03:59,866 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:03:59,930 - 

2024-02-17 13:03:59,931 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:04:07,624 - Epoch: [185][  100/  500]    Overall Loss 0.227796    Objective Loss 0.227796                                        LR 0.000125    Time 0.076873    
2024-02-17 13:04:14,788 - Epoch: [185][  200/  500]    Overall Loss 0.223938    Objective Loss 0.223938                                        LR 0.000125    Time 0.074233    
2024-02-17 13:04:21,911 - Epoch: [185][  300/  500]    Overall Loss 0.226002    Objective Loss 0.226002                                        LR 0.000125    Time 0.073219    
2024-02-17 13:04:29,059 - Epoch: [185][  400/  500]    Overall Loss 0.225594    Objective Loss 0.225594                                        LR 0.000125    Time 0.072773    
2024-02-17 13:04:36,238 - Epoch: [185][  500/  500]    Overall Loss 0.225943    Objective Loss 0.225943    Top1 93.500000    Top5 100.000000    LR 0.000125    Time 0.072567    
2024-02-17 13:04:36,363 - --- validate (epoch=185)-----------
2024-02-17 13:04:36,363 - 10000 samples (100 per mini-batch)
2024-02-17 13:04:39,309 - Epoch: [185][  100/  100]    Loss 1.967303    Top1 59.800000    Top5 85.760000    
2024-02-17 13:04:39,462 - ==> Top1: 59.800    Top5: 85.760    Loss: 1.967

2024-02-17 13:04:39,473 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:04:39,473 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:04:39,534 - 

2024-02-17 13:04:39,534 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:04:47,415 - Epoch: [186][  100/  500]    Overall Loss 0.210687    Objective Loss 0.210687                                        LR 0.000125    Time 0.078752    
2024-02-17 13:04:54,528 - Epoch: [186][  200/  500]    Overall Loss 0.218706    Objective Loss 0.218706                                        LR 0.000125    Time 0.074916    
2024-02-17 13:05:01,652 - Epoch: [186][  300/  500]    Overall Loss 0.220617    Objective Loss 0.220617                                        LR 0.000125    Time 0.073677    
2024-02-17 13:05:08,836 - Epoch: [186][  400/  500]    Overall Loss 0.221219    Objective Loss 0.221219                                        LR 0.000125    Time 0.073208    
2024-02-17 13:05:16,100 - Epoch: [186][  500/  500]    Overall Loss 0.224767    Objective Loss 0.224767    Top1 93.000000    Top5 99.500000    LR 0.000125    Time 0.073085    
2024-02-17 13:05:16,219 - --- validate (epoch=186)-----------
2024-02-17 13:05:16,220 - 10000 samples (100 per mini-batch)
2024-02-17 13:05:19,200 - Epoch: [186][  100/  100]    Loss 1.972442    Top1 59.790000    Top5 85.870000    
2024-02-17 13:05:19,307 - ==> Top1: 59.790    Top5: 85.870    Loss: 1.972

2024-02-17 13:05:19,317 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:05:19,317 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:05:19,383 - 

2024-02-17 13:05:19,384 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:05:26,911 - Epoch: [187][  100/  500]    Overall Loss 0.222190    Objective Loss 0.222190                                        LR 0.000125    Time 0.075216    
2024-02-17 13:05:33,952 - Epoch: [187][  200/  500]    Overall Loss 0.221114    Objective Loss 0.221114                                        LR 0.000125    Time 0.072791    
2024-02-17 13:05:40,997 - Epoch: [187][  300/  500]    Overall Loss 0.221460    Objective Loss 0.221460                                        LR 0.000125    Time 0.071997    
2024-02-17 13:05:47,978 - Epoch: [187][  400/  500]    Overall Loss 0.224399    Objective Loss 0.224399                                        LR 0.000125    Time 0.071441    
2024-02-17 13:05:54,875 - Epoch: [187][  500/  500]    Overall Loss 0.226769    Objective Loss 0.226769    Top1 93.000000    Top5 100.000000    LR 0.000125    Time 0.070941    
2024-02-17 13:05:55,076 - --- validate (epoch=187)-----------
2024-02-17 13:05:55,077 - 10000 samples (100 per mini-batch)
2024-02-17 13:05:58,160 - Epoch: [187][  100/  100]    Loss 1.984545    Top1 59.840000    Top5 85.680000    
2024-02-17 13:05:58,266 - ==> Top1: 59.840    Top5: 85.680    Loss: 1.985

2024-02-17 13:05:58,275 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:05:58,275 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:05:58,354 - 

2024-02-17 13:05:58,354 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:06:06,044 - Epoch: [188][  100/  500]    Overall Loss 0.213649    Objective Loss 0.213649                                        LR 0.000125    Time 0.076843    
2024-02-17 13:06:13,136 - Epoch: [188][  200/  500]    Overall Loss 0.218540    Objective Loss 0.218540                                        LR 0.000125    Time 0.073859    
2024-02-17 13:06:20,165 - Epoch: [188][  300/  500]    Overall Loss 0.218849    Objective Loss 0.218849                                        LR 0.000125    Time 0.072654    
2024-02-17 13:06:27,387 - Epoch: [188][  400/  500]    Overall Loss 0.223136    Objective Loss 0.223136                                        LR 0.000125    Time 0.072535    
2024-02-17 13:06:34,597 - Epoch: [188][  500/  500]    Overall Loss 0.225520    Objective Loss 0.225520    Top1 90.000000    Top5 100.000000    LR 0.000125    Time 0.072437    
2024-02-17 13:06:34,751 - --- validate (epoch=188)-----------
2024-02-17 13:06:34,752 - 10000 samples (100 per mini-batch)
2024-02-17 13:06:38,006 - Epoch: [188][  100/  100]    Loss 1.987643    Top1 59.360000    Top5 85.380000    
2024-02-17 13:06:38,100 - ==> Top1: 59.360    Top5: 85.380    Loss: 1.988

2024-02-17 13:06:38,111 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:06:38,111 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:06:38,187 - 

2024-02-17 13:06:38,188 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:06:45,843 - Epoch: [189][  100/  500]    Overall Loss 0.225949    Objective Loss 0.225949                                        LR 0.000125    Time 0.076497    
2024-02-17 13:06:52,958 - Epoch: [189][  200/  500]    Overall Loss 0.222795    Objective Loss 0.222795                                        LR 0.000125    Time 0.073804    
2024-02-17 13:07:00,130 - Epoch: [189][  300/  500]    Overall Loss 0.224683    Objective Loss 0.224683                                        LR 0.000125    Time 0.073093    
2024-02-17 13:07:07,278 - Epoch: [189][  400/  500]    Overall Loss 0.223539    Objective Loss 0.223539                                        LR 0.000125    Time 0.072679    
2024-02-17 13:07:14,369 - Epoch: [189][  500/  500]    Overall Loss 0.226418    Objective Loss 0.226418    Top1 93.000000    Top5 98.500000    LR 0.000125    Time 0.072317    
2024-02-17 13:07:14,522 - --- validate (epoch=189)-----------
2024-02-17 13:07:14,523 - 10000 samples (100 per mini-batch)
2024-02-17 13:07:17,540 - Epoch: [189][  100/  100]    Loss 1.986064    Top1 60.010000    Top5 85.360000    
2024-02-17 13:07:17,645 - ==> Top1: 60.010    Top5: 85.360    Loss: 1.986

2024-02-17 13:07:17,655 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:07:17,655 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:07:17,722 - 

2024-02-17 13:07:17,723 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:07:25,401 - Epoch: [190][  100/  500]    Overall Loss 0.211893    Objective Loss 0.211893                                        LR 0.000125    Time 0.076725    
2024-02-17 13:07:32,585 - Epoch: [190][  200/  500]    Overall Loss 0.216724    Objective Loss 0.216724                                        LR 0.000125    Time 0.074260    
2024-02-17 13:07:39,861 - Epoch: [190][  300/  500]    Overall Loss 0.217644    Objective Loss 0.217644                                        LR 0.000125    Time 0.073747    
2024-02-17 13:07:47,028 - Epoch: [190][  400/  500]    Overall Loss 0.220819    Objective Loss 0.220819                                        LR 0.000125    Time 0.073216    
2024-02-17 13:07:54,195 - Epoch: [190][  500/  500]    Overall Loss 0.222200    Objective Loss 0.222200    Top1 94.500000    Top5 99.500000    LR 0.000125    Time 0.072899    
2024-02-17 13:07:54,301 - --- validate (epoch=190)-----------
2024-02-17 13:07:54,302 - 10000 samples (100 per mini-batch)
2024-02-17 13:07:57,943 - Epoch: [190][  100/  100]    Loss 1.970894    Top1 59.700000    Top5 85.680000    
2024-02-17 13:07:58,063 - ==> Top1: 59.700    Top5: 85.680    Loss: 1.971

2024-02-17 13:07:58,070 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:07:58,070 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:07:58,126 - 

2024-02-17 13:07:58,126 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:08:05,714 - Epoch: [191][  100/  500]    Overall Loss 0.219249    Objective Loss 0.219249                                        LR 0.000125    Time 0.075817    
2024-02-17 13:08:12,820 - Epoch: [191][  200/  500]    Overall Loss 0.219000    Objective Loss 0.219000                                        LR 0.000125    Time 0.073419    
2024-02-17 13:08:19,904 - Epoch: [191][  300/  500]    Overall Loss 0.219732    Objective Loss 0.219732                                        LR 0.000125    Time 0.072546    
2024-02-17 13:08:27,011 - Epoch: [191][  400/  500]    Overall Loss 0.221630    Objective Loss 0.221630                                        LR 0.000125    Time 0.072164    
2024-02-17 13:08:34,148 - Epoch: [191][  500/  500]    Overall Loss 0.222612    Objective Loss 0.222612    Top1 95.000000    Top5 100.000000    LR 0.000125    Time 0.071998    
2024-02-17 13:08:34,324 - --- validate (epoch=191)-----------
2024-02-17 13:08:34,326 - 10000 samples (100 per mini-batch)
2024-02-17 13:08:37,537 - Epoch: [191][  100/  100]    Loss 1.991651    Top1 59.920000    Top5 85.580000    
2024-02-17 13:08:37,644 - ==> Top1: 59.920    Top5: 85.580    Loss: 1.992

2024-02-17 13:08:37,651 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:08:37,652 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:08:37,720 - 

2024-02-17 13:08:37,720 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:08:45,374 - Epoch: [192][  100/  500]    Overall Loss 0.218386    Objective Loss 0.218386                                        LR 0.000125    Time 0.076488    
2024-02-17 13:08:52,798 - Epoch: [192][  200/  500]    Overall Loss 0.217557    Objective Loss 0.217557                                        LR 0.000125    Time 0.075338    
2024-02-17 13:08:59,762 - Epoch: [192][  300/  500]    Overall Loss 0.215115    Objective Loss 0.215115                                        LR 0.000125    Time 0.073430    
2024-02-17 13:09:06,908 - Epoch: [192][  400/  500]    Overall Loss 0.217083    Objective Loss 0.217083                                        LR 0.000125    Time 0.072926    
2024-02-17 13:09:14,056 - Epoch: [192][  500/  500]    Overall Loss 0.219166    Objective Loss 0.219166    Top1 97.000000    Top5 100.000000    LR 0.000125    Time 0.072628    
2024-02-17 13:09:14,192 - --- validate (epoch=192)-----------
2024-02-17 13:09:14,193 - 10000 samples (100 per mini-batch)
2024-02-17 13:09:17,943 - Epoch: [192][  100/  100]    Loss 1.972215    Top1 59.890000    Top5 85.600000    
2024-02-17 13:09:18,051 - ==> Top1: 59.890    Top5: 85.600    Loss: 1.972

2024-02-17 13:09:18,060 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:09:18,060 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:09:18,121 - 

2024-02-17 13:09:18,122 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:09:25,929 - Epoch: [193][  100/  500]    Overall Loss 0.213690    Objective Loss 0.213690                                        LR 0.000125    Time 0.078013    
2024-02-17 13:09:33,134 - Epoch: [193][  200/  500]    Overall Loss 0.216133    Objective Loss 0.216133                                        LR 0.000125    Time 0.075011    
2024-02-17 13:09:40,363 - Epoch: [193][  300/  500]    Overall Loss 0.217282    Objective Loss 0.217282                                        LR 0.000125    Time 0.074089    
2024-02-17 13:09:47,557 - Epoch: [193][  400/  500]    Overall Loss 0.218157    Objective Loss 0.218157                                        LR 0.000125    Time 0.073543    
2024-02-17 13:09:54,716 - Epoch: [193][  500/  500]    Overall Loss 0.217944    Objective Loss 0.217944    Top1 95.000000    Top5 100.000000    LR 0.000125    Time 0.073143    
2024-02-17 13:09:54,848 - --- validate (epoch=193)-----------
2024-02-17 13:09:54,849 - 10000 samples (100 per mini-batch)
2024-02-17 13:09:57,859 - Epoch: [193][  100/  100]    Loss 1.982611    Top1 59.880000    Top5 85.910000    
2024-02-17 13:09:57,973 - ==> Top1: 59.880    Top5: 85.910    Loss: 1.983

2024-02-17 13:09:57,984 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:09:57,985 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:09:58,051 - 

2024-02-17 13:09:58,051 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:10:05,948 - Epoch: [194][  100/  500]    Overall Loss 0.203038    Objective Loss 0.203038                                        LR 0.000125    Time 0.078914    
2024-02-17 13:10:13,114 - Epoch: [194][  200/  500]    Overall Loss 0.209588    Objective Loss 0.209588                                        LR 0.000125    Time 0.075264    
2024-02-17 13:10:20,027 - Epoch: [194][  300/  500]    Overall Loss 0.212824    Objective Loss 0.212824                                        LR 0.000125    Time 0.073207    
2024-02-17 13:10:27,134 - Epoch: [194][  400/  500]    Overall Loss 0.214587    Objective Loss 0.214587                                        LR 0.000125    Time 0.072664    
2024-02-17 13:10:34,260 - Epoch: [194][  500/  500]    Overall Loss 0.216752    Objective Loss 0.216752    Top1 92.000000    Top5 99.500000    LR 0.000125    Time 0.072375    
2024-02-17 13:10:34,382 - --- validate (epoch=194)-----------
2024-02-17 13:10:34,382 - 10000 samples (100 per mini-batch)
2024-02-17 13:10:37,674 - Epoch: [194][  100/  100]    Loss 2.009548    Top1 59.420000    Top5 85.450000    
2024-02-17 13:10:37,854 - ==> Top1: 59.420    Top5: 85.450    Loss: 2.010

2024-02-17 13:10:37,859 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:10:37,860 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:10:37,917 - 

2024-02-17 13:10:37,918 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:10:45,571 - Epoch: [195][  100/  500]    Overall Loss 0.218217    Objective Loss 0.218217                                        LR 0.000125    Time 0.076475    
2024-02-17 13:10:52,604 - Epoch: [195][  200/  500]    Overall Loss 0.218245    Objective Loss 0.218245                                        LR 0.000125    Time 0.073382    
2024-02-17 13:10:59,638 - Epoch: [195][  300/  500]    Overall Loss 0.219620    Objective Loss 0.219620                                        LR 0.000125    Time 0.072355    
2024-02-17 13:11:06,754 - Epoch: [195][  400/  500]    Overall Loss 0.220032    Objective Loss 0.220032                                        LR 0.000125    Time 0.072047    
2024-02-17 13:11:13,824 - Epoch: [195][  500/  500]    Overall Loss 0.220848    Objective Loss 0.220848    Top1 93.000000    Top5 100.000000    LR 0.000125    Time 0.071769    
2024-02-17 13:11:13,962 - --- validate (epoch=195)-----------
2024-02-17 13:11:13,963 - 10000 samples (100 per mini-batch)
2024-02-17 13:11:17,273 - Epoch: [195][  100/  100]    Loss 1.998598    Top1 59.870000    Top5 85.750000    
2024-02-17 13:11:17,435 - ==> Top1: 59.870    Top5: 85.750    Loss: 1.999

2024-02-17 13:11:17,441 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:11:17,441 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:11:17,523 - 

2024-02-17 13:11:17,524 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:11:25,536 - Epoch: [196][  100/  500]    Overall Loss 0.215504    Objective Loss 0.215504                                        LR 0.000125    Time 0.080055    
2024-02-17 13:11:32,632 - Epoch: [196][  200/  500]    Overall Loss 0.215083    Objective Loss 0.215083                                        LR 0.000125    Time 0.075486    
2024-02-17 13:11:39,901 - Epoch: [196][  300/  500]    Overall Loss 0.214971    Objective Loss 0.214971                                        LR 0.000125    Time 0.074539    
2024-02-17 13:11:47,105 - Epoch: [196][  400/  500]    Overall Loss 0.215812    Objective Loss 0.215812                                        LR 0.000125    Time 0.073902    
2024-02-17 13:11:54,261 - Epoch: [196][  500/  500]    Overall Loss 0.216520    Objective Loss 0.216520    Top1 95.500000    Top5 100.000000    LR 0.000125    Time 0.073425    
2024-02-17 13:11:54,365 - --- validate (epoch=196)-----------
2024-02-17 13:11:54,365 - 10000 samples (100 per mini-batch)
2024-02-17 13:11:57,426 - Epoch: [196][  100/  100]    Loss 2.017509    Top1 59.410000    Top5 85.360000    
2024-02-17 13:11:57,525 - ==> Top1: 59.410    Top5: 85.360    Loss: 2.018

2024-02-17 13:11:57,540 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:11:57,540 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:11:57,605 - 

2024-02-17 13:11:57,605 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:12:05,282 - Epoch: [197][  100/  500]    Overall Loss 0.199093    Objective Loss 0.199093                                        LR 0.000125    Time 0.076704    
2024-02-17 13:12:12,224 - Epoch: [197][  200/  500]    Overall Loss 0.210532    Objective Loss 0.210532                                        LR 0.000125    Time 0.073044    
2024-02-17 13:12:19,384 - Epoch: [197][  300/  500]    Overall Loss 0.213149    Objective Loss 0.213149                                        LR 0.000125    Time 0.072546    
2024-02-17 13:12:26,464 - Epoch: [197][  400/  500]    Overall Loss 0.213956    Objective Loss 0.213956                                        LR 0.000125    Time 0.072098    
2024-02-17 13:12:33,598 - Epoch: [197][  500/  500]    Overall Loss 0.215248    Objective Loss 0.215248    Top1 93.000000    Top5 100.000000    LR 0.000125    Time 0.071937    
2024-02-17 13:12:33,720 - --- validate (epoch=197)-----------
2024-02-17 13:12:33,720 - 10000 samples (100 per mini-batch)
2024-02-17 13:12:37,063 - Epoch: [197][  100/  100]    Loss 1.980144    Top1 60.340000    Top5 85.590000    
2024-02-17 13:12:37,204 - ==> Top1: 60.340    Top5: 85.590    Loss: 1.980

2024-02-17 13:12:37,214 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:12:37,214 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:12:37,281 - 

2024-02-17 13:12:37,281 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:12:45,390 - Epoch: [198][  100/  500]    Overall Loss 0.212729    Objective Loss 0.212729                                        LR 0.000125    Time 0.081027    
2024-02-17 13:12:52,739 - Epoch: [198][  200/  500]    Overall Loss 0.210995    Objective Loss 0.210995                                        LR 0.000125    Time 0.077236    
2024-02-17 13:12:59,911 - Epoch: [198][  300/  500]    Overall Loss 0.211916    Objective Loss 0.211916                                        LR 0.000125    Time 0.075379    
2024-02-17 13:13:06,907 - Epoch: [198][  400/  500]    Overall Loss 0.213681    Objective Loss 0.213681                                        LR 0.000125    Time 0.074014    
2024-02-17 13:13:14,091 - Epoch: [198][  500/  500]    Overall Loss 0.216552    Objective Loss 0.216552    Top1 91.500000    Top5 100.000000    LR 0.000125    Time 0.073570    
2024-02-17 13:13:14,205 - --- validate (epoch=198)-----------
2024-02-17 13:13:14,207 - 10000 samples (100 per mini-batch)
2024-02-17 13:13:17,248 - Epoch: [198][  100/  100]    Loss 2.016967    Top1 59.390000    Top5 85.560000    
2024-02-17 13:13:17,362 - ==> Top1: 59.390    Top5: 85.560    Loss: 2.017

2024-02-17 13:13:17,376 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:13:17,376 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:13:17,438 - 

2024-02-17 13:13:17,438 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:13:25,082 - Epoch: [199][  100/  500]    Overall Loss 0.208800    Objective Loss 0.208800                                        LR 0.000125    Time 0.076385    
2024-02-17 13:13:32,158 - Epoch: [199][  200/  500]    Overall Loss 0.209824    Objective Loss 0.209824                                        LR 0.000125    Time 0.073552    
2024-02-17 13:13:39,274 - Epoch: [199][  300/  500]    Overall Loss 0.211934    Objective Loss 0.211934                                        LR 0.000125    Time 0.072739    
2024-02-17 13:13:46,312 - Epoch: [199][  400/  500]    Overall Loss 0.214977    Objective Loss 0.214977                                        LR 0.000125    Time 0.072140    
2024-02-17 13:13:53,547 - Epoch: [199][  500/  500]    Overall Loss 0.216143    Objective Loss 0.216143    Top1 93.500000    Top5 100.000000    LR 0.000125    Time 0.072173    
2024-02-17 13:13:53,667 - --- validate (epoch=199)-----------
2024-02-17 13:13:53,669 - 10000 samples (100 per mini-batch)
2024-02-17 13:13:56,569 - Epoch: [199][  100/  100]    Loss 1.993669    Top1 59.600000    Top5 85.480000    
2024-02-17 13:13:56,662 - ==> Top1: 59.600    Top5: 85.480    Loss: 1.994

2024-02-17 13:13:56,675 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:13:56,676 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:13:56,738 - 

2024-02-17 13:13:56,738 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:14:04,324 - Epoch: [200][  100/  500]    Overall Loss 0.203186    Objective Loss 0.203186                                        LR 0.000063    Time 0.075797    
2024-02-17 13:14:11,422 - Epoch: [200][  200/  500]    Overall Loss 0.200658    Objective Loss 0.200658                                        LR 0.000063    Time 0.073368    
2024-02-17 13:14:18,436 - Epoch: [200][  300/  500]    Overall Loss 0.199368    Objective Loss 0.199368                                        LR 0.000063    Time 0.072280    
2024-02-17 13:14:25,498 - Epoch: [200][  400/  500]    Overall Loss 0.198770    Objective Loss 0.198770                                        LR 0.000063    Time 0.071855    
2024-02-17 13:14:32,684 - Epoch: [200][  500/  500]    Overall Loss 0.200164    Objective Loss 0.200164    Top1 94.500000    Top5 100.000000    LR 0.000063    Time 0.071848    
2024-02-17 13:14:32,831 - --- validate (epoch=200)-----------
2024-02-17 13:14:32,832 - 10000 samples (100 per mini-batch)
2024-02-17 13:14:36,127 - Epoch: [200][  100/  100]    Loss 1.965052    Top1 59.960000    Top5 85.850000    
2024-02-17 13:14:36,224 - ==> Top1: 59.960    Top5: 85.850    Loss: 1.965

2024-02-17 13:14:36,231 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:14:36,231 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:14:36,284 - 

2024-02-17 13:14:36,284 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:14:43,935 - Epoch: [201][  100/  500]    Overall Loss 0.194777    Objective Loss 0.194777                                        LR 0.000063    Time 0.076445    
2024-02-17 13:14:50,995 - Epoch: [201][  200/  500]    Overall Loss 0.195865    Objective Loss 0.195865                                        LR 0.000063    Time 0.073502    
2024-02-17 13:14:58,054 - Epoch: [201][  300/  500]    Overall Loss 0.197402    Objective Loss 0.197402                                        LR 0.000063    Time 0.072517    
2024-02-17 13:15:05,110 - Epoch: [201][  400/  500]    Overall Loss 0.197737    Objective Loss 0.197737                                        LR 0.000063    Time 0.072019    
2024-02-17 13:15:12,215 - Epoch: [201][  500/  500]    Overall Loss 0.198817    Objective Loss 0.198817    Top1 92.000000    Top5 99.500000    LR 0.000063    Time 0.071817    
2024-02-17 13:15:12,318 - --- validate (epoch=201)-----------
2024-02-17 13:15:12,319 - 10000 samples (100 per mini-batch)
2024-02-17 13:15:15,330 - Epoch: [201][  100/  100]    Loss 1.979687    Top1 60.100000    Top5 85.590000    
2024-02-17 13:15:15,430 - ==> Top1: 60.100    Top5: 85.590    Loss: 1.980

2024-02-17 13:15:15,443 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:15:15,444 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:15:15,502 - 

2024-02-17 13:15:15,503 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:15:23,358 - Epoch: [202][  100/  500]    Overall Loss 0.196987    Objective Loss 0.196987                                        LR 0.000063    Time 0.078478    
2024-02-17 13:15:30,397 - Epoch: [202][  200/  500]    Overall Loss 0.198782    Objective Loss 0.198782                                        LR 0.000063    Time 0.074413    
2024-02-17 13:15:37,489 - Epoch: [202][  300/  500]    Overall Loss 0.199579    Objective Loss 0.199579                                        LR 0.000063    Time 0.073235    
2024-02-17 13:15:44,535 - Epoch: [202][  400/  500]    Overall Loss 0.202158    Objective Loss 0.202158                                        LR 0.000063    Time 0.072531    
2024-02-17 13:15:51,598 - Epoch: [202][  500/  500]    Overall Loss 0.202946    Objective Loss 0.202946    Top1 94.000000    Top5 100.000000    LR 0.000063    Time 0.072142    
2024-02-17 13:15:51,703 - --- validate (epoch=202)-----------
2024-02-17 13:15:51,703 - 10000 samples (100 per mini-batch)
2024-02-17 13:15:55,117 - Epoch: [202][  100/  100]    Loss 1.980951    Top1 60.160000    Top5 85.550000    
2024-02-17 13:15:55,215 - ==> Top1: 60.160    Top5: 85.550    Loss: 1.981

2024-02-17 13:15:55,226 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:15:55,226 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:15:55,288 - 

2024-02-17 13:15:55,288 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:16:02,990 - Epoch: [203][  100/  500]    Overall Loss 0.198696    Objective Loss 0.198696                                        LR 0.000063    Time 0.076962    
2024-02-17 13:16:09,945 - Epoch: [203][  200/  500]    Overall Loss 0.202378    Objective Loss 0.202378                                        LR 0.000063    Time 0.073236    
2024-02-17 13:16:17,011 - Epoch: [203][  300/  500]    Overall Loss 0.199438    Objective Loss 0.199438                                        LR 0.000063    Time 0.072363    
2024-02-17 13:16:24,111 - Epoch: [203][  400/  500]    Overall Loss 0.200565    Objective Loss 0.200565                                        LR 0.000063    Time 0.072012    
2024-02-17 13:16:31,363 - Epoch: [203][  500/  500]    Overall Loss 0.198712    Objective Loss 0.198712    Top1 93.500000    Top5 100.000000    LR 0.000063    Time 0.072105    
2024-02-17 13:16:31,527 - --- validate (epoch=203)-----------
2024-02-17 13:16:31,528 - 10000 samples (100 per mini-batch)
2024-02-17 13:16:34,508 - Epoch: [203][  100/  100]    Loss 1.983417    Top1 60.080000    Top5 85.490000    
2024-02-17 13:16:34,611 - ==> Top1: 60.080    Top5: 85.490    Loss: 1.983

2024-02-17 13:16:34,622 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:16:34,622 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:16:34,685 - 

2024-02-17 13:16:34,685 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:16:42,478 - Epoch: [204][  100/  500]    Overall Loss 0.194937    Objective Loss 0.194937                                        LR 0.000063    Time 0.077878    
2024-02-17 13:16:49,524 - Epoch: [204][  200/  500]    Overall Loss 0.194596    Objective Loss 0.194596                                        LR 0.000063    Time 0.074147    
2024-02-17 13:16:56,368 - Epoch: [204][  300/  500]    Overall Loss 0.192799    Objective Loss 0.192799                                        LR 0.000063    Time 0.072235    
2024-02-17 13:17:03,438 - Epoch: [204][  400/  500]    Overall Loss 0.195310    Objective Loss 0.195310                                        LR 0.000063    Time 0.071841    
2024-02-17 13:17:10,494 - Epoch: [204][  500/  500]    Overall Loss 0.196354    Objective Loss 0.196354    Top1 93.500000    Top5 99.500000    LR 0.000063    Time 0.071576    
2024-02-17 13:17:10,651 - --- validate (epoch=204)-----------
2024-02-17 13:17:10,652 - 10000 samples (100 per mini-batch)
2024-02-17 13:17:13,655 - Epoch: [204][  100/  100]    Loss 1.986956    Top1 60.240000    Top5 85.410000    
2024-02-17 13:17:13,771 - ==> Top1: 60.240    Top5: 85.410    Loss: 1.987

2024-02-17 13:17:13,785 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:17:13,786 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:17:13,849 - 

2024-02-17 13:17:13,849 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:17:21,495 - Epoch: [205][  100/  500]    Overall Loss 0.195522    Objective Loss 0.195522                                        LR 0.000063    Time 0.076406    
2024-02-17 13:17:28,736 - Epoch: [205][  200/  500]    Overall Loss 0.197703    Objective Loss 0.197703                                        LR 0.000063    Time 0.074384    
2024-02-17 13:17:35,974 - Epoch: [205][  300/  500]    Overall Loss 0.197991    Objective Loss 0.197991                                        LR 0.000063    Time 0.073703    
2024-02-17 13:17:43,054 - Epoch: [205][  400/  500]    Overall Loss 0.197787    Objective Loss 0.197787                                        LR 0.000063    Time 0.072965    
2024-02-17 13:17:50,071 - Epoch: [205][  500/  500]    Overall Loss 0.198011    Objective Loss 0.198011    Top1 96.000000    Top5 100.000000    LR 0.000063    Time 0.072399    
2024-02-17 13:17:50,206 - --- validate (epoch=205)-----------
2024-02-17 13:17:50,207 - 10000 samples (100 per mini-batch)
2024-02-17 13:17:53,140 - Epoch: [205][  100/  100]    Loss 1.993326    Top1 59.860000    Top5 85.530000    
2024-02-17 13:17:53,240 - ==> Top1: 59.860    Top5: 85.530    Loss: 1.993

2024-02-17 13:17:53,250 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:17:53,250 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:17:53,316 - 

2024-02-17 13:17:53,317 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:18:01,271 - Epoch: [206][  100/  500]    Overall Loss 0.187737    Objective Loss 0.187737                                        LR 0.000063    Time 0.079476    
2024-02-17 13:18:08,360 - Epoch: [206][  200/  500]    Overall Loss 0.190697    Objective Loss 0.190697                                        LR 0.000063    Time 0.075159    
2024-02-17 13:18:15,483 - Epoch: [206][  300/  500]    Overall Loss 0.191348    Objective Loss 0.191348                                        LR 0.000063    Time 0.073834    
2024-02-17 13:18:22,540 - Epoch: [206][  400/  500]    Overall Loss 0.193628    Objective Loss 0.193628                                        LR 0.000063    Time 0.073007    
2024-02-17 13:18:29,743 - Epoch: [206][  500/  500]    Overall Loss 0.194200    Objective Loss 0.194200    Top1 91.000000    Top5 100.000000    LR 0.000063    Time 0.072805    
2024-02-17 13:18:29,900 - --- validate (epoch=206)-----------
2024-02-17 13:18:29,901 - 10000 samples (100 per mini-batch)
2024-02-17 13:18:32,829 - Epoch: [206][  100/  100]    Loss 1.985267    Top1 59.780000    Top5 85.580000    
2024-02-17 13:18:32,940 - ==> Top1: 59.780    Top5: 85.580    Loss: 1.985

2024-02-17 13:18:32,951 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:18:32,952 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:18:33,015 - 

2024-02-17 13:18:33,016 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:18:40,790 - Epoch: [207][  100/  500]    Overall Loss 0.194213    Objective Loss 0.194213                                        LR 0.000063    Time 0.077683    
2024-02-17 13:18:47,874 - Epoch: [207][  200/  500]    Overall Loss 0.199908    Objective Loss 0.199908                                        LR 0.000063    Time 0.074241    
2024-02-17 13:18:54,982 - Epoch: [207][  300/  500]    Overall Loss 0.195590    Objective Loss 0.195590                                        LR 0.000063    Time 0.073173    
2024-02-17 13:19:02,004 - Epoch: [207][  400/  500]    Overall Loss 0.196481    Objective Loss 0.196481                                        LR 0.000063    Time 0.072427    
2024-02-17 13:19:09,131 - Epoch: [207][  500/  500]    Overall Loss 0.196540    Objective Loss 0.196540    Top1 96.500000    Top5 100.000000    LR 0.000063    Time 0.072188    
2024-02-17 13:19:09,284 - --- validate (epoch=207)-----------
2024-02-17 13:19:09,285 - 10000 samples (100 per mini-batch)
2024-02-17 13:19:12,539 - Epoch: [207][  100/  100]    Loss 1.995921    Top1 60.050000    Top5 85.430000    
2024-02-17 13:19:12,649 - ==> Top1: 60.050    Top5: 85.430    Loss: 1.996

2024-02-17 13:19:12,659 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:19:12,659 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:19:12,716 - 

2024-02-17 13:19:12,717 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:19:20,779 - Epoch: [208][  100/  500]    Overall Loss 0.186295    Objective Loss 0.186295                                        LR 0.000063    Time 0.080571    
2024-02-17 13:19:27,999 - Epoch: [208][  200/  500]    Overall Loss 0.191939    Objective Loss 0.191939                                        LR 0.000063    Time 0.076360    
2024-02-17 13:19:35,105 - Epoch: [208][  300/  500]    Overall Loss 0.191132    Objective Loss 0.191132                                        LR 0.000063    Time 0.074578    
2024-02-17 13:19:42,124 - Epoch: [208][  400/  500]    Overall Loss 0.191322    Objective Loss 0.191322                                        LR 0.000063    Time 0.073470    
2024-02-17 13:19:49,183 - Epoch: [208][  500/  500]    Overall Loss 0.191751    Objective Loss 0.191751    Top1 94.500000    Top5 99.500000    LR 0.000063    Time 0.072887    
2024-02-17 13:19:49,276 - --- validate (epoch=208)-----------
2024-02-17 13:19:49,277 - 10000 samples (100 per mini-batch)
2024-02-17 13:19:52,186 - Epoch: [208][  100/  100]    Loss 1.989587    Top1 59.850000    Top5 85.630000    
2024-02-17 13:19:52,291 - ==> Top1: 59.850    Top5: 85.630    Loss: 1.990

2024-02-17 13:19:52,305 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:19:52,305 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:19:52,370 - 

2024-02-17 13:19:52,370 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:19:59,929 - Epoch: [209][  100/  500]    Overall Loss 0.182289    Objective Loss 0.182289                                        LR 0.000063    Time 0.075535    
2024-02-17 13:20:07,111 - Epoch: [209][  200/  500]    Overall Loss 0.184634    Objective Loss 0.184634                                        LR 0.000063    Time 0.073653    
2024-02-17 13:20:14,228 - Epoch: [209][  300/  500]    Overall Loss 0.189662    Objective Loss 0.189662                                        LR 0.000063    Time 0.072811    
2024-02-17 13:20:21,495 - Epoch: [209][  400/  500]    Overall Loss 0.189371    Objective Loss 0.189371                                        LR 0.000063    Time 0.072764    
2024-02-17 13:20:28,752 - Epoch: [209][  500/  500]    Overall Loss 0.191692    Objective Loss 0.191692    Top1 97.500000    Top5 100.000000    LR 0.000063    Time 0.072718    
2024-02-17 13:20:28,924 - --- validate (epoch=209)-----------
2024-02-17 13:20:28,925 - 10000 samples (100 per mini-batch)
2024-02-17 13:20:31,839 - Epoch: [209][  100/  100]    Loss 1.987464    Top1 60.260000    Top5 85.570000    
2024-02-17 13:20:31,951 - ==> Top1: 60.260    Top5: 85.570    Loss: 1.987

2024-02-17 13:20:31,963 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:20:31,963 - Saving checkpoint to: logs/2024.02.17-110132/checkpoint.pth.tar
2024-02-17 13:20:32,025 - 

2024-02-17 13:20:32,025 - Initiating quantization aware training (QAT)...
2024-02-17 13:20:32,089 - 

2024-02-17 13:20:32,089 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:20:41,770 - Epoch: [210][  100/  500]    Overall Loss 1.805323    Objective Loss 1.805323                                        LR 0.000063    Time 0.096756    
2024-02-17 13:20:50,622 - Epoch: [210][  200/  500]    Overall Loss 1.426129    Objective Loss 1.426129                                        LR 0.000063    Time 0.092621    
2024-02-17 13:20:59,143 - Epoch: [210][  300/  500]    Overall Loss 1.275239    Objective Loss 1.275239                                        LR 0.000063    Time 0.090139    
2024-02-17 13:21:07,905 - Epoch: [210][  400/  500]    Overall Loss 1.191261    Objective Loss 1.191261                                        LR 0.000063    Time 0.089500    
2024-02-17 13:21:16,964 - Epoch: [210][  500/  500]    Overall Loss 1.132558    Objective Loss 1.132558    Top1 72.500000    Top5 96.500000    LR 0.000063    Time 0.089711    
2024-02-17 13:21:17,092 - --- validate (epoch=210)-----------
2024-02-17 13:21:17,093 - 10000 samples (100 per mini-batch)
2024-02-17 13:21:22,723 - Epoch: [210][  100/  100]    Loss 1.796575    Top1 53.590000    Top5 81.840000    
2024-02-17 13:21:22,838 - ==> Top1: 53.590    Top5: 81.840    Loss: 1.797

2024-02-17 13:21:22,848 - ==> Best [Top1: 53.590   Top5: 81.840   Sparsity:0.00   Params: 753952 on epoch: 210]
2024-02-17 13:21:22,849 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:21:22,903 - 

2024-02-17 13:21:22,903 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:21:32,092 - Epoch: [211][  100/  500]    Overall Loss 0.869171    Objective Loss 0.869171                                        LR 0.000063    Time 0.091844    
2024-02-17 13:21:40,765 - Epoch: [211][  200/  500]    Overall Loss 0.862219    Objective Loss 0.862219                                        LR 0.000063    Time 0.089265    
2024-02-17 13:21:49,496 - Epoch: [211][  300/  500]    Overall Loss 0.864063    Objective Loss 0.864063                                        LR 0.000063    Time 0.088602    
2024-02-17 13:21:58,321 - Epoch: [211][  400/  500]    Overall Loss 0.866157    Objective Loss 0.866157                                        LR 0.000063    Time 0.088503    
2024-02-17 13:22:07,208 - Epoch: [211][  500/  500]    Overall Loss 0.864143    Objective Loss 0.864143    Top1 76.500000    Top5 96.500000    LR 0.000063    Time 0.088569    
2024-02-17 13:22:07,327 - --- validate (epoch=211)-----------
2024-02-17 13:22:07,328 - 10000 samples (100 per mini-batch)
2024-02-17 13:22:12,779 - Epoch: [211][  100/  100]    Loss 1.726161    Top1 55.380000    Top5 83.230000    
2024-02-17 13:22:12,879 - ==> Top1: 55.380    Top5: 83.230    Loss: 1.726

2024-02-17 13:22:12,889 - ==> Best [Top1: 55.380   Top5: 83.230   Sparsity:0.00   Params: 753952 on epoch: 211]
2024-02-17 13:22:12,890 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:22:12,966 - 

2024-02-17 13:22:12,967 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:22:22,423 - Epoch: [212][  100/  500]    Overall Loss 0.839325    Objective Loss 0.839325                                        LR 0.000063    Time 0.094511    
2024-02-17 13:22:31,104 - Epoch: [212][  200/  500]    Overall Loss 0.829997    Objective Loss 0.829997                                        LR 0.000063    Time 0.090642    
2024-02-17 13:22:39,885 - Epoch: [212][  300/  500]    Overall Loss 0.838078    Objective Loss 0.838078                                        LR 0.000063    Time 0.089686    
2024-02-17 13:22:48,731 - Epoch: [212][  400/  500]    Overall Loss 0.843642    Objective Loss 0.843642                                        LR 0.000063    Time 0.089370    
2024-02-17 13:22:57,785 - Epoch: [212][  500/  500]    Overall Loss 0.850917    Objective Loss 0.850917    Top1 79.000000    Top5 95.500000    LR 0.000063    Time 0.089596    
2024-02-17 13:22:57,901 - --- validate (epoch=212)-----------
2024-02-17 13:22:57,902 - 10000 samples (100 per mini-batch)
2024-02-17 13:23:03,145 - Epoch: [212][  100/  100]    Loss 1.679131    Top1 56.230000    Top5 83.340000    
2024-02-17 13:23:03,241 - ==> Top1: 56.230    Top5: 83.340    Loss: 1.679

2024-02-17 13:23:03,252 - ==> Best [Top1: 56.230   Top5: 83.340   Sparsity:0.00   Params: 753952 on epoch: 212]
2024-02-17 13:23:03,252 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:23:03,319 - 

2024-02-17 13:23:03,319 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:23:12,713 - Epoch: [213][  100/  500]    Overall Loss 0.827652    Objective Loss 0.827652                                        LR 0.000063    Time 0.093882    
2024-02-17 13:23:21,366 - Epoch: [213][  200/  500]    Overall Loss 0.841936    Objective Loss 0.841936                                        LR 0.000063    Time 0.090187    
2024-02-17 13:23:30,334 - Epoch: [213][  300/  500]    Overall Loss 0.840917    Objective Loss 0.840917                                        LR 0.000063    Time 0.090005    
2024-02-17 13:23:39,304 - Epoch: [213][  400/  500]    Overall Loss 0.842650    Objective Loss 0.842650                                        LR 0.000063    Time 0.089918    
2024-02-17 13:23:48,268 - Epoch: [213][  500/  500]    Overall Loss 0.842578    Objective Loss 0.842578    Top1 72.500000    Top5 97.000000    LR 0.000063    Time 0.089854    
2024-02-17 13:23:48,387 - --- validate (epoch=213)-----------
2024-02-17 13:23:48,388 - 10000 samples (100 per mini-batch)
2024-02-17 13:23:54,287 - Epoch: [213][  100/  100]    Loss 1.653021    Top1 57.010000    Top5 84.430000    
2024-02-17 13:23:54,418 - ==> Top1: 57.010    Top5: 84.430    Loss: 1.653

2024-02-17 13:23:54,428 - ==> Best [Top1: 57.010   Top5: 84.430   Sparsity:0.00   Params: 753952 on epoch: 213]
2024-02-17 13:23:54,428 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:23:54,493 - 

2024-02-17 13:23:54,494 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:24:03,775 - Epoch: [214][  100/  500]    Overall Loss 0.820626    Objective Loss 0.820626                                        LR 0.000063    Time 0.092763    
2024-02-17 13:24:12,445 - Epoch: [214][  200/  500]    Overall Loss 0.832545    Objective Loss 0.832545                                        LR 0.000063    Time 0.089715    
2024-02-17 13:24:21,153 - Epoch: [214][  300/  500]    Overall Loss 0.827900    Objective Loss 0.827900                                        LR 0.000063    Time 0.088822    
2024-02-17 13:24:29,504 - Epoch: [214][  400/  500]    Overall Loss 0.833779    Objective Loss 0.833779                                        LR 0.000063    Time 0.087488    
2024-02-17 13:24:38,386 - Epoch: [214][  500/  500]    Overall Loss 0.838423    Objective Loss 0.838423    Top1 77.000000    Top5 97.000000    LR 0.000063    Time 0.087747    
2024-02-17 13:24:38,572 - --- validate (epoch=214)-----------
2024-02-17 13:24:38,573 - 10000 samples (100 per mini-batch)
2024-02-17 13:24:44,602 - Epoch: [214][  100/  100]    Loss 1.604667    Top1 57.610000    Top5 84.700000    
2024-02-17 13:24:44,730 - ==> Top1: 57.610    Top5: 84.700    Loss: 1.605

2024-02-17 13:24:44,742 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:24:44,742 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:24:44,810 - 

2024-02-17 13:24:44,810 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:24:54,163 - Epoch: [215][  100/  500]    Overall Loss 0.798591    Objective Loss 0.798591                                        LR 0.000063    Time 0.093476    
2024-02-17 13:25:03,217 - Epoch: [215][  200/  500]    Overall Loss 0.816431    Objective Loss 0.816431                                        LR 0.000063    Time 0.091984    
2024-02-17 13:25:12,371 - Epoch: [215][  300/  500]    Overall Loss 0.817956    Objective Loss 0.817956                                        LR 0.000063    Time 0.091825    
2024-02-17 13:25:21,461 - Epoch: [215][  400/  500]    Overall Loss 0.817901    Objective Loss 0.817901                                        LR 0.000063    Time 0.091581    
2024-02-17 13:25:30,355 - Epoch: [215][  500/  500]    Overall Loss 0.822834    Objective Loss 0.822834    Top1 72.000000    Top5 97.000000    LR 0.000063    Time 0.091045    
2024-02-17 13:25:30,502 - --- validate (epoch=215)-----------
2024-02-17 13:25:30,503 - 10000 samples (100 per mini-batch)
2024-02-17 13:25:35,438 - Epoch: [215][  100/  100]    Loss 1.699919    Top1 56.070000    Top5 83.290000    
2024-02-17 13:25:35,528 - ==> Top1: 56.070    Top5: 83.290    Loss: 1.700

2024-02-17 13:25:35,537 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:25:35,538 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:25:35,583 - 

2024-02-17 13:25:35,583 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:25:44,493 - Epoch: [216][  100/  500]    Overall Loss 0.820145    Objective Loss 0.820145                                        LR 0.000063    Time 0.089061    
2024-02-17 13:25:53,677 - Epoch: [216][  200/  500]    Overall Loss 0.819514    Objective Loss 0.819514                                        LR 0.000063    Time 0.090430    
2024-02-17 13:26:02,626 - Epoch: [216][  300/  500]    Overall Loss 0.831640    Objective Loss 0.831640                                        LR 0.000063    Time 0.090103    
2024-02-17 13:26:11,571 - Epoch: [216][  400/  500]    Overall Loss 0.829816    Objective Loss 0.829816                                        LR 0.000063    Time 0.089929    
2024-02-17 13:26:20,430 - Epoch: [216][  500/  500]    Overall Loss 0.830790    Objective Loss 0.830790    Top1 77.500000    Top5 98.000000    LR 0.000063    Time 0.089655    
2024-02-17 13:26:20,553 - --- validate (epoch=216)-----------
2024-02-17 13:26:20,553 - 10000 samples (100 per mini-batch)
2024-02-17 13:26:25,781 - Epoch: [216][  100/  100]    Loss 1.795203    Top1 54.560000    Top5 82.640000    
2024-02-17 13:26:25,882 - ==> Top1: 54.560    Top5: 82.640    Loss: 1.795

2024-02-17 13:26:25,891 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:26:25,892 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:26:25,942 - 

2024-02-17 13:26:25,943 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:26:35,736 - Epoch: [217][  100/  500]    Overall Loss 0.820370    Objective Loss 0.820370                                        LR 0.000063    Time 0.097875    
2024-02-17 13:26:44,747 - Epoch: [217][  200/  500]    Overall Loss 0.804473    Objective Loss 0.804473                                        LR 0.000063    Time 0.093974    
2024-02-17 13:26:53,639 - Epoch: [217][  300/  500]    Overall Loss 0.807452    Objective Loss 0.807452                                        LR 0.000063    Time 0.092277    
2024-02-17 13:27:02,595 - Epoch: [217][  400/  500]    Overall Loss 0.821734    Objective Loss 0.821734                                        LR 0.000063    Time 0.091586    
2024-02-17 13:27:11,647 - Epoch: [217][  500/  500]    Overall Loss 0.823199    Objective Loss 0.823199    Top1 76.500000    Top5 97.000000    LR 0.000063    Time 0.091366    
2024-02-17 13:27:11,847 - --- validate (epoch=217)-----------
2024-02-17 13:27:11,848 - 10000 samples (100 per mini-batch)
2024-02-17 13:27:17,157 - Epoch: [217][  100/  100]    Loss 1.688081    Top1 57.000000    Top5 83.930000    
2024-02-17 13:27:17,270 - ==> Top1: 57.000    Top5: 83.930    Loss: 1.688

2024-02-17 13:27:17,280 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:27:17,281 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:27:17,331 - 

2024-02-17 13:27:17,332 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:27:26,782 - Epoch: [218][  100/  500]    Overall Loss 0.795602    Objective Loss 0.795602                                        LR 0.000063    Time 0.094456    
2024-02-17 13:27:35,652 - Epoch: [218][  200/  500]    Overall Loss 0.801723    Objective Loss 0.801723                                        LR 0.000063    Time 0.091559    
2024-02-17 13:27:44,466 - Epoch: [218][  300/  500]    Overall Loss 0.808903    Objective Loss 0.808903                                        LR 0.000063    Time 0.090405    
2024-02-17 13:27:53,337 - Epoch: [218][  400/  500]    Overall Loss 0.818116    Objective Loss 0.818116                                        LR 0.000063    Time 0.089971    
2024-02-17 13:28:01,593 - Epoch: [218][  500/  500]    Overall Loss 0.821215    Objective Loss 0.821215    Top1 78.500000    Top5 95.500000    LR 0.000063    Time 0.088483    
2024-02-17 13:28:01,684 - --- validate (epoch=218)-----------
2024-02-17 13:28:01,685 - 10000 samples (100 per mini-batch)
2024-02-17 13:28:06,968 - Epoch: [218][  100/  100]    Loss 1.686649    Top1 57.140000    Top5 84.000000    
2024-02-17 13:28:07,112 - ==> Top1: 57.140    Top5: 84.000    Loss: 1.687

2024-02-17 13:28:07,123 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:28:07,124 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:28:07,174 - 

2024-02-17 13:28:07,175 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:28:16,440 - Epoch: [219][  100/  500]    Overall Loss 0.788783    Objective Loss 0.788783                                        LR 0.000063    Time 0.092604    
2024-02-17 13:28:24,782 - Epoch: [219][  200/  500]    Overall Loss 0.817112    Objective Loss 0.817112                                        LR 0.000063    Time 0.087994    
2024-02-17 13:28:33,409 - Epoch: [219][  300/  500]    Overall Loss 0.819733    Objective Loss 0.819733                                        LR 0.000063    Time 0.087410    
2024-02-17 13:28:41,303 - Epoch: [219][  400/  500]    Overall Loss 0.822602    Objective Loss 0.822602                                        LR 0.000063    Time 0.085287    
2024-02-17 13:28:50,186 - Epoch: [219][  500/  500]    Overall Loss 0.825111    Objective Loss 0.825111    Top1 71.500000    Top5 93.500000    LR 0.000063    Time 0.085988    
2024-02-17 13:28:50,307 - --- validate (epoch=219)-----------
2024-02-17 13:28:50,307 - 10000 samples (100 per mini-batch)
2024-02-17 13:28:56,303 - Epoch: [219][  100/  100]    Loss 1.710036    Top1 55.730000    Top5 83.880000    
2024-02-17 13:28:56,425 - ==> Top1: 55.730    Top5: 83.880    Loss: 1.710

2024-02-17 13:28:56,434 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:28:56,435 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:28:56,488 - 

2024-02-17 13:28:56,488 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:29:05,682 - Epoch: [220][  100/  500]    Overall Loss 0.801131    Objective Loss 0.801131                                        LR 0.000063    Time 0.091872    
2024-02-17 13:29:14,096 - Epoch: [220][  200/  500]    Overall Loss 0.803602    Objective Loss 0.803602                                        LR 0.000063    Time 0.087987    
2024-02-17 13:29:22,810 - Epoch: [220][  300/  500]    Overall Loss 0.807135    Objective Loss 0.807135                                        LR 0.000063    Time 0.087694    
2024-02-17 13:29:31,472 - Epoch: [220][  400/  500]    Overall Loss 0.816837    Objective Loss 0.816837                                        LR 0.000063    Time 0.087417    
2024-02-17 13:29:40,418 - Epoch: [220][  500/  500]    Overall Loss 0.822668    Objective Loss 0.822668    Top1 73.000000    Top5 96.000000    LR 0.000063    Time 0.087816    
2024-02-17 13:29:40,589 - --- validate (epoch=220)-----------
2024-02-17 13:29:40,589 - 10000 samples (100 per mini-batch)
2024-02-17 13:29:46,150 - Epoch: [220][  100/  100]    Loss 1.671188    Top1 56.680000    Top5 84.340000    
2024-02-17 13:29:46,280 - ==> Top1: 56.680    Top5: 84.340    Loss: 1.671

2024-02-17 13:29:46,289 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:29:46,289 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:29:46,341 - 

2024-02-17 13:29:46,342 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:29:55,749 - Epoch: [221][  100/  500]    Overall Loss 0.805463    Objective Loss 0.805463                                        LR 0.000063    Time 0.094020    
2024-02-17 13:30:04,485 - Epoch: [221][  200/  500]    Overall Loss 0.799991    Objective Loss 0.799991                                        LR 0.000063    Time 0.090670    
2024-02-17 13:30:13,229 - Epoch: [221][  300/  500]    Overall Loss 0.806440    Objective Loss 0.806440                                        LR 0.000063    Time 0.089581    
2024-02-17 13:30:22,205 - Epoch: [221][  400/  500]    Overall Loss 0.814603    Objective Loss 0.814603                                        LR 0.000063    Time 0.089616    
2024-02-17 13:30:31,124 - Epoch: [221][  500/  500]    Overall Loss 0.816976    Objective Loss 0.816976    Top1 77.000000    Top5 97.500000    LR 0.000063    Time 0.089521    
2024-02-17 13:30:31,275 - --- validate (epoch=221)-----------
2024-02-17 13:30:31,276 - 10000 samples (100 per mini-batch)
2024-02-17 13:30:36,616 - Epoch: [221][  100/  100]    Loss 1.818449    Top1 54.360000    Top5 82.500000    
2024-02-17 13:30:36,726 - ==> Top1: 54.360    Top5: 82.500    Loss: 1.818

2024-02-17 13:30:36,738 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:30:36,738 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:30:36,798 - 

2024-02-17 13:30:36,798 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:30:46,019 - Epoch: [222][  100/  500]    Overall Loss 0.815568    Objective Loss 0.815568                                        LR 0.000063    Time 0.092158    
2024-02-17 13:30:54,861 - Epoch: [222][  200/  500]    Overall Loss 0.811145    Objective Loss 0.811145                                        LR 0.000063    Time 0.090270    
2024-02-17 13:31:03,669 - Epoch: [222][  300/  500]    Overall Loss 0.814828    Objective Loss 0.814828                                        LR 0.000063    Time 0.089526    
2024-02-17 13:31:12,460 - Epoch: [222][  400/  500]    Overall Loss 0.818522    Objective Loss 0.818522                                        LR 0.000063    Time 0.089112    
2024-02-17 13:31:21,233 - Epoch: [222][  500/  500]    Overall Loss 0.827345    Objective Loss 0.827345    Top1 76.000000    Top5 98.000000    LR 0.000063    Time 0.088829    
2024-02-17 13:31:21,410 - --- validate (epoch=222)-----------
2024-02-17 13:31:21,411 - 10000 samples (100 per mini-batch)
2024-02-17 13:31:27,168 - Epoch: [222][  100/  100]    Loss 1.708830    Top1 56.300000    Top5 83.920000    
2024-02-17 13:31:27,262 - ==> Top1: 56.300    Top5: 83.920    Loss: 1.709

2024-02-17 13:31:27,272 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:31:27,272 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:31:27,322 - 

2024-02-17 13:31:27,323 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:31:36,044 - Epoch: [223][  100/  500]    Overall Loss 0.808696    Objective Loss 0.808696                                        LR 0.000063    Time 0.087174    
2024-02-17 13:31:44,566 - Epoch: [223][  200/  500]    Overall Loss 0.808954    Objective Loss 0.808954                                        LR 0.000063    Time 0.086179    
2024-02-17 13:31:53,360 - Epoch: [223][  300/  500]    Overall Loss 0.810988    Objective Loss 0.810988                                        LR 0.000063    Time 0.086754    
2024-02-17 13:32:02,133 - Epoch: [223][  400/  500]    Overall Loss 0.819608    Objective Loss 0.819608                                        LR 0.000063    Time 0.086987    
2024-02-17 13:32:11,031 - Epoch: [223][  500/  500]    Overall Loss 0.823536    Objective Loss 0.823536    Top1 73.500000    Top5 92.500000    LR 0.000063    Time 0.087378    
2024-02-17 13:32:11,218 - --- validate (epoch=223)-----------
2024-02-17 13:32:11,219 - 10000 samples (100 per mini-batch)
2024-02-17 13:32:17,034 - Epoch: [223][  100/  100]    Loss 1.694162    Top1 56.950000    Top5 83.880000    
2024-02-17 13:32:17,149 - ==> Top1: 56.950    Top5: 83.880    Loss: 1.694

2024-02-17 13:32:17,161 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:32:17,161 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:32:17,214 - 

2024-02-17 13:32:17,215 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:32:26,733 - Epoch: [224][  100/  500]    Overall Loss 0.818396    Objective Loss 0.818396                                        LR 0.000063    Time 0.095132    
2024-02-17 13:32:35,528 - Epoch: [224][  200/  500]    Overall Loss 0.812080    Objective Loss 0.812080                                        LR 0.000063    Time 0.091522    
2024-02-17 13:32:44,442 - Epoch: [224][  300/  500]    Overall Loss 0.822887    Objective Loss 0.822887                                        LR 0.000063    Time 0.090716    
2024-02-17 13:32:53,366 - Epoch: [224][  400/  500]    Overall Loss 0.824106    Objective Loss 0.824106                                        LR 0.000063    Time 0.090336    
2024-02-17 13:33:02,278 - Epoch: [224][  500/  500]    Overall Loss 0.828344    Objective Loss 0.828344    Top1 75.000000    Top5 96.000000    LR 0.000063    Time 0.090085    
2024-02-17 13:33:02,396 - --- validate (epoch=224)-----------
2024-02-17 13:33:02,396 - 10000 samples (100 per mini-batch)
2024-02-17 13:33:08,491 - Epoch: [224][  100/  100]    Loss 1.744004    Top1 55.490000    Top5 83.520000    
2024-02-17 13:33:08,594 - ==> Top1: 55.490    Top5: 83.520    Loss: 1.744

2024-02-17 13:33:08,603 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:33:08,603 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:33:08,656 - 

2024-02-17 13:33:08,657 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:33:18,035 - Epoch: [225][  100/  500]    Overall Loss 0.821697    Objective Loss 0.821697                                        LR 0.000063    Time 0.093737    
2024-02-17 13:33:26,953 - Epoch: [225][  200/  500]    Overall Loss 0.825257    Objective Loss 0.825257                                        LR 0.000063    Time 0.091436    
2024-02-17 13:33:35,853 - Epoch: [225][  300/  500]    Overall Loss 0.831995    Objective Loss 0.831995                                        LR 0.000063    Time 0.090613    
2024-02-17 13:33:44,658 - Epoch: [225][  400/  500]    Overall Loss 0.833357    Objective Loss 0.833357                                        LR 0.000063    Time 0.089962    
2024-02-17 13:33:53,454 - Epoch: [225][  500/  500]    Overall Loss 0.840126    Objective Loss 0.840126    Top1 71.000000    Top5 95.000000    LR 0.000063    Time 0.089555    
2024-02-17 13:33:53,604 - --- validate (epoch=225)-----------
2024-02-17 13:33:53,604 - 10000 samples (100 per mini-batch)
2024-02-17 13:33:58,865 - Epoch: [225][  100/  100]    Loss 1.700846    Top1 56.310000    Top5 83.100000    
2024-02-17 13:33:58,988 - ==> Top1: 56.310    Top5: 83.100    Loss: 1.701

2024-02-17 13:33:58,994 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:33:58,994 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:33:59,039 - 

2024-02-17 13:33:59,039 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:34:08,414 - Epoch: [226][  100/  500]    Overall Loss 0.799047    Objective Loss 0.799047                                        LR 0.000063    Time 0.093695    
2024-02-17 13:34:17,128 - Epoch: [226][  200/  500]    Overall Loss 0.815592    Objective Loss 0.815592                                        LR 0.000063    Time 0.090399    
2024-02-17 13:34:25,647 - Epoch: [226][  300/  500]    Overall Loss 0.822820    Objective Loss 0.822820                                        LR 0.000063    Time 0.088651    
2024-02-17 13:34:33,966 - Epoch: [226][  400/  500]    Overall Loss 0.826243    Objective Loss 0.826243                                        LR 0.000063    Time 0.087280    
2024-02-17 13:34:43,054 - Epoch: [226][  500/  500]    Overall Loss 0.831727    Objective Loss 0.831727    Top1 76.500000    Top5 95.000000    LR 0.000063    Time 0.087991    
2024-02-17 13:34:43,190 - --- validate (epoch=226)-----------
2024-02-17 13:34:43,191 - 10000 samples (100 per mini-batch)
2024-02-17 13:34:48,847 - Epoch: [226][  100/  100]    Loss 1.765071    Top1 55.340000    Top5 83.010000    
2024-02-17 13:34:48,952 - ==> Top1: 55.340    Top5: 83.010    Loss: 1.765

2024-02-17 13:34:48,965 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:34:48,965 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:34:49,016 - 

2024-02-17 13:34:49,016 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:34:58,303 - Epoch: [227][  100/  500]    Overall Loss 0.804113    Objective Loss 0.804113                                        LR 0.000063    Time 0.092821    
2024-02-17 13:35:06,989 - Epoch: [227][  200/  500]    Overall Loss 0.814855    Objective Loss 0.814855                                        LR 0.000063    Time 0.089819    
2024-02-17 13:35:15,897 - Epoch: [227][  300/  500]    Overall Loss 0.824620    Objective Loss 0.824620                                        LR 0.000063    Time 0.089562    
2024-02-17 13:35:24,981 - Epoch: [227][  400/  500]    Overall Loss 0.830794    Objective Loss 0.830794                                        LR 0.000063    Time 0.089871    
2024-02-17 13:35:33,827 - Epoch: [227][  500/  500]    Overall Loss 0.839587    Objective Loss 0.839587    Top1 74.000000    Top5 95.500000    LR 0.000063    Time 0.089580    
2024-02-17 13:35:33,950 - --- validate (epoch=227)-----------
2024-02-17 13:35:33,951 - 10000 samples (100 per mini-batch)
2024-02-17 13:35:39,637 - Epoch: [227][  100/  100]    Loss 1.739132    Top1 55.860000    Top5 83.190000    
2024-02-17 13:35:39,747 - ==> Top1: 55.860    Top5: 83.190    Loss: 1.739

2024-02-17 13:35:39,760 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:35:39,760 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:35:39,811 - 

2024-02-17 13:35:39,811 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:35:49,268 - Epoch: [228][  100/  500]    Overall Loss 0.814485    Objective Loss 0.814485                                        LR 0.000063    Time 0.094486    
2024-02-17 13:35:58,129 - Epoch: [228][  200/  500]    Overall Loss 0.808048    Objective Loss 0.808048                                        LR 0.000063    Time 0.091530    
2024-02-17 13:36:07,008 - Epoch: [228][  300/  500]    Overall Loss 0.805742    Objective Loss 0.805742                                        LR 0.000063    Time 0.090604    
2024-02-17 13:36:15,511 - Epoch: [228][  400/  500]    Overall Loss 0.815704    Objective Loss 0.815704                                        LR 0.000063    Time 0.089203    
2024-02-17 13:36:23,898 - Epoch: [228][  500/  500]    Overall Loss 0.822611    Objective Loss 0.822611    Top1 74.500000    Top5 96.500000    LR 0.000063    Time 0.088130    
2024-02-17 13:36:24,008 - --- validate (epoch=228)-----------
2024-02-17 13:36:24,009 - 10000 samples (100 per mini-batch)
2024-02-17 13:36:29,855 - Epoch: [228][  100/  100]    Loss 1.698285    Top1 56.650000    Top5 84.040000    
2024-02-17 13:36:29,978 - ==> Top1: 56.650    Top5: 84.040    Loss: 1.698

2024-02-17 13:36:29,987 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:36:29,988 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:36:30,039 - 

2024-02-17 13:36:30,040 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:36:39,348 - Epoch: [229][  100/  500]    Overall Loss 0.808437    Objective Loss 0.808437                                        LR 0.000063    Time 0.093032    
2024-02-17 13:36:48,263 - Epoch: [229][  200/  500]    Overall Loss 0.810218    Objective Loss 0.810218                                        LR 0.000063    Time 0.091070    
2024-02-17 13:36:56,795 - Epoch: [229][  300/  500]    Overall Loss 0.819466    Objective Loss 0.819466                                        LR 0.000063    Time 0.089145    
2024-02-17 13:37:05,805 - Epoch: [229][  400/  500]    Overall Loss 0.823990    Objective Loss 0.823990                                        LR 0.000063    Time 0.089372    
2024-02-17 13:37:14,659 - Epoch: [229][  500/  500]    Overall Loss 0.831888    Objective Loss 0.831888    Top1 77.500000    Top5 96.500000    LR 0.000063    Time 0.089196    
2024-02-17 13:37:14,766 - --- validate (epoch=229)-----------
2024-02-17 13:37:14,767 - 10000 samples (100 per mini-batch)
2024-02-17 13:37:20,059 - Epoch: [229][  100/  100]    Loss 1.673878    Top1 56.710000    Top5 83.410000    
2024-02-17 13:37:20,239 - ==> Top1: 56.710    Top5: 83.410    Loss: 1.674

2024-02-17 13:37:20,249 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:37:20,249 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:37:20,301 - 

2024-02-17 13:37:20,301 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:37:29,768 - Epoch: [230][  100/  500]    Overall Loss 0.830472    Objective Loss 0.830472                                        LR 0.000063    Time 0.094619    
2024-02-17 13:37:38,148 - Epoch: [230][  200/  500]    Overall Loss 0.830174    Objective Loss 0.830174                                        LR 0.000063    Time 0.089194    
2024-02-17 13:37:46,925 - Epoch: [230][  300/  500]    Overall Loss 0.825694    Objective Loss 0.825694                                        LR 0.000063    Time 0.088707    
2024-02-17 13:37:55,985 - Epoch: [230][  400/  500]    Overall Loss 0.827925    Objective Loss 0.827925                                        LR 0.000063    Time 0.089171    
2024-02-17 13:38:04,871 - Epoch: [230][  500/  500]    Overall Loss 0.832342    Objective Loss 0.832342    Top1 75.500000    Top5 97.000000    LR 0.000063    Time 0.089099    
2024-02-17 13:38:04,993 - --- validate (epoch=230)-----------
2024-02-17 13:38:04,994 - 10000 samples (100 per mini-batch)
2024-02-17 13:38:10,483 - Epoch: [230][  100/  100]    Loss 1.674150    Top1 57.290000    Top5 83.940000    
2024-02-17 13:38:10,662 - ==> Top1: 57.290    Top5: 83.940    Loss: 1.674

2024-02-17 13:38:10,673 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:38:10,673 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:38:10,724 - 

2024-02-17 13:38:10,725 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:38:19,899 - Epoch: [231][  100/  500]    Overall Loss 0.787391    Objective Loss 0.787391                                        LR 0.000063    Time 0.091695    
2024-02-17 13:38:28,688 - Epoch: [231][  200/  500]    Overall Loss 0.806921    Objective Loss 0.806921                                        LR 0.000063    Time 0.089772    
2024-02-17 13:38:37,372 - Epoch: [231][  300/  500]    Overall Loss 0.817839    Objective Loss 0.817839                                        LR 0.000063    Time 0.088784    
2024-02-17 13:38:46,352 - Epoch: [231][  400/  500]    Overall Loss 0.816155    Objective Loss 0.816155                                        LR 0.000063    Time 0.089028    
2024-02-17 13:38:55,105 - Epoch: [231][  500/  500]    Overall Loss 0.821859    Objective Loss 0.821859    Top1 80.500000    Top5 97.000000    LR 0.000063    Time 0.088720    
2024-02-17 13:38:55,235 - --- validate (epoch=231)-----------
2024-02-17 13:38:55,236 - 10000 samples (100 per mini-batch)
2024-02-17 13:39:00,467 - Epoch: [231][  100/  100]    Loss 1.678949    Top1 57.070000    Top5 83.900000    
2024-02-17 13:39:00,598 - ==> Top1: 57.070    Top5: 83.900    Loss: 1.679

2024-02-17 13:39:00,609 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:39:00,609 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:39:00,661 - 

2024-02-17 13:39:00,661 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:39:09,979 - Epoch: [232][  100/  500]    Overall Loss 0.831226    Objective Loss 0.831226                                        LR 0.000063    Time 0.093129    
2024-02-17 13:39:18,642 - Epoch: [232][  200/  500]    Overall Loss 0.825343    Objective Loss 0.825343                                        LR 0.000063    Time 0.089863    
2024-02-17 13:39:27,646 - Epoch: [232][  300/  500]    Overall Loss 0.824987    Objective Loss 0.824987                                        LR 0.000063    Time 0.089909    
2024-02-17 13:39:36,598 - Epoch: [232][  400/  500]    Overall Loss 0.828176    Objective Loss 0.828176                                        LR 0.000063    Time 0.089803    
2024-02-17 13:39:46,251 - Epoch: [232][  500/  500]    Overall Loss 0.826299    Objective Loss 0.826299    Top1 76.000000    Top5 98.000000    LR 0.000063    Time 0.091139    
2024-02-17 13:39:46,387 - --- validate (epoch=232)-----------
2024-02-17 13:39:46,387 - 10000 samples (100 per mini-batch)
2024-02-17 13:39:51,646 - Epoch: [232][  100/  100]    Loss 1.675870    Top1 56.740000    Top5 84.370000    
2024-02-17 13:39:51,769 - ==> Top1: 56.740    Top5: 84.370    Loss: 1.676

2024-02-17 13:39:51,780 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:39:51,780 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:39:51,831 - 

2024-02-17 13:39:51,832 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:40:02,078 - Epoch: [233][  100/  500]    Overall Loss 0.816195    Objective Loss 0.816195                                        LR 0.000063    Time 0.102406    
2024-02-17 13:40:11,649 - Epoch: [233][  200/  500]    Overall Loss 0.826937    Objective Loss 0.826937                                        LR 0.000063    Time 0.099038    
2024-02-17 13:40:20,683 - Epoch: [233][  300/  500]    Overall Loss 0.820987    Objective Loss 0.820987                                        LR 0.000063    Time 0.096127    
2024-02-17 13:40:29,704 - Epoch: [233][  400/  500]    Overall Loss 0.822797    Objective Loss 0.822797                                        LR 0.000063    Time 0.094637    
2024-02-17 13:40:38,567 - Epoch: [233][  500/  500]    Overall Loss 0.826451    Objective Loss 0.826451    Top1 70.500000    Top5 94.500000    LR 0.000063    Time 0.093429    
2024-02-17 13:40:38,695 - --- validate (epoch=233)-----------
2024-02-17 13:40:38,696 - 10000 samples (100 per mini-batch)
2024-02-17 13:40:43,959 - Epoch: [233][  100/  100]    Loss 1.734688    Top1 55.920000    Top5 83.440000    
2024-02-17 13:40:44,070 - ==> Top1: 55.920    Top5: 83.440    Loss: 1.735

2024-02-17 13:40:44,079 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:40:44,079 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:40:44,129 - 

2024-02-17 13:40:44,129 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:40:53,667 - Epoch: [234][  100/  500]    Overall Loss 0.807619    Objective Loss 0.807619                                        LR 0.000063    Time 0.095333    
2024-02-17 13:41:02,559 - Epoch: [234][  200/  500]    Overall Loss 0.820367    Objective Loss 0.820367                                        LR 0.000063    Time 0.092109    
2024-02-17 13:41:11,147 - Epoch: [234][  300/  500]    Overall Loss 0.827690    Objective Loss 0.827690                                        LR 0.000063    Time 0.090020    
2024-02-17 13:41:20,087 - Epoch: [234][  400/  500]    Overall Loss 0.836671    Objective Loss 0.836671                                        LR 0.000063    Time 0.089857    
2024-02-17 13:41:28,991 - Epoch: [234][  500/  500]    Overall Loss 0.835683    Objective Loss 0.835683    Top1 72.500000    Top5 94.500000    LR 0.000063    Time 0.089685    
2024-02-17 13:41:29,124 - --- validate (epoch=234)-----------
2024-02-17 13:41:29,126 - 10000 samples (100 per mini-batch)
2024-02-17 13:41:34,563 - Epoch: [234][  100/  100]    Loss 1.777891    Top1 55.380000    Top5 82.770000    
2024-02-17 13:41:34,743 - ==> Top1: 55.380    Top5: 82.770    Loss: 1.778

2024-02-17 13:41:34,754 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:41:34,754 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:41:34,806 - 

2024-02-17 13:41:34,806 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:41:44,160 - Epoch: [235][  100/  500]    Overall Loss 0.801357    Objective Loss 0.801357                                        LR 0.000063    Time 0.093488    
2024-02-17 13:41:53,056 - Epoch: [235][  200/  500]    Overall Loss 0.817764    Objective Loss 0.817764                                        LR 0.000063    Time 0.091204    
2024-02-17 13:42:01,853 - Epoch: [235][  300/  500]    Overall Loss 0.822597    Objective Loss 0.822597                                        LR 0.000063    Time 0.090110    
2024-02-17 13:42:10,629 - Epoch: [235][  400/  500]    Overall Loss 0.828709    Objective Loss 0.828709                                        LR 0.000063    Time 0.089513    
2024-02-17 13:42:19,754 - Epoch: [235][  500/  500]    Overall Loss 0.827531    Objective Loss 0.827531    Top1 72.000000    Top5 97.000000    LR 0.000063    Time 0.089854    
2024-02-17 13:42:19,877 - --- validate (epoch=235)-----------
2024-02-17 13:42:19,878 - 10000 samples (100 per mini-batch)
2024-02-17 13:42:25,806 - Epoch: [235][  100/  100]    Loss 1.812381    Top1 54.750000    Top5 82.880000    
2024-02-17 13:42:25,907 - ==> Top1: 54.750    Top5: 82.880    Loss: 1.812

2024-02-17 13:42:25,917 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:42:25,918 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:42:25,969 - 

2024-02-17 13:42:25,970 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:42:35,571 - Epoch: [236][  100/  500]    Overall Loss 0.809734    Objective Loss 0.809734                                        LR 0.000063    Time 0.095958    
2024-02-17 13:42:44,574 - Epoch: [236][  200/  500]    Overall Loss 0.826977    Objective Loss 0.826977                                        LR 0.000063    Time 0.092974    
2024-02-17 13:42:53,530 - Epoch: [236][  300/  500]    Overall Loss 0.832389    Objective Loss 0.832389                                        LR 0.000063    Time 0.091824    
2024-02-17 13:43:02,428 - Epoch: [236][  400/  500]    Overall Loss 0.828225    Objective Loss 0.828225                                        LR 0.000063    Time 0.091102    
2024-02-17 13:43:11,412 - Epoch: [236][  500/  500]    Overall Loss 0.827784    Objective Loss 0.827784    Top1 72.000000    Top5 95.500000    LR 0.000063    Time 0.090842    
2024-02-17 13:43:11,554 - --- validate (epoch=236)-----------
2024-02-17 13:43:11,555 - 10000 samples (100 per mini-batch)
2024-02-17 13:43:17,383 - Epoch: [236][  100/  100]    Loss 1.753512    Top1 55.660000    Top5 83.200000    
2024-02-17 13:43:17,532 - ==> Top1: 55.660    Top5: 83.200    Loss: 1.754

2024-02-17 13:43:17,541 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:43:17,541 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:43:17,592 - 

2024-02-17 13:43:17,592 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:43:26,721 - Epoch: [237][  100/  500]    Overall Loss 0.818560    Objective Loss 0.818560                                        LR 0.000063    Time 0.091241    
2024-02-17 13:43:35,508 - Epoch: [237][  200/  500]    Overall Loss 0.821037    Objective Loss 0.821037                                        LR 0.000063    Time 0.089533    
2024-02-17 13:43:44,321 - Epoch: [237][  300/  500]    Overall Loss 0.820061    Objective Loss 0.820061                                        LR 0.000063    Time 0.089053    
2024-02-17 13:43:52,948 - Epoch: [237][  400/  500]    Overall Loss 0.825511    Objective Loss 0.825511                                        LR 0.000063    Time 0.088350    
2024-02-17 13:44:01,332 - Epoch: [237][  500/  500]    Overall Loss 0.826618    Objective Loss 0.826618    Top1 69.500000    Top5 93.000000    LR 0.000063    Time 0.087441    
2024-02-17 13:44:01,451 - --- validate (epoch=237)-----------
2024-02-17 13:44:01,452 - 10000 samples (100 per mini-batch)
2024-02-17 13:44:07,070 - Epoch: [237][  100/  100]    Loss 1.791750    Top1 55.190000    Top5 82.940000    
2024-02-17 13:44:07,171 - ==> Top1: 55.190    Top5: 82.940    Loss: 1.792

2024-02-17 13:44:07,179 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:44:07,179 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:44:07,232 - 

2024-02-17 13:44:07,232 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:44:16,833 - Epoch: [238][  100/  500]    Overall Loss 0.785523    Objective Loss 0.785523                                        LR 0.000063    Time 0.095961    
2024-02-17 13:44:25,657 - Epoch: [238][  200/  500]    Overall Loss 0.815240    Objective Loss 0.815240                                        LR 0.000063    Time 0.092079    
2024-02-17 13:44:34,543 - Epoch: [238][  300/  500]    Overall Loss 0.822441    Objective Loss 0.822441                                        LR 0.000063    Time 0.090994    
2024-02-17 13:44:43,514 - Epoch: [238][  400/  500]    Overall Loss 0.821081    Objective Loss 0.821081                                        LR 0.000063    Time 0.090661    
2024-02-17 13:44:52,508 - Epoch: [238][  500/  500]    Overall Loss 0.824871    Objective Loss 0.824871    Top1 75.000000    Top5 95.000000    LR 0.000063    Time 0.090509    
2024-02-17 13:44:52,643 - --- validate (epoch=238)-----------
2024-02-17 13:44:52,644 - 10000 samples (100 per mini-batch)
2024-02-17 13:44:58,061 - Epoch: [238][  100/  100]    Loss 1.677992    Top1 56.950000    Top5 84.340000    
2024-02-17 13:44:58,163 - ==> Top1: 56.950    Top5: 84.340    Loss: 1.678

2024-02-17 13:44:58,172 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:44:58,173 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:44:58,222 - 

2024-02-17 13:44:58,222 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:45:07,453 - Epoch: [239][  100/  500]    Overall Loss 0.804739    Objective Loss 0.804739                                        LR 0.000063    Time 0.092264    
2024-02-17 13:45:16,126 - Epoch: [239][  200/  500]    Overall Loss 0.811940    Objective Loss 0.811940                                        LR 0.000063    Time 0.089475    
2024-02-17 13:45:24,899 - Epoch: [239][  300/  500]    Overall Loss 0.810345    Objective Loss 0.810345                                        LR 0.000063    Time 0.088880    
2024-02-17 13:45:33,393 - Epoch: [239][  400/  500]    Overall Loss 0.817107    Objective Loss 0.817107                                        LR 0.000063    Time 0.087887    
2024-02-17 13:45:41,957 - Epoch: [239][  500/  500]    Overall Loss 0.823503    Objective Loss 0.823503    Top1 68.500000    Top5 95.000000    LR 0.000063    Time 0.087431    
2024-02-17 13:45:42,082 - --- validate (epoch=239)-----------
2024-02-17 13:45:42,082 - 10000 samples (100 per mini-batch)
2024-02-17 13:45:47,408 - Epoch: [239][  100/  100]    Loss 1.650886    Top1 57.480000    Top5 84.260000    
2024-02-17 13:45:47,570 - ==> Top1: 57.480    Top5: 84.260    Loss: 1.651

2024-02-17 13:45:47,579 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:45:47,580 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:45:47,628 - 

2024-02-17 13:45:47,629 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:45:57,298 - Epoch: [240][  100/  500]    Overall Loss 0.795048    Objective Loss 0.795048                                        LR 0.000063    Time 0.096645    
2024-02-17 13:46:06,090 - Epoch: [240][  200/  500]    Overall Loss 0.809022    Objective Loss 0.809022                                        LR 0.000063    Time 0.092262    
2024-02-17 13:46:14,646 - Epoch: [240][  300/  500]    Overall Loss 0.824847    Objective Loss 0.824847                                        LR 0.000063    Time 0.090016    
2024-02-17 13:46:23,695 - Epoch: [240][  400/  500]    Overall Loss 0.829045    Objective Loss 0.829045                                        LR 0.000063    Time 0.090124    
2024-02-17 13:46:32,843 - Epoch: [240][  500/  500]    Overall Loss 0.827484    Objective Loss 0.827484    Top1 74.500000    Top5 99.000000    LR 0.000063    Time 0.090386    
2024-02-17 13:46:33,013 - --- validate (epoch=240)-----------
2024-02-17 13:46:33,014 - 10000 samples (100 per mini-batch)
2024-02-17 13:46:38,655 - Epoch: [240][  100/  100]    Loss 1.745353    Top1 56.520000    Top5 83.280000    
2024-02-17 13:46:38,761 - ==> Top1: 56.520    Top5: 83.280    Loss: 1.745

2024-02-17 13:46:38,770 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:46:38,771 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:46:38,822 - 

2024-02-17 13:46:38,822 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:46:47,983 - Epoch: [241][  100/  500]    Overall Loss 0.811869    Objective Loss 0.811869                                        LR 0.000063    Time 0.091556    
2024-02-17 13:46:56,524 - Epoch: [241][  200/  500]    Overall Loss 0.820573    Objective Loss 0.820573                                        LR 0.000063    Time 0.088470    
2024-02-17 13:47:05,515 - Epoch: [241][  300/  500]    Overall Loss 0.825222    Objective Loss 0.825222                                        LR 0.000063    Time 0.088934    
2024-02-17 13:47:14,600 - Epoch: [241][  400/  500]    Overall Loss 0.829415    Objective Loss 0.829415                                        LR 0.000063    Time 0.089404    
2024-02-17 13:47:23,711 - Epoch: [241][  500/  500]    Overall Loss 0.833021    Objective Loss 0.833021    Top1 66.500000    Top5 96.500000    LR 0.000063    Time 0.089739    
2024-02-17 13:47:23,830 - --- validate (epoch=241)-----------
2024-02-17 13:47:23,831 - 10000 samples (100 per mini-batch)
2024-02-17 13:47:29,451 - Epoch: [241][  100/  100]    Loss 1.705916    Top1 56.360000    Top5 83.600000    
2024-02-17 13:47:29,556 - ==> Top1: 56.360    Top5: 83.600    Loss: 1.706

2024-02-17 13:47:29,561 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:47:29,561 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:47:29,604 - 

2024-02-17 13:47:29,604 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:47:38,727 - Epoch: [242][  100/  500]    Overall Loss 0.802248    Objective Loss 0.802248                                        LR 0.000063    Time 0.091182    
2024-02-17 13:47:47,726 - Epoch: [242][  200/  500]    Overall Loss 0.811746    Objective Loss 0.811746                                        LR 0.000063    Time 0.090567    
2024-02-17 13:47:56,703 - Epoch: [242][  300/  500]    Overall Loss 0.808859    Objective Loss 0.808859                                        LR 0.000063    Time 0.090289    
2024-02-17 13:48:05,678 - Epoch: [242][  400/  500]    Overall Loss 0.816174    Objective Loss 0.816174                                        LR 0.000063    Time 0.090145    
2024-02-17 13:48:14,531 - Epoch: [242][  500/  500]    Overall Loss 0.817478    Objective Loss 0.817478    Top1 76.000000    Top5 95.500000    LR 0.000063    Time 0.089814    
2024-02-17 13:48:14,689 - --- validate (epoch=242)-----------
2024-02-17 13:48:14,689 - 10000 samples (100 per mini-batch)
2024-02-17 13:48:20,012 - Epoch: [242][  100/  100]    Loss 1.776710    Top1 56.050000    Top5 83.030000    
2024-02-17 13:48:20,112 - ==> Top1: 56.050    Top5: 83.030    Loss: 1.777

2024-02-17 13:48:20,122 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:48:20,122 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:48:20,362 - 

2024-02-17 13:48:20,362 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:48:29,548 - Epoch: [243][  100/  500]    Overall Loss 0.820373    Objective Loss 0.820373                                        LR 0.000063    Time 0.091804    
2024-02-17 13:48:38,253 - Epoch: [243][  200/  500]    Overall Loss 0.820214    Objective Loss 0.820214                                        LR 0.000063    Time 0.089412    
2024-02-17 13:48:47,109 - Epoch: [243][  300/  500]    Overall Loss 0.823566    Objective Loss 0.823566                                        LR 0.000063    Time 0.089114    
2024-02-17 13:48:56,167 - Epoch: [243][  400/  500]    Overall Loss 0.828750    Objective Loss 0.828750                                        LR 0.000063    Time 0.089470    
2024-02-17 13:49:05,130 - Epoch: [243][  500/  500]    Overall Loss 0.830889    Objective Loss 0.830889    Top1 70.500000    Top5 93.000000    LR 0.000063    Time 0.089495    
2024-02-17 13:49:05,265 - --- validate (epoch=243)-----------
2024-02-17 13:49:05,266 - 10000 samples (100 per mini-batch)
2024-02-17 13:49:10,702 - Epoch: [243][  100/  100]    Loss 1.662698    Top1 57.420000    Top5 84.450000    
2024-02-17 13:49:10,823 - ==> Top1: 57.420    Top5: 84.450    Loss: 1.663

2024-02-17 13:49:10,834 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:49:10,834 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:49:10,886 - 

2024-02-17 13:49:10,886 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:49:20,420 - Epoch: [244][  100/  500]    Overall Loss 0.794620    Objective Loss 0.794620                                        LR 0.000063    Time 0.095287    
2024-02-17 13:49:29,331 - Epoch: [244][  200/  500]    Overall Loss 0.803963    Objective Loss 0.803963                                        LR 0.000063    Time 0.092178    
2024-02-17 13:49:38,334 - Epoch: [244][  300/  500]    Overall Loss 0.809818    Objective Loss 0.809818                                        LR 0.000063    Time 0.091451    
2024-02-17 13:49:47,200 - Epoch: [244][  400/  500]    Overall Loss 0.809309    Objective Loss 0.809309                                        LR 0.000063    Time 0.090743    
2024-02-17 13:49:56,149 - Epoch: [244][  500/  500]    Overall Loss 0.815073    Objective Loss 0.815073    Top1 77.000000    Top5 94.500000    LR 0.000063    Time 0.090484    
2024-02-17 13:49:56,295 - --- validate (epoch=244)-----------
2024-02-17 13:49:56,295 - 10000 samples (100 per mini-batch)
2024-02-17 13:50:02,453 - Epoch: [244][  100/  100]    Loss 1.644878    Top1 57.590000    Top5 84.720000    
2024-02-17 13:50:02,565 - ==> Top1: 57.590    Top5: 84.720    Loss: 1.645

2024-02-17 13:50:02,577 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:50:02,577 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:50:02,628 - 

2024-02-17 13:50:02,628 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:50:11,802 - Epoch: [245][  100/  500]    Overall Loss 0.803331    Objective Loss 0.803331                                        LR 0.000063    Time 0.091689    
2024-02-17 13:50:20,619 - Epoch: [245][  200/  500]    Overall Loss 0.822945    Objective Loss 0.822945                                        LR 0.000063    Time 0.089911    
2024-02-17 13:50:29,645 - Epoch: [245][  300/  500]    Overall Loss 0.825849    Objective Loss 0.825849                                        LR 0.000063    Time 0.090014    
2024-02-17 13:50:38,576 - Epoch: [245][  400/  500]    Overall Loss 0.826585    Objective Loss 0.826585                                        LR 0.000063    Time 0.089827    
2024-02-17 13:50:47,478 - Epoch: [245][  500/  500]    Overall Loss 0.829005    Objective Loss 0.829005    Top1 83.000000    Top5 98.500000    LR 0.000063    Time 0.089658    
2024-02-17 13:50:47,592 - --- validate (epoch=245)-----------
2024-02-17 13:50:47,593 - 10000 samples (100 per mini-batch)
2024-02-17 13:50:52,917 - Epoch: [245][  100/  100]    Loss 1.799483    Top1 54.170000    Top5 82.870000    
2024-02-17 13:50:53,026 - ==> Top1: 54.170    Top5: 82.870    Loss: 1.799

2024-02-17 13:50:53,035 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:50:53,035 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:50:53,084 - 

2024-02-17 13:50:53,085 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:51:02,516 - Epoch: [246][  100/  500]    Overall Loss 0.826870    Objective Loss 0.826870                                        LR 0.000063    Time 0.094249    
2024-02-17 13:51:11,257 - Epoch: [246][  200/  500]    Overall Loss 0.834811    Objective Loss 0.834811                                        LR 0.000063    Time 0.090810    
2024-02-17 13:51:20,140 - Epoch: [246][  300/  500]    Overall Loss 0.830810    Objective Loss 0.830810                                        LR 0.000063    Time 0.090137    
2024-02-17 13:51:28,938 - Epoch: [246][  400/  500]    Overall Loss 0.827768    Objective Loss 0.827768                                        LR 0.000063    Time 0.089587    
2024-02-17 13:51:37,882 - Epoch: [246][  500/  500]    Overall Loss 0.832383    Objective Loss 0.832383    Top1 71.000000    Top5 93.000000    LR 0.000063    Time 0.089549    
2024-02-17 13:51:38,025 - --- validate (epoch=246)-----------
2024-02-17 13:51:38,026 - 10000 samples (100 per mini-batch)
2024-02-17 13:51:44,106 - Epoch: [246][  100/  100]    Loss 1.785898    Top1 55.290000    Top5 82.680000    
2024-02-17 13:51:44,286 - ==> Top1: 55.290    Top5: 82.680    Loss: 1.786

2024-02-17 13:51:44,296 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:51:44,297 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:51:44,348 - 

2024-02-17 13:51:44,348 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:51:53,327 - Epoch: [247][  100/  500]    Overall Loss 0.804140    Objective Loss 0.804140                                        LR 0.000063    Time 0.089739    
2024-02-17 13:52:01,887 - Epoch: [247][  200/  500]    Overall Loss 0.804272    Objective Loss 0.804272                                        LR 0.000063    Time 0.087655    
2024-02-17 13:52:10,801 - Epoch: [247][  300/  500]    Overall Loss 0.813150    Objective Loss 0.813150                                        LR 0.000063    Time 0.088136    
2024-02-17 13:52:19,497 - Epoch: [247][  400/  500]    Overall Loss 0.820161    Objective Loss 0.820161                                        LR 0.000063    Time 0.087832    
2024-02-17 13:52:28,235 - Epoch: [247][  500/  500]    Overall Loss 0.827741    Objective Loss 0.827741    Top1 78.500000    Top5 97.000000    LR 0.000063    Time 0.087734    
2024-02-17 13:52:28,341 - --- validate (epoch=247)-----------
2024-02-17 13:52:28,342 - 10000 samples (100 per mini-batch)
2024-02-17 13:52:34,178 - Epoch: [247][  100/  100]    Loss 1.736738    Top1 56.370000    Top5 83.750000    
2024-02-17 13:52:34,302 - ==> Top1: 56.370    Top5: 83.750    Loss: 1.737

2024-02-17 13:52:34,314 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:52:34,314 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:52:34,367 - 

2024-02-17 13:52:34,367 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:52:43,561 - Epoch: [248][  100/  500]    Overall Loss 0.794079    Objective Loss 0.794079                                        LR 0.000063    Time 0.091885    
2024-02-17 13:52:52,260 - Epoch: [248][  200/  500]    Overall Loss 0.804204    Objective Loss 0.804204                                        LR 0.000063    Time 0.089420    
2024-02-17 13:53:00,676 - Epoch: [248][  300/  500]    Overall Loss 0.806211    Objective Loss 0.806211                                        LR 0.000063    Time 0.087656    
2024-02-17 13:53:09,128 - Epoch: [248][  400/  500]    Overall Loss 0.810933    Objective Loss 0.810933                                        LR 0.000063    Time 0.086863    
2024-02-17 13:53:17,786 - Epoch: [248][  500/  500]    Overall Loss 0.820679    Objective Loss 0.820679    Top1 78.500000    Top5 96.000000    LR 0.000063    Time 0.086800    
2024-02-17 13:53:17,886 - --- validate (epoch=248)-----------
2024-02-17 13:53:17,887 - 10000 samples (100 per mini-batch)
2024-02-17 13:53:23,240 - Epoch: [248][  100/  100]    Loss 1.687997    Top1 56.680000    Top5 84.130000    
2024-02-17 13:53:23,398 - ==> Top1: 56.680    Top5: 84.130    Loss: 1.688

2024-02-17 13:53:23,406 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:53:23,406 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:53:23,664 - 

2024-02-17 13:53:23,665 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:53:33,078 - Epoch: [249][  100/  500]    Overall Loss 0.799847    Objective Loss 0.799847                                        LR 0.000063    Time 0.094080    
2024-02-17 13:53:41,739 - Epoch: [249][  200/  500]    Overall Loss 0.821040    Objective Loss 0.821040                                        LR 0.000063    Time 0.090329    
2024-02-17 13:53:50,387 - Epoch: [249][  300/  500]    Overall Loss 0.811825    Objective Loss 0.811825                                        LR 0.000063    Time 0.089032    
2024-02-17 13:53:59,022 - Epoch: [249][  400/  500]    Overall Loss 0.815384    Objective Loss 0.815384                                        LR 0.000063    Time 0.088353    
2024-02-17 13:54:07,773 - Epoch: [249][  500/  500]    Overall Loss 0.816700    Objective Loss 0.816700    Top1 80.500000    Top5 96.500000    LR 0.000063    Time 0.088177    
2024-02-17 13:54:07,870 - --- validate (epoch=249)-----------
2024-02-17 13:54:07,871 - 10000 samples (100 per mini-batch)
2024-02-17 13:54:13,500 - Epoch: [249][  100/  100]    Loss 1.805474    Top1 54.960000    Top5 82.980000    
2024-02-17 13:54:13,703 - ==> Top1: 54.960    Top5: 82.980    Loss: 1.805

2024-02-17 13:54:13,710 - ==> Best [Top1: 57.610   Top5: 84.700   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:54:13,710 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:54:13,764 - 

2024-02-17 13:54:13,764 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:54:23,521 - Epoch: [250][  100/  500]    Overall Loss 0.787593    Objective Loss 0.787593                                        LR 0.000063    Time 0.097519    
2024-02-17 13:54:32,400 - Epoch: [250][  200/  500]    Overall Loss 0.808917    Objective Loss 0.808917                                        LR 0.000063    Time 0.093132    
2024-02-17 13:54:41,316 - Epoch: [250][  300/  500]    Overall Loss 0.813889    Objective Loss 0.813889                                        LR 0.000063    Time 0.091795    
2024-02-17 13:54:49,971 - Epoch: [250][  400/  500]    Overall Loss 0.819793    Objective Loss 0.819793                                        LR 0.000063    Time 0.090474    
2024-02-17 13:54:58,746 - Epoch: [250][  500/  500]    Overall Loss 0.818840    Objective Loss 0.818840    Top1 76.000000    Top5 94.500000    LR 0.000063    Time 0.089921    
2024-02-17 13:54:58,871 - --- validate (epoch=250)-----------
2024-02-17 13:54:58,873 - 10000 samples (100 per mini-batch)
2024-02-17 13:55:04,183 - Epoch: [250][  100/  100]    Loss 1.628075    Top1 58.100000    Top5 85.090000    
2024-02-17 13:55:04,300 - ==> Top1: 58.100    Top5: 85.090    Loss: 1.628

2024-02-17 13:55:04,312 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 13:55:04,312 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:55:04,379 - 

2024-02-17 13:55:04,380 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:55:13,672 - Epoch: [251][  100/  500]    Overall Loss 0.789695    Objective Loss 0.789695                                        LR 0.000063    Time 0.092867    
2024-02-17 13:55:22,492 - Epoch: [251][  200/  500]    Overall Loss 0.803494    Objective Loss 0.803494                                        LR 0.000063    Time 0.090517    
2024-02-17 13:55:31,090 - Epoch: [251][  300/  500]    Overall Loss 0.810122    Objective Loss 0.810122                                        LR 0.000063    Time 0.088993    
2024-02-17 13:55:39,696 - Epoch: [251][  400/  500]    Overall Loss 0.813002    Objective Loss 0.813002                                        LR 0.000063    Time 0.088250    
2024-02-17 13:55:48,181 - Epoch: [251][  500/  500]    Overall Loss 0.817967    Objective Loss 0.817967    Top1 81.000000    Top5 96.000000    LR 0.000063    Time 0.087565    
2024-02-17 13:55:48,331 - --- validate (epoch=251)-----------
2024-02-17 13:55:48,332 - 10000 samples (100 per mini-batch)
2024-02-17 13:55:53,366 - Epoch: [251][  100/  100]    Loss 1.762333    Top1 55.960000    Top5 83.040000    
2024-02-17 13:55:53,454 - ==> Top1: 55.960    Top5: 83.040    Loss: 1.762

2024-02-17 13:55:53,463 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 13:55:53,463 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:55:53,511 - 

2024-02-17 13:55:53,512 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:56:02,742 - Epoch: [252][  100/  500]    Overall Loss 0.798981    Objective Loss 0.798981                                        LR 0.000063    Time 0.092238    
2024-02-17 13:56:11,660 - Epoch: [252][  200/  500]    Overall Loss 0.812654    Objective Loss 0.812654                                        LR 0.000063    Time 0.090687    
2024-02-17 13:56:20,716 - Epoch: [252][  300/  500]    Overall Loss 0.819725    Objective Loss 0.819725                                        LR 0.000063    Time 0.090631    
2024-02-17 13:56:29,844 - Epoch: [252][  400/  500]    Overall Loss 0.821419    Objective Loss 0.821419                                        LR 0.000063    Time 0.090784    
2024-02-17 13:56:38,949 - Epoch: [252][  500/  500]    Overall Loss 0.823987    Objective Loss 0.823987    Top1 77.000000    Top5 97.000000    LR 0.000063    Time 0.090828    
2024-02-17 13:56:39,097 - --- validate (epoch=252)-----------
2024-02-17 13:56:39,098 - 10000 samples (100 per mini-batch)
2024-02-17 13:56:44,923 - Epoch: [252][  100/  100]    Loss 1.657538    Top1 57.740000    Top5 84.110000    
2024-02-17 13:56:45,039 - ==> Top1: 57.740    Top5: 84.110    Loss: 1.658

2024-02-17 13:56:45,049 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 13:56:45,050 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:56:45,111 - 

2024-02-17 13:56:45,111 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:56:54,506 - Epoch: [253][  100/  500]    Overall Loss 0.810027    Objective Loss 0.810027                                        LR 0.000063    Time 0.093896    
2024-02-17 13:57:03,482 - Epoch: [253][  200/  500]    Overall Loss 0.802692    Objective Loss 0.802692                                        LR 0.000063    Time 0.091813    
2024-02-17 13:57:12,447 - Epoch: [253][  300/  500]    Overall Loss 0.809536    Objective Loss 0.809536                                        LR 0.000063    Time 0.091076    
2024-02-17 13:57:21,257 - Epoch: [253][  400/  500]    Overall Loss 0.810409    Objective Loss 0.810409                                        LR 0.000063    Time 0.090322    
2024-02-17 13:57:30,114 - Epoch: [253][  500/  500]    Overall Loss 0.811075    Objective Loss 0.811075    Top1 73.500000    Top5 95.000000    LR 0.000063    Time 0.089965    
2024-02-17 13:57:30,217 - --- validate (epoch=253)-----------
2024-02-17 13:57:30,218 - 10000 samples (100 per mini-batch)
2024-02-17 13:57:36,078 - Epoch: [253][  100/  100]    Loss 1.705152    Top1 57.050000    Top5 83.760000    
2024-02-17 13:57:36,205 - ==> Top1: 57.050    Top5: 83.760    Loss: 1.705

2024-02-17 13:57:36,215 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 13:57:36,215 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:57:36,266 - 

2024-02-17 13:57:36,266 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:57:45,481 - Epoch: [254][  100/  500]    Overall Loss 0.786346    Objective Loss 0.786346                                        LR 0.000063    Time 0.092102    
2024-02-17 13:57:54,208 - Epoch: [254][  200/  500]    Overall Loss 0.804809    Objective Loss 0.804809                                        LR 0.000063    Time 0.089667    
2024-02-17 13:58:03,043 - Epoch: [254][  300/  500]    Overall Loss 0.806793    Objective Loss 0.806793                                        LR 0.000063    Time 0.089214    
2024-02-17 13:58:12,013 - Epoch: [254][  400/  500]    Overall Loss 0.815298    Objective Loss 0.815298                                        LR 0.000063    Time 0.089324    
2024-02-17 13:58:21,097 - Epoch: [254][  500/  500]    Overall Loss 0.820930    Objective Loss 0.820930    Top1 72.500000    Top5 96.000000    LR 0.000063    Time 0.089618    
2024-02-17 13:58:21,268 - --- validate (epoch=254)-----------
2024-02-17 13:58:21,269 - 10000 samples (100 per mini-batch)
2024-02-17 13:58:26,487 - Epoch: [254][  100/  100]    Loss 1.636370    Top1 58.040000    Top5 84.780000    
2024-02-17 13:58:26,577 - ==> Top1: 58.040    Top5: 84.780    Loss: 1.636

2024-02-17 13:58:26,586 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 13:58:26,586 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:58:26,632 - 

2024-02-17 13:58:26,632 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:58:35,812 - Epoch: [255][  100/  500]    Overall Loss 0.790947    Objective Loss 0.790947                                        LR 0.000063    Time 0.091747    
2024-02-17 13:58:44,709 - Epoch: [255][  200/  500]    Overall Loss 0.786984    Objective Loss 0.786984                                        LR 0.000063    Time 0.090342    
2024-02-17 13:58:53,703 - Epoch: [255][  300/  500]    Overall Loss 0.808479    Objective Loss 0.808479                                        LR 0.000063    Time 0.090196    
2024-02-17 13:59:02,455 - Epoch: [255][  400/  500]    Overall Loss 0.813853    Objective Loss 0.813853                                        LR 0.000063    Time 0.089517    
2024-02-17 13:59:11,326 - Epoch: [255][  500/  500]    Overall Loss 0.818660    Objective Loss 0.818660    Top1 74.000000    Top5 96.500000    LR 0.000063    Time 0.089346    
2024-02-17 13:59:11,450 - --- validate (epoch=255)-----------
2024-02-17 13:59:11,451 - 10000 samples (100 per mini-batch)
2024-02-17 13:59:16,635 - Epoch: [255][  100/  100]    Loss 1.736305    Top1 56.620000    Top5 83.440000    
2024-02-17 13:59:16,791 - ==> Top1: 56.620    Top5: 83.440    Loss: 1.736

2024-02-17 13:59:16,801 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 13:59:16,802 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 13:59:16,854 - 

2024-02-17 13:59:16,854 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:59:26,693 - Epoch: [256][  100/  500]    Overall Loss 0.792843    Objective Loss 0.792843                                        LR 0.000063    Time 0.098332    
2024-02-17 13:59:35,676 - Epoch: [256][  200/  500]    Overall Loss 0.808288    Objective Loss 0.808288                                        LR 0.000063    Time 0.094065    
2024-02-17 13:59:44,694 - Epoch: [256][  300/  500]    Overall Loss 0.822132    Objective Loss 0.822132                                        LR 0.000063    Time 0.092755    
2024-02-17 13:59:53,548 - Epoch: [256][  400/  500]    Overall Loss 0.818839    Objective Loss 0.818839                                        LR 0.000063    Time 0.091693    
2024-02-17 14:00:02,466 - Epoch: [256][  500/  500]    Overall Loss 0.821030    Objective Loss 0.821030    Top1 74.000000    Top5 95.000000    LR 0.000063    Time 0.091181    
2024-02-17 14:00:02,589 - --- validate (epoch=256)-----------
2024-02-17 14:00:02,590 - 10000 samples (100 per mini-batch)
2024-02-17 14:00:08,012 - Epoch: [256][  100/  100]    Loss 1.705132    Top1 56.660000    Top5 83.960000    
2024-02-17 14:00:08,120 - ==> Top1: 56.660    Top5: 83.960    Loss: 1.705

2024-02-17 14:00:08,128 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:00:08,128 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:00:08,177 - 

2024-02-17 14:00:08,177 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:00:17,424 - Epoch: [257][  100/  500]    Overall Loss 0.787092    Objective Loss 0.787092                                        LR 0.000063    Time 0.092421    
2024-02-17 14:00:26,515 - Epoch: [257][  200/  500]    Overall Loss 0.800206    Objective Loss 0.800206                                        LR 0.000063    Time 0.091642    
2024-02-17 14:00:35,491 - Epoch: [257][  300/  500]    Overall Loss 0.808771    Objective Loss 0.808771                                        LR 0.000063    Time 0.091004    
2024-02-17 14:00:44,350 - Epoch: [257][  400/  500]    Overall Loss 0.816523    Objective Loss 0.816523                                        LR 0.000063    Time 0.090391    
2024-02-17 14:00:53,179 - Epoch: [257][  500/  500]    Overall Loss 0.816065    Objective Loss 0.816065    Top1 81.500000    Top5 96.000000    LR 0.000063    Time 0.089963    
2024-02-17 14:00:53,279 - --- validate (epoch=257)-----------
2024-02-17 14:00:53,280 - 10000 samples (100 per mini-batch)
2024-02-17 14:00:58,979 - Epoch: [257][  100/  100]    Loss 1.650241    Top1 57.540000    Top5 84.490000    
2024-02-17 14:00:59,083 - ==> Top1: 57.540    Top5: 84.490    Loss: 1.650

2024-02-17 14:00:59,093 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:00:59,093 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:00:59,148 - 

2024-02-17 14:00:59,148 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:01:08,637 - Epoch: [258][  100/  500]    Overall Loss 0.789888    Objective Loss 0.789888                                        LR 0.000063    Time 0.094846    
2024-02-17 14:01:17,568 - Epoch: [258][  200/  500]    Overall Loss 0.805765    Objective Loss 0.805765                                        LR 0.000063    Time 0.092056    
2024-02-17 14:01:26,582 - Epoch: [258][  300/  500]    Overall Loss 0.806829    Objective Loss 0.806829                                        LR 0.000063    Time 0.091404    
2024-02-17 14:01:35,547 - Epoch: [258][  400/  500]    Overall Loss 0.814166    Objective Loss 0.814166                                        LR 0.000063    Time 0.090957    
2024-02-17 14:01:44,514 - Epoch: [258][  500/  500]    Overall Loss 0.819009    Objective Loss 0.819009    Top1 74.500000    Top5 96.500000    LR 0.000063    Time 0.090692    
2024-02-17 14:01:44,634 - --- validate (epoch=258)-----------
2024-02-17 14:01:44,635 - 10000 samples (100 per mini-batch)
2024-02-17 14:01:50,698 - Epoch: [258][  100/  100]    Loss 1.699389    Top1 56.930000    Top5 84.110000    
2024-02-17 14:01:50,796 - ==> Top1: 56.930    Top5: 84.110    Loss: 1.699

2024-02-17 14:01:50,807 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:01:50,807 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:01:50,865 - 

2024-02-17 14:01:50,865 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:02:00,253 - Epoch: [259][  100/  500]    Overall Loss 0.769401    Objective Loss 0.769401                                        LR 0.000063    Time 0.093825    
2024-02-17 14:02:09,188 - Epoch: [259][  200/  500]    Overall Loss 0.792284    Objective Loss 0.792284                                        LR 0.000063    Time 0.091564    
2024-02-17 14:02:18,174 - Epoch: [259][  300/  500]    Overall Loss 0.803640    Objective Loss 0.803640                                        LR 0.000063    Time 0.090984    
2024-02-17 14:02:27,057 - Epoch: [259][  400/  500]    Overall Loss 0.809049    Objective Loss 0.809049                                        LR 0.000063    Time 0.090436    
2024-02-17 14:02:35,799 - Epoch: [259][  500/  500]    Overall Loss 0.817262    Objective Loss 0.817262    Top1 76.500000    Top5 95.000000    LR 0.000063    Time 0.089826    
2024-02-17 14:02:35,973 - --- validate (epoch=259)-----------
2024-02-17 14:02:35,974 - 10000 samples (100 per mini-batch)
2024-02-17 14:02:41,175 - Epoch: [259][  100/  100]    Loss 1.725611    Top1 56.260000    Top5 83.410000    
2024-02-17 14:02:41,280 - ==> Top1: 56.260    Top5: 83.410    Loss: 1.726

2024-02-17 14:02:41,291 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:02:41,291 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:02:41,342 - 

2024-02-17 14:02:41,343 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:02:50,882 - Epoch: [260][  100/  500]    Overall Loss 0.813062    Objective Loss 0.813062                                        LR 0.000063    Time 0.095341    
2024-02-17 14:02:59,803 - Epoch: [260][  200/  500]    Overall Loss 0.817227    Objective Loss 0.817227                                        LR 0.000063    Time 0.092254    
2024-02-17 14:03:08,813 - Epoch: [260][  300/  500]    Overall Loss 0.818387    Objective Loss 0.818387                                        LR 0.000063    Time 0.091522    
2024-02-17 14:03:17,659 - Epoch: [260][  400/  500]    Overall Loss 0.822773    Objective Loss 0.822773                                        LR 0.000063    Time 0.090746    
2024-02-17 14:03:26,627 - Epoch: [260][  500/  500]    Overall Loss 0.821498    Objective Loss 0.821498    Top1 73.000000    Top5 97.500000    LR 0.000063    Time 0.090525    
2024-02-17 14:03:26,784 - --- validate (epoch=260)-----------
2024-02-17 14:03:26,785 - 10000 samples (100 per mini-batch)
2024-02-17 14:03:32,132 - Epoch: [260][  100/  100]    Loss 1.677470    Top1 57.440000    Top5 84.520000    
2024-02-17 14:03:32,305 - ==> Top1: 57.440    Top5: 84.520    Loss: 1.677

2024-02-17 14:03:32,317 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:03:32,317 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:03:32,369 - 

2024-02-17 14:03:32,369 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:03:41,703 - Epoch: [261][  100/  500]    Overall Loss 0.826879    Objective Loss 0.826879                                        LR 0.000063    Time 0.093288    
2024-02-17 14:03:50,617 - Epoch: [261][  200/  500]    Overall Loss 0.811436    Objective Loss 0.811436                                        LR 0.000063    Time 0.091192    
2024-02-17 14:03:59,514 - Epoch: [261][  300/  500]    Overall Loss 0.813062    Objective Loss 0.813062                                        LR 0.000063    Time 0.090438    
2024-02-17 14:04:08,173 - Epoch: [261][  400/  500]    Overall Loss 0.816831    Objective Loss 0.816831                                        LR 0.000063    Time 0.089468    
2024-02-17 14:04:16,909 - Epoch: [261][  500/  500]    Overall Loss 0.821329    Objective Loss 0.821329    Top1 73.500000    Top5 94.000000    LR 0.000063    Time 0.089039    
2024-02-17 14:04:17,024 - --- validate (epoch=261)-----------
2024-02-17 14:04:17,024 - 10000 samples (100 per mini-batch)
2024-02-17 14:04:22,584 - Epoch: [261][  100/  100]    Loss 1.706736    Top1 57.110000    Top5 83.420000    
2024-02-17 14:04:22,708 - ==> Top1: 57.110    Top5: 83.420    Loss: 1.707

2024-02-17 14:04:22,718 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:04:22,719 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:04:22,770 - 

2024-02-17 14:04:22,770 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:04:32,303 - Epoch: [262][  100/  500]    Overall Loss 0.817717    Objective Loss 0.817717                                        LR 0.000063    Time 0.095276    
2024-02-17 14:04:41,193 - Epoch: [262][  200/  500]    Overall Loss 0.816306    Objective Loss 0.816306                                        LR 0.000063    Time 0.092066    
2024-02-17 14:04:50,026 - Epoch: [262][  300/  500]    Overall Loss 0.813959    Objective Loss 0.813959                                        LR 0.000063    Time 0.090808    
2024-02-17 14:04:58,705 - Epoch: [262][  400/  500]    Overall Loss 0.823587    Objective Loss 0.823587                                        LR 0.000063    Time 0.089795    
2024-02-17 14:05:07,489 - Epoch: [262][  500/  500]    Overall Loss 0.827720    Objective Loss 0.827720    Top1 70.500000    Top5 94.000000    LR 0.000063    Time 0.089397    
2024-02-17 14:05:07,600 - --- validate (epoch=262)-----------
2024-02-17 14:05:07,601 - 10000 samples (100 per mini-batch)
2024-02-17 14:05:12,854 - Epoch: [262][  100/  100]    Loss 1.729022    Top1 56.630000    Top5 83.440000    
2024-02-17 14:05:12,965 - ==> Top1: 56.630    Top5: 83.440    Loss: 1.729

2024-02-17 14:05:12,977 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:05:12,977 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:05:13,029 - 

2024-02-17 14:05:13,029 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:05:21,881 - Epoch: [263][  100/  500]    Overall Loss 0.796424    Objective Loss 0.796424                                        LR 0.000063    Time 0.088474    
2024-02-17 14:05:30,327 - Epoch: [263][  200/  500]    Overall Loss 0.809621    Objective Loss 0.809621                                        LR 0.000063    Time 0.086450    
2024-02-17 14:05:38,909 - Epoch: [263][  300/  500]    Overall Loss 0.812242    Objective Loss 0.812242                                        LR 0.000063    Time 0.086228    
2024-02-17 14:05:47,968 - Epoch: [263][  400/  500]    Overall Loss 0.821472    Objective Loss 0.821472                                        LR 0.000063    Time 0.087308    
2024-02-17 14:05:56,474 - Epoch: [263][  500/  500]    Overall Loss 0.824160    Objective Loss 0.824160    Top1 76.500000    Top5 96.500000    LR 0.000063    Time 0.086853    
2024-02-17 14:05:56,658 - --- validate (epoch=263)-----------
2024-02-17 14:05:56,659 - 10000 samples (100 per mini-batch)
2024-02-17 14:06:01,858 - Epoch: [263][  100/  100]    Loss 1.757634    Top1 56.060000    Top5 83.170000    
2024-02-17 14:06:01,954 - ==> Top1: 56.060    Top5: 83.170    Loss: 1.758

2024-02-17 14:06:01,963 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:06:01,964 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:06:02,018 - 

2024-02-17 14:06:02,019 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:06:11,597 - Epoch: [264][  100/  500]    Overall Loss 0.787857    Objective Loss 0.787857                                        LR 0.000063    Time 0.095728    
2024-02-17 14:06:20,237 - Epoch: [264][  200/  500]    Overall Loss 0.796261    Objective Loss 0.796261                                        LR 0.000063    Time 0.091049    
2024-02-17 14:06:29,089 - Epoch: [264][  300/  500]    Overall Loss 0.808894    Objective Loss 0.808894                                        LR 0.000063    Time 0.090191    
2024-02-17 14:06:38,164 - Epoch: [264][  400/  500]    Overall Loss 0.811708    Objective Loss 0.811708                                        LR 0.000063    Time 0.090321    
2024-02-17 14:06:46,771 - Epoch: [264][  500/  500]    Overall Loss 0.813623    Objective Loss 0.813623    Top1 73.000000    Top5 93.000000    LR 0.000063    Time 0.089465    
2024-02-17 14:06:46,871 - --- validate (epoch=264)-----------
2024-02-17 14:06:46,872 - 10000 samples (100 per mini-batch)
2024-02-17 14:06:51,945 - Epoch: [264][  100/  100]    Loss 1.683812    Top1 56.730000    Top5 84.030000    
2024-02-17 14:06:52,044 - ==> Top1: 56.730    Top5: 84.030    Loss: 1.684

2024-02-17 14:06:52,053 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:06:52,053 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:06:52,099 - 

2024-02-17 14:06:52,099 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:07:00,801 - Epoch: [265][  100/  500]    Overall Loss 0.776592    Objective Loss 0.776592                                        LR 0.000063    Time 0.086974    
2024-02-17 14:07:09,808 - Epoch: [265][  200/  500]    Overall Loss 0.792620    Objective Loss 0.792620                                        LR 0.000063    Time 0.088503    
2024-02-17 14:07:18,611 - Epoch: [265][  300/  500]    Overall Loss 0.806454    Objective Loss 0.806454                                        LR 0.000063    Time 0.088332    
2024-02-17 14:07:27,513 - Epoch: [265][  400/  500]    Overall Loss 0.807122    Objective Loss 0.807122                                        LR 0.000063    Time 0.088494    
2024-02-17 14:07:35,917 - Epoch: [265][  500/  500]    Overall Loss 0.812005    Objective Loss 0.812005    Top1 73.500000    Top5 97.000000    LR 0.000063    Time 0.087597    
2024-02-17 14:07:36,051 - --- validate (epoch=265)-----------
2024-02-17 14:07:36,052 - 10000 samples (100 per mini-batch)
2024-02-17 14:07:42,009 - Epoch: [265][  100/  100]    Loss 1.691001    Top1 56.710000    Top5 84.270000    
2024-02-17 14:07:42,105 - ==> Top1: 56.710    Top5: 84.270    Loss: 1.691

2024-02-17 14:07:42,115 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:07:42,115 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:07:42,166 - 

2024-02-17 14:07:42,166 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:07:51,695 - Epoch: [266][  100/  500]    Overall Loss 0.804337    Objective Loss 0.804337                                        LR 0.000063    Time 0.095239    
2024-02-17 14:08:00,562 - Epoch: [266][  200/  500]    Overall Loss 0.795152    Objective Loss 0.795152                                        LR 0.000063    Time 0.091932    
2024-02-17 14:08:09,308 - Epoch: [266][  300/  500]    Overall Loss 0.804032    Objective Loss 0.804032                                        LR 0.000063    Time 0.090427    
2024-02-17 14:08:18,163 - Epoch: [266][  400/  500]    Overall Loss 0.819606    Objective Loss 0.819606                                        LR 0.000063    Time 0.089950    
2024-02-17 14:08:26,588 - Epoch: [266][  500/  500]    Overall Loss 0.823633    Objective Loss 0.823633    Top1 69.500000    Top5 93.500000    LR 0.000063    Time 0.088802    
2024-02-17 14:08:26,690 - --- validate (epoch=266)-----------
2024-02-17 14:08:26,691 - 10000 samples (100 per mini-batch)
2024-02-17 14:08:31,867 - Epoch: [266][  100/  100]    Loss 1.697623    Top1 57.220000    Top5 84.420000    
2024-02-17 14:08:32,003 - ==> Top1: 57.220    Top5: 84.420    Loss: 1.698

2024-02-17 14:08:32,011 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:08:32,011 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:08:32,056 - 

2024-02-17 14:08:32,057 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:08:41,361 - Epoch: [267][  100/  500]    Overall Loss 0.806103    Objective Loss 0.806103                                        LR 0.000063    Time 0.092994    
2024-02-17 14:08:50,263 - Epoch: [267][  200/  500]    Overall Loss 0.806155    Objective Loss 0.806155                                        LR 0.000063    Time 0.090986    
2024-02-17 14:08:59,114 - Epoch: [267][  300/  500]    Overall Loss 0.822903    Objective Loss 0.822903                                        LR 0.000063    Time 0.090149    
2024-02-17 14:09:08,197 - Epoch: [267][  400/  500]    Overall Loss 0.823251    Objective Loss 0.823251                                        LR 0.000063    Time 0.090308    
2024-02-17 14:09:17,245 - Epoch: [267][  500/  500]    Overall Loss 0.826416    Objective Loss 0.826416    Top1 79.000000    Top5 95.000000    LR 0.000063    Time 0.090334    
2024-02-17 14:09:17,423 - --- validate (epoch=267)-----------
2024-02-17 14:09:17,424 - 10000 samples (100 per mini-batch)
2024-02-17 14:09:22,835 - Epoch: [267][  100/  100]    Loss 1.694257    Top1 56.960000    Top5 84.060000    
2024-02-17 14:09:22,936 - ==> Top1: 56.960    Top5: 84.060    Loss: 1.694

2024-02-17 14:09:22,946 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:09:22,946 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:09:22,997 - 

2024-02-17 14:09:22,998 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:09:32,297 - Epoch: [268][  100/  500]    Overall Loss 0.801530    Objective Loss 0.801530                                        LR 0.000063    Time 0.092939    
2024-02-17 14:09:40,826 - Epoch: [268][  200/  500]    Overall Loss 0.807159    Objective Loss 0.807159                                        LR 0.000063    Time 0.089102    
2024-02-17 14:09:49,328 - Epoch: [268][  300/  500]    Overall Loss 0.819618    Objective Loss 0.819618                                        LR 0.000063    Time 0.087730    
2024-02-17 14:09:57,670 - Epoch: [268][  400/  500]    Overall Loss 0.815754    Objective Loss 0.815754                                        LR 0.000063    Time 0.086644    
2024-02-17 14:10:06,753 - Epoch: [268][  500/  500]    Overall Loss 0.820045    Objective Loss 0.820045    Top1 73.500000    Top5 95.000000    LR 0.000063    Time 0.087473    
2024-02-17 14:10:06,905 - --- validate (epoch=268)-----------
2024-02-17 14:10:06,906 - 10000 samples (100 per mini-batch)
2024-02-17 14:10:12,502 - Epoch: [268][  100/  100]    Loss 1.640887    Top1 58.010000    Top5 84.680000    
2024-02-17 14:10:12,606 - ==> Top1: 58.010    Top5: 84.680    Loss: 1.641

2024-02-17 14:10:12,617 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:10:12,617 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:10:12,879 - 

2024-02-17 14:10:12,879 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:10:22,134 - Epoch: [269][  100/  500]    Overall Loss 0.797413    Objective Loss 0.797413                                        LR 0.000063    Time 0.092501    
2024-02-17 14:10:31,161 - Epoch: [269][  200/  500]    Overall Loss 0.806670    Objective Loss 0.806670                                        LR 0.000063    Time 0.091365    
2024-02-17 14:10:39,931 - Epoch: [269][  300/  500]    Overall Loss 0.811087    Objective Loss 0.811087                                        LR 0.000063    Time 0.090128    
2024-02-17 14:10:48,657 - Epoch: [269][  400/  500]    Overall Loss 0.815528    Objective Loss 0.815528                                        LR 0.000063    Time 0.089403    
2024-02-17 14:10:57,508 - Epoch: [269][  500/  500]    Overall Loss 0.823392    Objective Loss 0.823392    Top1 77.000000    Top5 94.000000    LR 0.000063    Time 0.089216    
2024-02-17 14:10:57,628 - --- validate (epoch=269)-----------
2024-02-17 14:10:57,629 - 10000 samples (100 per mini-batch)
2024-02-17 14:11:03,291 - Epoch: [269][  100/  100]    Loss 1.677516    Top1 57.030000    Top5 84.510000    
2024-02-17 14:11:03,386 - ==> Top1: 57.030    Top5: 84.510    Loss: 1.678

2024-02-17 14:11:03,399 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:11:03,400 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:11:03,463 - 

2024-02-17 14:11:03,464 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:11:13,064 - Epoch: [270][  100/  500]    Overall Loss 0.772017    Objective Loss 0.772017                                        LR 0.000063    Time 0.095933    
2024-02-17 14:11:21,889 - Epoch: [270][  200/  500]    Overall Loss 0.809423    Objective Loss 0.809423                                        LR 0.000063    Time 0.092072    
2024-02-17 14:11:30,612 - Epoch: [270][  300/  500]    Overall Loss 0.815304    Objective Loss 0.815304                                        LR 0.000063    Time 0.090446    
2024-02-17 14:11:39,318 - Epoch: [270][  400/  500]    Overall Loss 0.822545    Objective Loss 0.822545                                        LR 0.000063    Time 0.089589    
2024-02-17 14:11:48,039 - Epoch: [270][  500/  500]    Overall Loss 0.823923    Objective Loss 0.823923    Top1 69.500000    Top5 97.500000    LR 0.000063    Time 0.089107    
2024-02-17 14:11:48,142 - --- validate (epoch=270)-----------
2024-02-17 14:11:48,143 - 10000 samples (100 per mini-batch)
2024-02-17 14:11:53,405 - Epoch: [270][  100/  100]    Loss 1.706164    Top1 56.570000    Top5 84.140000    
2024-02-17 14:11:53,557 - ==> Top1: 56.570    Top5: 84.140    Loss: 1.706

2024-02-17 14:11:53,567 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:11:53,568 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:11:53,616 - 

2024-02-17 14:11:53,617 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:12:02,573 - Epoch: [271][  100/  500]    Overall Loss 0.800689    Objective Loss 0.800689                                        LR 0.000063    Time 0.089513    
2024-02-17 14:12:11,267 - Epoch: [271][  200/  500]    Overall Loss 0.818748    Objective Loss 0.818748                                        LR 0.000063    Time 0.088209    
2024-02-17 14:12:19,638 - Epoch: [271][  300/  500]    Overall Loss 0.818349    Objective Loss 0.818349                                        LR 0.000063    Time 0.086700    
2024-02-17 14:12:28,431 - Epoch: [271][  400/  500]    Overall Loss 0.826010    Objective Loss 0.826010                                        LR 0.000063    Time 0.086998    
2024-02-17 14:12:37,255 - Epoch: [271][  500/  500]    Overall Loss 0.828607    Objective Loss 0.828607    Top1 73.500000    Top5 96.000000    LR 0.000063    Time 0.087239    
2024-02-17 14:12:37,383 - --- validate (epoch=271)-----------
2024-02-17 14:12:37,384 - 10000 samples (100 per mini-batch)
2024-02-17 14:12:42,474 - Epoch: [271][  100/  100]    Loss 1.706022    Top1 57.110000    Top5 84.000000    
2024-02-17 14:12:42,588 - ==> Top1: 57.110    Top5: 84.000    Loss: 1.706

2024-02-17 14:12:42,599 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:12:42,600 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:12:42,651 - 

2024-02-17 14:12:42,651 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:12:51,820 - Epoch: [272][  100/  500]    Overall Loss 0.787880    Objective Loss 0.787880                                        LR 0.000063    Time 0.091639    
2024-02-17 14:13:00,748 - Epoch: [272][  200/  500]    Overall Loss 0.795106    Objective Loss 0.795106                                        LR 0.000063    Time 0.090438    
2024-02-17 14:13:09,564 - Epoch: [272][  300/  500]    Overall Loss 0.801771    Objective Loss 0.801771                                        LR 0.000063    Time 0.089666    
2024-02-17 14:13:18,363 - Epoch: [272][  400/  500]    Overall Loss 0.812687    Objective Loss 0.812687                                        LR 0.000063    Time 0.089238    
2024-02-17 14:13:27,348 - Epoch: [272][  500/  500]    Overall Loss 0.816641    Objective Loss 0.816641    Top1 75.500000    Top5 97.000000    LR 0.000063    Time 0.089354    
2024-02-17 14:13:27,448 - --- validate (epoch=272)-----------
2024-02-17 14:13:27,448 - 10000 samples (100 per mini-batch)
2024-02-17 14:13:33,108 - Epoch: [272][  100/  100]    Loss 1.784845    Top1 55.720000    Top5 83.380000    
2024-02-17 14:13:33,220 - ==> Top1: 55.720    Top5: 83.380    Loss: 1.785

2024-02-17 14:13:33,231 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:13:33,232 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:13:33,282 - 

2024-02-17 14:13:33,282 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:13:42,437 - Epoch: [273][  100/  500]    Overall Loss 0.813616    Objective Loss 0.813616                                        LR 0.000063    Time 0.091495    
2024-02-17 14:13:51,310 - Epoch: [273][  200/  500]    Overall Loss 0.820388    Objective Loss 0.820388                                        LR 0.000063    Time 0.090091    
2024-02-17 14:14:00,023 - Epoch: [273][  300/  500]    Overall Loss 0.822491    Objective Loss 0.822491                                        LR 0.000063    Time 0.089092    
2024-02-17 14:14:08,808 - Epoch: [273][  400/  500]    Overall Loss 0.834105    Objective Loss 0.834105                                        LR 0.000063    Time 0.088774    
2024-02-17 14:14:17,565 - Epoch: [273][  500/  500]    Overall Loss 0.833930    Objective Loss 0.833930    Top1 78.000000    Top5 98.500000    LR 0.000063    Time 0.088526    
2024-02-17 14:14:17,687 - --- validate (epoch=273)-----------
2024-02-17 14:14:17,687 - 10000 samples (100 per mini-batch)
2024-02-17 14:14:23,717 - Epoch: [273][  100/  100]    Loss 1.679380    Top1 57.450000    Top5 84.370000    
2024-02-17 14:14:23,814 - ==> Top1: 57.450    Top5: 84.370    Loss: 1.679

2024-02-17 14:14:23,825 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:14:23,826 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:14:23,877 - 

2024-02-17 14:14:23,877 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:14:33,131 - Epoch: [274][  100/  500]    Overall Loss 0.826900    Objective Loss 0.826900                                        LR 0.000063    Time 0.092487    
2024-02-17 14:14:41,706 - Epoch: [274][  200/  500]    Overall Loss 0.831165    Objective Loss 0.831165                                        LR 0.000063    Time 0.089098    
2024-02-17 14:14:50,018 - Epoch: [274][  300/  500]    Overall Loss 0.835881    Objective Loss 0.835881                                        LR 0.000063    Time 0.087095    
2024-02-17 14:14:58,571 - Epoch: [274][  400/  500]    Overall Loss 0.839448    Objective Loss 0.839448                                        LR 0.000063    Time 0.086696    
2024-02-17 14:15:07,294 - Epoch: [274][  500/  500]    Overall Loss 0.839769    Objective Loss 0.839769    Top1 70.000000    Top5 93.000000    LR 0.000063    Time 0.086794    
2024-02-17 14:15:07,416 - --- validate (epoch=274)-----------
2024-02-17 14:15:07,417 - 10000 samples (100 per mini-batch)
2024-02-17 14:15:13,493 - Epoch: [274][  100/  100]    Loss 1.715758    Top1 57.210000    Top5 83.630000    
2024-02-17 14:15:13,624 - ==> Top1: 57.210    Top5: 83.630    Loss: 1.716

2024-02-17 14:15:13,635 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:15:13,635 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:15:13,692 - 

2024-02-17 14:15:13,692 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:15:23,007 - Epoch: [275][  100/  500]    Overall Loss 0.801836    Objective Loss 0.801836                                        LR 0.000063    Time 0.093098    
2024-02-17 14:15:31,792 - Epoch: [275][  200/  500]    Overall Loss 0.808848    Objective Loss 0.808848                                        LR 0.000063    Time 0.090456    
2024-02-17 14:15:40,440 - Epoch: [275][  300/  500]    Overall Loss 0.811153    Objective Loss 0.811153                                        LR 0.000063    Time 0.089116    
2024-02-17 14:15:49,237 - Epoch: [275][  400/  500]    Overall Loss 0.821718    Objective Loss 0.821718                                        LR 0.000063    Time 0.088820    
2024-02-17 14:15:58,192 - Epoch: [275][  500/  500]    Overall Loss 0.828028    Objective Loss 0.828028    Top1 76.500000    Top5 97.000000    LR 0.000063    Time 0.088958    
2024-02-17 14:15:58,288 - --- validate (epoch=275)-----------
2024-02-17 14:15:58,290 - 10000 samples (100 per mini-batch)
2024-02-17 14:16:04,207 - Epoch: [275][  100/  100]    Loss 1.846611    Top1 55.250000    Top5 82.650000    
2024-02-17 14:16:04,326 - ==> Top1: 55.250    Top5: 82.650    Loss: 1.847

2024-02-17 14:16:04,337 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:16:04,337 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:16:04,388 - 

2024-02-17 14:16:04,388 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:16:13,864 - Epoch: [276][  100/  500]    Overall Loss 0.824597    Objective Loss 0.824597                                        LR 0.000063    Time 0.094711    
2024-02-17 14:16:22,775 - Epoch: [276][  200/  500]    Overall Loss 0.833112    Objective Loss 0.833112                                        LR 0.000063    Time 0.091888    
2024-02-17 14:16:31,476 - Epoch: [276][  300/  500]    Overall Loss 0.837324    Objective Loss 0.837324                                        LR 0.000063    Time 0.090250    
2024-02-17 14:16:40,163 - Epoch: [276][  400/  500]    Overall Loss 0.841776    Objective Loss 0.841776                                        LR 0.000063    Time 0.089396    
2024-02-17 14:16:48,876 - Epoch: [276][  500/  500]    Overall Loss 0.844843    Objective Loss 0.844843    Top1 76.000000    Top5 98.000000    LR 0.000063    Time 0.088934    
2024-02-17 14:16:49,026 - --- validate (epoch=276)-----------
2024-02-17 14:16:49,026 - 10000 samples (100 per mini-batch)
2024-02-17 14:16:54,408 - Epoch: [276][  100/  100]    Loss 1.702718    Top1 56.210000    Top5 84.040000    
2024-02-17 14:16:54,572 - ==> Top1: 56.210    Top5: 84.040    Loss: 1.703

2024-02-17 14:16:54,583 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:16:54,584 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:16:54,642 - 

2024-02-17 14:16:54,642 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:17:03,971 - Epoch: [277][  100/  500]    Overall Loss 0.799764    Objective Loss 0.799764                                        LR 0.000063    Time 0.093237    
2024-02-17 14:17:12,986 - Epoch: [277][  200/  500]    Overall Loss 0.819994    Objective Loss 0.819994                                        LR 0.000063    Time 0.091670    
2024-02-17 14:17:22,019 - Epoch: [277][  300/  500]    Overall Loss 0.829281    Objective Loss 0.829281                                        LR 0.000063    Time 0.091209    
2024-02-17 14:17:31,336 - Epoch: [277][  400/  500]    Overall Loss 0.836835    Objective Loss 0.836835                                        LR 0.000063    Time 0.091691    
2024-02-17 14:17:40,127 - Epoch: [277][  500/  500]    Overall Loss 0.837174    Objective Loss 0.837174    Top1 77.000000    Top5 96.500000    LR 0.000063    Time 0.090926    
2024-02-17 14:17:40,254 - --- validate (epoch=277)-----------
2024-02-17 14:17:40,254 - 10000 samples (100 per mini-batch)
2024-02-17 14:17:45,632 - Epoch: [277][  100/  100]    Loss 1.736226    Top1 56.570000    Top5 84.080000    
2024-02-17 14:17:45,773 - ==> Top1: 56.570    Top5: 84.080    Loss: 1.736

2024-02-17 14:17:45,779 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:17:45,780 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:17:45,825 - 

2024-02-17 14:17:45,825 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:17:55,269 - Epoch: [278][  100/  500]    Overall Loss 0.822232    Objective Loss 0.822232                                        LR 0.000063    Time 0.094374    
2024-02-17 14:18:04,155 - Epoch: [278][  200/  500]    Overall Loss 0.821614    Objective Loss 0.821614                                        LR 0.000063    Time 0.091597    
2024-02-17 14:18:13,088 - Epoch: [278][  300/  500]    Overall Loss 0.821581    Objective Loss 0.821581                                        LR 0.000063    Time 0.090828    
2024-02-17 14:18:21,965 - Epoch: [278][  400/  500]    Overall Loss 0.830131    Objective Loss 0.830131                                        LR 0.000063    Time 0.090304    
2024-02-17 14:18:30,895 - Epoch: [278][  500/  500]    Overall Loss 0.834910    Objective Loss 0.834910    Top1 74.500000    Top5 91.500000    LR 0.000063    Time 0.090095    
2024-02-17 14:18:31,085 - --- validate (epoch=278)-----------
2024-02-17 14:18:31,085 - 10000 samples (100 per mini-batch)
2024-02-17 14:18:37,199 - Epoch: [278][  100/  100]    Loss 1.675751    Top1 57.330000    Top5 84.330000    
2024-02-17 14:18:37,323 - ==> Top1: 57.330    Top5: 84.330    Loss: 1.676

2024-02-17 14:18:37,335 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:18:37,335 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:18:37,387 - 

2024-02-17 14:18:37,387 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:18:46,812 - Epoch: [279][  100/  500]    Overall Loss 0.808858    Objective Loss 0.808858                                        LR 0.000063    Time 0.094159    
2024-02-17 14:18:55,654 - Epoch: [279][  200/  500]    Overall Loss 0.814619    Objective Loss 0.814619                                        LR 0.000063    Time 0.091271    
2024-02-17 14:19:04,473 - Epoch: [279][  300/  500]    Overall Loss 0.812626    Objective Loss 0.812626                                        LR 0.000063    Time 0.090232    
2024-02-17 14:19:13,085 - Epoch: [279][  400/  500]    Overall Loss 0.826020    Objective Loss 0.826020                                        LR 0.000063    Time 0.089195    
2024-02-17 14:19:21,915 - Epoch: [279][  500/  500]    Overall Loss 0.833409    Objective Loss 0.833409    Top1 76.000000    Top5 94.000000    LR 0.000063    Time 0.089008    
2024-02-17 14:19:22,029 - --- validate (epoch=279)-----------
2024-02-17 14:19:22,030 - 10000 samples (100 per mini-batch)
2024-02-17 14:19:27,847 - Epoch: [279][  100/  100]    Loss 1.713956    Top1 56.430000    Top5 84.170000    
2024-02-17 14:19:27,948 - ==> Top1: 56.430    Top5: 84.170    Loss: 1.714

2024-02-17 14:19:27,957 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:19:27,958 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:19:28,012 - 

2024-02-17 14:19:28,012 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:19:37,687 - Epoch: [280][  100/  500]    Overall Loss 0.821177    Objective Loss 0.821177                                        LR 0.000063    Time 0.096694    
2024-02-17 14:19:46,608 - Epoch: [280][  200/  500]    Overall Loss 0.832355    Objective Loss 0.832355                                        LR 0.000063    Time 0.092932    
2024-02-17 14:19:55,307 - Epoch: [280][  300/  500]    Overall Loss 0.832108    Objective Loss 0.832108                                        LR 0.000063    Time 0.090938    
2024-02-17 14:20:04,020 - Epoch: [280][  400/  500]    Overall Loss 0.838777    Objective Loss 0.838777                                        LR 0.000063    Time 0.089977    
2024-02-17 14:20:12,497 - Epoch: [280][  500/  500]    Overall Loss 0.836241    Objective Loss 0.836241    Top1 73.000000    Top5 98.500000    LR 0.000063    Time 0.088928    
2024-02-17 14:20:12,618 - --- validate (epoch=280)-----------
2024-02-17 14:20:12,619 - 10000 samples (100 per mini-batch)
2024-02-17 14:20:18,435 - Epoch: [280][  100/  100]    Loss 1.636071    Top1 57.510000    Top5 84.540000    
2024-02-17 14:20:18,571 - ==> Top1: 57.510    Top5: 84.540    Loss: 1.636

2024-02-17 14:20:18,581 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:20:18,582 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:20:18,646 - 

2024-02-17 14:20:18,646 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:20:28,208 - Epoch: [281][  100/  500]    Overall Loss 0.795795    Objective Loss 0.795795                                        LR 0.000063    Time 0.095562    
2024-02-17 14:20:36,994 - Epoch: [281][  200/  500]    Overall Loss 0.824647    Objective Loss 0.824647                                        LR 0.000063    Time 0.091685    
2024-02-17 14:20:45,768 - Epoch: [281][  300/  500]    Overall Loss 0.823368    Objective Loss 0.823368                                        LR 0.000063    Time 0.090359    
2024-02-17 14:20:54,527 - Epoch: [281][  400/  500]    Overall Loss 0.822531    Objective Loss 0.822531                                        LR 0.000063    Time 0.089656    
2024-02-17 14:21:03,230 - Epoch: [281][  500/  500]    Overall Loss 0.827334    Objective Loss 0.827334    Top1 67.000000    Top5 94.000000    LR 0.000063    Time 0.089124    
2024-02-17 14:21:03,323 - --- validate (epoch=281)-----------
2024-02-17 14:21:03,324 - 10000 samples (100 per mini-batch)
2024-02-17 14:21:08,690 - Epoch: [281][  100/  100]    Loss 1.689035    Top1 57.260000    Top5 84.380000    
2024-02-17 14:21:08,807 - ==> Top1: 57.260    Top5: 84.380    Loss: 1.689

2024-02-17 14:21:08,818 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:21:08,818 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:21:08,872 - 

2024-02-17 14:21:08,872 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:21:18,216 - Epoch: [282][  100/  500]    Overall Loss 0.799900    Objective Loss 0.799900                                        LR 0.000063    Time 0.093381    
2024-02-17 14:21:27,143 - Epoch: [282][  200/  500]    Overall Loss 0.816708    Objective Loss 0.816708                                        LR 0.000063    Time 0.091306    
2024-02-17 14:21:36,013 - Epoch: [282][  300/  500]    Overall Loss 0.814767    Objective Loss 0.814767                                        LR 0.000063    Time 0.090423    
2024-02-17 14:21:44,668 - Epoch: [282][  400/  500]    Overall Loss 0.827920    Objective Loss 0.827920                                        LR 0.000063    Time 0.089447    
2024-02-17 14:21:53,363 - Epoch: [282][  500/  500]    Overall Loss 0.823381    Objective Loss 0.823381    Top1 76.500000    Top5 96.500000    LR 0.000063    Time 0.088941    
2024-02-17 14:21:53,484 - --- validate (epoch=282)-----------
2024-02-17 14:21:53,485 - 10000 samples (100 per mini-batch)
2024-02-17 14:21:59,239 - Epoch: [282][  100/  100]    Loss 1.705188    Top1 56.780000    Top5 83.960000    
2024-02-17 14:21:59,353 - ==> Top1: 56.780    Top5: 83.960    Loss: 1.705

2024-02-17 14:21:59,363 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:21:59,364 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:21:59,419 - 

2024-02-17 14:21:59,419 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:22:08,737 - Epoch: [283][  100/  500]    Overall Loss 0.800685    Objective Loss 0.800685                                        LR 0.000063    Time 0.093128    
2024-02-17 14:22:17,598 - Epoch: [283][  200/  500]    Overall Loss 0.803462    Objective Loss 0.803462                                        LR 0.000063    Time 0.090850    
2024-02-17 14:22:26,271 - Epoch: [283][  300/  500]    Overall Loss 0.815261    Objective Loss 0.815261                                        LR 0.000063    Time 0.089466    
2024-02-17 14:22:35,127 - Epoch: [283][  400/  500]    Overall Loss 0.819964    Objective Loss 0.819964                                        LR 0.000063    Time 0.089228    
2024-02-17 14:22:43,296 - Epoch: [283][  500/  500]    Overall Loss 0.820508    Objective Loss 0.820508    Top1 76.000000    Top5 98.000000    LR 0.000063    Time 0.087716    
2024-02-17 14:22:43,426 - --- validate (epoch=283)-----------
2024-02-17 14:22:43,427 - 10000 samples (100 per mini-batch)
2024-02-17 14:22:48,781 - Epoch: [283][  100/  100]    Loss 1.734397    Top1 56.900000    Top5 83.800000    
2024-02-17 14:22:48,974 - ==> Top1: 56.900    Top5: 83.800    Loss: 1.734

2024-02-17 14:22:48,985 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:22:48,985 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:22:49,038 - 

2024-02-17 14:22:49,038 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:22:58,879 - Epoch: [284][  100/  500]    Overall Loss 0.804171    Objective Loss 0.804171                                        LR 0.000063    Time 0.098361    
2024-02-17 14:23:07,731 - Epoch: [284][  200/  500]    Overall Loss 0.817947    Objective Loss 0.817947                                        LR 0.000063    Time 0.093419    
2024-02-17 14:23:16,432 - Epoch: [284][  300/  500]    Overall Loss 0.823996    Objective Loss 0.823996                                        LR 0.000063    Time 0.091270    
2024-02-17 14:23:25,316 - Epoch: [284][  400/  500]    Overall Loss 0.825795    Objective Loss 0.825795                                        LR 0.000063    Time 0.090652    
2024-02-17 14:23:34,113 - Epoch: [284][  500/  500]    Overall Loss 0.830154    Objective Loss 0.830154    Top1 80.000000    Top5 96.000000    LR 0.000063    Time 0.090108    
2024-02-17 14:23:34,248 - --- validate (epoch=284)-----------
2024-02-17 14:23:34,249 - 10000 samples (100 per mini-batch)
2024-02-17 14:23:39,649 - Epoch: [284][  100/  100]    Loss 1.701258    Top1 57.130000    Top5 84.180000    
2024-02-17 14:23:39,755 - ==> Top1: 57.130    Top5: 84.180    Loss: 1.701

2024-02-17 14:23:39,767 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:23:39,767 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:23:39,818 - 

2024-02-17 14:23:39,818 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:23:49,221 - Epoch: [285][  100/  500]    Overall Loss 0.804919    Objective Loss 0.804919                                        LR 0.000063    Time 0.093973    
2024-02-17 14:23:58,298 - Epoch: [285][  200/  500]    Overall Loss 0.805828    Objective Loss 0.805828                                        LR 0.000063    Time 0.092350    
2024-02-17 14:24:07,266 - Epoch: [285][  300/  500]    Overall Loss 0.819007    Objective Loss 0.819007                                        LR 0.000063    Time 0.091449    
2024-02-17 14:24:16,075 - Epoch: [285][  400/  500]    Overall Loss 0.824602    Objective Loss 0.824602                                        LR 0.000063    Time 0.090600    
2024-02-17 14:24:24,821 - Epoch: [285][  500/  500]    Overall Loss 0.832788    Objective Loss 0.832788    Top1 76.000000    Top5 94.500000    LR 0.000063    Time 0.089965    
2024-02-17 14:24:24,957 - --- validate (epoch=285)-----------
2024-02-17 14:24:24,958 - 10000 samples (100 per mini-batch)
2024-02-17 14:24:30,103 - Epoch: [285][  100/  100]    Loss 1.669694    Top1 58.050000    Top5 84.610000    
2024-02-17 14:24:30,202 - ==> Top1: 58.050    Top5: 84.610    Loss: 1.670

2024-02-17 14:24:30,211 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:24:30,211 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:24:30,253 - 

2024-02-17 14:24:30,254 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:24:39,501 - Epoch: [286][  100/  500]    Overall Loss 0.798006    Objective Loss 0.798006                                        LR 0.000063    Time 0.092427    
2024-02-17 14:24:47,809 - Epoch: [286][  200/  500]    Overall Loss 0.809458    Objective Loss 0.809458                                        LR 0.000063    Time 0.087741    
2024-02-17 14:24:56,553 - Epoch: [286][  300/  500]    Overall Loss 0.825995    Objective Loss 0.825995                                        LR 0.000063    Time 0.087625    
2024-02-17 14:25:05,493 - Epoch: [286][  400/  500]    Overall Loss 0.826033    Objective Loss 0.826033                                        LR 0.000063    Time 0.088059    
2024-02-17 14:25:14,417 - Epoch: [286][  500/  500]    Overall Loss 0.826003    Objective Loss 0.826003    Top1 68.500000    Top5 93.500000    LR 0.000063    Time 0.088286    
2024-02-17 14:25:14,578 - --- validate (epoch=286)-----------
2024-02-17 14:25:14,579 - 10000 samples (100 per mini-batch)
2024-02-17 14:25:19,827 - Epoch: [286][  100/  100]    Loss 1.667201    Top1 57.480000    Top5 84.450000    
2024-02-17 14:25:19,937 - ==> Top1: 57.480    Top5: 84.450    Loss: 1.667

2024-02-17 14:25:19,947 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:25:19,947 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:25:19,999 - 

2024-02-17 14:25:20,000 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:25:29,245 - Epoch: [287][  100/  500]    Overall Loss 0.810428    Objective Loss 0.810428                                        LR 0.000063    Time 0.092402    
2024-02-17 14:25:38,155 - Epoch: [287][  200/  500]    Overall Loss 0.817388    Objective Loss 0.817388                                        LR 0.000063    Time 0.090732    
2024-02-17 14:25:46,980 - Epoch: [287][  300/  500]    Overall Loss 0.820420    Objective Loss 0.820420                                        LR 0.000063    Time 0.089891    
2024-02-17 14:25:55,633 - Epoch: [287][  400/  500]    Overall Loss 0.821019    Objective Loss 0.821019                                        LR 0.000063    Time 0.089040    
2024-02-17 14:26:04,476 - Epoch: [287][  500/  500]    Overall Loss 0.822255    Objective Loss 0.822255    Top1 77.000000    Top5 97.500000    LR 0.000063    Time 0.088912    
2024-02-17 14:26:04,642 - --- validate (epoch=287)-----------
2024-02-17 14:26:04,642 - 10000 samples (100 per mini-batch)
2024-02-17 14:26:10,523 - Epoch: [287][  100/  100]    Loss 1.738902    Top1 56.210000    Top5 84.420000    
2024-02-17 14:26:10,623 - ==> Top1: 56.210    Top5: 84.420    Loss: 1.739

2024-02-17 14:26:10,634 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:26:10,635 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:26:10,689 - 

2024-02-17 14:26:10,689 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:26:20,081 - Epoch: [288][  100/  500]    Overall Loss 0.806490    Objective Loss 0.806490                                        LR 0.000063    Time 0.093869    
2024-02-17 14:26:28,901 - Epoch: [288][  200/  500]    Overall Loss 0.806438    Objective Loss 0.806438                                        LR 0.000063    Time 0.091014    
2024-02-17 14:26:37,315 - Epoch: [288][  300/  500]    Overall Loss 0.811759    Objective Loss 0.811759                                        LR 0.000063    Time 0.088713    
2024-02-17 14:26:46,443 - Epoch: [288][  400/  500]    Overall Loss 0.823174    Objective Loss 0.823174                                        LR 0.000063    Time 0.089346    
2024-02-17 14:26:55,194 - Epoch: [288][  500/  500]    Overall Loss 0.827558    Objective Loss 0.827558    Top1 70.500000    Top5 95.000000    LR 0.000063    Time 0.088970    
2024-02-17 14:26:55,386 - --- validate (epoch=288)-----------
2024-02-17 14:26:55,386 - 10000 samples (100 per mini-batch)
2024-02-17 14:27:00,900 - Epoch: [288][  100/  100]    Loss 1.727302    Top1 56.410000    Top5 83.860000    
2024-02-17 14:27:01,000 - ==> Top1: 56.410    Top5: 83.860    Loss: 1.727

2024-02-17 14:27:01,012 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:27:01,012 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:27:01,063 - 

2024-02-17 14:27:01,063 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:27:10,486 - Epoch: [289][  100/  500]    Overall Loss 0.784884    Objective Loss 0.784884                                        LR 0.000063    Time 0.094171    
2024-02-17 14:27:19,271 - Epoch: [289][  200/  500]    Overall Loss 0.803029    Objective Loss 0.803029                                        LR 0.000063    Time 0.090992    
2024-02-17 14:27:27,900 - Epoch: [289][  300/  500]    Overall Loss 0.811345    Objective Loss 0.811345                                        LR 0.000063    Time 0.089414    
2024-02-17 14:27:36,632 - Epoch: [289][  400/  500]    Overall Loss 0.816411    Objective Loss 0.816411                                        LR 0.000063    Time 0.088880    
2024-02-17 14:27:44,938 - Epoch: [289][  500/  500]    Overall Loss 0.819865    Objective Loss 0.819865    Top1 80.500000    Top5 97.000000    LR 0.000063    Time 0.087711    
2024-02-17 14:27:45,091 - --- validate (epoch=289)-----------
2024-02-17 14:27:45,092 - 10000 samples (100 per mini-batch)
2024-02-17 14:27:50,363 - Epoch: [289][  100/  100]    Loss 1.661886    Top1 57.110000    Top5 84.410000    
2024-02-17 14:27:50,469 - ==> Top1: 57.110    Top5: 84.410    Loss: 1.662

2024-02-17 14:27:50,480 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:27:50,480 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:27:50,531 - 

2024-02-17 14:27:50,532 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:27:59,960 - Epoch: [290][  100/  500]    Overall Loss 0.818738    Objective Loss 0.818738                                        LR 0.000063    Time 0.094235    
2024-02-17 14:28:08,917 - Epoch: [290][  200/  500]    Overall Loss 0.820877    Objective Loss 0.820877                                        LR 0.000063    Time 0.091882    
2024-02-17 14:28:17,736 - Epoch: [290][  300/  500]    Overall Loss 0.821142    Objective Loss 0.821142                                        LR 0.000063    Time 0.090640    
2024-02-17 14:28:26,548 - Epoch: [290][  400/  500]    Overall Loss 0.823528    Objective Loss 0.823528                                        LR 0.000063    Time 0.090001    
2024-02-17 14:28:35,356 - Epoch: [290][  500/  500]    Overall Loss 0.823600    Objective Loss 0.823600    Top1 75.000000    Top5 95.000000    LR 0.000063    Time 0.089608    
2024-02-17 14:28:35,448 - --- validate (epoch=290)-----------
2024-02-17 14:28:35,448 - 10000 samples (100 per mini-batch)
2024-02-17 14:28:41,121 - Epoch: [290][  100/  100]    Loss 1.737671    Top1 56.110000    Top5 83.480000    
2024-02-17 14:28:41,219 - ==> Top1: 56.110    Top5: 83.480    Loss: 1.738

2024-02-17 14:28:41,229 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:28:41,230 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:28:41,291 - 

2024-02-17 14:28:41,291 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:28:50,220 - Epoch: [291][  100/  500]    Overall Loss 0.811202    Objective Loss 0.811202                                        LR 0.000063    Time 0.089236    
2024-02-17 14:28:58,884 - Epoch: [291][  200/  500]    Overall Loss 0.809986    Objective Loss 0.809986                                        LR 0.000063    Time 0.087922    
2024-02-17 14:29:07,819 - Epoch: [291][  300/  500]    Overall Loss 0.808985    Objective Loss 0.808985                                        LR 0.000063    Time 0.088386    
2024-02-17 14:29:16,709 - Epoch: [291][  400/  500]    Overall Loss 0.813577    Objective Loss 0.813577                                        LR 0.000063    Time 0.088503    
2024-02-17 14:29:25,612 - Epoch: [291][  500/  500]    Overall Loss 0.823185    Objective Loss 0.823185    Top1 75.500000    Top5 94.000000    LR 0.000063    Time 0.088600    
2024-02-17 14:29:25,750 - --- validate (epoch=291)-----------
2024-02-17 14:29:25,751 - 10000 samples (100 per mini-batch)
2024-02-17 14:29:31,629 - Epoch: [291][  100/  100]    Loss 1.648312    Top1 57.700000    Top5 84.570000    
2024-02-17 14:29:31,725 - ==> Top1: 57.700    Top5: 84.570    Loss: 1.648

2024-02-17 14:29:31,735 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:29:31,735 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:29:31,791 - 

2024-02-17 14:29:31,791 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:29:41,412 - Epoch: [292][  100/  500]    Overall Loss 0.781506    Objective Loss 0.781506                                        LR 0.000063    Time 0.096154    
2024-02-17 14:29:50,177 - Epoch: [292][  200/  500]    Overall Loss 0.787585    Objective Loss 0.787585                                        LR 0.000063    Time 0.091883    
2024-02-17 14:29:59,156 - Epoch: [292][  300/  500]    Overall Loss 0.797174    Objective Loss 0.797174                                        LR 0.000063    Time 0.091174    
2024-02-17 14:30:07,970 - Epoch: [292][  400/  500]    Overall Loss 0.809245    Objective Loss 0.809245                                        LR 0.000063    Time 0.090405    
2024-02-17 14:30:16,831 - Epoch: [292][  500/  500]    Overall Loss 0.815586    Objective Loss 0.815586    Top1 71.000000    Top5 94.000000    LR 0.000063    Time 0.090039    
2024-02-17 14:30:16,986 - --- validate (epoch=292)-----------
2024-02-17 14:30:16,987 - 10000 samples (100 per mini-batch)
2024-02-17 14:30:22,392 - Epoch: [292][  100/  100]    Loss 1.662418    Top1 57.540000    Top5 84.540000    
2024-02-17 14:30:22,540 - ==> Top1: 57.540    Top5: 84.540    Loss: 1.662

2024-02-17 14:30:22,551 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:30:22,552 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:30:22,608 - 

2024-02-17 14:30:22,608 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:30:31,977 - Epoch: [293][  100/  500]    Overall Loss 0.784691    Objective Loss 0.784691                                        LR 0.000063    Time 0.093635    
2024-02-17 14:30:40,878 - Epoch: [293][  200/  500]    Overall Loss 0.801684    Objective Loss 0.801684                                        LR 0.000063    Time 0.091300    
2024-02-17 14:30:49,820 - Epoch: [293][  300/  500]    Overall Loss 0.816972    Objective Loss 0.816972                                        LR 0.000063    Time 0.090661    
2024-02-17 14:30:58,601 - Epoch: [293][  400/  500]    Overall Loss 0.822648    Objective Loss 0.822648                                        LR 0.000063    Time 0.089940    
2024-02-17 14:31:07,150 - Epoch: [293][  500/  500]    Overall Loss 0.826131    Objective Loss 0.826131    Top1 76.500000    Top5 97.000000    LR 0.000063    Time 0.089044    
2024-02-17 14:31:07,279 - --- validate (epoch=293)-----------
2024-02-17 14:31:07,279 - 10000 samples (100 per mini-batch)
2024-02-17 14:31:12,604 - Epoch: [293][  100/  100]    Loss 1.732351    Top1 56.510000    Top5 83.960000    
2024-02-17 14:31:12,704 - ==> Top1: 56.510    Top5: 83.960    Loss: 1.732

2024-02-17 14:31:12,715 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:31:12,716 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:31:12,770 - 

2024-02-17 14:31:12,770 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:31:22,227 - Epoch: [294][  100/  500]    Overall Loss 0.793658    Objective Loss 0.793658                                        LR 0.000063    Time 0.094520    
2024-02-17 14:31:31,263 - Epoch: [294][  200/  500]    Overall Loss 0.816941    Objective Loss 0.816941                                        LR 0.000063    Time 0.092418    
2024-02-17 14:31:40,254 - Epoch: [294][  300/  500]    Overall Loss 0.816221    Objective Loss 0.816221                                        LR 0.000063    Time 0.091571    
2024-02-17 14:31:49,246 - Epoch: [294][  400/  500]    Overall Loss 0.825939    Objective Loss 0.825939                                        LR 0.000063    Time 0.091146    
2024-02-17 14:31:58,272 - Epoch: [294][  500/  500]    Overall Loss 0.825459    Objective Loss 0.825459    Top1 79.000000    Top5 95.000000    LR 0.000063    Time 0.090961    
2024-02-17 14:31:58,463 - --- validate (epoch=294)-----------
2024-02-17 14:31:58,464 - 10000 samples (100 per mini-batch)
2024-02-17 14:32:03,820 - Epoch: [294][  100/  100]    Loss 1.766079    Top1 56.060000    Top5 83.670000    
2024-02-17 14:32:03,920 - ==> Top1: 56.060    Top5: 83.670    Loss: 1.766

2024-02-17 14:32:03,931 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:32:03,932 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:32:03,983 - 

2024-02-17 14:32:03,983 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:32:13,476 - Epoch: [295][  100/  500]    Overall Loss 0.804865    Objective Loss 0.804865                                        LR 0.000063    Time 0.094876    
2024-02-17 14:32:22,199 - Epoch: [295][  200/  500]    Overall Loss 0.800705    Objective Loss 0.800705                                        LR 0.000063    Time 0.091033    
2024-02-17 14:32:31,124 - Epoch: [295][  300/  500]    Overall Loss 0.808973    Objective Loss 0.808973                                        LR 0.000063    Time 0.090425    
2024-02-17 14:32:40,017 - Epoch: [295][  400/  500]    Overall Loss 0.817387    Objective Loss 0.817387                                        LR 0.000063    Time 0.090041    
2024-02-17 14:32:48,386 - Epoch: [295][  500/  500]    Overall Loss 0.819670    Objective Loss 0.819670    Top1 70.500000    Top5 95.500000    LR 0.000063    Time 0.088766    
2024-02-17 14:32:48,532 - --- validate (epoch=295)-----------
2024-02-17 14:32:48,533 - 10000 samples (100 per mini-batch)
2024-02-17 14:32:53,964 - Epoch: [295][  100/  100]    Loss 1.703854    Top1 57.100000    Top5 84.120000    
2024-02-17 14:32:54,092 - ==> Top1: 57.100    Top5: 84.120    Loss: 1.704

2024-02-17 14:32:54,102 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:32:54,103 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:32:54,154 - 

2024-02-17 14:32:54,155 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:33:03,486 - Epoch: [296][  100/  500]    Overall Loss 0.783173    Objective Loss 0.783173                                        LR 0.000063    Time 0.093257    
2024-02-17 14:33:12,426 - Epoch: [296][  200/  500]    Overall Loss 0.788514    Objective Loss 0.788514                                        LR 0.000063    Time 0.091309    
2024-02-17 14:33:21,383 - Epoch: [296][  300/  500]    Overall Loss 0.800844    Objective Loss 0.800844                                        LR 0.000063    Time 0.090716    
2024-02-17 14:33:30,344 - Epoch: [296][  400/  500]    Overall Loss 0.803457    Objective Loss 0.803457                                        LR 0.000063    Time 0.090429    
2024-02-17 14:33:39,214 - Epoch: [296][  500/  500]    Overall Loss 0.813440    Objective Loss 0.813440    Top1 81.500000    Top5 97.000000    LR 0.000063    Time 0.090074    
2024-02-17 14:33:39,311 - --- validate (epoch=296)-----------
2024-02-17 14:33:39,312 - 10000 samples (100 per mini-batch)
2024-02-17 14:33:44,855 - Epoch: [296][  100/  100]    Loss 1.861179    Top1 53.750000    Top5 82.150000    
2024-02-17 14:33:44,973 - ==> Top1: 53.750    Top5: 82.150    Loss: 1.861

2024-02-17 14:33:44,978 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:33:44,979 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:33:45,028 - 

2024-02-17 14:33:45,028 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:33:54,430 - Epoch: [297][  100/  500]    Overall Loss 0.786562    Objective Loss 0.786562                                        LR 0.000063    Time 0.093974    
2024-02-17 14:34:03,319 - Epoch: [297][  200/  500]    Overall Loss 0.787291    Objective Loss 0.787291                                        LR 0.000063    Time 0.091411    
2024-02-17 14:34:12,197 - Epoch: [297][  300/  500]    Overall Loss 0.798322    Objective Loss 0.798322                                        LR 0.000063    Time 0.090521    
2024-02-17 14:34:20,920 - Epoch: [297][  400/  500]    Overall Loss 0.803348    Objective Loss 0.803348                                        LR 0.000063    Time 0.089688    
2024-02-17 14:34:29,407 - Epoch: [297][  500/  500]    Overall Loss 0.813213    Objective Loss 0.813213    Top1 74.000000    Top5 98.000000    LR 0.000063    Time 0.088717    
2024-02-17 14:34:29,569 - --- validate (epoch=297)-----------
2024-02-17 14:34:29,569 - 10000 samples (100 per mini-batch)
2024-02-17 14:34:35,299 - Epoch: [297][  100/  100]    Loss 1.607378    Top1 58.090000    Top5 84.910000    
2024-02-17 14:34:35,410 - ==> Top1: 58.090    Top5: 84.910    Loss: 1.607

2024-02-17 14:34:35,420 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:34:35,420 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:34:35,477 - 

2024-02-17 14:34:35,477 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:34:45,058 - Epoch: [298][  100/  500]    Overall Loss 0.783780    Objective Loss 0.783780                                        LR 0.000063    Time 0.095751    
2024-02-17 14:34:53,600 - Epoch: [298][  200/  500]    Overall Loss 0.805611    Objective Loss 0.805611                                        LR 0.000063    Time 0.090571    
2024-02-17 14:35:02,230 - Epoch: [298][  300/  500]    Overall Loss 0.812969    Objective Loss 0.812969                                        LR 0.000063    Time 0.089132    
2024-02-17 14:35:10,774 - Epoch: [298][  400/  500]    Overall Loss 0.816581    Objective Loss 0.816581                                        LR 0.000063    Time 0.088203    
2024-02-17 14:35:19,179 - Epoch: [298][  500/  500]    Overall Loss 0.826126    Objective Loss 0.826126    Top1 74.500000    Top5 94.000000    LR 0.000063    Time 0.087366    
2024-02-17 14:35:19,305 - --- validate (epoch=298)-----------
2024-02-17 14:35:19,306 - 10000 samples (100 per mini-batch)
2024-02-17 14:35:24,620 - Epoch: [298][  100/  100]    Loss 1.694659    Top1 57.410000    Top5 84.130000    
2024-02-17 14:35:24,735 - ==> Top1: 57.410    Top5: 84.130    Loss: 1.695

2024-02-17 14:35:24,748 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:35:24,748 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:35:24,803 - 

2024-02-17 14:35:24,803 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:35:34,160 - Epoch: [299][  100/  500]    Overall Loss 0.795295    Objective Loss 0.795295                                        LR 0.000063    Time 0.093519    
2024-02-17 14:35:43,129 - Epoch: [299][  200/  500]    Overall Loss 0.813348    Objective Loss 0.813348                                        LR 0.000063    Time 0.091580    
2024-02-17 14:35:52,122 - Epoch: [299][  300/  500]    Overall Loss 0.806298    Objective Loss 0.806298                                        LR 0.000063    Time 0.091020    
2024-02-17 14:36:01,104 - Epoch: [299][  400/  500]    Overall Loss 0.821175    Objective Loss 0.821175                                        LR 0.000063    Time 0.090710    
2024-02-17 14:36:10,019 - Epoch: [299][  500/  500]    Overall Loss 0.820681    Objective Loss 0.820681    Top1 76.500000    Top5 96.000000    LR 0.000063    Time 0.090389    
2024-02-17 14:36:10,130 - --- validate (epoch=299)-----------
2024-02-17 14:36:10,131 - 10000 samples (100 per mini-batch)
2024-02-17 14:36:15,043 - Epoch: [299][  100/  100]    Loss 1.693079    Top1 57.080000    Top5 84.290000    
2024-02-17 14:36:15,146 - ==> Top1: 57.080    Top5: 84.290    Loss: 1.693

2024-02-17 14:36:15,156 - ==> Best [Top1: 58.100   Top5: 85.090   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 14:36:15,156 - Saving checkpoint to: logs/2024.02.17-110132/qat_checkpoint.pth.tar
2024-02-17 14:36:15,202 - --- test ---------------------
2024-02-17 14:36:15,202 - 10000 samples (100 per mini-batch)
2024-02-17 14:36:19,766 - Test: [  100/  100]    Loss 1.693079    Top1 57.080000    Top5 84.290000    
2024-02-17 14:36:19,857 - ==> Top1: 57.080    Top5: 84.290    Loss: 1.693

2024-02-17 14:36:19,869 - 
2024-02-17 14:36:19,869 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.02.17-110132/2024.02.17-110132.log
