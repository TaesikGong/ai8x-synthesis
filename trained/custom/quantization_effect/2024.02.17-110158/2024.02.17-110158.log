2024-02-17 11:01:58,271 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.02.17-110158/2024.02.17-110158.log
2024-02-17 11:02:03,560 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2024-02-17 11:02:03,561 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2024-02-17 11:02:06,048 - Dataset sizes:
	training=50000
	validation=10000
	test=10000
2024-02-17 11:02:06,050 - Reading compression schedule from: policies/schedule-cifar100-effnet2.yaml
2024-02-17 11:02:06,065 - 

2024-02-17 11:02:06,066 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:02:14,535 - Epoch: [0][  100/  500]    Overall Loss 4.275056    Objective Loss 4.275056                                        LR 0.001000    Time 0.084638    
2024-02-17 11:02:21,576 - Epoch: [0][  200/  500]    Overall Loss 4.121238    Objective Loss 4.121238                                        LR 0.001000    Time 0.077496    
2024-02-17 11:02:28,397 - Epoch: [0][  300/  500]    Overall Loss 4.013989    Objective Loss 4.013989                                        LR 0.001000    Time 0.074387    
2024-02-17 11:02:35,503 - Epoch: [0][  400/  500]    Overall Loss 3.929156    Objective Loss 3.929156                                        LR 0.001000    Time 0.073542    
2024-02-17 11:02:42,767 - Epoch: [0][  500/  500]    Overall Loss 3.866253    Objective Loss 3.866253    Top1 11.500000    Top5 46.000000    LR 0.001000    Time 0.073352    
2024-02-17 11:02:42,871 - --- validate (epoch=0)-----------
2024-02-17 11:02:42,871 - 10000 samples (100 per mini-batch)
2024-02-17 11:02:46,090 - Epoch: [0][  100/  100]    Loss 6.616838    Top1 1.510000    Top5 6.210000    
2024-02-17 11:02:46,193 - ==> Top1: 1.510    Top5: 6.210    Loss: 6.617

2024-02-17 11:02:46,199 - ==> Best [Top1: 1.510   Top5: 6.210   Sparsity:0.00   Params: 753952 on epoch: 0]
2024-02-17 11:02:46,199 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:02:46,265 - 

2024-02-17 11:02:46,265 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:02:53,984 - Epoch: [1][  100/  500]    Overall Loss 3.500466    Objective Loss 3.500466                                        LR 0.001000    Time 0.077128    
2024-02-17 11:03:01,051 - Epoch: [1][  200/  500]    Overall Loss 3.457511    Objective Loss 3.457511                                        LR 0.001000    Time 0.073876    
2024-02-17 11:03:08,112 - Epoch: [1][  300/  500]    Overall Loss 3.418027    Objective Loss 3.418027                                        LR 0.001000    Time 0.072771    
2024-02-17 11:03:15,228 - Epoch: [1][  400/  500]    Overall Loss 3.380266    Objective Loss 3.380266                                        LR 0.001000    Time 0.072358    
2024-02-17 11:03:22,362 - Epoch: [1][  500/  500]    Overall Loss 3.341529    Objective Loss 3.341529    Top1 16.000000    Top5 45.000000    LR 0.001000    Time 0.072146    
2024-02-17 11:03:22,473 - --- validate (epoch=1)-----------
2024-02-17 11:03:22,474 - 10000 samples (100 per mini-batch)
2024-02-17 11:03:25,460 - Epoch: [1][  100/  100]    Loss 3.431657    Top1 17.000000    Top5 44.840000    
2024-02-17 11:03:25,587 - ==> Top1: 17.000    Top5: 44.840    Loss: 3.432

2024-02-17 11:03:25,598 - ==> Best [Top1: 17.000   Top5: 44.840   Sparsity:0.00   Params: 753952 on epoch: 1]
2024-02-17 11:03:25,598 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:03:25,686 - 

2024-02-17 11:03:25,686 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:03:33,312 - Epoch: [2][  100/  500]    Overall Loss 3.099394    Objective Loss 3.099394                                        LR 0.001000    Time 0.076193    
2024-02-17 11:03:40,455 - Epoch: [2][  200/  500]    Overall Loss 3.083873    Objective Loss 3.083873                                        LR 0.001000    Time 0.073790    
2024-02-17 11:03:47,555 - Epoch: [2][  300/  500]    Overall Loss 3.067328    Objective Loss 3.067328                                        LR 0.001000    Time 0.072844    
2024-02-17 11:03:54,646 - Epoch: [2][  400/  500]    Overall Loss 3.046143    Objective Loss 3.046143                                        LR 0.001000    Time 0.072350    
2024-02-17 11:04:01,818 - Epoch: [2][  500/  500]    Overall Loss 3.019589    Objective Loss 3.019589    Top1 23.000000    Top5 60.000000    LR 0.001000    Time 0.072215    
2024-02-17 11:04:01,919 - --- validate (epoch=2)-----------
2024-02-17 11:04:01,920 - 10000 samples (100 per mini-batch)
2024-02-17 11:04:05,183 - Epoch: [2][  100/  100]    Loss 3.225702    Top1 22.180000    Top5 51.550000    
2024-02-17 11:04:05,329 - ==> Top1: 22.180    Top5: 51.550    Loss: 3.226

2024-02-17 11:04:05,339 - ==> Best [Top1: 22.180   Top5: 51.550   Sparsity:0.00   Params: 753952 on epoch: 2]
2024-02-17 11:04:05,339 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:04:05,412 - 

2024-02-17 11:04:05,413 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:04:13,341 - Epoch: [3][  100/  500]    Overall Loss 2.865246    Objective Loss 2.865246                                        LR 0.001000    Time 0.079227    
2024-02-17 11:04:20,648 - Epoch: [3][  200/  500]    Overall Loss 2.844952    Objective Loss 2.844952                                        LR 0.001000    Time 0.076125    
2024-02-17 11:04:27,791 - Epoch: [3][  300/  500]    Overall Loss 2.833951    Objective Loss 2.833951                                        LR 0.001000    Time 0.074545    
2024-02-17 11:04:34,894 - Epoch: [3][  400/  500]    Overall Loss 2.819812    Objective Loss 2.819812                                        LR 0.001000    Time 0.073654    
2024-02-17 11:04:42,014 - Epoch: [3][  500/  500]    Overall Loss 2.805881    Objective Loss 2.805881    Top1 25.000000    Top5 62.000000    LR 0.001000    Time 0.073155    
2024-02-17 11:04:42,123 - --- validate (epoch=3)-----------
2024-02-17 11:04:42,123 - 10000 samples (100 per mini-batch)
2024-02-17 11:04:45,141 - Epoch: [3][  100/  100]    Loss 3.079837    Top1 24.790000    Top5 56.140000    
2024-02-17 11:04:45,303 - ==> Top1: 24.790    Top5: 56.140    Loss: 3.080

2024-02-17 11:04:45,313 - ==> Best [Top1: 24.790   Top5: 56.140   Sparsity:0.00   Params: 753952 on epoch: 3]
2024-02-17 11:04:45,313 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:04:45,389 - 

2024-02-17 11:04:45,390 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:04:53,178 - Epoch: [4][  100/  500]    Overall Loss 2.697732    Objective Loss 2.697732                                        LR 0.001000    Time 0.077821    
2024-02-17 11:05:00,371 - Epoch: [4][  200/  500]    Overall Loss 2.681969    Objective Loss 2.681969                                        LR 0.001000    Time 0.074852    
2024-02-17 11:05:07,493 - Epoch: [4][  300/  500]    Overall Loss 2.668564    Objective Loss 2.668564                                        LR 0.001000    Time 0.073627    
2024-02-17 11:05:14,562 - Epoch: [4][  400/  500]    Overall Loss 2.648206    Objective Loss 2.648206                                        LR 0.001000    Time 0.072878    
2024-02-17 11:05:21,655 - Epoch: [4][  500/  500]    Overall Loss 2.628857    Objective Loss 2.628857    Top1 30.000000    Top5 64.500000    LR 0.001000    Time 0.072480    
2024-02-17 11:05:21,822 - --- validate (epoch=4)-----------
2024-02-17 11:05:21,823 - 10000 samples (100 per mini-batch)
2024-02-17 11:05:24,748 - Epoch: [4][  100/  100]    Loss 2.708755    Top1 30.410000    Top5 63.190000    
2024-02-17 11:05:24,931 - ==> Top1: 30.410    Top5: 63.190    Loss: 2.709

2024-02-17 11:05:24,942 - ==> Best [Top1: 30.410   Top5: 63.190   Sparsity:0.00   Params: 753952 on epoch: 4]
2024-02-17 11:05:24,942 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:05:25,019 - 

2024-02-17 11:05:25,019 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:05:32,550 - Epoch: [5][  100/  500]    Overall Loss 2.506355    Objective Loss 2.506355                                        LR 0.001000    Time 0.075255    
2024-02-17 11:05:39,587 - Epoch: [5][  200/  500]    Overall Loss 2.506461    Objective Loss 2.506461                                        LR 0.001000    Time 0.072791    
2024-02-17 11:05:46,625 - Epoch: [5][  300/  500]    Overall Loss 2.498337    Objective Loss 2.498337                                        LR 0.001000    Time 0.071973    
2024-02-17 11:05:53,685 - Epoch: [5][  400/  500]    Overall Loss 2.493585    Objective Loss 2.493585                                        LR 0.001000    Time 0.071618    
2024-02-17 11:06:00,776 - Epoch: [5][  500/  500]    Overall Loss 2.480782    Objective Loss 2.480782    Top1 38.000000    Top5 68.500000    LR 0.001000    Time 0.071467    
2024-02-17 11:06:00,991 - --- validate (epoch=5)-----------
2024-02-17 11:06:00,991 - 10000 samples (100 per mini-batch)
2024-02-17 11:06:04,064 - Epoch: [5][  100/  100]    Loss 2.529645    Top1 34.250000    Top5 67.130000    
2024-02-17 11:06:04,163 - ==> Top1: 34.250    Top5: 67.130    Loss: 2.530

2024-02-17 11:06:04,174 - ==> Best [Top1: 34.250   Top5: 67.130   Sparsity:0.00   Params: 753952 on epoch: 5]
2024-02-17 11:06:04,175 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:06:04,251 - 

2024-02-17 11:06:04,251 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:06:11,725 - Epoch: [6][  100/  500]    Overall Loss 2.366514    Objective Loss 2.366514                                        LR 0.001000    Time 0.074685    
2024-02-17 11:06:18,790 - Epoch: [6][  200/  500]    Overall Loss 2.383490    Objective Loss 2.383490                                        LR 0.001000    Time 0.072645    
2024-02-17 11:06:25,965 - Epoch: [6][  300/  500]    Overall Loss 2.374303    Objective Loss 2.374303                                        LR 0.001000    Time 0.072332    
2024-02-17 11:06:32,986 - Epoch: [6][  400/  500]    Overall Loss 2.364162    Objective Loss 2.364162                                        LR 0.001000    Time 0.071792    
2024-02-17 11:06:40,109 - Epoch: [6][  500/  500]    Overall Loss 2.355942    Objective Loss 2.355942    Top1 34.500000    Top5 65.000000    LR 0.001000    Time 0.071670    
2024-02-17 11:06:40,220 - --- validate (epoch=6)-----------
2024-02-17 11:06:40,221 - 10000 samples (100 per mini-batch)
2024-02-17 11:06:43,545 - Epoch: [6][  100/  100]    Loss 2.720212    Top1 32.220000    Top5 64.090000    
2024-02-17 11:06:43,653 - ==> Top1: 32.220    Top5: 64.090    Loss: 2.720

2024-02-17 11:06:43,664 - ==> Best [Top1: 34.250   Top5: 67.130   Sparsity:0.00   Params: 753952 on epoch: 5]
2024-02-17 11:06:43,664 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:06:43,729 - 

2024-02-17 11:06:43,729 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:06:51,269 - Epoch: [7][  100/  500]    Overall Loss 2.270916    Objective Loss 2.270916                                        LR 0.001000    Time 0.075327    
2024-02-17 11:06:58,539 - Epoch: [7][  200/  500]    Overall Loss 2.274110    Objective Loss 2.274110                                        LR 0.001000    Time 0.073988    
2024-02-17 11:07:05,620 - Epoch: [7][  300/  500]    Overall Loss 2.270804    Objective Loss 2.270804                                        LR 0.001000    Time 0.072915    
2024-02-17 11:07:12,622 - Epoch: [7][  400/  500]    Overall Loss 2.270610    Objective Loss 2.270610                                        LR 0.001000    Time 0.072180    
2024-02-17 11:07:19,727 - Epoch: [7][  500/  500]    Overall Loss 2.261209    Objective Loss 2.261209    Top1 35.000000    Top5 70.500000    LR 0.001000    Time 0.071947    
2024-02-17 11:07:19,904 - --- validate (epoch=7)-----------
2024-02-17 11:07:19,905 - 10000 samples (100 per mini-batch)
2024-02-17 11:07:22,897 - Epoch: [7][  100/  100]    Loss 2.439252    Top1 37.610000    Top5 69.580000    
2024-02-17 11:07:23,022 - ==> Top1: 37.610    Top5: 69.580    Loss: 2.439

2024-02-17 11:07:23,032 - ==> Best [Top1: 37.610   Top5: 69.580   Sparsity:0.00   Params: 753952 on epoch: 7]
2024-02-17 11:07:23,032 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:07:23,114 - 

2024-02-17 11:07:23,114 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:07:30,966 - Epoch: [8][  100/  500]    Overall Loss 2.151179    Objective Loss 2.151179                                        LR 0.001000    Time 0.078461    
2024-02-17 11:07:38,142 - Epoch: [8][  200/  500]    Overall Loss 2.173651    Objective Loss 2.173651                                        LR 0.001000    Time 0.075084    
2024-02-17 11:07:45,285 - Epoch: [8][  300/  500]    Overall Loss 2.162682    Objective Loss 2.162682                                        LR 0.001000    Time 0.073852    
2024-02-17 11:07:52,356 - Epoch: [8][  400/  500]    Overall Loss 2.168870    Objective Loss 2.168870                                        LR 0.001000    Time 0.073056    
2024-02-17 11:07:59,478 - Epoch: [8][  500/  500]    Overall Loss 2.163665    Objective Loss 2.163665    Top1 41.000000    Top5 75.500000    LR 0.001000    Time 0.072679    
2024-02-17 11:07:59,641 - --- validate (epoch=8)-----------
2024-02-17 11:07:59,643 - 10000 samples (100 per mini-batch)
2024-02-17 11:08:02,689 - Epoch: [8][  100/  100]    Loss 2.393116    Top1 38.040000    Top5 70.480000    
2024-02-17 11:08:02,794 - ==> Top1: 38.040    Top5: 70.480    Loss: 2.393

2024-02-17 11:08:02,803 - ==> Best [Top1: 38.040   Top5: 70.480   Sparsity:0.00   Params: 753952 on epoch: 8]
2024-02-17 11:08:02,803 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:08:02,887 - 

2024-02-17 11:08:02,887 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:08:10,523 - Epoch: [9][  100/  500]    Overall Loss 2.059027    Objective Loss 2.059027                                        LR 0.001000    Time 0.076303    
2024-02-17 11:08:17,672 - Epoch: [9][  200/  500]    Overall Loss 2.077117    Objective Loss 2.077117                                        LR 0.001000    Time 0.073875    
2024-02-17 11:08:24,723 - Epoch: [9][  300/  500]    Overall Loss 2.086387    Objective Loss 2.086387                                        LR 0.001000    Time 0.072736    
2024-02-17 11:08:31,966 - Epoch: [9][  400/  500]    Overall Loss 2.082988    Objective Loss 2.082988                                        LR 0.001000    Time 0.072651    
2024-02-17 11:08:39,169 - Epoch: [9][  500/  500]    Overall Loss 2.083254    Objective Loss 2.083254    Top1 47.000000    Top5 78.000000    LR 0.001000    Time 0.072518    
2024-02-17 11:08:39,318 - --- validate (epoch=9)-----------
2024-02-17 11:08:39,319 - 10000 samples (100 per mini-batch)
2024-02-17 11:08:42,284 - Epoch: [9][  100/  100]    Loss 2.423922    Top1 39.470000    Top5 70.630000    
2024-02-17 11:08:42,383 - ==> Top1: 39.470    Top5: 70.630    Loss: 2.424

2024-02-17 11:08:42,392 - ==> Best [Top1: 39.470   Top5: 70.630   Sparsity:0.00   Params: 753952 on epoch: 9]
2024-02-17 11:08:42,393 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:08:42,482 - 

2024-02-17 11:08:42,482 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:08:50,556 - Epoch: [10][  100/  500]    Overall Loss 2.034494    Objective Loss 2.034494                                        LR 0.001000    Time 0.080681    
2024-02-17 11:08:57,939 - Epoch: [10][  200/  500]    Overall Loss 2.030089    Objective Loss 2.030089                                        LR 0.001000    Time 0.077236    
2024-02-17 11:09:05,039 - Epoch: [10][  300/  500]    Overall Loss 2.026545    Objective Loss 2.026545                                        LR 0.001000    Time 0.075143    
2024-02-17 11:09:12,001 - Epoch: [10][  400/  500]    Overall Loss 2.018472    Objective Loss 2.018472                                        LR 0.001000    Time 0.073751    
2024-02-17 11:09:19,109 - Epoch: [10][  500/  500]    Overall Loss 2.021649    Objective Loss 2.021649    Top1 43.000000    Top5 74.500000    LR 0.001000    Time 0.073208    
2024-02-17 11:09:19,230 - --- validate (epoch=10)-----------
2024-02-17 11:09:19,231 - 10000 samples (100 per mini-batch)
2024-02-17 11:09:22,288 - Epoch: [10][  100/  100]    Loss 2.272644    Top1 40.380000    Top5 72.820000    
2024-02-17 11:09:22,414 - ==> Top1: 40.380    Top5: 72.820    Loss: 2.273

2024-02-17 11:09:22,420 - ==> Best [Top1: 40.380   Top5: 72.820   Sparsity:0.00   Params: 753952 on epoch: 10]
2024-02-17 11:09:22,420 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:09:22,493 - 

2024-02-17 11:09:22,494 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:09:30,116 - Epoch: [11][  100/  500]    Overall Loss 1.962647    Objective Loss 1.962647                                        LR 0.001000    Time 0.076163    
2024-02-17 11:09:37,321 - Epoch: [11][  200/  500]    Overall Loss 1.956996    Objective Loss 1.956996                                        LR 0.001000    Time 0.074083    
2024-02-17 11:09:44,552 - Epoch: [11][  300/  500]    Overall Loss 1.955405    Objective Loss 1.955405                                        LR 0.001000    Time 0.073476    
2024-02-17 11:09:51,662 - Epoch: [11][  400/  500]    Overall Loss 1.954840    Objective Loss 1.954840                                        LR 0.001000    Time 0.072871    
2024-02-17 11:09:58,740 - Epoch: [11][  500/  500]    Overall Loss 1.954866    Objective Loss 1.954866    Top1 53.000000    Top5 78.500000    LR 0.001000    Time 0.072444    
2024-02-17 11:09:58,841 - --- validate (epoch=11)-----------
2024-02-17 11:09:58,841 - 10000 samples (100 per mini-batch)
2024-02-17 11:10:01,884 - Epoch: [11][  100/  100]    Loss 2.191544    Top1 42.860000    Top5 74.630000    
2024-02-17 11:10:02,030 - ==> Top1: 42.860    Top5: 74.630    Loss: 2.192

2024-02-17 11:10:02,035 - ==> Best [Top1: 42.860   Top5: 74.630   Sparsity:0.00   Params: 753952 on epoch: 11]
2024-02-17 11:10:02,035 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:10:02,106 - 

2024-02-17 11:10:02,106 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:10:10,130 - Epoch: [12][  100/  500]    Overall Loss 1.905880    Objective Loss 1.905880                                        LR 0.001000    Time 0.080177    
2024-02-17 11:10:17,324 - Epoch: [12][  200/  500]    Overall Loss 1.911123    Objective Loss 1.911123                                        LR 0.001000    Time 0.076037    
2024-02-17 11:10:24,518 - Epoch: [12][  300/  500]    Overall Loss 1.905539    Objective Loss 1.905539                                        LR 0.001000    Time 0.074659    
2024-02-17 11:10:31,738 - Epoch: [12][  400/  500]    Overall Loss 1.901324    Objective Loss 1.901324                                        LR 0.001000    Time 0.074032    
2024-02-17 11:10:38,806 - Epoch: [12][  500/  500]    Overall Loss 1.901744    Objective Loss 1.901744    Top1 49.000000    Top5 80.000000    LR 0.001000    Time 0.073354    
2024-02-17 11:10:38,942 - --- validate (epoch=12)-----------
2024-02-17 11:10:38,943 - 10000 samples (100 per mini-batch)
2024-02-17 11:10:42,187 - Epoch: [12][  100/  100]    Loss 2.092381    Top1 45.170000    Top5 76.320000    
2024-02-17 11:10:42,338 - ==> Top1: 45.170    Top5: 76.320    Loss: 2.092

2024-02-17 11:10:42,348 - ==> Best [Top1: 45.170   Top5: 76.320   Sparsity:0.00   Params: 753952 on epoch: 12]
2024-02-17 11:10:42,348 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:10:42,429 - 

2024-02-17 11:10:42,430 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:10:49,988 - Epoch: [13][  100/  500]    Overall Loss 1.821593    Objective Loss 1.821593                                        LR 0.001000    Time 0.075524    
2024-02-17 11:10:57,170 - Epoch: [13][  200/  500]    Overall Loss 1.839496    Objective Loss 1.839496                                        LR 0.001000    Time 0.073651    
2024-02-17 11:11:04,222 - Epoch: [13][  300/  500]    Overall Loss 1.848061    Objective Loss 1.848061                                        LR 0.001000    Time 0.072593    
2024-02-17 11:11:11,355 - Epoch: [13][  400/  500]    Overall Loss 1.847638    Objective Loss 1.847638                                        LR 0.001000    Time 0.072266    
2024-02-17 11:11:18,494 - Epoch: [13][  500/  500]    Overall Loss 1.848399    Objective Loss 1.848399    Top1 49.000000    Top5 80.000000    LR 0.001000    Time 0.072084    
2024-02-17 11:11:18,619 - --- validate (epoch=13)-----------
2024-02-17 11:11:18,619 - 10000 samples (100 per mini-batch)
2024-02-17 11:11:21,863 - Epoch: [13][  100/  100]    Loss 2.009546    Top1 46.370000    Top5 77.500000    
2024-02-17 11:11:21,994 - ==> Top1: 46.370    Top5: 77.500    Loss: 2.010

2024-02-17 11:11:22,000 - ==> Best [Top1: 46.370   Top5: 77.500   Sparsity:0.00   Params: 753952 on epoch: 13]
2024-02-17 11:11:22,000 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:11:22,077 - 

2024-02-17 11:11:22,077 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:11:29,552 - Epoch: [14][  100/  500]    Overall Loss 1.775488    Objective Loss 1.775488                                        LR 0.001000    Time 0.074684    
2024-02-17 11:11:36,863 - Epoch: [14][  200/  500]    Overall Loss 1.784257    Objective Loss 1.784257                                        LR 0.001000    Time 0.073879    
2024-02-17 11:11:44,124 - Epoch: [14][  300/  500]    Overall Loss 1.796584    Objective Loss 1.796584                                        LR 0.001000    Time 0.073443    
2024-02-17 11:11:51,232 - Epoch: [14][  400/  500]    Overall Loss 1.795666    Objective Loss 1.795666                                        LR 0.001000    Time 0.072841    
2024-02-17 11:11:58,272 - Epoch: [14][  500/  500]    Overall Loss 1.796371    Objective Loss 1.796371    Top1 48.000000    Top5 74.500000    LR 0.001000    Time 0.072344    
2024-02-17 11:11:58,406 - --- validate (epoch=14)-----------
2024-02-17 11:11:58,407 - 10000 samples (100 per mini-batch)
2024-02-17 11:12:01,445 - Epoch: [14][  100/  100]    Loss 2.156361    Top1 43.530000    Top5 76.080000    
2024-02-17 11:12:01,574 - ==> Top1: 43.530    Top5: 76.080    Loss: 2.156

2024-02-17 11:12:01,827 - ==> Best [Top1: 46.370   Top5: 77.500   Sparsity:0.00   Params: 753952 on epoch: 13]
2024-02-17 11:12:01,827 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:12:01,887 - 

2024-02-17 11:12:01,887 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:12:09,413 - Epoch: [15][  100/  500]    Overall Loss 1.753944    Objective Loss 1.753944                                        LR 0.001000    Time 0.075204    
2024-02-17 11:12:16,393 - Epoch: [15][  200/  500]    Overall Loss 1.765189    Objective Loss 1.765189                                        LR 0.001000    Time 0.072480    
2024-02-17 11:12:23,374 - Epoch: [15][  300/  500]    Overall Loss 1.756644    Objective Loss 1.756644                                        LR 0.001000    Time 0.071575    
2024-02-17 11:12:30,538 - Epoch: [15][  400/  500]    Overall Loss 1.757607    Objective Loss 1.757607                                        LR 0.001000    Time 0.071580    
2024-02-17 11:12:37,651 - Epoch: [15][  500/  500]    Overall Loss 1.759161    Objective Loss 1.759161    Top1 54.000000    Top5 80.000000    LR 0.001000    Time 0.071481    
2024-02-17 11:12:37,784 - --- validate (epoch=15)-----------
2024-02-17 11:12:37,785 - 10000 samples (100 per mini-batch)
2024-02-17 11:12:40,800 - Epoch: [15][  100/  100]    Loss 2.198651    Top1 44.390000    Top5 74.910000    
2024-02-17 11:12:40,904 - ==> Top1: 44.390    Top5: 74.910    Loss: 2.199

2024-02-17 11:12:40,916 - ==> Best [Top1: 46.370   Top5: 77.500   Sparsity:0.00   Params: 753952 on epoch: 13]
2024-02-17 11:12:40,916 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:12:40,980 - 

2024-02-17 11:12:40,981 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:12:48,647 - Epoch: [16][  100/  500]    Overall Loss 1.711494    Objective Loss 1.711494                                        LR 0.001000    Time 0.076604    
2024-02-17 11:12:55,773 - Epoch: [16][  200/  500]    Overall Loss 1.725115    Objective Loss 1.725115                                        LR 0.001000    Time 0.073910    
2024-02-17 11:13:02,944 - Epoch: [16][  300/  500]    Overall Loss 1.719435    Objective Loss 1.719435                                        LR 0.001000    Time 0.073159    
2024-02-17 11:13:09,825 - Epoch: [16][  400/  500]    Overall Loss 1.718150    Objective Loss 1.718150                                        LR 0.001000    Time 0.072062    
2024-02-17 11:13:16,710 - Epoch: [16][  500/  500]    Overall Loss 1.720831    Objective Loss 1.720831    Top1 49.500000    Top5 82.500000    LR 0.001000    Time 0.071412    
2024-02-17 11:13:16,920 - --- validate (epoch=16)-----------
2024-02-17 11:13:16,920 - 10000 samples (100 per mini-batch)
2024-02-17 11:13:20,057 - Epoch: [16][  100/  100]    Loss 2.099510    Top1 45.580000    Top5 77.060000    
2024-02-17 11:13:20,169 - ==> Top1: 45.580    Top5: 77.060    Loss: 2.100

2024-02-17 11:13:20,180 - ==> Best [Top1: 46.370   Top5: 77.500   Sparsity:0.00   Params: 753952 on epoch: 13]
2024-02-17 11:13:20,181 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:13:20,250 - 

2024-02-17 11:13:20,251 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:13:27,922 - Epoch: [17][  100/  500]    Overall Loss 1.660831    Objective Loss 1.660831                                        LR 0.001000    Time 0.076658    
2024-02-17 11:13:35,082 - Epoch: [17][  200/  500]    Overall Loss 1.676588    Objective Loss 1.676588                                        LR 0.001000    Time 0.074106    
2024-02-17 11:13:42,360 - Epoch: [17][  300/  500]    Overall Loss 1.675820    Objective Loss 1.675820                                        LR 0.001000    Time 0.073648    
2024-02-17 11:13:49,816 - Epoch: [17][  400/  500]    Overall Loss 1.679714    Objective Loss 1.679714                                        LR 0.001000    Time 0.073867    
2024-02-17 11:13:56,989 - Epoch: [17][  500/  500]    Overall Loss 1.685002    Objective Loss 1.685002    Top1 53.500000    Top5 84.500000    LR 0.001000    Time 0.073430    
2024-02-17 11:13:57,096 - --- validate (epoch=17)-----------
2024-02-17 11:13:57,097 - 10000 samples (100 per mini-batch)
2024-02-17 11:13:59,967 - Epoch: [17][  100/  100]    Loss 2.105406    Top1 45.680000    Top5 77.190000    
2024-02-17 11:14:00,088 - ==> Top1: 45.680    Top5: 77.190    Loss: 2.105

2024-02-17 11:14:00,099 - ==> Best [Top1: 46.370   Top5: 77.500   Sparsity:0.00   Params: 753952 on epoch: 13]
2024-02-17 11:14:00,099 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:14:00,166 - 

2024-02-17 11:14:00,166 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:14:08,171 - Epoch: [18][  100/  500]    Overall Loss 1.621041    Objective Loss 1.621041                                        LR 0.001000    Time 0.079988    
2024-02-17 11:14:15,411 - Epoch: [18][  200/  500]    Overall Loss 1.613933    Objective Loss 1.613933                                        LR 0.001000    Time 0.076175    
2024-02-17 11:14:22,538 - Epoch: [18][  300/  500]    Overall Loss 1.632973    Objective Loss 1.632973                                        LR 0.001000    Time 0.074523    
2024-02-17 11:14:29,689 - Epoch: [18][  400/  500]    Overall Loss 1.638953    Objective Loss 1.638953                                        LR 0.001000    Time 0.073759    
2024-02-17 11:14:36,822 - Epoch: [18][  500/  500]    Overall Loss 1.642707    Objective Loss 1.642707    Top1 50.500000    Top5 84.000000    LR 0.001000    Time 0.073265    
2024-02-17 11:14:36,987 - --- validate (epoch=18)-----------
2024-02-17 11:14:36,988 - 10000 samples (100 per mini-batch)
2024-02-17 11:14:39,835 - Epoch: [18][  100/  100]    Loss 1.904714    Top1 49.260000    Top5 79.860000    
2024-02-17 11:14:39,990 - ==> Top1: 49.260    Top5: 79.860    Loss: 1.905

2024-02-17 11:14:40,003 - ==> Best [Top1: 49.260   Top5: 79.860   Sparsity:0.00   Params: 753952 on epoch: 18]
2024-02-17 11:14:40,003 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:14:40,093 - 

2024-02-17 11:14:40,093 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:14:47,935 - Epoch: [19][  100/  500]    Overall Loss 1.607954    Objective Loss 1.607954                                        LR 0.001000    Time 0.078361    
2024-02-17 11:14:55,300 - Epoch: [19][  200/  500]    Overall Loss 1.585529    Objective Loss 1.585529                                        LR 0.001000    Time 0.075980    
2024-02-17 11:15:02,761 - Epoch: [19][  300/  500]    Overall Loss 1.596851    Objective Loss 1.596851                                        LR 0.001000    Time 0.075511    
2024-02-17 11:15:09,875 - Epoch: [19][  400/  500]    Overall Loss 1.601803    Objective Loss 1.601803                                        LR 0.001000    Time 0.074405    
2024-02-17 11:15:16,985 - Epoch: [19][  500/  500]    Overall Loss 1.603009    Objective Loss 1.603009    Top1 52.500000    Top5 87.000000    LR 0.001000    Time 0.073737    
2024-02-17 11:15:17,118 - --- validate (epoch=19)-----------
2024-02-17 11:15:17,119 - 10000 samples (100 per mini-batch)
2024-02-17 11:15:20,043 - Epoch: [19][  100/  100]    Loss 1.988589    Top1 48.330000    Top5 79.250000    
2024-02-17 11:15:20,158 - ==> Top1: 48.330    Top5: 79.250    Loss: 1.989

2024-02-17 11:15:20,167 - ==> Best [Top1: 49.260   Top5: 79.860   Sparsity:0.00   Params: 753952 on epoch: 18]
2024-02-17 11:15:20,167 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:15:20,228 - 

2024-02-17 11:15:20,228 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:15:28,080 - Epoch: [20][  100/  500]    Overall Loss 1.564594    Objective Loss 1.564594                                        LR 0.001000    Time 0.078450    
2024-02-17 11:15:35,104 - Epoch: [20][  200/  500]    Overall Loss 1.562131    Objective Loss 1.562131                                        LR 0.001000    Time 0.074327    
2024-02-17 11:15:42,071 - Epoch: [20][  300/  500]    Overall Loss 1.562905    Objective Loss 1.562905                                        LR 0.001000    Time 0.072760    
2024-02-17 11:15:49,127 - Epoch: [20][  400/  500]    Overall Loss 1.571484    Objective Loss 1.571484                                        LR 0.001000    Time 0.072199    
2024-02-17 11:15:56,270 - Epoch: [20][  500/  500]    Overall Loss 1.575743    Objective Loss 1.575743    Top1 57.000000    Top5 87.500000    LR 0.001000    Time 0.072037    
2024-02-17 11:15:56,437 - --- validate (epoch=20)-----------
2024-02-17 11:15:56,438 - 10000 samples (100 per mini-batch)
2024-02-17 11:15:59,326 - Epoch: [20][  100/  100]    Loss 1.934092    Top1 49.540000    Top5 80.070000    
2024-02-17 11:15:59,445 - ==> Top1: 49.540    Top5: 80.070    Loss: 1.934

2024-02-17 11:15:59,456 - ==> Best [Top1: 49.540   Top5: 80.070   Sparsity:0.00   Params: 753952 on epoch: 20]
2024-02-17 11:15:59,457 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:15:59,545 - 

2024-02-17 11:15:59,545 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:16:07,136 - Epoch: [21][  100/  500]    Overall Loss 1.538992    Objective Loss 1.538992                                        LR 0.001000    Time 0.075845    
2024-02-17 11:16:14,392 - Epoch: [21][  200/  500]    Overall Loss 1.546913    Objective Loss 1.546913                                        LR 0.001000    Time 0.074180    
2024-02-17 11:16:21,809 - Epoch: [21][  300/  500]    Overall Loss 1.546955    Objective Loss 1.546955                                        LR 0.001000    Time 0.074163    
2024-02-17 11:16:29,207 - Epoch: [21][  400/  500]    Overall Loss 1.549520    Objective Loss 1.549520                                        LR 0.001000    Time 0.074107    
2024-02-17 11:16:36,417 - Epoch: [21][  500/  500]    Overall Loss 1.548122    Objective Loss 1.548122    Top1 51.500000    Top5 83.000000    LR 0.001000    Time 0.073697    
2024-02-17 11:16:36,569 - --- validate (epoch=21)-----------
2024-02-17 11:16:36,571 - 10000 samples (100 per mini-batch)
2024-02-17 11:16:39,480 - Epoch: [21][  100/  100]    Loss 2.062266    Top1 47.660000    Top5 77.650000    
2024-02-17 11:16:39,578 - ==> Top1: 47.660    Top5: 77.650    Loss: 2.062

2024-02-17 11:16:39,588 - ==> Best [Top1: 49.540   Top5: 80.070   Sparsity:0.00   Params: 753952 on epoch: 20]
2024-02-17 11:16:39,589 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:16:39,652 - 

2024-02-17 11:16:39,652 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:16:47,376 - Epoch: [22][  100/  500]    Overall Loss 1.496551    Objective Loss 1.496551                                        LR 0.001000    Time 0.077176    
2024-02-17 11:16:54,573 - Epoch: [22][  200/  500]    Overall Loss 1.495316    Objective Loss 1.495316                                        LR 0.001000    Time 0.074550    
2024-02-17 11:17:01,807 - Epoch: [22][  300/  500]    Overall Loss 1.502963    Objective Loss 1.502963                                        LR 0.001000    Time 0.073799    
2024-02-17 11:17:08,937 - Epoch: [22][  400/  500]    Overall Loss 1.511468    Objective Loss 1.511468                                        LR 0.001000    Time 0.073164    
2024-02-17 11:17:16,048 - Epoch: [22][  500/  500]    Overall Loss 1.516580    Objective Loss 1.516580    Top1 61.000000    Top5 85.500000    LR 0.001000    Time 0.072744    
2024-02-17 11:17:16,156 - --- validate (epoch=22)-----------
2024-02-17 11:17:16,157 - 10000 samples (100 per mini-batch)
2024-02-17 11:17:19,518 - Epoch: [22][  100/  100]    Loss 2.009924    Top1 49.130000    Top5 79.100000    
2024-02-17 11:17:19,617 - ==> Top1: 49.130    Top5: 79.100    Loss: 2.010

2024-02-17 11:17:19,629 - ==> Best [Top1: 49.540   Top5: 80.070   Sparsity:0.00   Params: 753952 on epoch: 20]
2024-02-17 11:17:19,629 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:17:19,690 - 

2024-02-17 11:17:19,691 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:17:27,243 - Epoch: [23][  100/  500]    Overall Loss 1.491574    Objective Loss 1.491574                                        LR 0.001000    Time 0.075473    
2024-02-17 11:17:34,406 - Epoch: [23][  200/  500]    Overall Loss 1.483728    Objective Loss 1.483728                                        LR 0.001000    Time 0.073526    
2024-02-17 11:17:41,723 - Epoch: [23][  300/  500]    Overall Loss 1.485817    Objective Loss 1.485817                                        LR 0.001000    Time 0.073395    
2024-02-17 11:17:49,008 - Epoch: [23][  400/  500]    Overall Loss 1.494041    Objective Loss 1.494041                                        LR 0.001000    Time 0.073246    
2024-02-17 11:17:56,076 - Epoch: [23][  500/  500]    Overall Loss 1.495390    Objective Loss 1.495390    Top1 51.500000    Top5 80.000000    LR 0.001000    Time 0.072724    
2024-02-17 11:17:56,196 - --- validate (epoch=23)-----------
2024-02-17 11:17:56,197 - 10000 samples (100 per mini-batch)
2024-02-17 11:17:59,243 - Epoch: [23][  100/  100]    Loss 1.921122    Top1 49.480000    Top5 80.280000    
2024-02-17 11:17:59,350 - ==> Top1: 49.480    Top5: 80.280    Loss: 1.921

2024-02-17 11:17:59,360 - ==> Best [Top1: 49.540   Top5: 80.070   Sparsity:0.00   Params: 753952 on epoch: 20]
2024-02-17 11:17:59,361 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:17:59,421 - 

2024-02-17 11:17:59,421 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:18:07,058 - Epoch: [24][  100/  500]    Overall Loss 1.432018    Objective Loss 1.432018                                        LR 0.001000    Time 0.076311    
2024-02-17 11:18:14,313 - Epoch: [24][  200/  500]    Overall Loss 1.439184    Objective Loss 1.439184                                        LR 0.001000    Time 0.074411    
2024-02-17 11:18:21,473 - Epoch: [24][  300/  500]    Overall Loss 1.446274    Objective Loss 1.446274                                        LR 0.001000    Time 0.073458    
2024-02-17 11:18:28,606 - Epoch: [24][  400/  500]    Overall Loss 1.455156    Objective Loss 1.455156                                        LR 0.001000    Time 0.072914    
2024-02-17 11:18:35,756 - Epoch: [24][  500/  500]    Overall Loss 1.462438    Objective Loss 1.462438    Top1 54.500000    Top5 88.000000    LR 0.001000    Time 0.072623    
2024-02-17 11:18:35,882 - --- validate (epoch=24)-----------
2024-02-17 11:18:35,883 - 10000 samples (100 per mini-batch)
2024-02-17 11:18:38,942 - Epoch: [24][  100/  100]    Loss 2.288580    Top1 45.180000    Top5 76.230000    
2024-02-17 11:18:39,043 - ==> Top1: 45.180    Top5: 76.230    Loss: 2.289

2024-02-17 11:18:39,050 - ==> Best [Top1: 49.540   Top5: 80.070   Sparsity:0.00   Params: 753952 on epoch: 20]
2024-02-17 11:18:39,051 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:18:39,107 - 

2024-02-17 11:18:39,108 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:18:46,613 - Epoch: [25][  100/  500]    Overall Loss 1.423045    Objective Loss 1.423045                                        LR 0.001000    Time 0.074998    
2024-02-17 11:18:53,767 - Epoch: [25][  200/  500]    Overall Loss 1.432119    Objective Loss 1.432119                                        LR 0.001000    Time 0.073249    
2024-02-17 11:19:00,847 - Epoch: [25][  300/  500]    Overall Loss 1.435971    Objective Loss 1.435971                                        LR 0.001000    Time 0.072416    
2024-02-17 11:19:07,982 - Epoch: [25][  400/  500]    Overall Loss 1.439155    Objective Loss 1.439155                                        LR 0.001000    Time 0.072138    
2024-02-17 11:19:15,043 - Epoch: [25][  500/  500]    Overall Loss 1.446353    Objective Loss 1.446353    Top1 59.000000    Top5 88.000000    LR 0.001000    Time 0.071824    
2024-02-17 11:19:15,197 - --- validate (epoch=25)-----------
2024-02-17 11:19:15,198 - 10000 samples (100 per mini-batch)
2024-02-17 11:19:18,135 - Epoch: [25][  100/  100]    Loss 1.934736    Top1 50.320000    Top5 80.460000    
2024-02-17 11:19:18,255 - ==> Top1: 50.320    Top5: 80.460    Loss: 1.935

2024-02-17 11:19:18,266 - ==> Best [Top1: 50.320   Top5: 80.460   Sparsity:0.00   Params: 753952 on epoch: 25]
2024-02-17 11:19:18,266 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:19:18,344 - 

2024-02-17 11:19:18,344 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:19:26,470 - Epoch: [26][  100/  500]    Overall Loss 1.370702    Objective Loss 1.370702                                        LR 0.001000    Time 0.081204    
2024-02-17 11:19:33,729 - Epoch: [26][  200/  500]    Overall Loss 1.381659    Objective Loss 1.381659                                        LR 0.001000    Time 0.076874    
2024-02-17 11:19:40,889 - Epoch: [26][  300/  500]    Overall Loss 1.401317    Objective Loss 1.401317                                        LR 0.001000    Time 0.075100    
2024-02-17 11:19:48,089 - Epoch: [26][  400/  500]    Overall Loss 1.416794    Objective Loss 1.416794                                        LR 0.001000    Time 0.074316    
2024-02-17 11:19:55,334 - Epoch: [26][  500/  500]    Overall Loss 1.416493    Objective Loss 1.416493    Top1 64.500000    Top5 84.500000    LR 0.001000    Time 0.073933    
2024-02-17 11:19:55,459 - --- validate (epoch=26)-----------
2024-02-17 11:19:55,460 - 10000 samples (100 per mini-batch)
2024-02-17 11:19:58,352 - Epoch: [26][  100/  100]    Loss 1.971634    Top1 49.450000    Top5 79.280000    
2024-02-17 11:19:58,508 - ==> Top1: 49.450    Top5: 79.280    Loss: 1.972

2024-02-17 11:19:58,519 - ==> Best [Top1: 50.320   Top5: 80.460   Sparsity:0.00   Params: 753952 on epoch: 25]
2024-02-17 11:19:58,519 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:19:58,578 - 

2024-02-17 11:19:58,579 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:20:06,152 - Epoch: [27][  100/  500]    Overall Loss 1.348677    Objective Loss 1.348677                                        LR 0.001000    Time 0.075681    
2024-02-17 11:20:13,246 - Epoch: [27][  200/  500]    Overall Loss 1.366492    Objective Loss 1.366492                                        LR 0.001000    Time 0.073287    
2024-02-17 11:20:20,252 - Epoch: [27][  300/  500]    Overall Loss 1.378561    Objective Loss 1.378561                                        LR 0.001000    Time 0.072196    
2024-02-17 11:20:27,534 - Epoch: [27][  400/  500]    Overall Loss 1.381536    Objective Loss 1.381536                                        LR 0.001000    Time 0.072341    
2024-02-17 11:20:34,953 - Epoch: [27][  500/  500]    Overall Loss 1.394964    Objective Loss 1.394964    Top1 60.500000    Top5 88.000000    LR 0.001000    Time 0.072703    
2024-02-17 11:20:35,109 - --- validate (epoch=27)-----------
2024-02-17 11:20:35,111 - 10000 samples (100 per mini-batch)
2024-02-17 11:20:38,018 - Epoch: [27][  100/  100]    Loss 2.050875    Top1 49.450000    Top5 78.590000    
2024-02-17 11:20:38,132 - ==> Top1: 49.450    Top5: 78.590    Loss: 2.051

2024-02-17 11:20:38,143 - ==> Best [Top1: 50.320   Top5: 80.460   Sparsity:0.00   Params: 753952 on epoch: 25]
2024-02-17 11:20:38,144 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:20:38,206 - 

2024-02-17 11:20:38,207 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:20:46,217 - Epoch: [28][  100/  500]    Overall Loss 1.328535    Objective Loss 1.328535                                        LR 0.001000    Time 0.080051    
2024-02-17 11:20:53,184 - Epoch: [28][  200/  500]    Overall Loss 1.350845    Objective Loss 1.350845                                        LR 0.001000    Time 0.074837    
2024-02-17 11:21:00,160 - Epoch: [28][  300/  500]    Overall Loss 1.362088    Objective Loss 1.362088                                        LR 0.001000    Time 0.073132    
2024-02-17 11:21:07,334 - Epoch: [28][  400/  500]    Overall Loss 1.363851    Objective Loss 1.363851                                        LR 0.001000    Time 0.072774    
2024-02-17 11:21:14,433 - Epoch: [28][  500/  500]    Overall Loss 1.367499    Objective Loss 1.367499    Top1 62.000000    Top5 87.500000    LR 0.001000    Time 0.072408    
2024-02-17 11:21:14,591 - --- validate (epoch=28)-----------
2024-02-17 11:21:14,591 - 10000 samples (100 per mini-batch)
2024-02-17 11:21:17,420 - Epoch: [28][  100/  100]    Loss 1.873124    Top1 51.380000    Top5 81.140000    
2024-02-17 11:21:17,571 - ==> Top1: 51.380    Top5: 81.140    Loss: 1.873

2024-02-17 11:21:17,582 - ==> Best [Top1: 51.380   Top5: 81.140   Sparsity:0.00   Params: 753952 on epoch: 28]
2024-02-17 11:21:17,582 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:21:17,660 - 

2024-02-17 11:21:17,661 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:21:25,289 - Epoch: [29][  100/  500]    Overall Loss 1.325078    Objective Loss 1.325078                                        LR 0.001000    Time 0.076225    
2024-02-17 11:21:32,478 - Epoch: [29][  200/  500]    Overall Loss 1.337117    Objective Loss 1.337117                                        LR 0.001000    Time 0.074035    
2024-02-17 11:21:39,542 - Epoch: [29][  300/  500]    Overall Loss 1.347000    Objective Loss 1.347000                                        LR 0.001000    Time 0.072892    
2024-02-17 11:21:46,660 - Epoch: [29][  400/  500]    Overall Loss 1.351536    Objective Loss 1.351536                                        LR 0.001000    Time 0.072450    
2024-02-17 11:21:54,055 - Epoch: [29][  500/  500]    Overall Loss 1.354046    Objective Loss 1.354046    Top1 63.000000    Top5 88.500000    LR 0.001000    Time 0.072742    
2024-02-17 11:21:54,180 - --- validate (epoch=29)-----------
2024-02-17 11:21:54,181 - 10000 samples (100 per mini-batch)
2024-02-17 11:21:57,042 - Epoch: [29][  100/  100]    Loss 1.797290    Top1 53.410000    Top5 81.720000    
2024-02-17 11:21:57,139 - ==> Top1: 53.410    Top5: 81.720    Loss: 1.797

2024-02-17 11:21:57,150 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:21:57,150 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:21:57,229 - 

2024-02-17 11:21:57,230 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:22:05,071 - Epoch: [30][  100/  500]    Overall Loss 1.287730    Objective Loss 1.287730                                        LR 0.001000    Time 0.078356    
2024-02-17 11:22:12,090 - Epoch: [30][  200/  500]    Overall Loss 1.299301    Objective Loss 1.299301                                        LR 0.001000    Time 0.074254    
2024-02-17 11:22:18,993 - Epoch: [30][  300/  500]    Overall Loss 1.313185    Objective Loss 1.313185                                        LR 0.001000    Time 0.072497    
2024-02-17 11:22:26,447 - Epoch: [30][  400/  500]    Overall Loss 1.322461    Objective Loss 1.322461                                        LR 0.001000    Time 0.072998    
2024-02-17 11:22:33,827 - Epoch: [30][  500/  500]    Overall Loss 1.331671    Objective Loss 1.331671    Top1 67.000000    Top5 89.000000    LR 0.001000    Time 0.073150    
2024-02-17 11:22:33,991 - --- validate (epoch=30)-----------
2024-02-17 11:22:33,992 - 10000 samples (100 per mini-batch)
2024-02-17 11:22:36,873 - Epoch: [30][  100/  100]    Loss 1.824264    Top1 52.090000    Top5 80.960000    
2024-02-17 11:22:37,044 - ==> Top1: 52.090    Top5: 80.960    Loss: 1.824

2024-02-17 11:22:37,054 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:22:37,054 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:22:37,119 - 

2024-02-17 11:22:37,119 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:22:44,806 - Epoch: [31][  100/  500]    Overall Loss 1.285199    Objective Loss 1.285199                                        LR 0.001000    Time 0.076810    
2024-02-17 11:22:51,998 - Epoch: [31][  200/  500]    Overall Loss 1.281403    Objective Loss 1.281403                                        LR 0.001000    Time 0.074346    
2024-02-17 11:22:59,168 - Epoch: [31][  300/  500]    Overall Loss 1.289112    Objective Loss 1.289112                                        LR 0.001000    Time 0.073449    
2024-02-17 11:23:06,314 - Epoch: [31][  400/  500]    Overall Loss 1.297153    Objective Loss 1.297153                                        LR 0.001000    Time 0.072941    
2024-02-17 11:23:13,416 - Epoch: [31][  500/  500]    Overall Loss 1.304195    Objective Loss 1.304195    Top1 65.000000    Top5 88.000000    LR 0.001000    Time 0.072547    
2024-02-17 11:23:13,585 - --- validate (epoch=31)-----------
2024-02-17 11:23:13,586 - 10000 samples (100 per mini-batch)
2024-02-17 11:23:16,477 - Epoch: [31][  100/  100]    Loss 1.831615    Top1 52.490000    Top5 81.970000    
2024-02-17 11:23:16,573 - ==> Top1: 52.490    Top5: 81.970    Loss: 1.832

2024-02-17 11:23:16,584 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:23:16,584 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:23:16,647 - 

2024-02-17 11:23:16,648 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:23:24,293 - Epoch: [32][  100/  500]    Overall Loss 1.256892    Objective Loss 1.256892                                        LR 0.001000    Time 0.076390    
2024-02-17 11:23:31,390 - Epoch: [32][  200/  500]    Overall Loss 1.268618    Objective Loss 1.268618                                        LR 0.001000    Time 0.073660    
2024-02-17 11:23:38,607 - Epoch: [32][  300/  500]    Overall Loss 1.278748    Objective Loss 1.278748                                        LR 0.001000    Time 0.073150    
2024-02-17 11:23:45,493 - Epoch: [32][  400/  500]    Overall Loss 1.282109    Objective Loss 1.282109                                        LR 0.001000    Time 0.072068    
2024-02-17 11:23:52,520 - Epoch: [32][  500/  500]    Overall Loss 1.286492    Objective Loss 1.286492    Top1 62.000000    Top5 92.000000    LR 0.001000    Time 0.071699    
2024-02-17 11:23:52,635 - --- validate (epoch=32)-----------
2024-02-17 11:23:52,635 - 10000 samples (100 per mini-batch)
2024-02-17 11:23:55,720 - Epoch: [32][  100/  100]    Loss 1.907953    Top1 51.330000    Top5 81.220000    
2024-02-17 11:23:55,821 - ==> Top1: 51.330    Top5: 81.220    Loss: 1.908

2024-02-17 11:23:55,831 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:23:55,832 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:23:55,894 - 

2024-02-17 11:23:55,894 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:24:03,454 - Epoch: [33][  100/  500]    Overall Loss 1.234660    Objective Loss 1.234660                                        LR 0.001000    Time 0.075521    
2024-02-17 11:24:10,486 - Epoch: [33][  200/  500]    Overall Loss 1.252759    Objective Loss 1.252759                                        LR 0.001000    Time 0.072900    
2024-02-17 11:24:17,549 - Epoch: [33][  300/  500]    Overall Loss 1.261966    Objective Loss 1.261966                                        LR 0.001000    Time 0.072130    
2024-02-17 11:24:24,622 - Epoch: [33][  400/  500]    Overall Loss 1.266817    Objective Loss 1.266817                                        LR 0.001000    Time 0.071767    
2024-02-17 11:24:31,761 - Epoch: [33][  500/  500]    Overall Loss 1.273002    Objective Loss 1.273002    Top1 59.500000    Top5 85.000000    LR 0.001000    Time 0.071685    
2024-02-17 11:24:31,933 - --- validate (epoch=33)-----------
2024-02-17 11:24:31,934 - 10000 samples (100 per mini-batch)
2024-02-17 11:24:34,854 - Epoch: [33][  100/  100]    Loss 1.876940    Top1 51.540000    Top5 81.710000    
2024-02-17 11:24:34,981 - ==> Top1: 51.540    Top5: 81.710    Loss: 1.877

2024-02-17 11:24:34,992 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:24:34,992 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:24:35,055 - 

2024-02-17 11:24:35,055 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:24:42,966 - Epoch: [34][  100/  500]    Overall Loss 1.218805    Objective Loss 1.218805                                        LR 0.001000    Time 0.079024    
2024-02-17 11:24:49,876 - Epoch: [34][  200/  500]    Overall Loss 1.223326    Objective Loss 1.223326                                        LR 0.001000    Time 0.074047    
2024-02-17 11:24:57,040 - Epoch: [34][  300/  500]    Overall Loss 1.228984    Objective Loss 1.228984                                        LR 0.001000    Time 0.073230    
2024-02-17 11:25:04,278 - Epoch: [34][  400/  500]    Overall Loss 1.236273    Objective Loss 1.236273                                        LR 0.001000    Time 0.073005    
2024-02-17 11:25:11,415 - Epoch: [34][  500/  500]    Overall Loss 1.248150    Objective Loss 1.248150    Top1 64.000000    Top5 90.500000    LR 0.001000    Time 0.072671    
2024-02-17 11:25:11,538 - --- validate (epoch=34)-----------
2024-02-17 11:25:11,539 - 10000 samples (100 per mini-batch)
2024-02-17 11:25:14,462 - Epoch: [34][  100/  100]    Loss 1.937653    Top1 52.230000    Top5 80.870000    
2024-02-17 11:25:14,632 - ==> Top1: 52.230    Top5: 80.870    Loss: 1.938

2024-02-17 11:25:14,644 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:25:14,644 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:25:14,708 - 

2024-02-17 11:25:14,708 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:25:22,535 - Epoch: [35][  100/  500]    Overall Loss 1.183362    Objective Loss 1.183362                                        LR 0.001000    Time 0.078211    
2024-02-17 11:25:29,700 - Epoch: [35][  200/  500]    Overall Loss 1.197224    Objective Loss 1.197224                                        LR 0.001000    Time 0.074909    
2024-02-17 11:25:36,722 - Epoch: [35][  300/  500]    Overall Loss 1.205798    Objective Loss 1.205798                                        LR 0.001000    Time 0.073333    
2024-02-17 11:25:43,915 - Epoch: [35][  400/  500]    Overall Loss 1.225171    Objective Loss 1.225171                                        LR 0.001000    Time 0.072970    
2024-02-17 11:25:50,956 - Epoch: [35][  500/  500]    Overall Loss 1.231215    Objective Loss 1.231215    Top1 63.000000    Top5 90.000000    LR 0.001000    Time 0.072450    
2024-02-17 11:25:51,073 - --- validate (epoch=35)-----------
2024-02-17 11:25:51,074 - 10000 samples (100 per mini-batch)
2024-02-17 11:25:53,956 - Epoch: [35][  100/  100]    Loss 1.796121    Top1 53.010000    Top5 82.540000    
2024-02-17 11:25:54,053 - ==> Top1: 53.010    Top5: 82.540    Loss: 1.796

2024-02-17 11:25:54,065 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:25:54,065 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:25:54,127 - 

2024-02-17 11:25:54,127 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:26:02,062 - Epoch: [36][  100/  500]    Overall Loss 1.157656    Objective Loss 1.157656                                        LR 0.001000    Time 0.079288    
2024-02-17 11:26:09,192 - Epoch: [36][  200/  500]    Overall Loss 1.183921    Objective Loss 1.183921                                        LR 0.001000    Time 0.075270    
2024-02-17 11:26:16,144 - Epoch: [36][  300/  500]    Overall Loss 1.203144    Objective Loss 1.203144                                        LR 0.001000    Time 0.073342    
2024-02-17 11:26:23,236 - Epoch: [36][  400/  500]    Overall Loss 1.207459    Objective Loss 1.207459                                        LR 0.001000    Time 0.072727    
2024-02-17 11:26:30,335 - Epoch: [36][  500/  500]    Overall Loss 1.211898    Objective Loss 1.211898    Top1 68.000000    Top5 90.500000    LR 0.001000    Time 0.072370    
2024-02-17 11:26:30,483 - --- validate (epoch=36)-----------
2024-02-17 11:26:30,483 - 10000 samples (100 per mini-batch)
2024-02-17 11:26:33,310 - Epoch: [36][  100/  100]    Loss 1.850014    Top1 52.670000    Top5 81.390000    
2024-02-17 11:26:33,459 - ==> Top1: 52.670    Top5: 81.390    Loss: 1.850

2024-02-17 11:26:33,471 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:26:33,471 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:26:33,534 - 

2024-02-17 11:26:33,535 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:26:41,121 - Epoch: [37][  100/  500]    Overall Loss 1.168624    Objective Loss 1.168624                                        LR 0.001000    Time 0.075805    
2024-02-17 11:26:48,091 - Epoch: [37][  200/  500]    Overall Loss 1.180387    Objective Loss 1.180387                                        LR 0.001000    Time 0.072731    
2024-02-17 11:26:55,236 - Epoch: [37][  300/  500]    Overall Loss 1.177521    Objective Loss 1.177521                                        LR 0.001000    Time 0.072292    
2024-02-17 11:27:02,314 - Epoch: [37][  400/  500]    Overall Loss 1.183165    Objective Loss 1.183165                                        LR 0.001000    Time 0.071903    
2024-02-17 11:27:09,403 - Epoch: [37][  500/  500]    Overall Loss 1.195524    Objective Loss 1.195524    Top1 57.000000    Top5 86.000000    LR 0.001000    Time 0.071692    
2024-02-17 11:27:09,567 - --- validate (epoch=37)-----------
2024-02-17 11:27:09,568 - 10000 samples (100 per mini-batch)
2024-02-17 11:27:12,398 - Epoch: [37][  100/  100]    Loss 1.863734    Top1 52.960000    Top5 81.110000    
2024-02-17 11:27:12,499 - ==> Top1: 52.960    Top5: 81.110    Loss: 1.864

2024-02-17 11:27:12,510 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:27:12,511 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:27:12,573 - 

2024-02-17 11:27:12,573 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:27:20,342 - Epoch: [38][  100/  500]    Overall Loss 1.126354    Objective Loss 1.126354                                        LR 0.001000    Time 0.077633    
2024-02-17 11:27:27,516 - Epoch: [38][  200/  500]    Overall Loss 1.148542    Objective Loss 1.148542                                        LR 0.001000    Time 0.074666    
2024-02-17 11:27:34,569 - Epoch: [38][  300/  500]    Overall Loss 1.159107    Objective Loss 1.159107                                        LR 0.001000    Time 0.073272    
2024-02-17 11:27:41,665 - Epoch: [38][  400/  500]    Overall Loss 1.166049    Objective Loss 1.166049                                        LR 0.001000    Time 0.072683    
2024-02-17 11:27:48,771 - Epoch: [38][  500/  500]    Overall Loss 1.173457    Objective Loss 1.173457    Top1 60.000000    Top5 89.500000    LR 0.001000    Time 0.072350    
2024-02-17 11:27:48,881 - --- validate (epoch=38)-----------
2024-02-17 11:27:48,883 - 10000 samples (100 per mini-batch)
2024-02-17 11:27:51,794 - Epoch: [38][  100/  100]    Loss 1.933706    Top1 51.900000    Top5 81.390000    
2024-02-17 11:27:51,884 - ==> Top1: 51.900    Top5: 81.390    Loss: 1.934

2024-02-17 11:27:51,896 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:27:51,896 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:27:51,960 - 

2024-02-17 11:27:51,960 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:27:59,689 - Epoch: [39][  100/  500]    Overall Loss 1.136036    Objective Loss 1.136036                                        LR 0.001000    Time 0.077225    
2024-02-17 11:28:06,960 - Epoch: [39][  200/  500]    Overall Loss 1.146980    Objective Loss 1.146980                                        LR 0.001000    Time 0.074943    
2024-02-17 11:28:14,111 - Epoch: [39][  300/  500]    Overall Loss 1.155615    Objective Loss 1.155615                                        LR 0.001000    Time 0.073788    
2024-02-17 11:28:21,319 - Epoch: [39][  400/  500]    Overall Loss 1.160542    Objective Loss 1.160542                                        LR 0.001000    Time 0.073348    
2024-02-17 11:28:28,396 - Epoch: [39][  500/  500]    Overall Loss 1.164606    Objective Loss 1.164606    Top1 64.500000    Top5 92.500000    LR 0.001000    Time 0.072824    
2024-02-17 11:28:28,526 - --- validate (epoch=39)-----------
2024-02-17 11:28:28,527 - 10000 samples (100 per mini-batch)
2024-02-17 11:28:31,455 - Epoch: [39][  100/  100]    Loss 1.959881    Top1 52.020000    Top5 80.500000    
2024-02-17 11:28:31,582 - ==> Top1: 52.020    Top5: 80.500    Loss: 1.960

2024-02-17 11:28:31,593 - ==> Best [Top1: 53.410   Top5: 81.720   Sparsity:0.00   Params: 753952 on epoch: 29]
2024-02-17 11:28:31,593 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:28:31,656 - 

2024-02-17 11:28:31,656 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:28:39,556 - Epoch: [40][  100/  500]    Overall Loss 1.099555    Objective Loss 1.099555                                        LR 0.001000    Time 0.078944    
2024-02-17 11:28:46,721 - Epoch: [40][  200/  500]    Overall Loss 1.107798    Objective Loss 1.107798                                        LR 0.001000    Time 0.075273    
2024-02-17 11:28:53,927 - Epoch: [40][  300/  500]    Overall Loss 1.119186    Objective Loss 1.119186                                        LR 0.001000    Time 0.074188    
2024-02-17 11:29:00,996 - Epoch: [40][  400/  500]    Overall Loss 1.135157    Objective Loss 1.135157                                        LR 0.001000    Time 0.073304    
2024-02-17 11:29:08,095 - Epoch: [40][  500/  500]    Overall Loss 1.139456    Objective Loss 1.139456    Top1 67.500000    Top5 91.000000    LR 0.001000    Time 0.072832    
2024-02-17 11:29:08,195 - --- validate (epoch=40)-----------
2024-02-17 11:29:08,196 - 10000 samples (100 per mini-batch)
2024-02-17 11:29:11,059 - Epoch: [40][  100/  100]    Loss 1.839705    Top1 53.910000    Top5 82.340000    
2024-02-17 11:29:11,214 - ==> Top1: 53.910    Top5: 82.340    Loss: 1.840

2024-02-17 11:29:11,224 - ==> Best [Top1: 53.910   Top5: 82.340   Sparsity:0.00   Params: 753952 on epoch: 40]
2024-02-17 11:29:11,225 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:29:11,301 - 

2024-02-17 11:29:11,301 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:29:18,875 - Epoch: [41][  100/  500]    Overall Loss 1.098113    Objective Loss 1.098113                                        LR 0.001000    Time 0.075677    
2024-02-17 11:29:25,926 - Epoch: [41][  200/  500]    Overall Loss 1.099937    Objective Loss 1.099937                                        LR 0.001000    Time 0.073072    
2024-02-17 11:29:32,869 - Epoch: [41][  300/  500]    Overall Loss 1.105861    Objective Loss 1.105861                                        LR 0.001000    Time 0.071847    
2024-02-17 11:29:39,981 - Epoch: [41][  400/  500]    Overall Loss 1.114760    Objective Loss 1.114760                                        LR 0.001000    Time 0.071653    
2024-02-17 11:29:47,040 - Epoch: [41][  500/  500]    Overall Loss 1.122951    Objective Loss 1.122951    Top1 68.000000    Top5 91.500000    LR 0.001000    Time 0.071433    
2024-02-17 11:29:47,186 - --- validate (epoch=41)-----------
2024-02-17 11:29:47,187 - 10000 samples (100 per mini-batch)
2024-02-17 11:29:50,030 - Epoch: [41][  100/  100]    Loss 1.782460    Top1 54.690000    Top5 82.720000    
2024-02-17 11:29:50,141 - ==> Top1: 54.690    Top5: 82.720    Loss: 1.782

2024-02-17 11:29:50,153 - ==> Best [Top1: 54.690   Top5: 82.720   Sparsity:0.00   Params: 753952 on epoch: 41]
2024-02-17 11:29:50,154 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:29:50,230 - 

2024-02-17 11:29:50,230 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:29:57,788 - Epoch: [42][  100/  500]    Overall Loss 1.089304    Objective Loss 1.089304                                        LR 0.001000    Time 0.075520    
2024-02-17 11:30:04,974 - Epoch: [42][  200/  500]    Overall Loss 1.088567    Objective Loss 1.088567                                        LR 0.001000    Time 0.073668    
2024-02-17 11:30:12,363 - Epoch: [42][  300/  500]    Overall Loss 1.090409    Objective Loss 1.090409                                        LR 0.001000    Time 0.073728    
2024-02-17 11:30:19,336 - Epoch: [42][  400/  500]    Overall Loss 1.103778    Objective Loss 1.103778                                        LR 0.001000    Time 0.072718    
2024-02-17 11:30:26,460 - Epoch: [42][  500/  500]    Overall Loss 1.108860    Objective Loss 1.108860    Top1 71.500000    Top5 90.000000    LR 0.001000    Time 0.072414    
2024-02-17 11:30:26,588 - --- validate (epoch=42)-----------
2024-02-17 11:30:26,589 - 10000 samples (100 per mini-batch)
2024-02-17 11:30:29,533 - Epoch: [42][  100/  100]    Loss 1.801879    Top1 54.520000    Top5 82.590000    
2024-02-17 11:30:29,628 - ==> Top1: 54.520    Top5: 82.590    Loss: 1.802

2024-02-17 11:30:29,876 - ==> Best [Top1: 54.690   Top5: 82.720   Sparsity:0.00   Params: 753952 on epoch: 41]
2024-02-17 11:30:29,876 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:30:29,931 - 

2024-02-17 11:30:29,931 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:30:37,582 - Epoch: [43][  100/  500]    Overall Loss 1.059019    Objective Loss 1.059019                                        LR 0.001000    Time 0.076455    
2024-02-17 11:30:44,594 - Epoch: [43][  200/  500]    Overall Loss 1.070833    Objective Loss 1.070833                                        LR 0.001000    Time 0.073268    
2024-02-17 11:30:51,652 - Epoch: [43][  300/  500]    Overall Loss 1.076827    Objective Loss 1.076827                                        LR 0.001000    Time 0.072355    
2024-02-17 11:30:58,636 - Epoch: [43][  400/  500]    Overall Loss 1.089423    Objective Loss 1.089423                                        LR 0.001000    Time 0.071717    
2024-02-17 11:31:05,781 - Epoch: [43][  500/  500]    Overall Loss 1.100200    Objective Loss 1.100200    Top1 63.000000    Top5 89.000000    LR 0.001000    Time 0.071655    
2024-02-17 11:31:05,902 - --- validate (epoch=43)-----------
2024-02-17 11:31:05,902 - 10000 samples (100 per mini-batch)
2024-02-17 11:31:08,829 - Epoch: [43][  100/  100]    Loss 1.852823    Top1 53.770000    Top5 83.190000    
2024-02-17 11:31:08,930 - ==> Top1: 53.770    Top5: 83.190    Loss: 1.853

2024-02-17 11:31:08,942 - ==> Best [Top1: 54.690   Top5: 82.720   Sparsity:0.00   Params: 753952 on epoch: 41]
2024-02-17 11:31:08,943 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:31:09,003 - 

2024-02-17 11:31:09,003 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:31:16,597 - Epoch: [44][  100/  500]    Overall Loss 1.047904    Objective Loss 1.047904                                        LR 0.001000    Time 0.075878    
2024-02-17 11:31:23,883 - Epoch: [44][  200/  500]    Overall Loss 1.062250    Objective Loss 1.062250                                        LR 0.001000    Time 0.074344    
2024-02-17 11:31:30,897 - Epoch: [44][  300/  500]    Overall Loss 1.065268    Objective Loss 1.065268                                        LR 0.001000    Time 0.072931    
2024-02-17 11:31:37,964 - Epoch: [44][  400/  500]    Overall Loss 1.066647    Objective Loss 1.066647                                        LR 0.001000    Time 0.072355    
2024-02-17 11:31:45,168 - Epoch: [44][  500/  500]    Overall Loss 1.078985    Objective Loss 1.078985    Top1 67.000000    Top5 87.500000    LR 0.001000    Time 0.072283    
2024-02-17 11:31:45,296 - --- validate (epoch=44)-----------
2024-02-17 11:31:45,297 - 10000 samples (100 per mini-batch)
2024-02-17 11:31:48,546 - Epoch: [44][  100/  100]    Loss 1.821990    Top1 54.740000    Top5 82.280000    
2024-02-17 11:31:48,646 - ==> Top1: 54.740    Top5: 82.280    Loss: 1.822

2024-02-17 11:31:48,657 - ==> Best [Top1: 54.740   Top5: 82.280   Sparsity:0.00   Params: 753952 on epoch: 44]
2024-02-17 11:31:48,657 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:31:48,739 - 

2024-02-17 11:31:48,739 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:31:56,234 - Epoch: [45][  100/  500]    Overall Loss 1.003887    Objective Loss 1.003887                                        LR 0.001000    Time 0.074893    
2024-02-17 11:32:03,541 - Epoch: [45][  200/  500]    Overall Loss 1.031504    Objective Loss 1.031504                                        LR 0.001000    Time 0.073960    
2024-02-17 11:32:10,964 - Epoch: [45][  300/  500]    Overall Loss 1.046354    Objective Loss 1.046354                                        LR 0.001000    Time 0.074035    
2024-02-17 11:32:18,190 - Epoch: [45][  400/  500]    Overall Loss 1.056035    Objective Loss 1.056035                                        LR 0.001000    Time 0.073581    
2024-02-17 11:32:25,383 - Epoch: [45][  500/  500]    Overall Loss 1.068227    Objective Loss 1.068227    Top1 66.000000    Top5 89.000000    LR 0.001000    Time 0.073242    
2024-02-17 11:32:25,495 - --- validate (epoch=45)-----------
2024-02-17 11:32:25,495 - 10000 samples (100 per mini-batch)
2024-02-17 11:32:28,594 - Epoch: [45][  100/  100]    Loss 1.828801    Top1 54.110000    Top5 82.480000    
2024-02-17 11:32:28,689 - ==> Top1: 54.110    Top5: 82.480    Loss: 1.829

2024-02-17 11:32:28,701 - ==> Best [Top1: 54.740   Top5: 82.280   Sparsity:0.00   Params: 753952 on epoch: 44]
2024-02-17 11:32:28,701 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:32:28,760 - 

2024-02-17 11:32:28,760 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:32:36,387 - Epoch: [46][  100/  500]    Overall Loss 1.009476    Objective Loss 1.009476                                        LR 0.001000    Time 0.076209    
2024-02-17 11:32:43,393 - Epoch: [46][  200/  500]    Overall Loss 1.027685    Objective Loss 1.027685                                        LR 0.001000    Time 0.073113    
2024-02-17 11:32:50,416 - Epoch: [46][  300/  500]    Overall Loss 1.036520    Objective Loss 1.036520                                        LR 0.001000    Time 0.072141    
2024-02-17 11:32:57,520 - Epoch: [46][  400/  500]    Overall Loss 1.042552    Objective Loss 1.042552                                        LR 0.001000    Time 0.071853    
2024-02-17 11:33:04,654 - Epoch: [46][  500/  500]    Overall Loss 1.052531    Objective Loss 1.052531    Top1 73.000000    Top5 96.000000    LR 0.001000    Time 0.071741    
2024-02-17 11:33:04,800 - --- validate (epoch=46)-----------
2024-02-17 11:33:04,801 - 10000 samples (100 per mini-batch)
2024-02-17 11:33:08,114 - Epoch: [46][  100/  100]    Loss 1.830720    Top1 53.730000    Top5 82.630000    
2024-02-17 11:33:08,216 - ==> Top1: 53.730    Top5: 82.630    Loss: 1.831

2024-02-17 11:33:08,227 - ==> Best [Top1: 54.740   Top5: 82.280   Sparsity:0.00   Params: 753952 on epoch: 44]
2024-02-17 11:33:08,227 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:33:08,292 - 

2024-02-17 11:33:08,293 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:33:16,156 - Epoch: [47][  100/  500]    Overall Loss 0.999660    Objective Loss 0.999660                                        LR 0.001000    Time 0.078572    
2024-02-17 11:33:23,546 - Epoch: [47][  200/  500]    Overall Loss 1.003852    Objective Loss 1.003852                                        LR 0.001000    Time 0.076213    
2024-02-17 11:33:30,775 - Epoch: [47][  300/  500]    Overall Loss 1.014563    Objective Loss 1.014563                                        LR 0.001000    Time 0.074891    
2024-02-17 11:33:38,000 - Epoch: [47][  400/  500]    Overall Loss 1.020945    Objective Loss 1.020945                                        LR 0.001000    Time 0.074220    
2024-02-17 11:33:45,307 - Epoch: [47][  500/  500]    Overall Loss 1.030677    Objective Loss 1.030677    Top1 70.000000    Top5 95.000000    LR 0.001000    Time 0.073981    
2024-02-17 11:33:45,405 - --- validate (epoch=47)-----------
2024-02-17 11:33:45,405 - 10000 samples (100 per mini-batch)
2024-02-17 11:33:48,296 - Epoch: [47][  100/  100]    Loss 1.896644    Top1 54.310000    Top5 82.350000    
2024-02-17 11:33:48,437 - ==> Top1: 54.310    Top5: 82.350    Loss: 1.897

2024-02-17 11:33:48,448 - ==> Best [Top1: 54.740   Top5: 82.280   Sparsity:0.00   Params: 753952 on epoch: 44]
2024-02-17 11:33:48,449 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:33:48,512 - 

2024-02-17 11:33:48,512 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:33:56,143 - Epoch: [48][  100/  500]    Overall Loss 1.005949    Objective Loss 1.005949                                        LR 0.001000    Time 0.076250    
2024-02-17 11:34:03,337 - Epoch: [48][  200/  500]    Overall Loss 1.002851    Objective Loss 1.002851                                        LR 0.001000    Time 0.074075    
2024-02-17 11:34:10,606 - Epoch: [48][  300/  500]    Overall Loss 1.013319    Objective Loss 1.013319                                        LR 0.001000    Time 0.073599    
2024-02-17 11:34:17,692 - Epoch: [48][  400/  500]    Overall Loss 1.015516    Objective Loss 1.015516                                        LR 0.001000    Time 0.072902    
2024-02-17 11:34:25,003 - Epoch: [48][  500/  500]    Overall Loss 1.024054    Objective Loss 1.024054    Top1 69.000000    Top5 93.000000    LR 0.001000    Time 0.072935    
2024-02-17 11:34:25,162 - --- validate (epoch=48)-----------
2024-02-17 11:34:25,163 - 10000 samples (100 per mini-batch)
2024-02-17 11:34:28,343 - Epoch: [48][  100/  100]    Loss 1.873272    Top1 54.180000    Top5 82.180000    
2024-02-17 11:34:28,514 - ==> Top1: 54.180    Top5: 82.180    Loss: 1.873

2024-02-17 11:34:28,519 - ==> Best [Top1: 54.740   Top5: 82.280   Sparsity:0.00   Params: 753952 on epoch: 44]
2024-02-17 11:34:28,520 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:34:28,581 - 

2024-02-17 11:34:28,581 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:34:36,323 - Epoch: [49][  100/  500]    Overall Loss 0.991793    Objective Loss 0.991793                                        LR 0.001000    Time 0.077364    
2024-02-17 11:34:43,572 - Epoch: [49][  200/  500]    Overall Loss 0.981546    Objective Loss 0.981546                                        LR 0.001000    Time 0.074902    
2024-02-17 11:34:50,874 - Epoch: [49][  300/  500]    Overall Loss 0.990645    Objective Loss 0.990645                                        LR 0.001000    Time 0.074261    
2024-02-17 11:34:58,219 - Epoch: [49][  400/  500]    Overall Loss 1.000108    Objective Loss 1.000108                                        LR 0.001000    Time 0.074048    
2024-02-17 11:35:05,426 - Epoch: [49][  500/  500]    Overall Loss 1.004350    Objective Loss 1.004350    Top1 66.000000    Top5 91.000000    LR 0.001000    Time 0.073644    
2024-02-17 11:35:05,538 - --- validate (epoch=49)-----------
2024-02-17 11:35:05,539 - 10000 samples (100 per mini-batch)
2024-02-17 11:35:08,571 - Epoch: [49][  100/  100]    Loss 1.923390    Top1 53.300000    Top5 82.380000    
2024-02-17 11:35:08,664 - ==> Top1: 53.300    Top5: 82.380    Loss: 1.923

2024-02-17 11:35:08,674 - ==> Best [Top1: 54.740   Top5: 82.280   Sparsity:0.00   Params: 753952 on epoch: 44]
2024-02-17 11:35:08,674 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:35:08,734 - 

2024-02-17 11:35:08,734 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:35:16,682 - Epoch: [50][  100/  500]    Overall Loss 0.891832    Objective Loss 0.891832                                        LR 0.000500    Time 0.079420    
2024-02-17 11:35:23,899 - Epoch: [50][  200/  500]    Overall Loss 0.875344    Objective Loss 0.875344                                        LR 0.000500    Time 0.075772    
2024-02-17 11:35:30,808 - Epoch: [50][  300/  500]    Overall Loss 0.871916    Objective Loss 0.871916                                        LR 0.000500    Time 0.073531    
2024-02-17 11:35:37,820 - Epoch: [50][  400/  500]    Overall Loss 0.872488    Objective Loss 0.872488                                        LR 0.000500    Time 0.072669    
2024-02-17 11:35:45,013 - Epoch: [50][  500/  500]    Overall Loss 0.877727    Objective Loss 0.877727    Top1 69.000000    Top5 92.500000    LR 0.000500    Time 0.072513    
2024-02-17 11:35:45,152 - --- validate (epoch=50)-----------
2024-02-17 11:35:45,153 - 10000 samples (100 per mini-batch)
2024-02-17 11:35:48,309 - Epoch: [50][  100/  100]    Loss 1.586217    Top1 58.580000    Top5 85.530000    
2024-02-17 11:35:48,452 - ==> Top1: 58.580    Top5: 85.530    Loss: 1.586

2024-02-17 11:35:48,463 - ==> Best [Top1: 58.580   Top5: 85.530   Sparsity:0.00   Params: 753952 on epoch: 50]
2024-02-17 11:35:48,464 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:35:48,542 - 

2024-02-17 11:35:48,542 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:35:56,115 - Epoch: [51][  100/  500]    Overall Loss 0.807896    Objective Loss 0.807896                                        LR 0.000500    Time 0.075665    
2024-02-17 11:36:03,574 - Epoch: [51][  200/  500]    Overall Loss 0.833882    Objective Loss 0.833882                                        LR 0.000500    Time 0.075105    
2024-02-17 11:36:10,979 - Epoch: [51][  300/  500]    Overall Loss 0.841966    Objective Loss 0.841966                                        LR 0.000500    Time 0.074740    
2024-02-17 11:36:18,191 - Epoch: [51][  400/  500]    Overall Loss 0.849738    Objective Loss 0.849738                                        LR 0.000500    Time 0.074075    
2024-02-17 11:36:25,397 - Epoch: [51][  500/  500]    Overall Loss 0.849891    Objective Loss 0.849891    Top1 72.000000    Top5 95.500000    LR 0.000500    Time 0.073662    
2024-02-17 11:36:25,535 - --- validate (epoch=51)-----------
2024-02-17 11:36:25,536 - 10000 samples (100 per mini-batch)
2024-02-17 11:36:28,847 - Epoch: [51][  100/  100]    Loss 1.623510    Top1 58.210000    Top5 85.200000    
2024-02-17 11:36:28,952 - ==> Top1: 58.210    Top5: 85.200    Loss: 1.624

2024-02-17 11:36:28,964 - ==> Best [Top1: 58.580   Top5: 85.530   Sparsity:0.00   Params: 753952 on epoch: 50]
2024-02-17 11:36:28,964 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:36:29,029 - 

2024-02-17 11:36:29,029 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:36:37,009 - Epoch: [52][  100/  500]    Overall Loss 0.800980    Objective Loss 0.800980                                        LR 0.000500    Time 0.079729    
2024-02-17 11:36:44,170 - Epoch: [52][  200/  500]    Overall Loss 0.809246    Objective Loss 0.809246                                        LR 0.000500    Time 0.075647    
2024-02-17 11:36:51,211 - Epoch: [52][  300/  500]    Overall Loss 0.815269    Objective Loss 0.815269                                        LR 0.000500    Time 0.073885    
2024-02-17 11:36:58,369 - Epoch: [52][  400/  500]    Overall Loss 0.828829    Objective Loss 0.828829                                        LR 0.000500    Time 0.073299    
2024-02-17 11:37:05,527 - Epoch: [52][  500/  500]    Overall Loss 0.833809    Objective Loss 0.833809    Top1 72.000000    Top5 93.000000    LR 0.000500    Time 0.072945    
2024-02-17 11:37:05,655 - --- validate (epoch=52)-----------
2024-02-17 11:37:05,656 - 10000 samples (100 per mini-batch)
2024-02-17 11:37:08,900 - Epoch: [52][  100/  100]    Loss 1.649348    Top1 58.060000    Top5 85.130000    
2024-02-17 11:37:09,007 - ==> Top1: 58.060    Top5: 85.130    Loss: 1.649

2024-02-17 11:37:09,014 - ==> Best [Top1: 58.580   Top5: 85.530   Sparsity:0.00   Params: 753952 on epoch: 50]
2024-02-17 11:37:09,015 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:37:09,083 - 

2024-02-17 11:37:09,084 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:37:16,720 - Epoch: [53][  100/  500]    Overall Loss 0.789713    Objective Loss 0.789713                                        LR 0.000500    Time 0.076293    
2024-02-17 11:37:23,817 - Epoch: [53][  200/  500]    Overall Loss 0.799589    Objective Loss 0.799589                                        LR 0.000500    Time 0.073609    
2024-02-17 11:37:30,853 - Epoch: [53][  300/  500]    Overall Loss 0.810559    Objective Loss 0.810559                                        LR 0.000500    Time 0.072513    
2024-02-17 11:37:37,965 - Epoch: [53][  400/  500]    Overall Loss 0.815939    Objective Loss 0.815939                                        LR 0.000500    Time 0.072155    
2024-02-17 11:37:45,054 - Epoch: [53][  500/  500]    Overall Loss 0.820874    Objective Loss 0.820874    Top1 70.000000    Top5 95.000000    LR 0.000500    Time 0.071893    
2024-02-17 11:37:45,183 - --- validate (epoch=53)-----------
2024-02-17 11:37:45,183 - 10000 samples (100 per mini-batch)
2024-02-17 11:37:48,390 - Epoch: [53][  100/  100]    Loss 1.603811    Top1 58.960000    Top5 85.370000    
2024-02-17 11:37:48,490 - ==> Top1: 58.960    Top5: 85.370    Loss: 1.604

2024-02-17 11:37:48,499 - ==> Best [Top1: 58.960   Top5: 85.370   Sparsity:0.00   Params: 753952 on epoch: 53]
2024-02-17 11:37:48,499 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:37:48,580 - 

2024-02-17 11:37:48,580 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:37:56,286 - Epoch: [54][  100/  500]    Overall Loss 0.796024    Objective Loss 0.796024                                        LR 0.000500    Time 0.076985    
2024-02-17 11:38:03,426 - Epoch: [54][  200/  500]    Overall Loss 0.797079    Objective Loss 0.797079                                        LR 0.000500    Time 0.074170    
2024-02-17 11:38:10,557 - Epoch: [54][  300/  500]    Overall Loss 0.806079    Objective Loss 0.806079                                        LR 0.000500    Time 0.073202    
2024-02-17 11:38:17,616 - Epoch: [54][  400/  500]    Overall Loss 0.813309    Objective Loss 0.813309                                        LR 0.000500    Time 0.072538    
2024-02-17 11:38:24,774 - Epoch: [54][  500/  500]    Overall Loss 0.818176    Objective Loss 0.818176    Top1 76.500000    Top5 96.000000    LR 0.000500    Time 0.072337    
2024-02-17 11:38:24,938 - --- validate (epoch=54)-----------
2024-02-17 11:38:24,939 - 10000 samples (100 per mini-batch)
2024-02-17 11:38:28,222 - Epoch: [54][  100/  100]    Loss 1.654874    Top1 58.370000    Top5 84.970000    
2024-02-17 11:38:28,326 - ==> Top1: 58.370    Top5: 84.970    Loss: 1.655

2024-02-17 11:38:28,336 - ==> Best [Top1: 58.960   Top5: 85.370   Sparsity:0.00   Params: 753952 on epoch: 53]
2024-02-17 11:38:28,337 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:38:28,400 - 

2024-02-17 11:38:28,400 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:38:36,042 - Epoch: [55][  100/  500]    Overall Loss 0.769146    Objective Loss 0.769146                                        LR 0.000500    Time 0.076361    
2024-02-17 11:38:43,162 - Epoch: [55][  200/  500]    Overall Loss 0.780171    Objective Loss 0.780171                                        LR 0.000500    Time 0.073759    
2024-02-17 11:38:50,231 - Epoch: [55][  300/  500]    Overall Loss 0.789821    Objective Loss 0.789821                                        LR 0.000500    Time 0.072720    
2024-02-17 11:38:57,274 - Epoch: [55][  400/  500]    Overall Loss 0.797652    Objective Loss 0.797652                                        LR 0.000500    Time 0.072137    
2024-02-17 11:39:04,455 - Epoch: [55][  500/  500]    Overall Loss 0.802727    Objective Loss 0.802727    Top1 80.000000    Top5 95.500000    LR 0.000500    Time 0.072062    
2024-02-17 11:39:04,615 - --- validate (epoch=55)-----------
2024-02-17 11:39:04,616 - 10000 samples (100 per mini-batch)
2024-02-17 11:39:07,773 - Epoch: [55][  100/  100]    Loss 1.577737    Top1 59.690000    Top5 85.980000    
2024-02-17 11:39:07,870 - ==> Top1: 59.690    Top5: 85.980    Loss: 1.578

2024-02-17 11:39:07,881 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:39:07,881 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:39:07,957 - 

2024-02-17 11:39:07,957 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:39:15,758 - Epoch: [56][  100/  500]    Overall Loss 0.767662    Objective Loss 0.767662                                        LR 0.000500    Time 0.077945    
2024-02-17 11:39:22,895 - Epoch: [56][  200/  500]    Overall Loss 0.775037    Objective Loss 0.775037                                        LR 0.000500    Time 0.074638    
2024-02-17 11:39:30,293 - Epoch: [56][  300/  500]    Overall Loss 0.782996    Objective Loss 0.782996                                        LR 0.000500    Time 0.074405    
2024-02-17 11:39:37,401 - Epoch: [56][  400/  500]    Overall Loss 0.793193    Objective Loss 0.793193                                        LR 0.000500    Time 0.073563    
2024-02-17 11:39:44,569 - Epoch: [56][  500/  500]    Overall Loss 0.798635    Objective Loss 0.798635    Top1 76.500000    Top5 95.500000    LR 0.000500    Time 0.073177    
2024-02-17 11:39:44,694 - --- validate (epoch=56)-----------
2024-02-17 11:39:44,695 - 10000 samples (100 per mini-batch)
2024-02-17 11:39:47,771 - Epoch: [56][  100/  100]    Loss 1.677374    Top1 57.960000    Top5 85.060000    
2024-02-17 11:39:47,889 - ==> Top1: 57.960    Top5: 85.060    Loss: 1.677

2024-02-17 11:39:47,901 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:39:47,901 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:39:47,967 - 

2024-02-17 11:39:47,968 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:39:55,626 - Epoch: [57][  100/  500]    Overall Loss 0.748854    Objective Loss 0.748854                                        LR 0.000500    Time 0.076521    
2024-02-17 11:40:02,771 - Epoch: [57][  200/  500]    Overall Loss 0.757948    Objective Loss 0.757948                                        LR 0.000500    Time 0.073964    
2024-02-17 11:40:10,008 - Epoch: [57][  300/  500]    Overall Loss 0.768813    Objective Loss 0.768813                                        LR 0.000500    Time 0.073422    
2024-02-17 11:40:17,083 - Epoch: [57][  400/  500]    Overall Loss 0.777358    Objective Loss 0.777358                                        LR 0.000500    Time 0.072741    
2024-02-17 11:40:24,331 - Epoch: [57][  500/  500]    Overall Loss 0.782456    Objective Loss 0.782456    Top1 78.000000    Top5 95.000000    LR 0.000500    Time 0.072681    
2024-02-17 11:40:24,441 - --- validate (epoch=57)-----------
2024-02-17 11:40:24,442 - 10000 samples (100 per mini-batch)
2024-02-17 11:40:27,309 - Epoch: [57][  100/  100]    Loss 1.637951    Top1 58.710000    Top5 85.710000    
2024-02-17 11:40:27,417 - ==> Top1: 58.710    Top5: 85.710    Loss: 1.638

2024-02-17 11:40:27,423 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:40:27,423 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:40:27,482 - 

2024-02-17 11:40:27,483 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:40:35,301 - Epoch: [58][  100/  500]    Overall Loss 0.757408    Objective Loss 0.757408                                        LR 0.000500    Time 0.078128    
2024-02-17 11:40:42,502 - Epoch: [58][  200/  500]    Overall Loss 0.752679    Objective Loss 0.752679                                        LR 0.000500    Time 0.075048    
2024-02-17 11:40:49,691 - Epoch: [58][  300/  500]    Overall Loss 0.761102    Objective Loss 0.761102                                        LR 0.000500    Time 0.073979    
2024-02-17 11:40:56,844 - Epoch: [58][  400/  500]    Overall Loss 0.769629    Objective Loss 0.769629                                        LR 0.000500    Time 0.073357    
2024-02-17 11:41:04,079 - Epoch: [58][  500/  500]    Overall Loss 0.776744    Objective Loss 0.776744    Top1 75.500000    Top5 94.500000    LR 0.000500    Time 0.073146    
2024-02-17 11:41:04,237 - --- validate (epoch=58)-----------
2024-02-17 11:41:04,238 - 10000 samples (100 per mini-batch)
2024-02-17 11:41:07,205 - Epoch: [58][  100/  100]    Loss 1.693725    Top1 58.040000    Top5 84.660000    
2024-02-17 11:41:07,299 - ==> Top1: 58.040    Top5: 84.660    Loss: 1.694

2024-02-17 11:41:07,305 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:41:07,305 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:41:07,360 - 

2024-02-17 11:41:07,361 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:41:15,197 - Epoch: [59][  100/  500]    Overall Loss 0.725762    Objective Loss 0.725762                                        LR 0.000500    Time 0.078303    
2024-02-17 11:41:22,314 - Epoch: [59][  200/  500]    Overall Loss 0.742909    Objective Loss 0.742909                                        LR 0.000500    Time 0.074716    
2024-02-17 11:41:29,380 - Epoch: [59][  300/  500]    Overall Loss 0.745777    Objective Loss 0.745777                                        LR 0.000500    Time 0.073348    
2024-02-17 11:41:36,747 - Epoch: [59][  400/  500]    Overall Loss 0.756595    Objective Loss 0.756595                                        LR 0.000500    Time 0.073418    
2024-02-17 11:41:44,054 - Epoch: [59][  500/  500]    Overall Loss 0.762398    Objective Loss 0.762398    Top1 74.500000    Top5 96.000000    LR 0.000500    Time 0.073342    
2024-02-17 11:41:44,183 - --- validate (epoch=59)-----------
2024-02-17 11:41:44,183 - 10000 samples (100 per mini-batch)
2024-02-17 11:41:47,065 - Epoch: [59][  100/  100]    Loss 1.658248    Top1 58.860000    Top5 85.150000    
2024-02-17 11:41:47,158 - ==> Top1: 58.860    Top5: 85.150    Loss: 1.658

2024-02-17 11:41:47,170 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:41:47,170 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:41:47,232 - 

2024-02-17 11:41:47,232 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:41:55,108 - Epoch: [60][  100/  500]    Overall Loss 0.717924    Objective Loss 0.717924                                        LR 0.000500    Time 0.078696    
2024-02-17 11:42:02,212 - Epoch: [60][  200/  500]    Overall Loss 0.740746    Objective Loss 0.740746                                        LR 0.000500    Time 0.074845    
2024-02-17 11:42:09,097 - Epoch: [60][  300/  500]    Overall Loss 0.746455    Objective Loss 0.746455                                        LR 0.000500    Time 0.072833    
2024-02-17 11:42:16,253 - Epoch: [60][  400/  500]    Overall Loss 0.755625    Objective Loss 0.755625                                        LR 0.000500    Time 0.072503    
2024-02-17 11:42:23,377 - Epoch: [60][  500/  500]    Overall Loss 0.761782    Objective Loss 0.761782    Top1 77.000000    Top5 96.000000    LR 0.000500    Time 0.072241    
2024-02-17 11:42:23,531 - --- validate (epoch=60)-----------
2024-02-17 11:42:23,531 - 10000 samples (100 per mini-batch)
2024-02-17 11:42:26,430 - Epoch: [60][  100/  100]    Loss 1.683339    Top1 57.680000    Top5 85.160000    
2024-02-17 11:42:26,552 - ==> Top1: 57.680    Top5: 85.160    Loss: 1.683

2024-02-17 11:42:26,565 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:42:26,566 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:42:26,628 - 

2024-02-17 11:42:26,628 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:42:34,207 - Epoch: [61][  100/  500]    Overall Loss 0.723786    Objective Loss 0.723786                                        LR 0.000500    Time 0.075740    
2024-02-17 11:42:41,411 - Epoch: [61][  200/  500]    Overall Loss 0.725941    Objective Loss 0.725941                                        LR 0.000500    Time 0.073867    
2024-02-17 11:42:48,533 - Epoch: [61][  300/  500]    Overall Loss 0.736931    Objective Loss 0.736931                                        LR 0.000500    Time 0.072971    
2024-02-17 11:42:55,538 - Epoch: [61][  400/  500]    Overall Loss 0.743888    Objective Loss 0.743888                                        LR 0.000500    Time 0.072230    
2024-02-17 11:43:02,624 - Epoch: [61][  500/  500]    Overall Loss 0.750233    Objective Loss 0.750233    Top1 76.500000    Top5 97.000000    LR 0.000500    Time 0.071947    
2024-02-17 11:43:02,742 - --- validate (epoch=61)-----------
2024-02-17 11:43:02,743 - 10000 samples (100 per mini-batch)
2024-02-17 11:43:05,757 - Epoch: [61][  100/  100]    Loss 1.703892    Top1 58.030000    Top5 84.660000    
2024-02-17 11:43:05,877 - ==> Top1: 58.030    Top5: 84.660    Loss: 1.704

2024-02-17 11:43:05,888 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:43:05,889 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:43:05,949 - 

2024-02-17 11:43:05,950 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:43:13,801 - Epoch: [62][  100/  500]    Overall Loss 0.716316    Objective Loss 0.716316                                        LR 0.000500    Time 0.078460    
2024-02-17 11:43:20,980 - Epoch: [62][  200/  500]    Overall Loss 0.725154    Objective Loss 0.725154                                        LR 0.000500    Time 0.075099    
2024-02-17 11:43:28,114 - Epoch: [62][  300/  500]    Overall Loss 0.726598    Objective Loss 0.726598                                        LR 0.000500    Time 0.073830    
2024-02-17 11:43:35,107 - Epoch: [62][  400/  500]    Overall Loss 0.732765    Objective Loss 0.732765                                        LR 0.000500    Time 0.072846    
2024-02-17 11:43:42,186 - Epoch: [62][  500/  500]    Overall Loss 0.735718    Objective Loss 0.735718    Top1 77.500000    Top5 97.000000    LR 0.000500    Time 0.072426    
2024-02-17 11:43:42,351 - --- validate (epoch=62)-----------
2024-02-17 11:43:42,351 - 10000 samples (100 per mini-batch)
2024-02-17 11:43:45,353 - Epoch: [62][  100/  100]    Loss 1.695187    Top1 58.080000    Top5 84.730000    
2024-02-17 11:43:45,458 - ==> Top1: 58.080    Top5: 84.730    Loss: 1.695

2024-02-17 11:43:45,463 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:43:45,464 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:43:45,520 - 

2024-02-17 11:43:45,520 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:43:53,241 - Epoch: [63][  100/  500]    Overall Loss 0.708055    Objective Loss 0.708055                                        LR 0.000500    Time 0.077153    
2024-02-17 11:44:00,618 - Epoch: [63][  200/  500]    Overall Loss 0.707810    Objective Loss 0.707810                                        LR 0.000500    Time 0.075441    
2024-02-17 11:44:07,935 - Epoch: [63][  300/  500]    Overall Loss 0.714700    Objective Loss 0.714700                                        LR 0.000500    Time 0.074668    
2024-02-17 11:44:15,005 - Epoch: [63][  400/  500]    Overall Loss 0.722657    Objective Loss 0.722657                                        LR 0.000500    Time 0.073664    
2024-02-17 11:44:22,091 - Epoch: [63][  500/  500]    Overall Loss 0.728273    Objective Loss 0.728273    Top1 78.000000    Top5 94.500000    LR 0.000500    Time 0.073094    
2024-02-17 11:44:22,204 - --- validate (epoch=63)-----------
2024-02-17 11:44:22,205 - 10000 samples (100 per mini-batch)
2024-02-17 11:44:25,293 - Epoch: [63][  100/  100]    Loss 1.712738    Top1 58.530000    Top5 85.230000    
2024-02-17 11:44:25,399 - ==> Top1: 58.530    Top5: 85.230    Loss: 1.713

2024-02-17 11:44:25,408 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:44:25,409 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:44:25,473 - 

2024-02-17 11:44:25,474 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:44:33,315 - Epoch: [64][  100/  500]    Overall Loss 0.699875    Objective Loss 0.699875                                        LR 0.000500    Time 0.078358    
2024-02-17 11:44:40,444 - Epoch: [64][  200/  500]    Overall Loss 0.711025    Objective Loss 0.711025                                        LR 0.000500    Time 0.074798    
2024-02-17 11:44:47,600 - Epoch: [64][  300/  500]    Overall Loss 0.711866    Objective Loss 0.711866                                        LR 0.000500    Time 0.073705    
2024-02-17 11:44:54,774 - Epoch: [64][  400/  500]    Overall Loss 0.718434    Objective Loss 0.718434                                        LR 0.000500    Time 0.073203    
2024-02-17 11:45:01,993 - Epoch: [64][  500/  500]    Overall Loss 0.721861    Objective Loss 0.721861    Top1 80.000000    Top5 97.500000    LR 0.000500    Time 0.072993    
2024-02-17 11:45:02,112 - --- validate (epoch=64)-----------
2024-02-17 11:45:02,113 - 10000 samples (100 per mini-batch)
2024-02-17 11:45:05,130 - Epoch: [64][  100/  100]    Loss 1.672986    Top1 58.580000    Top5 85.230000    
2024-02-17 11:45:05,295 - ==> Top1: 58.580    Top5: 85.230    Loss: 1.673

2024-02-17 11:45:05,303 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:45:05,304 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:45:05,372 - 

2024-02-17 11:45:05,372 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:45:12,921 - Epoch: [65][  100/  500]    Overall Loss 0.685307    Objective Loss 0.685307                                        LR 0.000500    Time 0.075432    
2024-02-17 11:45:19,984 - Epoch: [65][  200/  500]    Overall Loss 0.691732    Objective Loss 0.691732                                        LR 0.000500    Time 0.073008    
2024-02-17 11:45:27,009 - Epoch: [65][  300/  500]    Overall Loss 0.702046    Objective Loss 0.702046                                        LR 0.000500    Time 0.072076    
2024-02-17 11:45:34,035 - Epoch: [65][  400/  500]    Overall Loss 0.707181    Objective Loss 0.707181                                        LR 0.000500    Time 0.071610    
2024-02-17 11:45:41,097 - Epoch: [65][  500/  500]    Overall Loss 0.714602    Objective Loss 0.714602    Top1 77.000000    Top5 96.000000    LR 0.000500    Time 0.071406    
2024-02-17 11:45:41,226 - --- validate (epoch=65)-----------
2024-02-17 11:45:41,227 - 10000 samples (100 per mini-batch)
2024-02-17 11:45:44,122 - Epoch: [65][  100/  100]    Loss 1.658163    Top1 58.720000    Top5 85.510000    
2024-02-17 11:45:44,226 - ==> Top1: 58.720    Top5: 85.510    Loss: 1.658

2024-02-17 11:45:44,236 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:45:44,236 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:45:44,292 - 

2024-02-17 11:45:44,292 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:45:51,914 - Epoch: [66][  100/  500]    Overall Loss 0.674366    Objective Loss 0.674366                                        LR 0.000500    Time 0.076155    
2024-02-17 11:45:58,973 - Epoch: [66][  200/  500]    Overall Loss 0.684975    Objective Loss 0.684975                                        LR 0.000500    Time 0.073352    
2024-02-17 11:46:05,969 - Epoch: [66][  300/  500]    Overall Loss 0.694046    Objective Loss 0.694046                                        LR 0.000500    Time 0.072206    
2024-02-17 11:46:13,044 - Epoch: [66][  400/  500]    Overall Loss 0.699602    Objective Loss 0.699602                                        LR 0.000500    Time 0.071833    
2024-02-17 11:46:20,092 - Epoch: [66][  500/  500]    Overall Loss 0.704723    Objective Loss 0.704723    Top1 74.000000    Top5 93.000000    LR 0.000500    Time 0.071554    
2024-02-17 11:46:20,208 - --- validate (epoch=66)-----------
2024-02-17 11:46:20,209 - 10000 samples (100 per mini-batch)
2024-02-17 11:46:23,101 - Epoch: [66][  100/  100]    Loss 1.779810    Top1 57.320000    Top5 84.590000    
2024-02-17 11:46:23,208 - ==> Top1: 57.320    Top5: 84.590    Loss: 1.780

2024-02-17 11:46:23,220 - ==> Best [Top1: 59.690   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 55]
2024-02-17 11:46:23,220 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:46:23,282 - 

2024-02-17 11:46:23,283 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:46:31,041 - Epoch: [67][  100/  500]    Overall Loss 0.667879    Objective Loss 0.667879                                        LR 0.000500    Time 0.077534    
2024-02-17 11:46:38,231 - Epoch: [67][  200/  500]    Overall Loss 0.670391    Objective Loss 0.670391                                        LR 0.000500    Time 0.074694    
2024-02-17 11:46:45,465 - Epoch: [67][  300/  500]    Overall Loss 0.682634    Objective Loss 0.682634                                        LR 0.000500    Time 0.073895    
2024-02-17 11:46:52,504 - Epoch: [67][  400/  500]    Overall Loss 0.693407    Objective Loss 0.693407                                        LR 0.000500    Time 0.073007    
2024-02-17 11:46:59,563 - Epoch: [67][  500/  500]    Overall Loss 0.701100    Objective Loss 0.701100    Top1 82.000000    Top5 97.500000    LR 0.000500    Time 0.072515    
2024-02-17 11:46:59,663 - --- validate (epoch=67)-----------
2024-02-17 11:46:59,663 - 10000 samples (100 per mini-batch)
2024-02-17 11:47:02,579 - Epoch: [67][  100/  100]    Loss 1.624452    Top1 59.700000    Top5 85.970000    
2024-02-17 11:47:02,677 - ==> Top1: 59.700    Top5: 85.970    Loss: 1.624

2024-02-17 11:47:02,685 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:47:02,685 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:47:02,758 - 

2024-02-17 11:47:02,759 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:47:10,429 - Epoch: [68][  100/  500]    Overall Loss 0.666309    Objective Loss 0.666309                                        LR 0.000500    Time 0.076653    
2024-02-17 11:47:17,672 - Epoch: [68][  200/  500]    Overall Loss 0.671433    Objective Loss 0.671433                                        LR 0.000500    Time 0.074515    
2024-02-17 11:47:24,883 - Epoch: [68][  300/  500]    Overall Loss 0.681416    Objective Loss 0.681416                                        LR 0.000500    Time 0.073699    
2024-02-17 11:47:32,005 - Epoch: [68][  400/  500]    Overall Loss 0.681536    Objective Loss 0.681536                                        LR 0.000500    Time 0.073070    
2024-02-17 11:47:39,041 - Epoch: [68][  500/  500]    Overall Loss 0.689015    Objective Loss 0.689015    Top1 81.000000    Top5 96.500000    LR 0.000500    Time 0.072519    
2024-02-17 11:47:39,193 - --- validate (epoch=68)-----------
2024-02-17 11:47:39,193 - 10000 samples (100 per mini-batch)
2024-02-17 11:47:42,199 - Epoch: [68][  100/  100]    Loss 1.762042    Top1 57.330000    Top5 84.180000    
2024-02-17 11:47:42,299 - ==> Top1: 57.330    Top5: 84.180    Loss: 1.762

2024-02-17 11:47:42,309 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:47:42,310 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:47:42,371 - 

2024-02-17 11:47:42,371 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:47:50,019 - Epoch: [69][  100/  500]    Overall Loss 0.670748    Objective Loss 0.670748                                        LR 0.000500    Time 0.076422    
2024-02-17 11:47:57,144 - Epoch: [69][  200/  500]    Overall Loss 0.678400    Objective Loss 0.678400                                        LR 0.000500    Time 0.073812    
2024-02-17 11:48:04,286 - Epoch: [69][  300/  500]    Overall Loss 0.680186    Objective Loss 0.680186                                        LR 0.000500    Time 0.072999    
2024-02-17 11:48:11,196 - Epoch: [69][  400/  500]    Overall Loss 0.683780    Objective Loss 0.683780                                        LR 0.000500    Time 0.072017    
2024-02-17 11:48:18,242 - Epoch: [69][  500/  500]    Overall Loss 0.688127    Objective Loss 0.688127    Top1 75.000000    Top5 95.500000    LR 0.000500    Time 0.071696    
2024-02-17 11:48:18,342 - --- validate (epoch=69)-----------
2024-02-17 11:48:18,343 - 10000 samples (100 per mini-batch)
2024-02-17 11:48:21,272 - Epoch: [69][  100/  100]    Loss 1.698582    Top1 58.650000    Top5 85.260000    
2024-02-17 11:48:21,379 - ==> Top1: 58.650    Top5: 85.260    Loss: 1.699

2024-02-17 11:48:21,391 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:48:21,392 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:48:21,457 - 

2024-02-17 11:48:21,458 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:48:29,405 - Epoch: [70][  100/  500]    Overall Loss 0.654303    Objective Loss 0.654303                                        LR 0.000500    Time 0.079416    
2024-02-17 11:48:36,598 - Epoch: [70][  200/  500]    Overall Loss 0.663836    Objective Loss 0.663836                                        LR 0.000500    Time 0.075653    
2024-02-17 11:48:43,841 - Epoch: [70][  300/  500]    Overall Loss 0.670636    Objective Loss 0.670636                                        LR 0.000500    Time 0.074563    
2024-02-17 11:48:50,871 - Epoch: [70][  400/  500]    Overall Loss 0.675985    Objective Loss 0.675985                                        LR 0.000500    Time 0.073488    
2024-02-17 11:48:57,902 - Epoch: [70][  500/  500]    Overall Loss 0.682836    Objective Loss 0.682836    Top1 80.000000    Top5 96.500000    LR 0.000500    Time 0.072845    
2024-02-17 11:48:58,061 - --- validate (epoch=70)-----------
2024-02-17 11:48:58,063 - 10000 samples (100 per mini-batch)
2024-02-17 11:49:00,938 - Epoch: [70][  100/  100]    Loss 1.732679    Top1 58.640000    Top5 84.830000    
2024-02-17 11:49:01,062 - ==> Top1: 58.640    Top5: 84.830    Loss: 1.733

2024-02-17 11:49:01,072 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:49:01,073 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:49:01,136 - 

2024-02-17 11:49:01,136 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:49:08,658 - Epoch: [71][  100/  500]    Overall Loss 0.640584    Objective Loss 0.640584                                        LR 0.000500    Time 0.075170    
2024-02-17 11:49:15,984 - Epoch: [71][  200/  500]    Overall Loss 0.644814    Objective Loss 0.644814                                        LR 0.000500    Time 0.074194    
2024-02-17 11:49:23,207 - Epoch: [71][  300/  500]    Overall Loss 0.658395    Objective Loss 0.658395                                        LR 0.000500    Time 0.073525    
2024-02-17 11:49:30,110 - Epoch: [71][  400/  500]    Overall Loss 0.664975    Objective Loss 0.664975                                        LR 0.000500    Time 0.072390    
2024-02-17 11:49:37,260 - Epoch: [71][  500/  500]    Overall Loss 0.673418    Objective Loss 0.673418    Top1 76.500000    Top5 97.500000    LR 0.000500    Time 0.072204    
2024-02-17 11:49:37,377 - --- validate (epoch=71)-----------
2024-02-17 11:49:37,378 - 10000 samples (100 per mini-batch)
2024-02-17 11:49:40,361 - Epoch: [71][  100/  100]    Loss 1.694343    Top1 59.110000    Top5 85.550000    
2024-02-17 11:49:40,544 - ==> Top1: 59.110    Top5: 85.550    Loss: 1.694

2024-02-17 11:49:40,554 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:49:40,554 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:49:40,617 - 

2024-02-17 11:49:40,618 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:49:48,392 - Epoch: [72][  100/  500]    Overall Loss 0.634068    Objective Loss 0.634068                                        LR 0.000500    Time 0.077687    
2024-02-17 11:49:55,541 - Epoch: [72][  200/  500]    Overall Loss 0.639392    Objective Loss 0.639392                                        LR 0.000500    Time 0.074565    
2024-02-17 11:50:02,733 - Epoch: [72][  300/  500]    Overall Loss 0.649119    Objective Loss 0.649119                                        LR 0.000500    Time 0.073669    
2024-02-17 11:50:09,464 - Epoch: [72][  400/  500]    Overall Loss 0.660243    Objective Loss 0.660243                                        LR 0.000500    Time 0.072070    
2024-02-17 11:50:16,324 - Epoch: [72][  500/  500]    Overall Loss 0.662036    Objective Loss 0.662036    Top1 75.500000    Top5 97.000000    LR 0.000500    Time 0.071369    
2024-02-17 11:50:16,488 - --- validate (epoch=72)-----------
2024-02-17 11:50:16,488 - 10000 samples (100 per mini-batch)
2024-02-17 11:50:19,388 - Epoch: [72][  100/  100]    Loss 1.789423    Top1 58.120000    Top5 84.880000    
2024-02-17 11:50:19,494 - ==> Top1: 58.120    Top5: 84.880    Loss: 1.789

2024-02-17 11:50:19,506 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:50:19,507 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:50:19,571 - 

2024-02-17 11:50:19,571 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:50:27,477 - Epoch: [73][  100/  500]    Overall Loss 0.635874    Objective Loss 0.635874                                        LR 0.000500    Time 0.078998    
2024-02-17 11:50:34,364 - Epoch: [73][  200/  500]    Overall Loss 0.634746    Objective Loss 0.634746                                        LR 0.000500    Time 0.073919    
2024-02-17 11:50:41,171 - Epoch: [73][  300/  500]    Overall Loss 0.643160    Objective Loss 0.643160                                        LR 0.000500    Time 0.071957    
2024-02-17 11:50:48,302 - Epoch: [73][  400/  500]    Overall Loss 0.652762    Objective Loss 0.652762                                        LR 0.000500    Time 0.071783    
2024-02-17 11:50:55,521 - Epoch: [73][  500/  500]    Overall Loss 0.657868    Objective Loss 0.657868    Top1 81.000000    Top5 97.000000    LR 0.000500    Time 0.071857    
2024-02-17 11:50:55,657 - --- validate (epoch=73)-----------
2024-02-17 11:50:55,657 - 10000 samples (100 per mini-batch)
2024-02-17 11:50:58,538 - Epoch: [73][  100/  100]    Loss 1.707734    Top1 58.790000    Top5 85.400000    
2024-02-17 11:50:58,654 - ==> Top1: 58.790    Top5: 85.400    Loss: 1.708

2024-02-17 11:50:58,665 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:50:58,665 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:50:58,726 - 

2024-02-17 11:50:58,727 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:51:06,271 - Epoch: [74][  100/  500]    Overall Loss 0.609792    Objective Loss 0.609792                                        LR 0.000500    Time 0.075382    
2024-02-17 11:51:13,401 - Epoch: [74][  200/  500]    Overall Loss 0.622190    Objective Loss 0.622190                                        LR 0.000500    Time 0.073318    
2024-02-17 11:51:20,540 - Epoch: [74][  300/  500]    Overall Loss 0.634530    Objective Loss 0.634530                                        LR 0.000500    Time 0.072663    
2024-02-17 11:51:27,769 - Epoch: [74][  400/  500]    Overall Loss 0.644153    Objective Loss 0.644153                                        LR 0.000500    Time 0.072557    
2024-02-17 11:51:34,893 - Epoch: [74][  500/  500]    Overall Loss 0.649697    Objective Loss 0.649697    Top1 84.000000    Top5 98.000000    LR 0.000500    Time 0.072285    
2024-02-17 11:51:35,003 - --- validate (epoch=74)-----------
2024-02-17 11:51:35,004 - 10000 samples (100 per mini-batch)
2024-02-17 11:51:37,923 - Epoch: [74][  100/  100]    Loss 1.735290    Top1 58.450000    Top5 85.140000    
2024-02-17 11:51:38,110 - ==> Top1: 58.450    Top5: 85.140    Loss: 1.735

2024-02-17 11:51:38,346 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:51:38,347 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:51:38,400 - 

2024-02-17 11:51:38,400 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:51:45,856 - Epoch: [75][  100/  500]    Overall Loss 0.620486    Objective Loss 0.620486                                        LR 0.000500    Time 0.074508    
2024-02-17 11:51:52,880 - Epoch: [75][  200/  500]    Overall Loss 0.633876    Objective Loss 0.633876                                        LR 0.000500    Time 0.072352    
2024-02-17 11:51:59,957 - Epoch: [75][  300/  500]    Overall Loss 0.637861    Objective Loss 0.637861                                        LR 0.000500    Time 0.071809    
2024-02-17 11:52:07,055 - Epoch: [75][  400/  500]    Overall Loss 0.641115    Objective Loss 0.641115                                        LR 0.000500    Time 0.071593    
2024-02-17 11:52:14,159 - Epoch: [75][  500/  500]    Overall Loss 0.644184    Objective Loss 0.644184    Top1 79.500000    Top5 97.000000    LR 0.000500    Time 0.071474    
2024-02-17 11:52:14,270 - --- validate (epoch=75)-----------
2024-02-17 11:52:14,271 - 10000 samples (100 per mini-batch)
2024-02-17 11:52:17,101 - Epoch: [75][  100/  100]    Loss 1.705996    Top1 59.100000    Top5 85.270000    
2024-02-17 11:52:17,288 - ==> Top1: 59.100    Top5: 85.270    Loss: 1.706

2024-02-17 11:52:17,299 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:52:17,300 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:52:17,362 - 

2024-02-17 11:52:17,362 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:52:24,843 - Epoch: [76][  100/  500]    Overall Loss 0.610907    Objective Loss 0.610907                                        LR 0.000500    Time 0.074752    
2024-02-17 11:52:31,857 - Epoch: [76][  200/  500]    Overall Loss 0.624727    Objective Loss 0.624727                                        LR 0.000500    Time 0.072425    
2024-02-17 11:52:39,054 - Epoch: [76][  300/  500]    Overall Loss 0.624320    Objective Loss 0.624320                                        LR 0.000500    Time 0.072258    
2024-02-17 11:52:46,155 - Epoch: [76][  400/  500]    Overall Loss 0.631084    Objective Loss 0.631084                                        LR 0.000500    Time 0.071935    
2024-02-17 11:52:53,231 - Epoch: [76][  500/  500]    Overall Loss 0.636151    Objective Loss 0.636151    Top1 77.500000    Top5 98.500000    LR 0.000500    Time 0.071692    
2024-02-17 11:52:53,366 - --- validate (epoch=76)-----------
2024-02-17 11:52:53,367 - 10000 samples (100 per mini-batch)
2024-02-17 11:52:56,565 - Epoch: [76][  100/  100]    Loss 1.739712    Top1 58.250000    Top5 85.250000    
2024-02-17 11:52:56,662 - ==> Top1: 58.250    Top5: 85.250    Loss: 1.740

2024-02-17 11:52:56,672 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:52:56,673 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:52:56,735 - 

2024-02-17 11:52:56,735 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:53:04,168 - Epoch: [77][  100/  500]    Overall Loss 0.597206    Objective Loss 0.597206                                        LR 0.000500    Time 0.074280    
2024-02-17 11:53:11,105 - Epoch: [77][  200/  500]    Overall Loss 0.601952    Objective Loss 0.601952                                        LR 0.000500    Time 0.071802    
2024-02-17 11:53:18,292 - Epoch: [77][  300/  500]    Overall Loss 0.609414    Objective Loss 0.609414                                        LR 0.000500    Time 0.071808    
2024-02-17 11:53:25,422 - Epoch: [77][  400/  500]    Overall Loss 0.621360    Objective Loss 0.621360                                        LR 0.000500    Time 0.071670    
2024-02-17 11:53:32,507 - Epoch: [77][  500/  500]    Overall Loss 0.630023    Objective Loss 0.630023    Top1 84.000000    Top5 99.000000    LR 0.000500    Time 0.071497    
2024-02-17 11:53:32,672 - --- validate (epoch=77)-----------
2024-02-17 11:53:32,673 - 10000 samples (100 per mini-batch)
2024-02-17 11:53:35,673 - Epoch: [77][  100/  100]    Loss 1.715761    Top1 58.450000    Top5 85.800000    
2024-02-17 11:53:35,784 - ==> Top1: 58.450    Top5: 85.800    Loss: 1.716

2024-02-17 11:53:35,796 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:53:35,797 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:53:35,858 - 

2024-02-17 11:53:35,859 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:53:43,300 - Epoch: [78][  100/  500]    Overall Loss 0.602155    Objective Loss 0.602155                                        LR 0.000500    Time 0.074361    
2024-02-17 11:53:50,262 - Epoch: [78][  200/  500]    Overall Loss 0.619924    Objective Loss 0.619924                                        LR 0.000500    Time 0.071968    
2024-02-17 11:53:57,395 - Epoch: [78][  300/  500]    Overall Loss 0.618334    Objective Loss 0.618334                                        LR 0.000500    Time 0.071740    
2024-02-17 11:54:04,577 - Epoch: [78][  400/  500]    Overall Loss 0.623308    Objective Loss 0.623308                                        LR 0.000500    Time 0.071749    
2024-02-17 11:54:11,664 - Epoch: [78][  500/  500]    Overall Loss 0.626740    Objective Loss 0.626740    Top1 73.500000    Top5 97.500000    LR 0.000500    Time 0.071564    
2024-02-17 11:54:11,772 - --- validate (epoch=78)-----------
2024-02-17 11:54:11,773 - 10000 samples (100 per mini-batch)
2024-02-17 11:54:15,035 - Epoch: [78][  100/  100]    Loss 1.753041    Top1 57.960000    Top5 85.190000    
2024-02-17 11:54:15,158 - ==> Top1: 57.960    Top5: 85.190    Loss: 1.753

2024-02-17 11:54:15,174 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:54:15,175 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:54:15,246 - 

2024-02-17 11:54:15,246 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:54:23,177 - Epoch: [79][  100/  500]    Overall Loss 0.591097    Objective Loss 0.591097                                        LR 0.000500    Time 0.079248    
2024-02-17 11:54:30,610 - Epoch: [79][  200/  500]    Overall Loss 0.602335    Objective Loss 0.602335                                        LR 0.000500    Time 0.076769    
2024-02-17 11:54:37,754 - Epoch: [79][  300/  500]    Overall Loss 0.608733    Objective Loss 0.608733                                        LR 0.000500    Time 0.074977    
2024-02-17 11:54:44,899 - Epoch: [79][  400/  500]    Overall Loss 0.613700    Objective Loss 0.613700                                        LR 0.000500    Time 0.074085    
2024-02-17 11:54:52,029 - Epoch: [79][  500/  500]    Overall Loss 0.619532    Objective Loss 0.619532    Top1 76.000000    Top5 97.000000    LR 0.000500    Time 0.073520    
2024-02-17 11:54:52,141 - --- validate (epoch=79)-----------
2024-02-17 11:54:52,142 - 10000 samples (100 per mini-batch)
2024-02-17 11:54:55,034 - Epoch: [79][  100/  100]    Loss 1.748150    Top1 58.340000    Top5 85.160000    
2024-02-17 11:54:55,128 - ==> Top1: 58.340    Top5: 85.160    Loss: 1.748

2024-02-17 11:54:55,139 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:54:55,140 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:54:55,202 - 

2024-02-17 11:54:55,203 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:55:02,942 - Epoch: [80][  100/  500]    Overall Loss 0.586069    Objective Loss 0.586069                                        LR 0.000500    Time 0.077342    
2024-02-17 11:55:09,949 - Epoch: [80][  200/  500]    Overall Loss 0.584702    Objective Loss 0.584702                                        LR 0.000500    Time 0.073684    
2024-02-17 11:55:17,015 - Epoch: [80][  300/  500]    Overall Loss 0.597921    Objective Loss 0.597921                                        LR 0.000500    Time 0.072661    
2024-02-17 11:55:24,057 - Epoch: [80][  400/  500]    Overall Loss 0.604512    Objective Loss 0.604512                                        LR 0.000500    Time 0.072089    
2024-02-17 11:55:31,199 - Epoch: [80][  500/  500]    Overall Loss 0.610524    Objective Loss 0.610524    Top1 83.500000    Top5 97.500000    LR 0.000500    Time 0.071947    
2024-02-17 11:55:31,322 - --- validate (epoch=80)-----------
2024-02-17 11:55:31,323 - 10000 samples (100 per mini-batch)
2024-02-17 11:55:34,211 - Epoch: [80][  100/  100]    Loss 1.771340    Top1 58.120000    Top5 84.990000    
2024-02-17 11:55:34,308 - ==> Top1: 58.120    Top5: 84.990    Loss: 1.771

2024-02-17 11:55:34,320 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:55:34,320 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:55:34,381 - 

2024-02-17 11:55:34,382 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:55:41,858 - Epoch: [81][  100/  500]    Overall Loss 0.581604    Objective Loss 0.581604                                        LR 0.000500    Time 0.074711    
2024-02-17 11:55:48,863 - Epoch: [81][  200/  500]    Overall Loss 0.582514    Objective Loss 0.582514                                        LR 0.000500    Time 0.072358    
2024-02-17 11:55:56,000 - Epoch: [81][  300/  500]    Overall Loss 0.598248    Objective Loss 0.598248                                        LR 0.000500    Time 0.072012    
2024-02-17 11:56:03,062 - Epoch: [81][  400/  500]    Overall Loss 0.606046    Objective Loss 0.606046                                        LR 0.000500    Time 0.071653    
2024-02-17 11:56:10,311 - Epoch: [81][  500/  500]    Overall Loss 0.609403    Objective Loss 0.609403    Top1 82.000000    Top5 97.000000    LR 0.000500    Time 0.071812    
2024-02-17 11:56:10,447 - --- validate (epoch=81)-----------
2024-02-17 11:56:10,447 - 10000 samples (100 per mini-batch)
2024-02-17 11:56:13,404 - Epoch: [81][  100/  100]    Loss 1.777510    Top1 58.730000    Top5 84.850000    
2024-02-17 11:56:13,505 - ==> Top1: 58.730    Top5: 84.850    Loss: 1.778

2024-02-17 11:56:13,517 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:56:13,517 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:56:13,579 - 

2024-02-17 11:56:13,579 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:56:21,375 - Epoch: [82][  100/  500]    Overall Loss 0.580655    Objective Loss 0.580655                                        LR 0.000500    Time 0.077900    
2024-02-17 11:56:28,428 - Epoch: [82][  200/  500]    Overall Loss 0.586616    Objective Loss 0.586616                                        LR 0.000500    Time 0.074195    
2024-02-17 11:56:35,610 - Epoch: [82][  300/  500]    Overall Loss 0.591215    Objective Loss 0.591215                                        LR 0.000500    Time 0.073388    
2024-02-17 11:56:42,679 - Epoch: [82][  400/  500]    Overall Loss 0.591351    Objective Loss 0.591351                                        LR 0.000500    Time 0.072704    
2024-02-17 11:56:49,893 - Epoch: [82][  500/  500]    Overall Loss 0.597625    Objective Loss 0.597625    Top1 83.500000    Top5 99.000000    LR 0.000500    Time 0.072583    
2024-02-17 11:56:49,997 - --- validate (epoch=82)-----------
2024-02-17 11:56:49,998 - 10000 samples (100 per mini-batch)
2024-02-17 11:56:52,913 - Epoch: [82][  100/  100]    Loss 1.825459    Top1 57.690000    Top5 84.730000    
2024-02-17 11:56:53,075 - ==> Top1: 57.690    Top5: 84.730    Loss: 1.825

2024-02-17 11:56:53,086 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:56:53,086 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:56:53,148 - 

2024-02-17 11:56:53,148 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:57:00,671 - Epoch: [83][  100/  500]    Overall Loss 0.563809    Objective Loss 0.563809                                        LR 0.000500    Time 0.075176    
2024-02-17 11:57:07,721 - Epoch: [83][  200/  500]    Overall Loss 0.573816    Objective Loss 0.573816                                        LR 0.000500    Time 0.072816    
2024-02-17 11:57:15,014 - Epoch: [83][  300/  500]    Overall Loss 0.576394    Objective Loss 0.576394                                        LR 0.000500    Time 0.072837    
2024-02-17 11:57:22,083 - Epoch: [83][  400/  500]    Overall Loss 0.585076    Objective Loss 0.585076                                        LR 0.000500    Time 0.072289    
2024-02-17 11:57:29,262 - Epoch: [83][  500/  500]    Overall Loss 0.590422    Objective Loss 0.590422    Top1 83.000000    Top5 97.000000    LR 0.000500    Time 0.072180    
2024-02-17 11:57:29,448 - --- validate (epoch=83)-----------
2024-02-17 11:57:29,448 - 10000 samples (100 per mini-batch)
2024-02-17 11:57:32,335 - Epoch: [83][  100/  100]    Loss 1.793144    Top1 58.300000    Top5 85.080000    
2024-02-17 11:57:32,438 - ==> Top1: 58.300    Top5: 85.080    Loss: 1.793

2024-02-17 11:57:32,449 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:57:32,449 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:57:32,510 - 

2024-02-17 11:57:32,510 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:57:40,265 - Epoch: [84][  100/  500]    Overall Loss 0.537897    Objective Loss 0.537897                                        LR 0.000500    Time 0.077495    
2024-02-17 11:57:47,342 - Epoch: [84][  200/  500]    Overall Loss 0.562149    Objective Loss 0.562149                                        LR 0.000500    Time 0.074106    
2024-02-17 11:57:54,566 - Epoch: [84][  300/  500]    Overall Loss 0.570473    Objective Loss 0.570473                                        LR 0.000500    Time 0.073470    
2024-02-17 11:58:01,673 - Epoch: [84][  400/  500]    Overall Loss 0.577448    Objective Loss 0.577448                                        LR 0.000500    Time 0.072860    
2024-02-17 11:58:08,558 - Epoch: [84][  500/  500]    Overall Loss 0.587891    Objective Loss 0.587891    Top1 81.000000    Top5 97.000000    LR 0.000500    Time 0.072051    
2024-02-17 11:58:08,712 - --- validate (epoch=84)-----------
2024-02-17 11:58:08,713 - 10000 samples (100 per mini-batch)
2024-02-17 11:58:11,665 - Epoch: [84][  100/  100]    Loss 1.739641    Top1 58.970000    Top5 85.100000    
2024-02-17 11:58:11,787 - ==> Top1: 58.970    Top5: 85.100    Loss: 1.740

2024-02-17 11:58:11,800 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:58:11,800 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:58:11,864 - 

2024-02-17 11:58:11,865 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:58:19,792 - Epoch: [85][  100/  500]    Overall Loss 0.554587    Objective Loss 0.554587                                        LR 0.000500    Time 0.079168    
2024-02-17 11:58:27,247 - Epoch: [85][  200/  500]    Overall Loss 0.572589    Objective Loss 0.572589                                        LR 0.000500    Time 0.076838    
2024-02-17 11:58:34,192 - Epoch: [85][  300/  500]    Overall Loss 0.576759    Objective Loss 0.576759                                        LR 0.000500    Time 0.074361    
2024-02-17 11:58:41,324 - Epoch: [85][  400/  500]    Overall Loss 0.583177    Objective Loss 0.583177                                        LR 0.000500    Time 0.073590    
2024-02-17 11:58:48,489 - Epoch: [85][  500/  500]    Overall Loss 0.589292    Objective Loss 0.589292    Top1 81.000000    Top5 97.000000    LR 0.000500    Time 0.073193    
2024-02-17 11:58:48,631 - --- validate (epoch=85)-----------
2024-02-17 11:58:48,632 - 10000 samples (100 per mini-batch)
2024-02-17 11:58:51,604 - Epoch: [85][  100/  100]    Loss 1.771237    Top1 58.690000    Top5 85.650000    
2024-02-17 11:58:51,710 - ==> Top1: 58.690    Top5: 85.650    Loss: 1.771

2024-02-17 11:58:51,716 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:58:51,716 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:58:51,773 - 

2024-02-17 11:58:51,773 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:58:59,528 - Epoch: [86][  100/  500]    Overall Loss 0.539606    Objective Loss 0.539606                                        LR 0.000500    Time 0.077497    
2024-02-17 11:59:06,535 - Epoch: [86][  200/  500]    Overall Loss 0.546295    Objective Loss 0.546295                                        LR 0.000500    Time 0.073759    
2024-02-17 11:59:13,685 - Epoch: [86][  300/  500]    Overall Loss 0.558972    Objective Loss 0.558972                                        LR 0.000500    Time 0.072992    
2024-02-17 11:59:20,752 - Epoch: [86][  400/  500]    Overall Loss 0.567789    Objective Loss 0.567789                                        LR 0.000500    Time 0.072402    
2024-02-17 11:59:27,873 - Epoch: [86][  500/  500]    Overall Loss 0.574149    Objective Loss 0.574149    Top1 82.000000    Top5 97.000000    LR 0.000500    Time 0.072155    
2024-02-17 11:59:27,979 - --- validate (epoch=86)-----------
2024-02-17 11:59:27,980 - 10000 samples (100 per mini-batch)
2024-02-17 11:59:31,074 - Epoch: [86][  100/  100]    Loss 1.823585    Top1 57.930000    Top5 84.860000    
2024-02-17 11:59:31,173 - ==> Top1: 57.930    Top5: 84.860    Loss: 1.824

2024-02-17 11:59:31,184 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 11:59:31,184 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 11:59:31,244 - 

2024-02-17 11:59:31,244 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 11:59:39,094 - Epoch: [87][  100/  500]    Overall Loss 0.556764    Objective Loss 0.556764                                        LR 0.000500    Time 0.078448    
2024-02-17 11:59:46,401 - Epoch: [87][  200/  500]    Overall Loss 0.549502    Objective Loss 0.549502                                        LR 0.000500    Time 0.075736    
2024-02-17 11:59:53,531 - Epoch: [87][  300/  500]    Overall Loss 0.558405    Objective Loss 0.558405                                        LR 0.000500    Time 0.074244    
2024-02-17 12:00:00,585 - Epoch: [87][  400/  500]    Overall Loss 0.566278    Objective Loss 0.566278                                        LR 0.000500    Time 0.073307    
2024-02-17 12:00:07,756 - Epoch: [87][  500/  500]    Overall Loss 0.571798    Objective Loss 0.571798    Top1 79.000000    Top5 97.500000    LR 0.000500    Time 0.072980    
2024-02-17 12:00:07,916 - --- validate (epoch=87)-----------
2024-02-17 12:00:07,917 - 10000 samples (100 per mini-batch)
2024-02-17 12:00:11,028 - Epoch: [87][  100/  100]    Loss 1.827984    Top1 58.240000    Top5 84.840000    
2024-02-17 12:00:11,132 - ==> Top1: 58.240    Top5: 84.840    Loss: 1.828

2024-02-17 12:00:11,143 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:00:11,143 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:00:11,205 - 

2024-02-17 12:00:11,205 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:00:19,014 - Epoch: [88][  100/  500]    Overall Loss 0.547444    Objective Loss 0.547444                                        LR 0.000500    Time 0.078034    
2024-02-17 12:00:26,096 - Epoch: [88][  200/  500]    Overall Loss 0.558846    Objective Loss 0.558846                                        LR 0.000500    Time 0.074405    
2024-02-17 12:00:33,247 - Epoch: [88][  300/  500]    Overall Loss 0.563672    Objective Loss 0.563672                                        LR 0.000500    Time 0.073427    
2024-02-17 12:00:40,345 - Epoch: [88][  400/  500]    Overall Loss 0.568608    Objective Loss 0.568608                                        LR 0.000500    Time 0.072803    
2024-02-17 12:00:47,438 - Epoch: [88][  500/  500]    Overall Loss 0.572333    Objective Loss 0.572333    Top1 87.000000    Top5 98.000000    LR 0.000500    Time 0.072421    
2024-02-17 12:00:47,541 - --- validate (epoch=88)-----------
2024-02-17 12:00:47,542 - 10000 samples (100 per mini-batch)
2024-02-17 12:00:50,404 - Epoch: [88][  100/  100]    Loss 1.751668    Top1 58.800000    Top5 85.800000    
2024-02-17 12:00:50,575 - ==> Top1: 58.800    Top5: 85.800    Loss: 1.752

2024-02-17 12:00:50,586 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:00:50,587 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:00:50,647 - 

2024-02-17 12:00:50,647 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:00:58,272 - Epoch: [89][  100/  500]    Overall Loss 0.537153    Objective Loss 0.537153                                        LR 0.000500    Time 0.076191    
2024-02-17 12:01:05,395 - Epoch: [89][  200/  500]    Overall Loss 0.542063    Objective Loss 0.542063                                        LR 0.000500    Time 0.073688    
2024-02-17 12:01:12,507 - Epoch: [89][  300/  500]    Overall Loss 0.545037    Objective Loss 0.545037                                        LR 0.000500    Time 0.072819    
2024-02-17 12:01:19,705 - Epoch: [89][  400/  500]    Overall Loss 0.550915    Objective Loss 0.550915                                        LR 0.000500    Time 0.072598    
2024-02-17 12:01:27,065 - Epoch: [89][  500/  500]    Overall Loss 0.556556    Objective Loss 0.556556    Top1 87.000000    Top5 97.000000    LR 0.000500    Time 0.072790    
2024-02-17 12:01:27,239 - --- validate (epoch=89)-----------
2024-02-17 12:01:27,239 - 10000 samples (100 per mini-batch)
2024-02-17 12:01:30,230 - Epoch: [89][  100/  100]    Loss 1.848719    Top1 57.640000    Top5 84.930000    
2024-02-17 12:01:30,388 - ==> Top1: 57.640    Top5: 84.930    Loss: 1.849

2024-02-17 12:01:30,400 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:01:30,400 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:01:30,462 - 

2024-02-17 12:01:30,462 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:01:38,437 - Epoch: [90][  100/  500]    Overall Loss 0.529408    Objective Loss 0.529408                                        LR 0.000500    Time 0.079688    
2024-02-17 12:01:45,774 - Epoch: [90][  200/  500]    Overall Loss 0.539266    Objective Loss 0.539266                                        LR 0.000500    Time 0.076506    
2024-02-17 12:01:53,036 - Epoch: [90][  300/  500]    Overall Loss 0.540605    Objective Loss 0.540605                                        LR 0.000500    Time 0.075195    
2024-02-17 12:02:00,145 - Epoch: [90][  400/  500]    Overall Loss 0.546338    Objective Loss 0.546338                                        LR 0.000500    Time 0.074157    
2024-02-17 12:02:07,311 - Epoch: [90][  500/  500]    Overall Loss 0.556061    Objective Loss 0.556061    Top1 83.000000    Top5 97.500000    LR 0.000500    Time 0.073650    
2024-02-17 12:02:07,435 - --- validate (epoch=90)-----------
2024-02-17 12:02:07,436 - 10000 samples (100 per mini-batch)
2024-02-17 12:02:10,593 - Epoch: [90][  100/  100]    Loss 1.862603    Top1 57.940000    Top5 84.360000    
2024-02-17 12:02:10,728 - ==> Top1: 57.940    Top5: 84.360    Loss: 1.863

2024-02-17 12:02:10,733 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:02:10,734 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:02:10,789 - 

2024-02-17 12:02:10,790 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:02:18,253 - Epoch: [91][  100/  500]    Overall Loss 0.518050    Objective Loss 0.518050                                        LR 0.000500    Time 0.074571    
2024-02-17 12:02:25,321 - Epoch: [91][  200/  500]    Overall Loss 0.528958    Objective Loss 0.528958                                        LR 0.000500    Time 0.072605    
2024-02-17 12:02:32,426 - Epoch: [91][  300/  500]    Overall Loss 0.539271    Objective Loss 0.539271                                        LR 0.000500    Time 0.072073    
2024-02-17 12:02:39,498 - Epoch: [91][  400/  500]    Overall Loss 0.544243    Objective Loss 0.544243                                        LR 0.000500    Time 0.071724    
2024-02-17 12:02:46,593 - Epoch: [91][  500/  500]    Overall Loss 0.549738    Objective Loss 0.549738    Top1 87.000000    Top5 99.000000    LR 0.000500    Time 0.071561    
2024-02-17 12:02:46,764 - --- validate (epoch=91)-----------
2024-02-17 12:02:46,765 - 10000 samples (100 per mini-batch)
2024-02-17 12:02:50,030 - Epoch: [91][  100/  100]    Loss 1.888735    Top1 57.300000    Top5 84.190000    
2024-02-17 12:02:50,180 - ==> Top1: 57.300    Top5: 84.190    Loss: 1.889

2024-02-17 12:02:50,190 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:02:50,191 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:02:50,254 - 

2024-02-17 12:02:50,254 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:02:58,120 - Epoch: [92][  100/  500]    Overall Loss 0.512143    Objective Loss 0.512143                                        LR 0.000500    Time 0.078601    
2024-02-17 12:03:05,190 - Epoch: [92][  200/  500]    Overall Loss 0.524299    Objective Loss 0.524299                                        LR 0.000500    Time 0.074629    
2024-02-17 12:03:12,370 - Epoch: [92][  300/  500]    Overall Loss 0.532035    Objective Loss 0.532035                                        LR 0.000500    Time 0.073671    
2024-02-17 12:03:19,467 - Epoch: [92][  400/  500]    Overall Loss 0.539652    Objective Loss 0.539652                                        LR 0.000500    Time 0.072985    
2024-02-17 12:03:26,464 - Epoch: [92][  500/  500]    Overall Loss 0.544782    Objective Loss 0.544782    Top1 77.500000    Top5 96.500000    LR 0.000500    Time 0.072373    
2024-02-17 12:03:26,631 - --- validate (epoch=92)-----------
2024-02-17 12:03:26,632 - 10000 samples (100 per mini-batch)
2024-02-17 12:03:29,855 - Epoch: [92][  100/  100]    Loss 1.756985    Top1 59.200000    Top5 85.740000    
2024-02-17 12:03:29,977 - ==> Top1: 59.200    Top5: 85.740    Loss: 1.757

2024-02-17 12:03:29,983 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:03:29,983 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:03:30,046 - 

2024-02-17 12:03:30,046 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:03:37,737 - Epoch: [93][  100/  500]    Overall Loss 0.506487    Objective Loss 0.506487                                        LR 0.000500    Time 0.076851    
2024-02-17 12:03:44,871 - Epoch: [93][  200/  500]    Overall Loss 0.517290    Objective Loss 0.517290                                        LR 0.000500    Time 0.074073    
2024-02-17 12:03:52,053 - Epoch: [93][  300/  500]    Overall Loss 0.525091    Objective Loss 0.525091                                        LR 0.000500    Time 0.073306    
2024-02-17 12:03:58,996 - Epoch: [93][  400/  500]    Overall Loss 0.529492    Objective Loss 0.529492                                        LR 0.000500    Time 0.072328    
2024-02-17 12:04:06,072 - Epoch: [93][  500/  500]    Overall Loss 0.537910    Objective Loss 0.537910    Top1 83.000000    Top5 99.000000    LR 0.000500    Time 0.072007    
2024-02-17 12:04:06,257 - --- validate (epoch=93)-----------
2024-02-17 12:04:06,258 - 10000 samples (100 per mini-batch)
2024-02-17 12:04:09,196 - Epoch: [93][  100/  100]    Loss 1.832554    Top1 58.650000    Top5 84.650000    
2024-02-17 12:04:09,361 - ==> Top1: 58.650    Top5: 84.650    Loss: 1.833

2024-02-17 12:04:09,372 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:04:09,373 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:04:09,442 - 

2024-02-17 12:04:09,443 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:04:17,291 - Epoch: [94][  100/  500]    Overall Loss 0.500161    Objective Loss 0.500161                                        LR 0.000500    Time 0.078413    
2024-02-17 12:04:24,530 - Epoch: [94][  200/  500]    Overall Loss 0.509390    Objective Loss 0.509390                                        LR 0.000500    Time 0.075382    
2024-02-17 12:04:31,531 - Epoch: [94][  300/  500]    Overall Loss 0.520344    Objective Loss 0.520344                                        LR 0.000500    Time 0.073579    
2024-02-17 12:04:38,607 - Epoch: [94][  400/  500]    Overall Loss 0.527340    Objective Loss 0.527340                                        LR 0.000500    Time 0.072864    
2024-02-17 12:04:45,827 - Epoch: [94][  500/  500]    Overall Loss 0.534484    Objective Loss 0.534484    Top1 84.000000    Top5 98.000000    LR 0.000500    Time 0.072722    
2024-02-17 12:04:45,969 - --- validate (epoch=94)-----------
2024-02-17 12:04:45,969 - 10000 samples (100 per mini-batch)
2024-02-17 12:04:48,750 - Epoch: [94][  100/  100]    Loss 1.857067    Top1 57.960000    Top5 84.550000    
2024-02-17 12:04:48,852 - ==> Top1: 57.960    Top5: 84.550    Loss: 1.857

2024-02-17 12:04:48,863 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:04:48,864 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:04:48,927 - 

2024-02-17 12:04:48,928 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:04:56,533 - Epoch: [95][  100/  500]    Overall Loss 0.505093    Objective Loss 0.505093                                        LR 0.000500    Time 0.076000    
2024-02-17 12:05:03,579 - Epoch: [95][  200/  500]    Overall Loss 0.510239    Objective Loss 0.510239                                        LR 0.000500    Time 0.073206    
2024-02-17 12:05:10,702 - Epoch: [95][  300/  500]    Overall Loss 0.512875    Objective Loss 0.512875                                        LR 0.000500    Time 0.072531    
2024-02-17 12:05:17,755 - Epoch: [95][  400/  500]    Overall Loss 0.522760    Objective Loss 0.522760                                        LR 0.000500    Time 0.072021    
2024-02-17 12:05:24,987 - Epoch: [95][  500/  500]    Overall Loss 0.532060    Objective Loss 0.532060    Top1 84.000000    Top5 97.000000    LR 0.000500    Time 0.072074    
2024-02-17 12:05:25,117 - --- validate (epoch=95)-----------
2024-02-17 12:05:25,118 - 10000 samples (100 per mini-batch)
2024-02-17 12:05:27,864 - Epoch: [95][  100/  100]    Loss 1.794046    Top1 58.590000    Top5 85.450000    
2024-02-17 12:05:27,965 - ==> Top1: 58.590    Top5: 85.450    Loss: 1.794

2024-02-17 12:05:27,976 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:05:27,976 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:05:28,039 - 

2024-02-17 12:05:28,039 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:05:35,778 - Epoch: [96][  100/  500]    Overall Loss 0.506437    Objective Loss 0.506437                                        LR 0.000500    Time 0.077338    
2024-02-17 12:05:42,834 - Epoch: [96][  200/  500]    Overall Loss 0.510144    Objective Loss 0.510144                                        LR 0.000500    Time 0.073928    
2024-02-17 12:05:50,092 - Epoch: [96][  300/  500]    Overall Loss 0.513762    Objective Loss 0.513762                                        LR 0.000500    Time 0.073460    
2024-02-17 12:05:57,405 - Epoch: [96][  400/  500]    Overall Loss 0.521659    Objective Loss 0.521659                                        LR 0.000500    Time 0.073369    
2024-02-17 12:06:04,555 - Epoch: [96][  500/  500]    Overall Loss 0.523922    Objective Loss 0.523922    Top1 86.000000    Top5 98.500000    LR 0.000500    Time 0.072987    
2024-02-17 12:06:04,660 - --- validate (epoch=96)-----------
2024-02-17 12:06:04,660 - 10000 samples (100 per mini-batch)
2024-02-17 12:06:07,573 - Epoch: [96][  100/  100]    Loss 1.863796    Top1 57.850000    Top5 84.610000    
2024-02-17 12:06:07,669 - ==> Top1: 57.850    Top5: 84.610    Loss: 1.864

2024-02-17 12:06:07,681 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:06:07,682 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:06:07,743 - 

2024-02-17 12:06:07,744 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:06:15,282 - Epoch: [97][  100/  500]    Overall Loss 0.492927    Objective Loss 0.492927                                        LR 0.000500    Time 0.075323    
2024-02-17 12:06:22,419 - Epoch: [97][  200/  500]    Overall Loss 0.510134    Objective Loss 0.510134                                        LR 0.000500    Time 0.073323    
2024-02-17 12:06:29,258 - Epoch: [97][  300/  500]    Overall Loss 0.511359    Objective Loss 0.511359                                        LR 0.000500    Time 0.071668    
2024-02-17 12:06:36,288 - Epoch: [97][  400/  500]    Overall Loss 0.517971    Objective Loss 0.517971                                        LR 0.000500    Time 0.071315    
2024-02-17 12:06:43,420 - Epoch: [97][  500/  500]    Overall Loss 0.522544    Objective Loss 0.522544    Top1 80.000000    Top5 96.500000    LR 0.000500    Time 0.071308    
2024-02-17 12:06:43,584 - --- validate (epoch=97)-----------
2024-02-17 12:06:43,585 - 10000 samples (100 per mini-batch)
2024-02-17 12:06:46,512 - Epoch: [97][  100/  100]    Loss 1.799315    Top1 58.500000    Top5 85.210000    
2024-02-17 12:06:46,645 - ==> Top1: 58.500    Top5: 85.210    Loss: 1.799

2024-02-17 12:06:46,653 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:06:46,653 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:06:46,716 - 

2024-02-17 12:06:46,717 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:06:54,661 - Epoch: [98][  100/  500]    Overall Loss 0.492416    Objective Loss 0.492416                                        LR 0.000500    Time 0.079391    
2024-02-17 12:07:02,024 - Epoch: [98][  200/  500]    Overall Loss 0.509689    Objective Loss 0.509689                                        LR 0.000500    Time 0.076487    
2024-02-17 12:07:08,968 - Epoch: [98][  300/  500]    Overall Loss 0.514820    Objective Loss 0.514820                                        LR 0.000500    Time 0.074126    
2024-02-17 12:07:15,699 - Epoch: [98][  400/  500]    Overall Loss 0.519473    Objective Loss 0.519473                                        LR 0.000500    Time 0.072413    
2024-02-17 12:07:23,088 - Epoch: [98][  500/  500]    Overall Loss 0.520509    Objective Loss 0.520509    Top1 78.500000    Top5 96.000000    LR 0.000500    Time 0.072701    
2024-02-17 12:07:23,218 - --- validate (epoch=98)-----------
2024-02-17 12:07:23,219 - 10000 samples (100 per mini-batch)
2024-02-17 12:07:26,170 - Epoch: [98][  100/  100]    Loss 1.888811    Top1 57.210000    Top5 84.670000    
2024-02-17 12:07:26,358 - ==> Top1: 57.210    Top5: 84.670    Loss: 1.889

2024-02-17 12:07:26,370 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:07:26,371 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:07:26,434 - 

2024-02-17 12:07:26,434 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:07:33,972 - Epoch: [99][  100/  500]    Overall Loss 0.491538    Objective Loss 0.491538                                        LR 0.000500    Time 0.075325    
2024-02-17 12:07:41,174 - Epoch: [99][  200/  500]    Overall Loss 0.495809    Objective Loss 0.495809                                        LR 0.000500    Time 0.073646    
2024-02-17 12:07:48,429 - Epoch: [99][  300/  500]    Overall Loss 0.503789    Objective Loss 0.503789                                        LR 0.000500    Time 0.073267    
2024-02-17 12:07:55,426 - Epoch: [99][  400/  500]    Overall Loss 0.509748    Objective Loss 0.509748                                        LR 0.000500    Time 0.072434    
2024-02-17 12:08:02,374 - Epoch: [99][  500/  500]    Overall Loss 0.509678    Objective Loss 0.509678    Top1 83.000000    Top5 98.000000    LR 0.000500    Time 0.071834    
2024-02-17 12:08:02,498 - --- validate (epoch=99)-----------
2024-02-17 12:08:02,499 - 10000 samples (100 per mini-batch)
2024-02-17 12:08:05,426 - Epoch: [99][  100/  100]    Loss 1.846241    Top1 58.420000    Top5 85.290000    
2024-02-17 12:08:05,529 - ==> Top1: 58.420    Top5: 85.290    Loss: 1.846

2024-02-17 12:08:05,540 - ==> Best [Top1: 59.700   Top5: 85.970   Sparsity:0.00   Params: 753952 on epoch: 67]
2024-02-17 12:08:05,541 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:08:05,605 - 

2024-02-17 12:08:05,605 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:08:13,192 - Epoch: [100][  100/  500]    Overall Loss 0.451988    Objective Loss 0.451988                                        LR 0.000250    Time 0.075811    
2024-02-17 12:08:20,284 - Epoch: [100][  200/  500]    Overall Loss 0.452809    Objective Loss 0.452809                                        LR 0.000250    Time 0.073342    
2024-02-17 12:08:27,564 - Epoch: [100][  300/  500]    Overall Loss 0.448110    Objective Loss 0.448110                                        LR 0.000250    Time 0.073146    
2024-02-17 12:08:34,436 - Epoch: [100][  400/  500]    Overall Loss 0.446328    Objective Loss 0.446328                                        LR 0.000250    Time 0.072030    
2024-02-17 12:08:41,580 - Epoch: [100][  500/  500]    Overall Loss 0.444304    Objective Loss 0.444304    Top1 86.000000    Top5 99.500000    LR 0.000250    Time 0.071905    
2024-02-17 12:08:41,739 - --- validate (epoch=100)-----------
2024-02-17 12:08:41,740 - 10000 samples (100 per mini-batch)
2024-02-17 12:08:45,044 - Epoch: [100][  100/  100]    Loss 1.728898    Top1 59.900000    Top5 85.980000    
2024-02-17 12:08:45,147 - ==> Top1: 59.900    Top5: 85.980    Loss: 1.729

2024-02-17 12:08:45,159 - ==> Best [Top1: 59.900   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 100]
2024-02-17 12:08:45,160 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:08:45,236 - 

2024-02-17 12:08:45,236 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:08:52,762 - Epoch: [101][  100/  500]    Overall Loss 0.418033    Objective Loss 0.418033                                        LR 0.000250    Time 0.075199    
2024-02-17 12:08:59,883 - Epoch: [101][  200/  500]    Overall Loss 0.421524    Objective Loss 0.421524                                        LR 0.000250    Time 0.073180    
2024-02-17 12:09:07,082 - Epoch: [101][  300/  500]    Overall Loss 0.426896    Objective Loss 0.426896                                        LR 0.000250    Time 0.072769    
2024-02-17 12:09:14,144 - Epoch: [101][  400/  500]    Overall Loss 0.427725    Objective Loss 0.427725                                        LR 0.000250    Time 0.072222    
2024-02-17 12:09:21,337 - Epoch: [101][  500/  500]    Overall Loss 0.429468    Objective Loss 0.429468    Top1 86.000000    Top5 98.500000    LR 0.000250    Time 0.072155    
2024-02-17 12:09:21,446 - --- validate (epoch=101)-----------
2024-02-17 12:09:21,447 - 10000 samples (100 per mini-batch)
2024-02-17 12:09:24,490 - Epoch: [101][  100/  100]    Loss 1.757559    Top1 59.600000    Top5 86.150000    
2024-02-17 12:09:24,598 - ==> Top1: 59.600    Top5: 86.150    Loss: 1.758

2024-02-17 12:09:24,609 - ==> Best [Top1: 59.900   Top5: 85.980   Sparsity:0.00   Params: 753952 on epoch: 100]
2024-02-17 12:09:24,610 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:09:24,672 - 

2024-02-17 12:09:24,672 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:09:32,474 - Epoch: [102][  100/  500]    Overall Loss 0.416229    Objective Loss 0.416229                                        LR 0.000250    Time 0.077965    
2024-02-17 12:09:39,570 - Epoch: [102][  200/  500]    Overall Loss 0.413444    Objective Loss 0.413444                                        LR 0.000250    Time 0.074439    
2024-02-17 12:09:46,763 - Epoch: [102][  300/  500]    Overall Loss 0.420871    Objective Loss 0.420871                                        LR 0.000250    Time 0.073589    
2024-02-17 12:09:53,744 - Epoch: [102][  400/  500]    Overall Loss 0.427168    Objective Loss 0.427168                                        LR 0.000250    Time 0.072635    
2024-02-17 12:10:00,738 - Epoch: [102][  500/  500]    Overall Loss 0.427152    Objective Loss 0.427152    Top1 89.000000    Top5 99.500000    LR 0.000250    Time 0.072088    
2024-02-17 12:10:00,881 - --- validate (epoch=102)-----------
2024-02-17 12:10:00,881 - 10000 samples (100 per mini-batch)
2024-02-17 12:10:03,957 - Epoch: [102][  100/  100]    Loss 1.739574    Top1 60.070000    Top5 86.210000    
2024-02-17 12:10:04,055 - ==> Top1: 60.070    Top5: 86.210    Loss: 1.740

2024-02-17 12:10:04,065 - ==> Best [Top1: 60.070   Top5: 86.210   Sparsity:0.00   Params: 753952 on epoch: 102]
2024-02-17 12:10:04,065 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:10:04,145 - 

2024-02-17 12:10:04,145 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:10:11,927 - Epoch: [103][  100/  500]    Overall Loss 0.403438    Objective Loss 0.403438                                        LR 0.000250    Time 0.077766    
2024-02-17 12:10:19,178 - Epoch: [103][  200/  500]    Overall Loss 0.401079    Objective Loss 0.401079                                        LR 0.000250    Time 0.075118    
2024-02-17 12:10:26,257 - Epoch: [103][  300/  500]    Overall Loss 0.409565    Objective Loss 0.409565                                        LR 0.000250    Time 0.073660    
2024-02-17 12:10:33,280 - Epoch: [103][  400/  500]    Overall Loss 0.415376    Objective Loss 0.415376                                        LR 0.000250    Time 0.072794    
2024-02-17 12:10:40,430 - Epoch: [103][  500/  500]    Overall Loss 0.418406    Objective Loss 0.418406    Top1 89.000000    Top5 99.500000    LR 0.000250    Time 0.072525    
2024-02-17 12:10:40,603 - --- validate (epoch=103)-----------
2024-02-17 12:10:40,604 - 10000 samples (100 per mini-batch)
2024-02-17 12:10:43,666 - Epoch: [103][  100/  100]    Loss 1.754008    Top1 59.800000    Top5 85.760000    
2024-02-17 12:10:43,761 - ==> Top1: 59.800    Top5: 85.760    Loss: 1.754

2024-02-17 12:10:43,766 - ==> Best [Top1: 60.070   Top5: 86.210   Sparsity:0.00   Params: 753952 on epoch: 102]
2024-02-17 12:10:43,766 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:10:43,819 - 

2024-02-17 12:10:43,819 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:10:51,627 - Epoch: [104][  100/  500]    Overall Loss 0.394743    Objective Loss 0.394743                                        LR 0.000250    Time 0.078028    
2024-02-17 12:10:58,699 - Epoch: [104][  200/  500]    Overall Loss 0.400930    Objective Loss 0.400930                                        LR 0.000250    Time 0.074351    
2024-02-17 12:11:05,704 - Epoch: [104][  300/  500]    Overall Loss 0.403892    Objective Loss 0.403892                                        LR 0.000250    Time 0.072902    
2024-02-17 12:11:12,474 - Epoch: [104][  400/  500]    Overall Loss 0.407532    Objective Loss 0.407532                                        LR 0.000250    Time 0.071590    
2024-02-17 12:11:19,510 - Epoch: [104][  500/  500]    Overall Loss 0.412013    Objective Loss 0.412013    Top1 87.000000    Top5 99.000000    LR 0.000250    Time 0.071334    
2024-02-17 12:11:19,653 - --- validate (epoch=104)-----------
2024-02-17 12:11:19,655 - 10000 samples (100 per mini-batch)
2024-02-17 12:11:22,612 - Epoch: [104][  100/  100]    Loss 1.762275    Top1 60.200000    Top5 86.020000    
2024-02-17 12:11:22,712 - ==> Top1: 60.200    Top5: 86.020    Loss: 1.762

2024-02-17 12:11:22,722 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:11:22,723 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:11:22,817 - 

2024-02-17 12:11:22,818 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:11:30,421 - Epoch: [105][  100/  500]    Overall Loss 0.393309    Objective Loss 0.393309                                        LR 0.000250    Time 0.075972    
2024-02-17 12:11:37,600 - Epoch: [105][  200/  500]    Overall Loss 0.400789    Objective Loss 0.400789                                        LR 0.000250    Time 0.073857    
2024-02-17 12:11:44,715 - Epoch: [105][  300/  500]    Overall Loss 0.402713    Objective Loss 0.402713                                        LR 0.000250    Time 0.072941    
2024-02-17 12:11:51,727 - Epoch: [105][  400/  500]    Overall Loss 0.410494    Objective Loss 0.410494                                        LR 0.000250    Time 0.072225    
2024-02-17 12:11:58,819 - Epoch: [105][  500/  500]    Overall Loss 0.411551    Objective Loss 0.411551    Top1 88.500000    Top5 99.500000    LR 0.000250    Time 0.071956    
2024-02-17 12:11:58,936 - --- validate (epoch=105)-----------
2024-02-17 12:11:58,937 - 10000 samples (100 per mini-batch)
2024-02-17 12:12:01,903 - Epoch: [105][  100/  100]    Loss 1.748009    Top1 59.960000    Top5 86.130000    
2024-02-17 12:12:02,017 - ==> Top1: 59.960    Top5: 86.130    Loss: 1.748

2024-02-17 12:12:02,028 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:12:02,029 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:12:02,091 - 

2024-02-17 12:12:02,092 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:12:09,561 - Epoch: [106][  100/  500]    Overall Loss 0.394131    Objective Loss 0.394131                                        LR 0.000250    Time 0.074647    
2024-02-17 12:12:16,534 - Epoch: [106][  200/  500]    Overall Loss 0.396050    Objective Loss 0.396050                                        LR 0.000250    Time 0.072167    
2024-02-17 12:12:23,595 - Epoch: [106][  300/  500]    Overall Loss 0.398229    Objective Loss 0.398229                                        LR 0.000250    Time 0.071637    
2024-02-17 12:12:30,689 - Epoch: [106][  400/  500]    Overall Loss 0.399904    Objective Loss 0.399904                                        LR 0.000250    Time 0.071451    
2024-02-17 12:12:37,913 - Epoch: [106][  500/  500]    Overall Loss 0.403155    Objective Loss 0.403155    Top1 86.500000    Top5 99.500000    LR 0.000250    Time 0.071597    
2024-02-17 12:12:38,036 - --- validate (epoch=106)-----------
2024-02-17 12:12:38,037 - 10000 samples (100 per mini-batch)
2024-02-17 12:12:41,113 - Epoch: [106][  100/  100]    Loss 1.801233    Top1 59.730000    Top5 85.940000    
2024-02-17 12:12:41,323 - ==> Top1: 59.730    Top5: 85.940    Loss: 1.801

2024-02-17 12:12:41,335 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:12:41,335 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:12:41,397 - 

2024-02-17 12:12:41,398 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:12:49,108 - Epoch: [107][  100/  500]    Overall Loss 0.401932    Objective Loss 0.401932                                        LR 0.000250    Time 0.077047    
2024-02-17 12:12:56,277 - Epoch: [107][  200/  500]    Overall Loss 0.399534    Objective Loss 0.399534                                        LR 0.000250    Time 0.074348    
2024-02-17 12:13:03,420 - Epoch: [107][  300/  500]    Overall Loss 0.402269    Objective Loss 0.402269                                        LR 0.000250    Time 0.073358    
2024-02-17 12:13:10,371 - Epoch: [107][  400/  500]    Overall Loss 0.400546    Objective Loss 0.400546                                        LR 0.000250    Time 0.072387    
2024-02-17 12:13:17,420 - Epoch: [107][  500/  500]    Overall Loss 0.405310    Objective Loss 0.405310    Top1 87.000000    Top5 100.000000    LR 0.000250    Time 0.071999    
2024-02-17 12:13:17,538 - --- validate (epoch=107)-----------
2024-02-17 12:13:17,539 - 10000 samples (100 per mini-batch)
2024-02-17 12:13:20,647 - Epoch: [107][  100/  100]    Loss 1.785273    Top1 59.360000    Top5 86.040000    
2024-02-17 12:13:20,816 - ==> Top1: 59.360    Top5: 86.040    Loss: 1.785

2024-02-17 12:13:20,827 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:13:20,828 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:13:20,890 - 

2024-02-17 12:13:20,890 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:13:28,316 - Epoch: [108][  100/  500]    Overall Loss 0.384186    Objective Loss 0.384186                                        LR 0.000250    Time 0.074208    
2024-02-17 12:13:35,280 - Epoch: [108][  200/  500]    Overall Loss 0.387768    Objective Loss 0.387768                                        LR 0.000250    Time 0.071902    
2024-02-17 12:13:42,197 - Epoch: [108][  300/  500]    Overall Loss 0.390102    Objective Loss 0.390102                                        LR 0.000250    Time 0.070979    
2024-02-17 12:13:49,419 - Epoch: [108][  400/  500]    Overall Loss 0.397599    Objective Loss 0.397599                                        LR 0.000250    Time 0.071276    
2024-02-17 12:13:56,510 - Epoch: [108][  500/  500]    Overall Loss 0.399284    Objective Loss 0.399284    Top1 87.000000    Top5 98.500000    LR 0.000250    Time 0.071194    
2024-02-17 12:13:56,633 - --- validate (epoch=108)-----------
2024-02-17 12:13:56,633 - 10000 samples (100 per mini-batch)
2024-02-17 12:14:00,083 - Epoch: [108][  100/  100]    Loss 1.784492    Top1 60.080000    Top5 85.660000    
2024-02-17 12:14:00,192 - ==> Top1: 60.080    Top5: 85.660    Loss: 1.784

2024-02-17 12:14:00,203 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:14:00,204 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:14:00,266 - 

2024-02-17 12:14:00,266 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:14:07,752 - Epoch: [109][  100/  500]    Overall Loss 0.383582    Objective Loss 0.383582                                        LR 0.000250    Time 0.074798    
2024-02-17 12:14:14,876 - Epoch: [109][  200/  500]    Overall Loss 0.395178    Objective Loss 0.395178                                        LR 0.000250    Time 0.073000    
2024-02-17 12:14:21,861 - Epoch: [109][  300/  500]    Overall Loss 0.394470    Objective Loss 0.394470                                        LR 0.000250    Time 0.071936    
2024-02-17 12:14:28,988 - Epoch: [109][  400/  500]    Overall Loss 0.396336    Objective Loss 0.396336                                        LR 0.000250    Time 0.071759    
2024-02-17 12:14:36,309 - Epoch: [109][  500/  500]    Overall Loss 0.397936    Objective Loss 0.397936    Top1 92.500000    Top5 99.500000    LR 0.000250    Time 0.072039    
2024-02-17 12:14:36,450 - --- validate (epoch=109)-----------
2024-02-17 12:14:36,452 - 10000 samples (100 per mini-batch)
2024-02-17 12:14:39,605 - Epoch: [109][  100/  100]    Loss 1.760970    Top1 60.100000    Top5 86.090000    
2024-02-17 12:14:39,709 - ==> Top1: 60.100    Top5: 86.090    Loss: 1.761

2024-02-17 12:14:39,720 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:14:39,720 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:14:39,781 - 

2024-02-17 12:14:39,782 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:14:47,617 - Epoch: [110][  100/  500]    Overall Loss 0.375902    Objective Loss 0.375902                                        LR 0.000250    Time 0.078297    
2024-02-17 12:14:54,851 - Epoch: [110][  200/  500]    Overall Loss 0.385555    Objective Loss 0.385555                                        LR 0.000250    Time 0.075297    
2024-02-17 12:15:01,908 - Epoch: [110][  300/  500]    Overall Loss 0.391136    Objective Loss 0.391136                                        LR 0.000250    Time 0.073706    
2024-02-17 12:15:09,073 - Epoch: [110][  400/  500]    Overall Loss 0.393006    Objective Loss 0.393006                                        LR 0.000250    Time 0.073182    
2024-02-17 12:15:16,284 - Epoch: [110][  500/  500]    Overall Loss 0.395539    Objective Loss 0.395539    Top1 88.000000    Top5 98.500000    LR 0.000250    Time 0.072959    
2024-02-17 12:15:16,409 - --- validate (epoch=110)-----------
2024-02-17 12:15:16,409 - 10000 samples (100 per mini-batch)
2024-02-17 12:15:19,488 - Epoch: [110][  100/  100]    Loss 1.768408    Top1 59.650000    Top5 85.850000    
2024-02-17 12:15:19,600 - ==> Top1: 59.650    Top5: 85.850    Loss: 1.768

2024-02-17 12:15:19,611 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:15:19,611 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:15:19,675 - 

2024-02-17 12:15:19,675 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:15:27,308 - Epoch: [111][  100/  500]    Overall Loss 0.376204    Objective Loss 0.376204                                        LR 0.000250    Time 0.076273    
2024-02-17 12:15:34,501 - Epoch: [111][  200/  500]    Overall Loss 0.375309    Objective Loss 0.375309                                        LR 0.000250    Time 0.074081    
2024-02-17 12:15:41,564 - Epoch: [111][  300/  500]    Overall Loss 0.379431    Objective Loss 0.379431                                        LR 0.000250    Time 0.072914    
2024-02-17 12:15:48,783 - Epoch: [111][  400/  500]    Overall Loss 0.384439    Objective Loss 0.384439                                        LR 0.000250    Time 0.072723    
2024-02-17 12:15:56,058 - Epoch: [111][  500/  500]    Overall Loss 0.384868    Objective Loss 0.384868    Top1 91.500000    Top5 99.500000    LR 0.000250    Time 0.072720    
2024-02-17 12:15:56,248 - --- validate (epoch=111)-----------
2024-02-17 12:15:56,248 - 10000 samples (100 per mini-batch)
2024-02-17 12:15:59,491 - Epoch: [111][  100/  100]    Loss 1.789759    Top1 59.550000    Top5 85.860000    
2024-02-17 12:15:59,607 - ==> Top1: 59.550    Top5: 85.860    Loss: 1.790

2024-02-17 12:15:59,616 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:15:59,616 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:15:59,684 - 

2024-02-17 12:15:59,684 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:16:07,782 - Epoch: [112][  100/  500]    Overall Loss 0.372916    Objective Loss 0.372916                                        LR 0.000250    Time 0.080924    
2024-02-17 12:16:15,132 - Epoch: [112][  200/  500]    Overall Loss 0.377291    Objective Loss 0.377291                                        LR 0.000250    Time 0.077189    
2024-02-17 12:16:22,504 - Epoch: [112][  300/  500]    Overall Loss 0.377563    Objective Loss 0.377563                                        LR 0.000250    Time 0.076021    
2024-02-17 12:16:29,786 - Epoch: [112][  400/  500]    Overall Loss 0.383397    Objective Loss 0.383397                                        LR 0.000250    Time 0.075208    
2024-02-17 12:16:36,804 - Epoch: [112][  500/  500]    Overall Loss 0.386634    Objective Loss 0.386634    Top1 85.500000    Top5 99.500000    LR 0.000250    Time 0.074195    
2024-02-17 12:16:36,925 - --- validate (epoch=112)-----------
2024-02-17 12:16:36,927 - 10000 samples (100 per mini-batch)
2024-02-17 12:16:40,103 - Epoch: [112][  100/  100]    Loss 1.808364    Top1 59.860000    Top5 86.170000    
2024-02-17 12:16:40,264 - ==> Top1: 59.860    Top5: 86.170    Loss: 1.808

2024-02-17 12:16:40,276 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:16:40,277 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:16:40,344 - 

2024-02-17 12:16:40,344 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:16:48,002 - Epoch: [113][  100/  500]    Overall Loss 0.379976    Objective Loss 0.379976                                        LR 0.000250    Time 0.076515    
2024-02-17 12:16:55,100 - Epoch: [113][  200/  500]    Overall Loss 0.376639    Objective Loss 0.376639                                        LR 0.000250    Time 0.073727    
2024-02-17 12:17:02,108 - Epoch: [113][  300/  500]    Overall Loss 0.377383    Objective Loss 0.377383                                        LR 0.000250    Time 0.072498    
2024-02-17 12:17:09,177 - Epoch: [113][  400/  500]    Overall Loss 0.382346    Objective Loss 0.382346                                        LR 0.000250    Time 0.072034    
2024-02-17 12:17:16,446 - Epoch: [113][  500/  500]    Overall Loss 0.384519    Objective Loss 0.384519    Top1 86.000000    Top5 99.500000    LR 0.000250    Time 0.072157    
2024-02-17 12:17:16,577 - --- validate (epoch=113)-----------
2024-02-17 12:17:16,578 - 10000 samples (100 per mini-batch)
2024-02-17 12:17:19,737 - Epoch: [113][  100/  100]    Loss 1.799925    Top1 59.570000    Top5 85.930000    
2024-02-17 12:17:19,845 - ==> Top1: 59.570    Top5: 85.930    Loss: 1.800

2024-02-17 12:17:19,854 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:17:19,854 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:17:19,916 - 

2024-02-17 12:17:19,916 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:17:27,825 - Epoch: [114][  100/  500]    Overall Loss 0.362519    Objective Loss 0.362519                                        LR 0.000250    Time 0.079030    
2024-02-17 12:17:34,992 - Epoch: [114][  200/  500]    Overall Loss 0.368233    Objective Loss 0.368233                                        LR 0.000250    Time 0.075327    
2024-02-17 12:17:42,040 - Epoch: [114][  300/  500]    Overall Loss 0.368697    Objective Loss 0.368697                                        LR 0.000250    Time 0.073698    
2024-02-17 12:17:48,957 - Epoch: [114][  400/  500]    Overall Loss 0.373663    Objective Loss 0.373663                                        LR 0.000250    Time 0.072556    
2024-02-17 12:17:56,116 - Epoch: [114][  500/  500]    Overall Loss 0.380259    Objective Loss 0.380259    Top1 90.000000    Top5 99.000000    LR 0.000250    Time 0.072354    
2024-02-17 12:17:56,291 - --- validate (epoch=114)-----------
2024-02-17 12:17:56,292 - 10000 samples (100 per mini-batch)
2024-02-17 12:17:59,356 - Epoch: [114][  100/  100]    Loss 1.824426    Top1 59.310000    Top5 85.840000    
2024-02-17 12:17:59,496 - ==> Top1: 59.310    Top5: 85.840    Loss: 1.824

2024-02-17 12:17:59,503 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:17:59,503 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:17:59,565 - 

2024-02-17 12:17:59,565 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:18:07,127 - Epoch: [115][  100/  500]    Overall Loss 0.372828    Objective Loss 0.372828                                        LR 0.000250    Time 0.075557    
2024-02-17 12:18:14,252 - Epoch: [115][  200/  500]    Overall Loss 0.373907    Objective Loss 0.373907                                        LR 0.000250    Time 0.073383    
2024-02-17 12:18:21,389 - Epoch: [115][  300/  500]    Overall Loss 0.372180    Objective Loss 0.372180                                        LR 0.000250    Time 0.072696    
2024-02-17 12:18:28,462 - Epoch: [115][  400/  500]    Overall Loss 0.377547    Objective Loss 0.377547                                        LR 0.000250    Time 0.072194    
2024-02-17 12:18:35,637 - Epoch: [115][  500/  500]    Overall Loss 0.378982    Objective Loss 0.378982    Top1 85.000000    Top5 98.500000    LR 0.000250    Time 0.072097    
2024-02-17 12:18:35,770 - --- validate (epoch=115)-----------
2024-02-17 12:18:35,771 - 10000 samples (100 per mini-batch)
2024-02-17 12:18:38,560 - Epoch: [115][  100/  100]    Loss 1.803950    Top1 59.830000    Top5 85.820000    
2024-02-17 12:18:38,716 - ==> Top1: 59.830    Top5: 85.820    Loss: 1.804

2024-02-17 12:18:38,727 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:18:38,727 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:18:38,789 - 

2024-02-17 12:18:38,790 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:18:46,319 - Epoch: [116][  100/  500]    Overall Loss 0.370018    Objective Loss 0.370018                                        LR 0.000250    Time 0.075241    
2024-02-17 12:18:53,256 - Epoch: [116][  200/  500]    Overall Loss 0.365850    Objective Loss 0.365850                                        LR 0.000250    Time 0.072285    
2024-02-17 12:19:00,027 - Epoch: [116][  300/  500]    Overall Loss 0.368996    Objective Loss 0.368996                                        LR 0.000250    Time 0.070746    
2024-02-17 12:19:06,830 - Epoch: [116][  400/  500]    Overall Loss 0.370000    Objective Loss 0.370000                                        LR 0.000250    Time 0.070060    
2024-02-17 12:19:13,938 - Epoch: [116][  500/  500]    Overall Loss 0.374056    Objective Loss 0.374056    Top1 85.000000    Top5 99.500000    LR 0.000250    Time 0.070256    
2024-02-17 12:19:14,090 - --- validate (epoch=116)-----------
2024-02-17 12:19:14,091 - 10000 samples (100 per mini-batch)
2024-02-17 12:19:17,185 - Epoch: [116][  100/  100]    Loss 1.815954    Top1 59.720000    Top5 85.730000    
2024-02-17 12:19:17,305 - ==> Top1: 59.720    Top5: 85.730    Loss: 1.816

2024-02-17 12:19:17,317 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:19:17,317 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:19:17,594 - 

2024-02-17 12:19:17,594 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:19:25,272 - Epoch: [117][  100/  500]    Overall Loss 0.357760    Objective Loss 0.357760                                        LR 0.000250    Time 0.076725    
2024-02-17 12:19:32,357 - Epoch: [117][  200/  500]    Overall Loss 0.356955    Objective Loss 0.356955                                        LR 0.000250    Time 0.073767    
2024-02-17 12:19:39,578 - Epoch: [117][  300/  500]    Overall Loss 0.364442    Objective Loss 0.364442                                        LR 0.000250    Time 0.073235    
2024-02-17 12:19:46,703 - Epoch: [117][  400/  500]    Overall Loss 0.370243    Objective Loss 0.370243                                        LR 0.000250    Time 0.072726    
2024-02-17 12:19:53,984 - Epoch: [117][  500/  500]    Overall Loss 0.372194    Objective Loss 0.372194    Top1 89.500000    Top5 99.500000    LR 0.000250    Time 0.072736    
2024-02-17 12:19:54,139 - --- validate (epoch=117)-----------
2024-02-17 12:19:54,140 - 10000 samples (100 per mini-batch)
2024-02-17 12:19:57,279 - Epoch: [117][  100/  100]    Loss 1.813385    Top1 60.020000    Top5 85.900000    
2024-02-17 12:19:57,383 - ==> Top1: 60.020    Top5: 85.900    Loss: 1.813

2024-02-17 12:19:57,389 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:19:57,389 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:19:57,445 - 

2024-02-17 12:19:57,445 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:20:05,375 - Epoch: [118][  100/  500]    Overall Loss 0.357653    Objective Loss 0.357653                                        LR 0.000250    Time 0.079246    
2024-02-17 12:20:12,409 - Epoch: [118][  200/  500]    Overall Loss 0.353822    Objective Loss 0.353822                                        LR 0.000250    Time 0.074770    
2024-02-17 12:20:19,535 - Epoch: [118][  300/  500]    Overall Loss 0.354413    Objective Loss 0.354413                                        LR 0.000250    Time 0.073587    
2024-02-17 12:20:26,626 - Epoch: [118][  400/  500]    Overall Loss 0.362473    Objective Loss 0.362473                                        LR 0.000250    Time 0.072906    
2024-02-17 12:20:33,774 - Epoch: [118][  500/  500]    Overall Loss 0.368500    Objective Loss 0.368500    Top1 89.000000    Top5 99.500000    LR 0.000250    Time 0.072607    
2024-02-17 12:20:33,903 - --- validate (epoch=118)-----------
2024-02-17 12:20:33,903 - 10000 samples (100 per mini-batch)
2024-02-17 12:20:36,937 - Epoch: [118][  100/  100]    Loss 1.849383    Top1 59.390000    Top5 85.570000    
2024-02-17 12:20:37,063 - ==> Top1: 59.390    Top5: 85.570    Loss: 1.849

2024-02-17 12:20:37,069 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:20:37,069 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:20:37,126 - 

2024-02-17 12:20:37,127 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:20:44,799 - Epoch: [119][  100/  500]    Overall Loss 0.345665    Objective Loss 0.345665                                        LR 0.000250    Time 0.076660    
2024-02-17 12:20:51,838 - Epoch: [119][  200/  500]    Overall Loss 0.359003    Objective Loss 0.359003                                        LR 0.000250    Time 0.073504    
2024-02-17 12:20:58,951 - Epoch: [119][  300/  500]    Overall Loss 0.360961    Objective Loss 0.360961                                        LR 0.000250    Time 0.072697    
2024-02-17 12:21:06,164 - Epoch: [119][  400/  500]    Overall Loss 0.365260    Objective Loss 0.365260                                        LR 0.000250    Time 0.072545    
2024-02-17 12:21:13,380 - Epoch: [119][  500/  500]    Overall Loss 0.368261    Objective Loss 0.368261    Top1 88.000000    Top5 99.500000    LR 0.000250    Time 0.072459    
2024-02-17 12:21:13,550 - --- validate (epoch=119)-----------
2024-02-17 12:21:13,551 - 10000 samples (100 per mini-batch)
2024-02-17 12:21:16,465 - Epoch: [119][  100/  100]    Loss 1.855645    Top1 59.090000    Top5 85.700000    
2024-02-17 12:21:16,558 - ==> Top1: 59.090    Top5: 85.700    Loss: 1.856

2024-02-17 12:21:16,569 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:21:16,570 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:21:16,635 - 

2024-02-17 12:21:16,635 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:21:24,237 - Epoch: [120][  100/  500]    Overall Loss 0.339139    Objective Loss 0.339139                                        LR 0.000250    Time 0.075955    
2024-02-17 12:21:31,308 - Epoch: [120][  200/  500]    Overall Loss 0.346002    Objective Loss 0.346002                                        LR 0.000250    Time 0.073310    
2024-02-17 12:21:38,471 - Epoch: [120][  300/  500]    Overall Loss 0.356069    Objective Loss 0.356069                                        LR 0.000250    Time 0.072736    
2024-02-17 12:21:45,391 - Epoch: [120][  400/  500]    Overall Loss 0.359483    Objective Loss 0.359483                                        LR 0.000250    Time 0.071843    
2024-02-17 12:21:52,431 - Epoch: [120][  500/  500]    Overall Loss 0.365972    Objective Loss 0.365972    Top1 89.000000    Top5 98.500000    LR 0.000250    Time 0.071545    
2024-02-17 12:21:52,537 - --- validate (epoch=120)-----------
2024-02-17 12:21:52,538 - 10000 samples (100 per mini-batch)
2024-02-17 12:21:55,583 - Epoch: [120][  100/  100]    Loss 1.849578    Top1 59.150000    Top5 85.530000    
2024-02-17 12:21:55,679 - ==> Top1: 59.150    Top5: 85.530    Loss: 1.850

2024-02-17 12:21:55,905 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:21:55,906 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:21:55,958 - 

2024-02-17 12:21:55,959 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:22:03,526 - Epoch: [121][  100/  500]    Overall Loss 0.353543    Objective Loss 0.353543                                        LR 0.000250    Time 0.075622    
2024-02-17 12:22:10,609 - Epoch: [121][  200/  500]    Overall Loss 0.355225    Objective Loss 0.355225                                        LR 0.000250    Time 0.073204    
2024-02-17 12:22:17,672 - Epoch: [121][  300/  500]    Overall Loss 0.357259    Objective Loss 0.357259                                        LR 0.000250    Time 0.072330    
2024-02-17 12:22:24,882 - Epoch: [121][  400/  500]    Overall Loss 0.362525    Objective Loss 0.362525                                        LR 0.000250    Time 0.072263    
2024-02-17 12:22:32,133 - Epoch: [121][  500/  500]    Overall Loss 0.365523    Objective Loss 0.365523    Top1 92.500000    Top5 99.500000    LR 0.000250    Time 0.072303    
2024-02-17 12:22:32,257 - --- validate (epoch=121)-----------
2024-02-17 12:22:32,259 - 10000 samples (100 per mini-batch)
2024-02-17 12:22:35,431 - Epoch: [121][  100/  100]    Loss 1.824821    Top1 59.830000    Top5 86.040000    
2024-02-17 12:22:35,570 - ==> Top1: 59.830    Top5: 86.040    Loss: 1.825

2024-02-17 12:22:35,581 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:22:35,582 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:22:35,645 - 

2024-02-17 12:22:35,645 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:22:43,136 - Epoch: [122][  100/  500]    Overall Loss 0.342752    Objective Loss 0.342752                                        LR 0.000250    Time 0.074858    
2024-02-17 12:22:50,007 - Epoch: [122][  200/  500]    Overall Loss 0.351260    Objective Loss 0.351260                                        LR 0.000250    Time 0.071762    
2024-02-17 12:22:57,025 - Epoch: [122][  300/  500]    Overall Loss 0.356273    Objective Loss 0.356273                                        LR 0.000250    Time 0.071219    
2024-02-17 12:23:04,279 - Epoch: [122][  400/  500]    Overall Loss 0.360148    Objective Loss 0.360148                                        LR 0.000250    Time 0.071539    
2024-02-17 12:23:11,473 - Epoch: [122][  500/  500]    Overall Loss 0.361480    Objective Loss 0.361480    Top1 86.500000    Top5 99.500000    LR 0.000250    Time 0.071611    
2024-02-17 12:23:11,597 - --- validate (epoch=122)-----------
2024-02-17 12:23:11,598 - 10000 samples (100 per mini-batch)
2024-02-17 12:23:14,941 - Epoch: [122][  100/  100]    Loss 1.836520    Top1 59.600000    Top5 85.500000    
2024-02-17 12:23:15,049 - ==> Top1: 59.600    Top5: 85.500    Loss: 1.837

2024-02-17 12:23:15,059 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:23:15,060 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:23:15,123 - 

2024-02-17 12:23:15,124 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:23:22,709 - Epoch: [123][  100/  500]    Overall Loss 0.335022    Objective Loss 0.335022                                        LR 0.000250    Time 0.075772    
2024-02-17 12:23:29,676 - Epoch: [123][  200/  500]    Overall Loss 0.342234    Objective Loss 0.342234                                        LR 0.000250    Time 0.072698    
2024-02-17 12:23:36,696 - Epoch: [123][  300/  500]    Overall Loss 0.348723    Objective Loss 0.348723                                        LR 0.000250    Time 0.071852    
2024-02-17 12:23:43,783 - Epoch: [123][  400/  500]    Overall Loss 0.351100    Objective Loss 0.351100                                        LR 0.000250    Time 0.071597    
2024-02-17 12:23:50,798 - Epoch: [123][  500/  500]    Overall Loss 0.353314    Objective Loss 0.353314    Top1 84.500000    Top5 100.000000    LR 0.000250    Time 0.071299    
2024-02-17 12:23:51,000 - --- validate (epoch=123)-----------
2024-02-17 12:23:51,001 - 10000 samples (100 per mini-batch)
2024-02-17 12:23:54,002 - Epoch: [123][  100/  100]    Loss 1.858697    Top1 59.060000    Top5 85.740000    
2024-02-17 12:23:54,111 - ==> Top1: 59.060    Top5: 85.740    Loss: 1.859

2024-02-17 12:23:54,125 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:23:54,126 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:23:54,191 - 

2024-02-17 12:23:54,191 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:24:01,701 - Epoch: [124][  100/  500]    Overall Loss 0.339974    Objective Loss 0.339974                                        LR 0.000250    Time 0.075048    
2024-02-17 12:24:08,683 - Epoch: [124][  200/  500]    Overall Loss 0.345771    Objective Loss 0.345771                                        LR 0.000250    Time 0.072416    
2024-02-17 12:24:15,832 - Epoch: [124][  300/  500]    Overall Loss 0.345579    Objective Loss 0.345579                                        LR 0.000250    Time 0.072096    
2024-02-17 12:24:22,996 - Epoch: [124][  400/  500]    Overall Loss 0.349166    Objective Loss 0.349166                                        LR 0.000250    Time 0.071972    
2024-02-17 12:24:30,191 - Epoch: [124][  500/  500]    Overall Loss 0.353267    Objective Loss 0.353267    Top1 87.000000    Top5 99.000000    LR 0.000250    Time 0.071957    
2024-02-17 12:24:30,310 - --- validate (epoch=124)-----------
2024-02-17 12:24:30,311 - 10000 samples (100 per mini-batch)
2024-02-17 12:24:33,281 - Epoch: [124][  100/  100]    Loss 1.852374    Top1 59.910000    Top5 85.740000    
2024-02-17 12:24:33,467 - ==> Top1: 59.910    Top5: 85.740    Loss: 1.852

2024-02-17 12:24:33,478 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:24:33,478 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:24:33,544 - 

2024-02-17 12:24:33,544 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:24:41,115 - Epoch: [125][  100/  500]    Overall Loss 0.342641    Objective Loss 0.342641                                        LR 0.000250    Time 0.075648    
2024-02-17 12:24:48,196 - Epoch: [125][  200/  500]    Overall Loss 0.339652    Objective Loss 0.339652                                        LR 0.000250    Time 0.073210    
2024-02-17 12:24:55,237 - Epoch: [125][  300/  500]    Overall Loss 0.342186    Objective Loss 0.342186                                        LR 0.000250    Time 0.072263    
2024-02-17 12:25:02,347 - Epoch: [125][  400/  500]    Overall Loss 0.347361    Objective Loss 0.347361                                        LR 0.000250    Time 0.071962    
2024-02-17 12:25:09,526 - Epoch: [125][  500/  500]    Overall Loss 0.349484    Objective Loss 0.349484    Top1 92.500000    Top5 100.000000    LR 0.000250    Time 0.071919    
2024-02-17 12:25:09,654 - --- validate (epoch=125)-----------
2024-02-17 12:25:09,655 - 10000 samples (100 per mini-batch)
2024-02-17 12:25:12,554 - Epoch: [125][  100/  100]    Loss 1.864347    Top1 59.370000    Top5 85.530000    
2024-02-17 12:25:12,659 - ==> Top1: 59.370    Top5: 85.530    Loss: 1.864

2024-02-17 12:25:12,670 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:25:12,671 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:25:12,733 - 

2024-02-17 12:25:12,733 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:25:20,600 - Epoch: [126][  100/  500]    Overall Loss 0.328410    Objective Loss 0.328410                                        LR 0.000250    Time 0.078616    
2024-02-17 12:25:27,697 - Epoch: [126][  200/  500]    Overall Loss 0.342120    Objective Loss 0.342120                                        LR 0.000250    Time 0.074771    
2024-02-17 12:25:34,752 - Epoch: [126][  300/  500]    Overall Loss 0.346524    Objective Loss 0.346524                                        LR 0.000250    Time 0.073348    
2024-02-17 12:25:41,934 - Epoch: [126][  400/  500]    Overall Loss 0.350320    Objective Loss 0.350320                                        LR 0.000250    Time 0.072955    
2024-02-17 12:25:48,989 - Epoch: [126][  500/  500]    Overall Loss 0.349495    Objective Loss 0.349495    Top1 89.500000    Top5 98.500000    LR 0.000250    Time 0.072466    
2024-02-17 12:25:49,107 - --- validate (epoch=126)-----------
2024-02-17 12:25:49,108 - 10000 samples (100 per mini-batch)
2024-02-17 12:25:52,032 - Epoch: [126][  100/  100]    Loss 1.874391    Top1 58.960000    Top5 85.670000    
2024-02-17 12:25:52,143 - ==> Top1: 58.960    Top5: 85.670    Loss: 1.874

2024-02-17 12:25:52,154 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:25:52,155 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:25:52,218 - 

2024-02-17 12:25:52,218 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:25:59,667 - Epoch: [127][  100/  500]    Overall Loss 0.337505    Objective Loss 0.337505                                        LR 0.000250    Time 0.074439    
2024-02-17 12:26:06,467 - Epoch: [127][  200/  500]    Overall Loss 0.341469    Objective Loss 0.341469                                        LR 0.000250    Time 0.071202    
2024-02-17 12:26:13,507 - Epoch: [127][  300/  500]    Overall Loss 0.342730    Objective Loss 0.342730                                        LR 0.000250    Time 0.070920    
2024-02-17 12:26:20,245 - Epoch: [127][  400/  500]    Overall Loss 0.347414    Objective Loss 0.347414                                        LR 0.000250    Time 0.070028    
2024-02-17 12:26:26,970 - Epoch: [127][  500/  500]    Overall Loss 0.350394    Objective Loss 0.350394    Top1 87.500000    Top5 99.000000    LR 0.000250    Time 0.069467    
2024-02-17 12:26:27,098 - --- validate (epoch=127)-----------
2024-02-17 12:26:27,099 - 10000 samples (100 per mini-batch)
2024-02-17 12:26:30,008 - Epoch: [127][  100/  100]    Loss 1.868177    Top1 59.330000    Top5 85.890000    
2024-02-17 12:26:30,125 - ==> Top1: 59.330    Top5: 85.890    Loss: 1.868

2024-02-17 12:26:30,135 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:26:30,136 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:26:30,186 - 

2024-02-17 12:26:30,186 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:26:37,687 - Epoch: [128][  100/  500]    Overall Loss 0.321977    Objective Loss 0.321977                                        LR 0.000250    Time 0.074956    
2024-02-17 12:26:44,734 - Epoch: [128][  200/  500]    Overall Loss 0.328395    Objective Loss 0.328395                                        LR 0.000250    Time 0.072695    
2024-02-17 12:26:51,846 - Epoch: [128][  300/  500]    Overall Loss 0.335899    Objective Loss 0.335899                                        LR 0.000250    Time 0.072155    
2024-02-17 12:26:59,020 - Epoch: [128][  400/  500]    Overall Loss 0.340811    Objective Loss 0.340811                                        LR 0.000250    Time 0.072040    
2024-02-17 12:27:06,509 - Epoch: [128][  500/  500]    Overall Loss 0.344051    Objective Loss 0.344051    Top1 90.000000    Top5 100.000000    LR 0.000250    Time 0.072600    
2024-02-17 12:27:06,613 - --- validate (epoch=128)-----------
2024-02-17 12:27:06,614 - 10000 samples (100 per mini-batch)
2024-02-17 12:27:09,590 - Epoch: [128][  100/  100]    Loss 1.862338    Top1 59.780000    Top5 86.150000    
2024-02-17 12:27:09,705 - ==> Top1: 59.780    Top5: 86.150    Loss: 1.862

2024-02-17 12:27:09,711 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:27:09,711 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:27:09,768 - 

2024-02-17 12:27:09,769 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:27:17,654 - Epoch: [129][  100/  500]    Overall Loss 0.327128    Objective Loss 0.327128                                        LR 0.000250    Time 0.078791    
2024-02-17 12:27:25,014 - Epoch: [129][  200/  500]    Overall Loss 0.330816    Objective Loss 0.330816                                        LR 0.000250    Time 0.076175    
2024-02-17 12:27:32,130 - Epoch: [129][  300/  500]    Overall Loss 0.334286    Objective Loss 0.334286                                        LR 0.000250    Time 0.074491    
2024-02-17 12:27:39,455 - Epoch: [129][  400/  500]    Overall Loss 0.336386    Objective Loss 0.336386                                        LR 0.000250    Time 0.074167    
2024-02-17 12:27:46,791 - Epoch: [129][  500/  500]    Overall Loss 0.339072    Objective Loss 0.339072    Top1 89.500000    Top5 100.000000    LR 0.000250    Time 0.073997    
2024-02-17 12:27:46,927 - --- validate (epoch=129)-----------
2024-02-17 12:27:46,928 - 10000 samples (100 per mini-batch)
2024-02-17 12:27:49,861 - Epoch: [129][  100/  100]    Loss 1.913295    Top1 59.320000    Top5 85.110000    
2024-02-17 12:27:50,048 - ==> Top1: 59.320    Top5: 85.110    Loss: 1.913

2024-02-17 12:27:50,059 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:27:50,059 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:27:50,122 - 

2024-02-17 12:27:50,122 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:27:58,019 - Epoch: [130][  100/  500]    Overall Loss 0.334636    Objective Loss 0.334636                                        LR 0.000250    Time 0.078908    
2024-02-17 12:28:05,237 - Epoch: [130][  200/  500]    Overall Loss 0.330480    Objective Loss 0.330480                                        LR 0.000250    Time 0.075522    
2024-02-17 12:28:12,134 - Epoch: [130][  300/  500]    Overall Loss 0.327833    Objective Loss 0.327833                                        LR 0.000250    Time 0.073327    
2024-02-17 12:28:19,022 - Epoch: [130][  400/  500]    Overall Loss 0.331335    Objective Loss 0.331335                                        LR 0.000250    Time 0.072207    
2024-02-17 12:28:25,764 - Epoch: [130][  500/  500]    Overall Loss 0.335437    Objective Loss 0.335437    Top1 90.000000    Top5 98.500000    LR 0.000250    Time 0.071242    
2024-02-17 12:28:25,951 - --- validate (epoch=130)-----------
2024-02-17 12:28:25,952 - 10000 samples (100 per mini-batch)
2024-02-17 12:28:29,084 - Epoch: [130][  100/  100]    Loss 1.907367    Top1 59.130000    Top5 85.190000    
2024-02-17 12:28:29,182 - ==> Top1: 59.130    Top5: 85.190    Loss: 1.907

2024-02-17 12:28:29,192 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:28:29,193 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:28:29,258 - 

2024-02-17 12:28:29,259 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:28:36,796 - Epoch: [131][  100/  500]    Overall Loss 0.317695    Objective Loss 0.317695                                        LR 0.000250    Time 0.075320    
2024-02-17 12:28:43,714 - Epoch: [131][  200/  500]    Overall Loss 0.322430    Objective Loss 0.322430                                        LR 0.000250    Time 0.072227    
2024-02-17 12:28:50,866 - Epoch: [131][  300/  500]    Overall Loss 0.333403    Objective Loss 0.333403                                        LR 0.000250    Time 0.071977    
2024-02-17 12:28:57,904 - Epoch: [131][  400/  500]    Overall Loss 0.334155    Objective Loss 0.334155                                        LR 0.000250    Time 0.071568    
2024-02-17 12:29:05,022 - Epoch: [131][  500/  500]    Overall Loss 0.337027    Objective Loss 0.337027    Top1 92.500000    Top5 99.500000    LR 0.000250    Time 0.071481    
2024-02-17 12:29:05,167 - --- validate (epoch=131)-----------
2024-02-17 12:29:05,167 - 10000 samples (100 per mini-batch)
2024-02-17 12:29:07,973 - Epoch: [131][  100/  100]    Loss 1.899934    Top1 59.270000    Top5 85.610000    
2024-02-17 12:29:08,076 - ==> Top1: 59.270    Top5: 85.610    Loss: 1.900

2024-02-17 12:29:08,088 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:29:08,089 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:29:08,152 - 

2024-02-17 12:29:08,153 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:29:15,623 - Epoch: [132][  100/  500]    Overall Loss 0.324536    Objective Loss 0.324536                                        LR 0.000250    Time 0.074649    
2024-02-17 12:29:22,852 - Epoch: [132][  200/  500]    Overall Loss 0.326878    Objective Loss 0.326878                                        LR 0.000250    Time 0.073447    
2024-02-17 12:29:29,753 - Epoch: [132][  300/  500]    Overall Loss 0.329249    Objective Loss 0.329249                                        LR 0.000250    Time 0.071955    
2024-02-17 12:29:36,824 - Epoch: [132][  400/  500]    Overall Loss 0.333414    Objective Loss 0.333414                                        LR 0.000250    Time 0.071634    
2024-02-17 12:29:43,911 - Epoch: [132][  500/  500]    Overall Loss 0.336415    Objective Loss 0.336415    Top1 88.000000    Top5 99.500000    LR 0.000250    Time 0.071473    
2024-02-17 12:29:44,042 - --- validate (epoch=132)-----------
2024-02-17 12:29:44,042 - 10000 samples (100 per mini-batch)
2024-02-17 12:29:47,176 - Epoch: [132][  100/  100]    Loss 1.892687    Top1 59.240000    Top5 85.570000    
2024-02-17 12:29:47,273 - ==> Top1: 59.240    Top5: 85.570    Loss: 1.893

2024-02-17 12:29:47,284 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:29:47,284 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:29:47,347 - 

2024-02-17 12:29:47,347 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:29:54,892 - Epoch: [133][  100/  500]    Overall Loss 0.314647    Objective Loss 0.314647                                        LR 0.000250    Time 0.075393    
2024-02-17 12:30:02,100 - Epoch: [133][  200/  500]    Overall Loss 0.318626    Objective Loss 0.318626                                        LR 0.000250    Time 0.073712    
2024-02-17 12:30:09,289 - Epoch: [133][  300/  500]    Overall Loss 0.325949    Objective Loss 0.325949                                        LR 0.000250    Time 0.073091    
2024-02-17 12:30:16,534 - Epoch: [133][  400/  500]    Overall Loss 0.330286    Objective Loss 0.330286                                        LR 0.000250    Time 0.072919    
2024-02-17 12:30:23,935 - Epoch: [133][  500/  500]    Overall Loss 0.332519    Objective Loss 0.332519    Top1 92.500000    Top5 100.000000    LR 0.000250    Time 0.073128    
2024-02-17 12:30:24,112 - --- validate (epoch=133)-----------
2024-02-17 12:30:24,112 - 10000 samples (100 per mini-batch)
2024-02-17 12:30:27,205 - Epoch: [133][  100/  100]    Loss 1.894338    Top1 59.280000    Top5 85.410000    
2024-02-17 12:30:27,299 - ==> Top1: 59.280    Top5: 85.410    Loss: 1.894

2024-02-17 12:30:27,310 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:30:27,310 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:30:27,380 - 

2024-02-17 12:30:27,381 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:30:35,642 - Epoch: [134][  100/  500]    Overall Loss 0.323956    Objective Loss 0.323956                                        LR 0.000250    Time 0.082562    
2024-02-17 12:30:43,040 - Epoch: [134][  200/  500]    Overall Loss 0.318200    Objective Loss 0.318200                                        LR 0.000250    Time 0.078246    
2024-02-17 12:30:50,245 - Epoch: [134][  300/  500]    Overall Loss 0.323282    Objective Loss 0.323282                                        LR 0.000250    Time 0.076169    
2024-02-17 12:30:57,410 - Epoch: [134][  400/  500]    Overall Loss 0.326481    Objective Loss 0.326481                                        LR 0.000250    Time 0.075028    
2024-02-17 12:31:04,465 - Epoch: [134][  500/  500]    Overall Loss 0.330671    Objective Loss 0.330671    Top1 89.500000    Top5 99.000000    LR 0.000250    Time 0.074125    
2024-02-17 12:31:04,621 - --- validate (epoch=134)-----------
2024-02-17 12:31:04,622 - 10000 samples (100 per mini-batch)
2024-02-17 12:31:07,681 - Epoch: [134][  100/  100]    Loss 1.921497    Top1 59.110000    Top5 85.390000    
2024-02-17 12:31:07,778 - ==> Top1: 59.110    Top5: 85.390    Loss: 1.921

2024-02-17 12:31:07,788 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:31:07,788 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:31:07,850 - 

2024-02-17 12:31:07,851 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:31:15,802 - Epoch: [135][  100/  500]    Overall Loss 0.319096    Objective Loss 0.319096                                        LR 0.000250    Time 0.079453    
2024-02-17 12:31:23,172 - Epoch: [135][  200/  500]    Overall Loss 0.320444    Objective Loss 0.320444                                        LR 0.000250    Time 0.076560    
2024-02-17 12:31:30,336 - Epoch: [135][  300/  500]    Overall Loss 0.323427    Objective Loss 0.323427                                        LR 0.000250    Time 0.074906    
2024-02-17 12:31:37,423 - Epoch: [135][  400/  500]    Overall Loss 0.329200    Objective Loss 0.329200                                        LR 0.000250    Time 0.073886    
2024-02-17 12:31:44,580 - Epoch: [135][  500/  500]    Overall Loss 0.330982    Objective Loss 0.330982    Top1 90.000000    Top5 99.500000    LR 0.000250    Time 0.073415    
2024-02-17 12:31:44,728 - --- validate (epoch=135)-----------
2024-02-17 12:31:44,729 - 10000 samples (100 per mini-batch)
2024-02-17 12:31:47,844 - Epoch: [135][  100/  100]    Loss 1.874001    Top1 59.550000    Top5 85.790000    
2024-02-17 12:31:47,952 - ==> Top1: 59.550    Top5: 85.790    Loss: 1.874

2024-02-17 12:31:47,959 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:31:47,959 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:31:48,014 - 

2024-02-17 12:31:48,014 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:31:55,852 - Epoch: [136][  100/  500]    Overall Loss 0.312774    Objective Loss 0.312774                                        LR 0.000250    Time 0.078320    
2024-02-17 12:32:02,915 - Epoch: [136][  200/  500]    Overall Loss 0.318205    Objective Loss 0.318205                                        LR 0.000250    Time 0.074458    
2024-02-17 12:32:10,037 - Epoch: [136][  300/  500]    Overall Loss 0.322399    Objective Loss 0.322399                                        LR 0.000250    Time 0.073366    
2024-02-17 12:32:17,194 - Epoch: [136][  400/  500]    Overall Loss 0.329251    Objective Loss 0.329251                                        LR 0.000250    Time 0.072905    
2024-02-17 12:32:24,276 - Epoch: [136][  500/  500]    Overall Loss 0.331625    Objective Loss 0.331625    Top1 91.000000    Top5 99.500000    LR 0.000250    Time 0.072479    
2024-02-17 12:32:24,457 - --- validate (epoch=136)-----------
2024-02-17 12:32:24,459 - 10000 samples (100 per mini-batch)
2024-02-17 12:32:27,441 - Epoch: [136][  100/  100]    Loss 1.928612    Top1 58.880000    Top5 85.260000    
2024-02-17 12:32:27,540 - ==> Top1: 58.880    Top5: 85.260    Loss: 1.929

2024-02-17 12:32:27,551 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:32:27,552 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:32:27,616 - 

2024-02-17 12:32:27,616 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:32:35,433 - Epoch: [137][  100/  500]    Overall Loss 0.320526    Objective Loss 0.320526                                        LR 0.000250    Time 0.078119    
2024-02-17 12:32:42,923 - Epoch: [137][  200/  500]    Overall Loss 0.319600    Objective Loss 0.319600                                        LR 0.000250    Time 0.076484    
2024-02-17 12:32:50,137 - Epoch: [137][  300/  500]    Overall Loss 0.321798    Objective Loss 0.321798                                        LR 0.000250    Time 0.075024    
2024-02-17 12:32:57,205 - Epoch: [137][  400/  500]    Overall Loss 0.324250    Objective Loss 0.324250                                        LR 0.000250    Time 0.073928    
2024-02-17 12:33:04,282 - Epoch: [137][  500/  500]    Overall Loss 0.326408    Objective Loss 0.326408    Top1 92.500000    Top5 99.500000    LR 0.000250    Time 0.073287    
2024-02-17 12:33:04,506 - --- validate (epoch=137)-----------
2024-02-17 12:33:04,506 - 10000 samples (100 per mini-batch)
2024-02-17 12:33:07,409 - Epoch: [137][  100/  100]    Loss 1.924970    Top1 59.600000    Top5 85.320000    
2024-02-17 12:33:07,504 - ==> Top1: 59.600    Top5: 85.320    Loss: 1.925

2024-02-17 12:33:07,516 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:33:07,517 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:33:07,579 - 

2024-02-17 12:33:07,579 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:33:15,350 - Epoch: [138][  100/  500]    Overall Loss 0.312813    Objective Loss 0.312813                                        LR 0.000250    Time 0.077657    
2024-02-17 12:33:22,455 - Epoch: [138][  200/  500]    Overall Loss 0.319293    Objective Loss 0.319293                                        LR 0.000250    Time 0.074333    
2024-02-17 12:33:29,534 - Epoch: [138][  300/  500]    Overall Loss 0.317928    Objective Loss 0.317928                                        LR 0.000250    Time 0.073138    
2024-02-17 12:33:36,566 - Epoch: [138][  400/  500]    Overall Loss 0.321298    Objective Loss 0.321298                                        LR 0.000250    Time 0.072423    
2024-02-17 12:33:43,719 - Epoch: [138][  500/  500]    Overall Loss 0.325568    Objective Loss 0.325568    Top1 89.500000    Top5 99.500000    LR 0.000250    Time 0.072236    
2024-02-17 12:33:43,912 - --- validate (epoch=138)-----------
2024-02-17 12:33:43,914 - 10000 samples (100 per mini-batch)
2024-02-17 12:33:46,813 - Epoch: [138][  100/  100]    Loss 1.897192    Top1 59.600000    Top5 85.370000    
2024-02-17 12:33:46,997 - ==> Top1: 59.600    Top5: 85.370    Loss: 1.897

2024-02-17 12:33:47,009 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:33:47,010 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:33:47,072 - 

2024-02-17 12:33:47,072 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:33:54,720 - Epoch: [139][  100/  500]    Overall Loss 0.313691    Objective Loss 0.313691                                        LR 0.000250    Time 0.076425    
2024-02-17 12:34:01,692 - Epoch: [139][  200/  500]    Overall Loss 0.316112    Objective Loss 0.316112                                        LR 0.000250    Time 0.073048    
2024-02-17 12:34:08,761 - Epoch: [139][  300/  500]    Overall Loss 0.321306    Objective Loss 0.321306                                        LR 0.000250    Time 0.072251    
2024-02-17 12:34:15,727 - Epoch: [139][  400/  500]    Overall Loss 0.322199    Objective Loss 0.322199                                        LR 0.000250    Time 0.071592    
2024-02-17 12:34:22,831 - Epoch: [139][  500/  500]    Overall Loss 0.323036    Objective Loss 0.323036    Top1 88.500000    Top5 99.000000    LR 0.000250    Time 0.071474    
2024-02-17 12:34:22,936 - --- validate (epoch=139)-----------
2024-02-17 12:34:22,937 - 10000 samples (100 per mini-batch)
2024-02-17 12:34:25,893 - Epoch: [139][  100/  100]    Loss 1.915504    Top1 59.400000    Top5 85.500000    
2024-02-17 12:34:25,991 - ==> Top1: 59.400    Top5: 85.500    Loss: 1.916

2024-02-17 12:34:26,003 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:34:26,004 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:34:26,066 - 

2024-02-17 12:34:26,067 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:34:33,907 - Epoch: [140][  100/  500]    Overall Loss 0.308380    Objective Loss 0.308380                                        LR 0.000250    Time 0.078346    
2024-02-17 12:34:41,049 - Epoch: [140][  200/  500]    Overall Loss 0.316878    Objective Loss 0.316878                                        LR 0.000250    Time 0.074860    
2024-02-17 12:34:48,060 - Epoch: [140][  300/  500]    Overall Loss 0.317922    Objective Loss 0.317922                                        LR 0.000250    Time 0.073260    
2024-02-17 12:34:55,078 - Epoch: [140][  400/  500]    Overall Loss 0.319950    Objective Loss 0.319950                                        LR 0.000250    Time 0.072480    
2024-02-17 12:35:02,482 - Epoch: [140][  500/  500]    Overall Loss 0.322267    Objective Loss 0.322267    Top1 92.000000    Top5 99.500000    LR 0.000250    Time 0.072784    
2024-02-17 12:35:02,605 - --- validate (epoch=140)-----------
2024-02-17 12:35:02,606 - 10000 samples (100 per mini-batch)
2024-02-17 12:35:05,597 - Epoch: [140][  100/  100]    Loss 1.888656    Top1 59.530000    Top5 85.440000    
2024-02-17 12:35:05,722 - ==> Top1: 59.530    Top5: 85.440    Loss: 1.889

2024-02-17 12:35:05,733 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:35:05,733 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:35:05,795 - 

2024-02-17 12:35:05,795 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:35:13,420 - Epoch: [141][  100/  500]    Overall Loss 0.303841    Objective Loss 0.303841                                        LR 0.000250    Time 0.076190    
2024-02-17 12:35:20,556 - Epoch: [141][  200/  500]    Overall Loss 0.305617    Objective Loss 0.305617                                        LR 0.000250    Time 0.073750    
2024-02-17 12:35:27,613 - Epoch: [141][  300/  500]    Overall Loss 0.307391    Objective Loss 0.307391                                        LR 0.000250    Time 0.072675    
2024-02-17 12:35:34,799 - Epoch: [141][  400/  500]    Overall Loss 0.311601    Objective Loss 0.311601                                        LR 0.000250    Time 0.072461    
2024-02-17 12:35:42,123 - Epoch: [141][  500/  500]    Overall Loss 0.314979    Objective Loss 0.314979    Top1 92.000000    Top5 99.500000    LR 0.000250    Time 0.072608    
2024-02-17 12:35:42,246 - --- validate (epoch=141)-----------
2024-02-17 12:35:42,246 - 10000 samples (100 per mini-batch)
2024-02-17 12:35:45,412 - Epoch: [141][  100/  100]    Loss 1.918699    Top1 59.460000    Top5 85.740000    
2024-02-17 12:35:45,549 - ==> Top1: 59.460    Top5: 85.740    Loss: 1.919

2024-02-17 12:35:45,558 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:35:45,558 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:35:45,613 - 

2024-02-17 12:35:45,613 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:35:53,634 - Epoch: [142][  100/  500]    Overall Loss 0.301498    Objective Loss 0.301498                                        LR 0.000250    Time 0.080154    
2024-02-17 12:36:01,046 - Epoch: [142][  200/  500]    Overall Loss 0.307584    Objective Loss 0.307584                                        LR 0.000250    Time 0.077118    
2024-02-17 12:36:08,384 - Epoch: [142][  300/  500]    Overall Loss 0.312257    Objective Loss 0.312257                                        LR 0.000250    Time 0.075860    
2024-02-17 12:36:15,641 - Epoch: [142][  400/  500]    Overall Loss 0.315166    Objective Loss 0.315166                                        LR 0.000250    Time 0.075027    
2024-02-17 12:36:22,831 - Epoch: [142][  500/  500]    Overall Loss 0.318074    Objective Loss 0.318074    Top1 90.000000    Top5 100.000000    LR 0.000250    Time 0.074393    
2024-02-17 12:36:23,007 - --- validate (epoch=142)-----------
2024-02-17 12:36:23,008 - 10000 samples (100 per mini-batch)
2024-02-17 12:36:26,089 - Epoch: [142][  100/  100]    Loss 1.891356    Top1 59.630000    Top5 85.490000    
2024-02-17 12:36:26,208 - ==> Top1: 59.630    Top5: 85.490    Loss: 1.891

2024-02-17 12:36:26,215 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:36:26,215 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:36:26,273 - 

2024-02-17 12:36:26,273 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:36:33,905 - Epoch: [143][  100/  500]    Overall Loss 0.305017    Objective Loss 0.305017                                        LR 0.000250    Time 0.076262    
2024-02-17 12:36:41,080 - Epoch: [143][  200/  500]    Overall Loss 0.305078    Objective Loss 0.305078                                        LR 0.000250    Time 0.073985    
2024-02-17 12:36:48,286 - Epoch: [143][  300/  500]    Overall Loss 0.305630    Objective Loss 0.305630                                        LR 0.000250    Time 0.073328    
2024-02-17 12:36:55,401 - Epoch: [143][  400/  500]    Overall Loss 0.308593    Objective Loss 0.308593                                        LR 0.000250    Time 0.072774    
2024-02-17 12:37:02,501 - Epoch: [143][  500/  500]    Overall Loss 0.310301    Objective Loss 0.310301    Top1 92.000000    Top5 99.500000    LR 0.000250    Time 0.072410    
2024-02-17 12:37:02,688 - --- validate (epoch=143)-----------
2024-02-17 12:37:02,689 - 10000 samples (100 per mini-batch)
2024-02-17 12:37:05,793 - Epoch: [143][  100/  100]    Loss 1.907204    Top1 60.080000    Top5 85.530000    
2024-02-17 12:37:05,960 - ==> Top1: 60.080    Top5: 85.530    Loss: 1.907

2024-02-17 12:37:05,972 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:37:05,972 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:37:06,032 - 

2024-02-17 12:37:06,033 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:37:13,814 - Epoch: [144][  100/  500]    Overall Loss 0.296074    Objective Loss 0.296074                                        LR 0.000250    Time 0.077765    
2024-02-17 12:37:20,852 - Epoch: [144][  200/  500]    Overall Loss 0.292306    Objective Loss 0.292306                                        LR 0.000250    Time 0.074049    
2024-02-17 12:37:27,953 - Epoch: [144][  300/  500]    Overall Loss 0.299635    Objective Loss 0.299635                                        LR 0.000250    Time 0.073022    
2024-02-17 12:37:35,130 - Epoch: [144][  400/  500]    Overall Loss 0.302514    Objective Loss 0.302514                                        LR 0.000250    Time 0.072698    
2024-02-17 12:37:42,235 - Epoch: [144][  500/  500]    Overall Loss 0.306938    Objective Loss 0.306938    Top1 91.000000    Top5 100.000000    LR 0.000250    Time 0.072359    
2024-02-17 12:37:42,392 - --- validate (epoch=144)-----------
2024-02-17 12:37:42,393 - 10000 samples (100 per mini-batch)
2024-02-17 12:37:45,393 - Epoch: [144][  100/  100]    Loss 1.907192    Top1 59.280000    Top5 85.520000    
2024-02-17 12:37:45,534 - ==> Top1: 59.280    Top5: 85.520    Loss: 1.907

2024-02-17 12:37:45,542 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:37:45,542 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:37:45,598 - 

2024-02-17 12:37:45,598 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:37:53,094 - Epoch: [145][  100/  500]    Overall Loss 0.282330    Objective Loss 0.282330                                        LR 0.000250    Time 0.074901    
2024-02-17 12:37:59,974 - Epoch: [145][  200/  500]    Overall Loss 0.292444    Objective Loss 0.292444                                        LR 0.000250    Time 0.071827    
2024-02-17 12:38:06,853 - Epoch: [145][  300/  500]    Overall Loss 0.299170    Objective Loss 0.299170                                        LR 0.000250    Time 0.070801    
2024-02-17 12:38:13,981 - Epoch: [145][  400/  500]    Overall Loss 0.304370    Objective Loss 0.304370                                        LR 0.000250    Time 0.070910    
2024-02-17 12:38:20,895 - Epoch: [145][  500/  500]    Overall Loss 0.308219    Objective Loss 0.308219    Top1 92.500000    Top5 98.500000    LR 0.000250    Time 0.070549    
2024-02-17 12:38:21,014 - --- validate (epoch=145)-----------
2024-02-17 12:38:21,015 - 10000 samples (100 per mini-batch)
2024-02-17 12:38:23,900 - Epoch: [145][  100/  100]    Loss 1.923322    Top1 59.140000    Top5 85.370000    
2024-02-17 12:38:24,019 - ==> Top1: 59.140    Top5: 85.370    Loss: 1.923

2024-02-17 12:38:24,031 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:38:24,031 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:38:24,095 - 

2024-02-17 12:38:24,095 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:38:31,643 - Epoch: [146][  100/  500]    Overall Loss 0.295699    Objective Loss 0.295699                                        LR 0.000250    Time 0.075422    
2024-02-17 12:38:38,699 - Epoch: [146][  200/  500]    Overall Loss 0.296993    Objective Loss 0.296993                                        LR 0.000250    Time 0.072970    
2024-02-17 12:38:45,630 - Epoch: [146][  300/  500]    Overall Loss 0.299033    Objective Loss 0.299033                                        LR 0.000250    Time 0.071738    
2024-02-17 12:38:52,516 - Epoch: [146][  400/  500]    Overall Loss 0.301379    Objective Loss 0.301379                                        LR 0.000250    Time 0.071007    
2024-02-17 12:38:59,745 - Epoch: [146][  500/  500]    Overall Loss 0.305143    Objective Loss 0.305143    Top1 88.500000    Top5 99.500000    LR 0.000250    Time 0.071256    
2024-02-17 12:38:59,876 - --- validate (epoch=146)-----------
2024-02-17 12:38:59,877 - 10000 samples (100 per mini-batch)
2024-02-17 12:39:03,199 - Epoch: [146][  100/  100]    Loss 1.971627    Top1 58.810000    Top5 85.000000    
2024-02-17 12:39:03,325 - ==> Top1: 58.810    Top5: 85.000    Loss: 1.972

2024-02-17 12:39:03,332 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:39:03,332 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:39:03,393 - 

2024-02-17 12:39:03,393 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:39:11,028 - Epoch: [147][  100/  500]    Overall Loss 0.309138    Objective Loss 0.309138                                        LR 0.000250    Time 0.076297    
2024-02-17 12:39:18,390 - Epoch: [147][  200/  500]    Overall Loss 0.306801    Objective Loss 0.306801                                        LR 0.000250    Time 0.074936    
2024-02-17 12:39:25,664 - Epoch: [147][  300/  500]    Overall Loss 0.305706    Objective Loss 0.305706                                        LR 0.000250    Time 0.074188    
2024-02-17 12:39:33,023 - Epoch: [147][  400/  500]    Overall Loss 0.307134    Objective Loss 0.307134                                        LR 0.000250    Time 0.074028    
2024-02-17 12:39:40,351 - Epoch: [147][  500/  500]    Overall Loss 0.307280    Objective Loss 0.307280    Top1 94.000000    Top5 99.500000    LR 0.000250    Time 0.073869    
2024-02-17 12:39:40,452 - --- validate (epoch=147)-----------
2024-02-17 12:39:40,452 - 10000 samples (100 per mini-batch)
2024-02-17 12:39:43,549 - Epoch: [147][  100/  100]    Loss 1.915154    Top1 59.680000    Top5 85.160000    
2024-02-17 12:39:43,659 - ==> Top1: 59.680    Top5: 85.160    Loss: 1.915

2024-02-17 12:39:43,669 - ==> Best [Top1: 60.200   Top5: 86.020   Sparsity:0.00   Params: 753952 on epoch: 104]
2024-02-17 12:39:43,669 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:39:43,736 - 

2024-02-17 12:39:43,736 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:39:51,324 - Epoch: [148][  100/  500]    Overall Loss 0.302657    Objective Loss 0.302657                                        LR 0.000250    Time 0.075822    
2024-02-17 12:39:58,300 - Epoch: [148][  200/  500]    Overall Loss 0.302141    Objective Loss 0.302141                                        LR 0.000250    Time 0.072770    
2024-02-17 12:40:05,414 - Epoch: [148][  300/  500]    Overall Loss 0.303867    Objective Loss 0.303867                                        LR 0.000250    Time 0.072213    
2024-02-17 12:40:12,589 - Epoch: [148][  400/  500]    Overall Loss 0.304256    Objective Loss 0.304256                                        LR 0.000250    Time 0.072088    
2024-02-17 12:40:19,732 - Epoch: [148][  500/  500]    Overall Loss 0.306868    Objective Loss 0.306868    Top1 88.000000    Top5 100.000000    LR 0.000250    Time 0.071947    
2024-02-17 12:40:19,846 - --- validate (epoch=148)-----------
2024-02-17 12:40:19,847 - 10000 samples (100 per mini-batch)
2024-02-17 12:40:23,041 - Epoch: [148][  100/  100]    Loss 1.888129    Top1 60.240000    Top5 85.950000    
2024-02-17 12:40:23,153 - ==> Top1: 60.240    Top5: 85.950    Loss: 1.888

2024-02-17 12:40:23,165 - ==> Best [Top1: 60.240   Top5: 85.950   Sparsity:0.00   Params: 753952 on epoch: 148]
2024-02-17 12:40:23,165 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:40:23,244 - 

2024-02-17 12:40:23,244 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:40:30,800 - Epoch: [149][  100/  500]    Overall Loss 0.288324    Objective Loss 0.288324                                        LR 0.000250    Time 0.075501    
2024-02-17 12:40:37,973 - Epoch: [149][  200/  500]    Overall Loss 0.292148    Objective Loss 0.292148                                        LR 0.000250    Time 0.073591    
2024-02-17 12:40:45,003 - Epoch: [149][  300/  500]    Overall Loss 0.299755    Objective Loss 0.299755                                        LR 0.000250    Time 0.072480    
2024-02-17 12:40:52,114 - Epoch: [149][  400/  500]    Overall Loss 0.303344    Objective Loss 0.303344                                        LR 0.000250    Time 0.072129    
2024-02-17 12:40:59,203 - Epoch: [149][  500/  500]    Overall Loss 0.304277    Objective Loss 0.304277    Top1 90.000000    Top5 99.000000    LR 0.000250    Time 0.071873    
2024-02-17 12:40:59,346 - --- validate (epoch=149)-----------
2024-02-17 12:40:59,347 - 10000 samples (100 per mini-batch)
2024-02-17 12:41:02,439 - Epoch: [149][  100/  100]    Loss 1.935919    Top1 59.500000    Top5 85.440000    
2024-02-17 12:41:02,534 - ==> Top1: 59.500    Top5: 85.440    Loss: 1.936

2024-02-17 12:41:02,547 - ==> Best [Top1: 60.240   Top5: 85.950   Sparsity:0.00   Params: 753952 on epoch: 148]
2024-02-17 12:41:02,548 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:41:02,611 - 

2024-02-17 12:41:02,611 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:41:10,767 - Epoch: [150][  100/  500]    Overall Loss 0.273165    Objective Loss 0.273165                                        LR 0.000125    Time 0.081508    
2024-02-17 12:41:18,258 - Epoch: [150][  200/  500]    Overall Loss 0.268597    Objective Loss 0.268597                                        LR 0.000125    Time 0.078186    
2024-02-17 12:41:25,307 - Epoch: [150][  300/  500]    Overall Loss 0.272199    Objective Loss 0.272199                                        LR 0.000125    Time 0.075608    
2024-02-17 12:41:32,354 - Epoch: [150][  400/  500]    Overall Loss 0.273386    Objective Loss 0.273386                                        LR 0.000125    Time 0.074312    
2024-02-17 12:41:39,322 - Epoch: [150][  500/  500]    Overall Loss 0.273813    Objective Loss 0.273813    Top1 90.500000    Top5 99.500000    LR 0.000125    Time 0.073379    
2024-02-17 12:41:39,456 - --- validate (epoch=150)-----------
2024-02-17 12:41:39,457 - 10000 samples (100 per mini-batch)
2024-02-17 12:41:42,496 - Epoch: [150][  100/  100]    Loss 1.883507    Top1 60.170000    Top5 85.790000    
2024-02-17 12:41:42,635 - ==> Top1: 60.170    Top5: 85.790    Loss: 1.884

2024-02-17 12:41:42,647 - ==> Best [Top1: 60.240   Top5: 85.950   Sparsity:0.00   Params: 753952 on epoch: 148]
2024-02-17 12:41:42,647 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:41:42,708 - 

2024-02-17 12:41:42,708 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:41:50,281 - Epoch: [151][  100/  500]    Overall Loss 0.265116    Objective Loss 0.265116                                        LR 0.000125    Time 0.075663    
2024-02-17 12:41:57,477 - Epoch: [151][  200/  500]    Overall Loss 0.262215    Objective Loss 0.262215                                        LR 0.000125    Time 0.073788    
2024-02-17 12:42:04,537 - Epoch: [151][  300/  500]    Overall Loss 0.262135    Objective Loss 0.262135                                        LR 0.000125    Time 0.072712    
2024-02-17 12:42:11,558 - Epoch: [151][  400/  500]    Overall Loss 0.263139    Objective Loss 0.263139                                        LR 0.000125    Time 0.072076    
2024-02-17 12:42:18,400 - Epoch: [151][  500/  500]    Overall Loss 0.265369    Objective Loss 0.265369    Top1 94.500000    Top5 100.000000    LR 0.000125    Time 0.071339    
2024-02-17 12:42:18,501 - --- validate (epoch=151)-----------
2024-02-17 12:42:18,502 - 10000 samples (100 per mini-batch)
2024-02-17 12:42:21,558 - Epoch: [151][  100/  100]    Loss 1.877078    Top1 60.580000    Top5 85.860000    
2024-02-17 12:42:21,676 - ==> Top1: 60.580    Top5: 85.860    Loss: 1.877

2024-02-17 12:42:21,684 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:42:21,684 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:42:21,763 - 

2024-02-17 12:42:21,763 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:42:29,508 - Epoch: [152][  100/  500]    Overall Loss 0.258555    Objective Loss 0.258555                                        LR 0.000125    Time 0.077391    
2024-02-17 12:42:36,665 - Epoch: [152][  200/  500]    Overall Loss 0.261813    Objective Loss 0.261813                                        LR 0.000125    Time 0.074463    
2024-02-17 12:42:43,774 - Epoch: [152][  300/  500]    Overall Loss 0.261460    Objective Loss 0.261460                                        LR 0.000125    Time 0.073324    
2024-02-17 12:42:50,934 - Epoch: [152][  400/  500]    Overall Loss 0.264246    Objective Loss 0.264246                                        LR 0.000125    Time 0.072882    
2024-02-17 12:42:58,006 - Epoch: [152][  500/  500]    Overall Loss 0.264172    Objective Loss 0.264172    Top1 95.000000    Top5 100.000000    LR 0.000125    Time 0.072442    
2024-02-17 12:42:58,107 - --- validate (epoch=152)-----------
2024-02-17 12:42:58,107 - 10000 samples (100 per mini-batch)
2024-02-17 12:43:01,067 - Epoch: [152][  100/  100]    Loss 1.897309    Top1 60.000000    Top5 86.000000    
2024-02-17 12:43:01,176 - ==> Top1: 60.000    Top5: 86.000    Loss: 1.897

2024-02-17 12:43:01,183 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:43:01,183 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:43:01,240 - 

2024-02-17 12:43:01,241 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:43:08,798 - Epoch: [153][  100/  500]    Overall Loss 0.254941    Objective Loss 0.254941                                        LR 0.000125    Time 0.075519    
2024-02-17 12:43:15,819 - Epoch: [153][  200/  500]    Overall Loss 0.253205    Objective Loss 0.253205                                        LR 0.000125    Time 0.072842    
2024-02-17 12:43:22,882 - Epoch: [153][  300/  500]    Overall Loss 0.257455    Objective Loss 0.257455                                        LR 0.000125    Time 0.072092    
2024-02-17 12:43:29,975 - Epoch: [153][  400/  500]    Overall Loss 0.258693    Objective Loss 0.258693                                        LR 0.000125    Time 0.071791    
2024-02-17 12:43:36,947 - Epoch: [153][  500/  500]    Overall Loss 0.261254    Objective Loss 0.261254    Top1 95.500000    Top5 100.000000    LR 0.000125    Time 0.071369    
2024-02-17 12:43:37,085 - --- validate (epoch=153)-----------
2024-02-17 12:43:37,086 - 10000 samples (100 per mini-batch)
2024-02-17 12:43:39,910 - Epoch: [153][  100/  100]    Loss 1.893290    Top1 60.220000    Top5 85.750000    
2024-02-17 12:43:40,010 - ==> Top1: 60.220    Top5: 85.750    Loss: 1.893

2024-02-17 12:43:40,022 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:43:40,022 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:43:40,087 - 

2024-02-17 12:43:40,087 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:43:47,901 - Epoch: [154][  100/  500]    Overall Loss 0.248995    Objective Loss 0.248995                                        LR 0.000125    Time 0.078083    
2024-02-17 12:43:54,744 - Epoch: [154][  200/  500]    Overall Loss 0.251812    Objective Loss 0.251812                                        LR 0.000125    Time 0.073239    
2024-02-17 12:44:02,054 - Epoch: [154][  300/  500]    Overall Loss 0.256797    Objective Loss 0.256797                                        LR 0.000125    Time 0.073176    
2024-02-17 12:44:09,431 - Epoch: [154][  400/  500]    Overall Loss 0.259046    Objective Loss 0.259046                                        LR 0.000125    Time 0.073314    
2024-02-17 12:44:16,838 - Epoch: [154][  500/  500]    Overall Loss 0.261252    Objective Loss 0.261252    Top1 89.500000    Top5 100.000000    LR 0.000125    Time 0.073456    
2024-02-17 12:44:17,009 - --- validate (epoch=154)-----------
2024-02-17 12:44:17,010 - 10000 samples (100 per mini-batch)
2024-02-17 12:44:19,920 - Epoch: [154][  100/  100]    Loss 1.909400    Top1 59.950000    Top5 85.850000    
2024-02-17 12:44:20,079 - ==> Top1: 59.950    Top5: 85.850    Loss: 1.909

2024-02-17 12:44:20,090 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:44:20,091 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:44:20,153 - 

2024-02-17 12:44:20,153 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:44:27,740 - Epoch: [155][  100/  500]    Overall Loss 0.246360    Objective Loss 0.246360                                        LR 0.000125    Time 0.075813    
2024-02-17 12:44:34,503 - Epoch: [155][  200/  500]    Overall Loss 0.251740    Objective Loss 0.251740                                        LR 0.000125    Time 0.071701    
2024-02-17 12:44:41,824 - Epoch: [155][  300/  500]    Overall Loss 0.255473    Objective Loss 0.255473                                        LR 0.000125    Time 0.072193    
2024-02-17 12:44:49,209 - Epoch: [155][  400/  500]    Overall Loss 0.254105    Objective Loss 0.254105                                        LR 0.000125    Time 0.072597    
2024-02-17 12:44:56,646 - Epoch: [155][  500/  500]    Overall Loss 0.255617    Objective Loss 0.255617    Top1 95.500000    Top5 100.000000    LR 0.000125    Time 0.072943    
2024-02-17 12:44:56,785 - --- validate (epoch=155)-----------
2024-02-17 12:44:56,785 - 10000 samples (100 per mini-batch)
2024-02-17 12:44:59,685 - Epoch: [155][  100/  100]    Loss 1.907299    Top1 59.790000    Top5 85.910000    
2024-02-17 12:44:59,832 - ==> Top1: 59.790    Top5: 85.910    Loss: 1.907

2024-02-17 12:44:59,842 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:44:59,842 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:44:59,905 - 

2024-02-17 12:44:59,906 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:45:07,569 - Epoch: [156][  100/  500]    Overall Loss 0.252096    Objective Loss 0.252096                                        LR 0.000125    Time 0.076579    
2024-02-17 12:45:14,778 - Epoch: [156][  200/  500]    Overall Loss 0.250345    Objective Loss 0.250345                                        LR 0.000125    Time 0.074310    
2024-02-17 12:45:21,877 - Epoch: [156][  300/  500]    Overall Loss 0.254333    Objective Loss 0.254333                                        LR 0.000125    Time 0.073191    
2024-02-17 12:45:29,026 - Epoch: [156][  400/  500]    Overall Loss 0.256664    Objective Loss 0.256664                                        LR 0.000125    Time 0.072754    
2024-02-17 12:45:36,164 - Epoch: [156][  500/  500]    Overall Loss 0.256761    Objective Loss 0.256761    Top1 95.000000    Top5 100.000000    LR 0.000125    Time 0.072471    
2024-02-17 12:45:36,292 - --- validate (epoch=156)-----------
2024-02-17 12:45:36,293 - 10000 samples (100 per mini-batch)
2024-02-17 12:45:39,061 - Epoch: [156][  100/  100]    Loss 1.902428    Top1 60.500000    Top5 85.740000    
2024-02-17 12:45:39,178 - ==> Top1: 60.500    Top5: 85.740    Loss: 1.902

2024-02-17 12:45:39,191 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:45:39,191 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:45:39,461 - 

2024-02-17 12:45:39,461 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:45:47,072 - Epoch: [157][  100/  500]    Overall Loss 0.245394    Objective Loss 0.245394                                        LR 0.000125    Time 0.076056    
2024-02-17 12:45:54,029 - Epoch: [157][  200/  500]    Overall Loss 0.251643    Objective Loss 0.251643                                        LR 0.000125    Time 0.072791    
2024-02-17 12:46:00,899 - Epoch: [157][  300/  500]    Overall Loss 0.251384    Objective Loss 0.251384                                        LR 0.000125    Time 0.071416    
2024-02-17 12:46:07,671 - Epoch: [157][  400/  500]    Overall Loss 0.255264    Objective Loss 0.255264                                        LR 0.000125    Time 0.070484    
2024-02-17 12:46:14,634 - Epoch: [157][  500/  500]    Overall Loss 0.256547    Objective Loss 0.256547    Top1 95.500000    Top5 99.500000    LR 0.000125    Time 0.070306    
2024-02-17 12:46:14,763 - --- validate (epoch=157)-----------
2024-02-17 12:46:14,764 - 10000 samples (100 per mini-batch)
2024-02-17 12:46:17,625 - Epoch: [157][  100/  100]    Loss 1.920516    Top1 59.920000    Top5 85.730000    
2024-02-17 12:46:17,731 - ==> Top1: 59.920    Top5: 85.730    Loss: 1.921

2024-02-17 12:46:17,746 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:46:17,746 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:46:17,807 - 

2024-02-17 12:46:17,808 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:46:25,563 - Epoch: [158][  100/  500]    Overall Loss 0.243929    Objective Loss 0.243929                                        LR 0.000125    Time 0.077505    
2024-02-17 12:46:32,748 - Epoch: [158][  200/  500]    Overall Loss 0.251566    Objective Loss 0.251566                                        LR 0.000125    Time 0.074654    
2024-02-17 12:46:40,021 - Epoch: [158][  300/  500]    Overall Loss 0.253808    Objective Loss 0.253808                                        LR 0.000125    Time 0.073998    
2024-02-17 12:46:47,288 - Epoch: [158][  400/  500]    Overall Loss 0.257415    Objective Loss 0.257415                                        LR 0.000125    Time 0.073656    
2024-02-17 12:46:54,649 - Epoch: [158][  500/  500]    Overall Loss 0.258460    Objective Loss 0.258460    Top1 89.500000    Top5 100.000000    LR 0.000125    Time 0.073638    
2024-02-17 12:46:54,767 - --- validate (epoch=158)-----------
2024-02-17 12:46:54,768 - 10000 samples (100 per mini-batch)
2024-02-17 12:46:57,598 - Epoch: [158][  100/  100]    Loss 1.905086    Top1 60.190000    Top5 85.770000    
2024-02-17 12:46:57,714 - ==> Top1: 60.190    Top5: 85.770    Loss: 1.905

2024-02-17 12:46:57,724 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:46:57,725 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:46:57,776 - 

2024-02-17 12:46:57,776 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:47:05,031 - Epoch: [159][  100/  500]    Overall Loss 0.240422    Objective Loss 0.240422                                        LR 0.000125    Time 0.072498    
2024-02-17 12:47:12,181 - Epoch: [159][  200/  500]    Overall Loss 0.247656    Objective Loss 0.247656                                        LR 0.000125    Time 0.071978    
2024-02-17 12:47:19,288 - Epoch: [159][  300/  500]    Overall Loss 0.253079    Objective Loss 0.253079                                        LR 0.000125    Time 0.071661    
2024-02-17 12:47:26,305 - Epoch: [159][  400/  500]    Overall Loss 0.255299    Objective Loss 0.255299                                        LR 0.000125    Time 0.071279    
2024-02-17 12:47:33,207 - Epoch: [159][  500/  500]    Overall Loss 0.253614    Objective Loss 0.253614    Top1 92.500000    Top5 99.500000    LR 0.000125    Time 0.070819    
2024-02-17 12:47:33,374 - --- validate (epoch=159)-----------
2024-02-17 12:47:33,375 - 10000 samples (100 per mini-batch)
2024-02-17 12:47:36,352 - Epoch: [159][  100/  100]    Loss 1.922087    Top1 59.890000    Top5 85.770000    
2024-02-17 12:47:36,538 - ==> Top1: 59.890    Top5: 85.770    Loss: 1.922

2024-02-17 12:47:36,548 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:47:36,549 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:47:36,612 - 

2024-02-17 12:47:36,613 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:47:43,992 - Epoch: [160][  100/  500]    Overall Loss 0.250435    Objective Loss 0.250435                                        LR 0.000125    Time 0.073743    
2024-02-17 12:47:51,070 - Epoch: [160][  200/  500]    Overall Loss 0.255699    Objective Loss 0.255699                                        LR 0.000125    Time 0.072238    
2024-02-17 12:47:58,112 - Epoch: [160][  300/  500]    Overall Loss 0.255090    Objective Loss 0.255090                                        LR 0.000125    Time 0.071618    
2024-02-17 12:48:05,006 - Epoch: [160][  400/  500]    Overall Loss 0.253042    Objective Loss 0.253042                                        LR 0.000125    Time 0.070936    
2024-02-17 12:48:11,952 - Epoch: [160][  500/  500]    Overall Loss 0.252557    Objective Loss 0.252557    Top1 94.500000    Top5 100.000000    LR 0.000125    Time 0.070634    
2024-02-17 12:48:12,122 - --- validate (epoch=160)-----------
2024-02-17 12:48:12,123 - 10000 samples (100 per mini-batch)
2024-02-17 12:48:15,279 - Epoch: [160][  100/  100]    Loss 1.924850    Top1 60.170000    Top5 85.660000    
2024-02-17 12:48:15,375 - ==> Top1: 60.170    Top5: 85.660    Loss: 1.925

2024-02-17 12:48:15,384 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:48:15,384 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:48:15,444 - 

2024-02-17 12:48:15,444 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:48:23,177 - Epoch: [161][  100/  500]    Overall Loss 0.242642    Objective Loss 0.242642                                        LR 0.000125    Time 0.077273    
2024-02-17 12:48:30,458 - Epoch: [161][  200/  500]    Overall Loss 0.249146    Objective Loss 0.249146                                        LR 0.000125    Time 0.075021    
2024-02-17 12:48:37,568 - Epoch: [161][  300/  500]    Overall Loss 0.249139    Objective Loss 0.249139                                        LR 0.000125    Time 0.073700    
2024-02-17 12:48:44,616 - Epoch: [161][  400/  500]    Overall Loss 0.250134    Objective Loss 0.250134                                        LR 0.000125    Time 0.072885    
2024-02-17 12:48:51,725 - Epoch: [161][  500/  500]    Overall Loss 0.251034    Objective Loss 0.251034    Top1 93.500000    Top5 100.000000    LR 0.000125    Time 0.072517    
2024-02-17 12:48:51,909 - --- validate (epoch=161)-----------
2024-02-17 12:48:51,911 - 10000 samples (100 per mini-batch)
2024-02-17 12:48:54,843 - Epoch: [161][  100/  100]    Loss 1.905843    Top1 60.080000    Top5 85.760000    
2024-02-17 12:48:54,955 - ==> Top1: 60.080    Top5: 85.760    Loss: 1.906

2024-02-17 12:48:54,968 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:48:54,968 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:48:55,030 - 

2024-02-17 12:48:55,030 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:49:02,847 - Epoch: [162][  100/  500]    Overall Loss 0.234019    Objective Loss 0.234019                                        LR 0.000125    Time 0.078117    
2024-02-17 12:49:09,918 - Epoch: [162][  200/  500]    Overall Loss 0.244368    Objective Loss 0.244368                                        LR 0.000125    Time 0.074393    
2024-02-17 12:49:16,943 - Epoch: [162][  300/  500]    Overall Loss 0.245240    Objective Loss 0.245240                                        LR 0.000125    Time 0.072997    
2024-02-17 12:49:23,915 - Epoch: [162][  400/  500]    Overall Loss 0.245359    Objective Loss 0.245359                                        LR 0.000125    Time 0.072169    
2024-02-17 12:49:30,843 - Epoch: [162][  500/  500]    Overall Loss 0.247341    Objective Loss 0.247341    Top1 91.000000    Top5 99.500000    LR 0.000125    Time 0.071583    
2024-02-17 12:49:30,944 - --- validate (epoch=162)-----------
2024-02-17 12:49:30,945 - 10000 samples (100 per mini-batch)
2024-02-17 12:49:33,904 - Epoch: [162][  100/  100]    Loss 1.919250    Top1 60.410000    Top5 85.600000    
2024-02-17 12:49:34,031 - ==> Top1: 60.410    Top5: 85.600    Loss: 1.919

2024-02-17 12:49:34,042 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:49:34,042 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:49:34,103 - 

2024-02-17 12:49:34,104 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:49:41,687 - Epoch: [163][  100/  500]    Overall Loss 0.238383    Objective Loss 0.238383                                        LR 0.000125    Time 0.075776    
2024-02-17 12:49:48,745 - Epoch: [163][  200/  500]    Overall Loss 0.246432    Objective Loss 0.246432                                        LR 0.000125    Time 0.073157    
2024-02-17 12:49:55,807 - Epoch: [163][  300/  500]    Overall Loss 0.249270    Objective Loss 0.249270                                        LR 0.000125    Time 0.072298    
2024-02-17 12:50:03,043 - Epoch: [163][  400/  500]    Overall Loss 0.248447    Objective Loss 0.248447                                        LR 0.000125    Time 0.072303    
2024-02-17 12:50:10,281 - Epoch: [163][  500/  500]    Overall Loss 0.248890    Objective Loss 0.248890    Top1 93.500000    Top5 99.500000    LR 0.000125    Time 0.072310    
2024-02-17 12:50:10,424 - --- validate (epoch=163)-----------
2024-02-17 12:50:10,425 - 10000 samples (100 per mini-batch)
2024-02-17 12:50:13,401 - Epoch: [163][  100/  100]    Loss 1.931699    Top1 59.970000    Top5 85.960000    
2024-02-17 12:50:13,523 - ==> Top1: 59.970    Top5: 85.960    Loss: 1.932

2024-02-17 12:50:13,531 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:50:13,531 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:50:13,593 - 

2024-02-17 12:50:13,594 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:50:21,098 - Epoch: [164][  100/  500]    Overall Loss 0.241553    Objective Loss 0.241553                                        LR 0.000125    Time 0.074992    
2024-02-17 12:50:28,233 - Epoch: [164][  200/  500]    Overall Loss 0.247464    Objective Loss 0.247464                                        LR 0.000125    Time 0.073146    
2024-02-17 12:50:35,383 - Epoch: [164][  300/  500]    Overall Loss 0.247544    Objective Loss 0.247544                                        LR 0.000125    Time 0.072584    
2024-02-17 12:50:42,715 - Epoch: [164][  400/  500]    Overall Loss 0.249057    Objective Loss 0.249057                                        LR 0.000125    Time 0.072758    
2024-02-17 12:50:49,742 - Epoch: [164][  500/  500]    Overall Loss 0.249866    Objective Loss 0.249866    Top1 95.000000    Top5 100.000000    LR 0.000125    Time 0.072253    
2024-02-17 12:50:49,842 - --- validate (epoch=164)-----------
2024-02-17 12:50:49,843 - 10000 samples (100 per mini-batch)
2024-02-17 12:50:52,965 - Epoch: [164][  100/  100]    Loss 1.929833    Top1 59.980000    Top5 85.800000    
2024-02-17 12:50:53,098 - ==> Top1: 59.980    Top5: 85.800    Loss: 1.930

2024-02-17 12:50:53,111 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:50:53,111 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:50:53,174 - 

2024-02-17 12:50:53,175 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:51:00,694 - Epoch: [165][  100/  500]    Overall Loss 0.238838    Objective Loss 0.238838                                        LR 0.000125    Time 0.075131    
2024-02-17 12:51:08,013 - Epoch: [165][  200/  500]    Overall Loss 0.240462    Objective Loss 0.240462                                        LR 0.000125    Time 0.074140    
2024-02-17 12:51:15,214 - Epoch: [165][  300/  500]    Overall Loss 0.243240    Objective Loss 0.243240                                        LR 0.000125    Time 0.073414    
2024-02-17 12:51:22,320 - Epoch: [165][  400/  500]    Overall Loss 0.244188    Objective Loss 0.244188                                        LR 0.000125    Time 0.072817    
2024-02-17 12:51:29,426 - Epoch: [165][  500/  500]    Overall Loss 0.244595    Objective Loss 0.244595    Top1 92.500000    Top5 100.000000    LR 0.000125    Time 0.072457    
2024-02-17 12:51:29,611 - --- validate (epoch=165)-----------
2024-02-17 12:51:29,612 - 10000 samples (100 per mini-batch)
2024-02-17 12:51:32,428 - Epoch: [165][  100/  100]    Loss 1.920215    Top1 60.150000    Top5 85.790000    
2024-02-17 12:51:32,623 - ==> Top1: 60.150    Top5: 85.790    Loss: 1.920

2024-02-17 12:51:32,636 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:51:32,636 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:51:32,697 - 

2024-02-17 12:51:32,697 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:51:40,449 - Epoch: [166][  100/  500]    Overall Loss 0.245896    Objective Loss 0.245896                                        LR 0.000125    Time 0.077459    
2024-02-17 12:51:47,526 - Epoch: [166][  200/  500]    Overall Loss 0.247981    Objective Loss 0.247981                                        LR 0.000125    Time 0.074097    
2024-02-17 12:51:54,551 - Epoch: [166][  300/  500]    Overall Loss 0.246354    Objective Loss 0.246354                                        LR 0.000125    Time 0.072798    
2024-02-17 12:52:01,627 - Epoch: [166][  400/  500]    Overall Loss 0.246137    Objective Loss 0.246137                                        LR 0.000125    Time 0.072278    
2024-02-17 12:52:08,830 - Epoch: [166][  500/  500]    Overall Loss 0.246829    Objective Loss 0.246829    Top1 91.000000    Top5 100.000000    LR 0.000125    Time 0.072221    
2024-02-17 12:52:08,957 - --- validate (epoch=166)-----------
2024-02-17 12:52:08,958 - 10000 samples (100 per mini-batch)
2024-02-17 12:52:11,870 - Epoch: [166][  100/  100]    Loss 1.935571    Top1 59.970000    Top5 85.610000    
2024-02-17 12:52:12,009 - ==> Top1: 59.970    Top5: 85.610    Loss: 1.936

2024-02-17 12:52:12,022 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:52:12,022 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:52:12,084 - 

2024-02-17 12:52:12,084 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:52:19,538 - Epoch: [167][  100/  500]    Overall Loss 0.236638    Objective Loss 0.236638                                        LR 0.000125    Time 0.074478    
2024-02-17 12:52:26,575 - Epoch: [167][  200/  500]    Overall Loss 0.237886    Objective Loss 0.237886                                        LR 0.000125    Time 0.072406    
2024-02-17 12:52:33,659 - Epoch: [167][  300/  500]    Overall Loss 0.240551    Objective Loss 0.240551                                        LR 0.000125    Time 0.071868    
2024-02-17 12:52:40,718 - Epoch: [167][  400/  500]    Overall Loss 0.241208    Objective Loss 0.241208                                        LR 0.000125    Time 0.071539    
2024-02-17 12:52:47,867 - Epoch: [167][  500/  500]    Overall Loss 0.244225    Objective Loss 0.244225    Top1 88.500000    Top5 100.000000    LR 0.000125    Time 0.071521    
2024-02-17 12:52:48,042 - --- validate (epoch=167)-----------
2024-02-17 12:52:48,043 - 10000 samples (100 per mini-batch)
2024-02-17 12:52:50,931 - Epoch: [167][  100/  100]    Loss 1.930874    Top1 60.170000    Top5 85.860000    
2024-02-17 12:52:51,031 - ==> Top1: 60.170    Top5: 85.860    Loss: 1.931

2024-02-17 12:52:51,044 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:52:51,044 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:52:51,107 - 

2024-02-17 12:52:51,107 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:52:59,025 - Epoch: [168][  100/  500]    Overall Loss 0.232428    Objective Loss 0.232428                                        LR 0.000125    Time 0.079112    
2024-02-17 12:53:06,052 - Epoch: [168][  200/  500]    Overall Loss 0.237562    Objective Loss 0.237562                                        LR 0.000125    Time 0.074669    
2024-02-17 12:53:13,206 - Epoch: [168][  300/  500]    Overall Loss 0.240998    Objective Loss 0.240998                                        LR 0.000125    Time 0.073612    
2024-02-17 12:53:20,283 - Epoch: [168][  400/  500]    Overall Loss 0.243139    Objective Loss 0.243139                                        LR 0.000125    Time 0.072890    
2024-02-17 12:53:27,404 - Epoch: [168][  500/  500]    Overall Loss 0.242613    Objective Loss 0.242613    Top1 94.000000    Top5 99.500000    LR 0.000125    Time 0.072545    
2024-02-17 12:53:27,522 - --- validate (epoch=168)-----------
2024-02-17 12:53:27,524 - 10000 samples (100 per mini-batch)
2024-02-17 12:53:30,388 - Epoch: [168][  100/  100]    Loss 1.940044    Top1 60.110000    Top5 85.750000    
2024-02-17 12:53:30,515 - ==> Top1: 60.110    Top5: 85.750    Loss: 1.940

2024-02-17 12:53:30,527 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:53:30,528 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:53:30,589 - 

2024-02-17 12:53:30,590 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:53:38,114 - Epoch: [169][  100/  500]    Overall Loss 0.233130    Objective Loss 0.233130                                        LR 0.000125    Time 0.075182    
2024-02-17 12:53:45,218 - Epoch: [169][  200/  500]    Overall Loss 0.239286    Objective Loss 0.239286                                        LR 0.000125    Time 0.073089    
2024-02-17 12:53:52,402 - Epoch: [169][  300/  500]    Overall Loss 0.239623    Objective Loss 0.239623                                        LR 0.000125    Time 0.072661    
2024-02-17 12:53:59,472 - Epoch: [169][  400/  500]    Overall Loss 0.240834    Objective Loss 0.240834                                        LR 0.000125    Time 0.072159    
2024-02-17 12:54:06,581 - Epoch: [169][  500/  500]    Overall Loss 0.241750    Objective Loss 0.241750    Top1 90.500000    Top5 100.000000    LR 0.000125    Time 0.071937    
2024-02-17 12:54:06,748 - --- validate (epoch=169)-----------
2024-02-17 12:54:06,749 - 10000 samples (100 per mini-batch)
2024-02-17 12:54:09,734 - Epoch: [169][  100/  100]    Loss 1.938254    Top1 60.110000    Top5 85.580000    
2024-02-17 12:54:09,887 - ==> Top1: 60.110    Top5: 85.580    Loss: 1.938

2024-02-17 12:54:09,899 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:54:09,900 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:54:09,961 - 

2024-02-17 12:54:09,962 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:54:17,911 - Epoch: [170][  100/  500]    Overall Loss 0.229916    Objective Loss 0.229916                                        LR 0.000125    Time 0.079436    
2024-02-17 12:54:25,106 - Epoch: [170][  200/  500]    Overall Loss 0.236218    Objective Loss 0.236218                                        LR 0.000125    Time 0.075673    
2024-02-17 12:54:32,255 - Epoch: [170][  300/  500]    Overall Loss 0.237610    Objective Loss 0.237610                                        LR 0.000125    Time 0.074265    
2024-02-17 12:54:39,226 - Epoch: [170][  400/  500]    Overall Loss 0.239643    Objective Loss 0.239643                                        LR 0.000125    Time 0.073116    
2024-02-17 12:54:46,418 - Epoch: [170][  500/  500]    Overall Loss 0.242543    Objective Loss 0.242543    Top1 94.000000    Top5 100.000000    LR 0.000125    Time 0.072870    
2024-02-17 12:54:46,552 - --- validate (epoch=170)-----------
2024-02-17 12:54:46,553 - 10000 samples (100 per mini-batch)
2024-02-17 12:54:49,730 - Epoch: [170][  100/  100]    Loss 1.934659    Top1 60.060000    Top5 85.940000    
2024-02-17 12:54:49,826 - ==> Top1: 60.060    Top5: 85.940    Loss: 1.935

2024-02-17 12:54:49,832 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:54:49,832 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:54:49,888 - 

2024-02-17 12:54:49,888 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:54:57,674 - Epoch: [171][  100/  500]    Overall Loss 0.222846    Objective Loss 0.222846                                        LR 0.000125    Time 0.077804    
2024-02-17 12:55:05,045 - Epoch: [171][  200/  500]    Overall Loss 0.231142    Objective Loss 0.231142                                        LR 0.000125    Time 0.075730    
2024-02-17 12:55:12,198 - Epoch: [171][  300/  500]    Overall Loss 0.233896    Objective Loss 0.233896                                        LR 0.000125    Time 0.074317    
2024-02-17 12:55:19,187 - Epoch: [171][  400/  500]    Overall Loss 0.235802    Objective Loss 0.235802                                        LR 0.000125    Time 0.073200    
2024-02-17 12:55:26,298 - Epoch: [171][  500/  500]    Overall Loss 0.236756    Objective Loss 0.236756    Top1 92.000000    Top5 99.500000    LR 0.000125    Time 0.072773    
2024-02-17 12:55:26,427 - --- validate (epoch=171)-----------
2024-02-17 12:55:26,427 - 10000 samples (100 per mini-batch)
2024-02-17 12:55:29,447 - Epoch: [171][  100/  100]    Loss 1.950507    Top1 59.930000    Top5 85.490000    
2024-02-17 12:55:29,574 - ==> Top1: 59.930    Top5: 85.490    Loss: 1.951

2024-02-17 12:55:29,585 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:55:29,586 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:55:29,649 - 

2024-02-17 12:55:29,649 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:55:37,207 - Epoch: [172][  100/  500]    Overall Loss 0.233213    Objective Loss 0.233213                                        LR 0.000125    Time 0.075519    
2024-02-17 12:55:44,293 - Epoch: [172][  200/  500]    Overall Loss 0.233640    Objective Loss 0.233640                                        LR 0.000125    Time 0.073168    
2024-02-17 12:55:51,337 - Epoch: [172][  300/  500]    Overall Loss 0.234297    Objective Loss 0.234297                                        LR 0.000125    Time 0.072244    
2024-02-17 12:55:58,425 - Epoch: [172][  400/  500]    Overall Loss 0.237797    Objective Loss 0.237797                                        LR 0.000125    Time 0.071893    
2024-02-17 12:56:05,570 - Epoch: [172][  500/  500]    Overall Loss 0.237228    Objective Loss 0.237228    Top1 94.500000    Top5 100.000000    LR 0.000125    Time 0.071796    
2024-02-17 12:56:05,708 - --- validate (epoch=172)-----------
2024-02-17 12:56:05,709 - 10000 samples (100 per mini-batch)
2024-02-17 12:56:08,581 - Epoch: [172][  100/  100]    Loss 1.936042    Top1 59.900000    Top5 85.730000    
2024-02-17 12:56:08,684 - ==> Top1: 59.900    Top5: 85.730    Loss: 1.936

2024-02-17 12:56:08,904 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:56:08,905 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:56:08,959 - 

2024-02-17 12:56:08,959 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:56:16,528 - Epoch: [173][  100/  500]    Overall Loss 0.222833    Objective Loss 0.222833                                        LR 0.000125    Time 0.075630    
2024-02-17 12:56:23,676 - Epoch: [173][  200/  500]    Overall Loss 0.226368    Objective Loss 0.226368                                        LR 0.000125    Time 0.073531    
2024-02-17 12:56:30,796 - Epoch: [173][  300/  500]    Overall Loss 0.233803    Objective Loss 0.233803                                        LR 0.000125    Time 0.072740    
2024-02-17 12:56:37,840 - Epoch: [173][  400/  500]    Overall Loss 0.236299    Objective Loss 0.236299                                        LR 0.000125    Time 0.072155    
2024-02-17 12:56:45,009 - Epoch: [173][  500/  500]    Overall Loss 0.237052    Objective Loss 0.237052    Top1 94.000000    Top5 100.000000    LR 0.000125    Time 0.072053    
2024-02-17 12:56:45,120 - --- validate (epoch=173)-----------
2024-02-17 12:56:45,120 - 10000 samples (100 per mini-batch)
2024-02-17 12:56:48,133 - Epoch: [173][  100/  100]    Loss 1.957766    Top1 59.780000    Top5 85.580000    
2024-02-17 12:56:48,238 - ==> Top1: 59.780    Top5: 85.580    Loss: 1.958

2024-02-17 12:56:48,250 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:56:48,251 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:56:48,312 - 

2024-02-17 12:56:48,312 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:56:55,855 - Epoch: [174][  100/  500]    Overall Loss 0.225888    Objective Loss 0.225888                                        LR 0.000125    Time 0.075370    
2024-02-17 12:57:02,921 - Epoch: [174][  200/  500]    Overall Loss 0.232191    Objective Loss 0.232191                                        LR 0.000125    Time 0.072997    
2024-02-17 12:57:10,057 - Epoch: [174][  300/  500]    Overall Loss 0.234775    Objective Loss 0.234775                                        LR 0.000125    Time 0.072437    
2024-02-17 12:57:17,046 - Epoch: [174][  400/  500]    Overall Loss 0.236206    Objective Loss 0.236206                                        LR 0.000125    Time 0.071788    
2024-02-17 12:57:24,215 - Epoch: [174][  500/  500]    Overall Loss 0.237734    Objective Loss 0.237734    Top1 92.500000    Top5 99.500000    LR 0.000125    Time 0.071759    
2024-02-17 12:57:24,398 - --- validate (epoch=174)-----------
2024-02-17 12:57:24,399 - 10000 samples (100 per mini-batch)
2024-02-17 12:57:27,862 - Epoch: [174][  100/  100]    Loss 1.943706    Top1 59.710000    Top5 85.770000    
2024-02-17 12:57:27,968 - ==> Top1: 59.710    Top5: 85.770    Loss: 1.944

2024-02-17 12:57:27,976 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:57:27,977 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:57:28,033 - 

2024-02-17 12:57:28,034 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:57:35,522 - Epoch: [175][  100/  500]    Overall Loss 0.225993    Objective Loss 0.225993                                        LR 0.000125    Time 0.074828    
2024-02-17 12:57:42,544 - Epoch: [175][  200/  500]    Overall Loss 0.233519    Objective Loss 0.233519                                        LR 0.000125    Time 0.072500    
2024-02-17 12:57:49,669 - Epoch: [175][  300/  500]    Overall Loss 0.234766    Objective Loss 0.234766                                        LR 0.000125    Time 0.072070    
2024-02-17 12:57:56,776 - Epoch: [175][  400/  500]    Overall Loss 0.235085    Objective Loss 0.235085                                        LR 0.000125    Time 0.071810    
2024-02-17 12:58:03,906 - Epoch: [175][  500/  500]    Overall Loss 0.236514    Objective Loss 0.236514    Top1 94.000000    Top5 100.000000    LR 0.000125    Time 0.071698    
2024-02-17 12:58:04,053 - --- validate (epoch=175)-----------
2024-02-17 12:58:04,054 - 10000 samples (100 per mini-batch)
2024-02-17 12:58:06,995 - Epoch: [175][  100/  100]    Loss 1.950114    Top1 59.730000    Top5 85.760000    
2024-02-17 12:58:07,124 - ==> Top1: 59.730    Top5: 85.760    Loss: 1.950

2024-02-17 12:58:07,131 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:58:07,132 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:58:07,194 - 

2024-02-17 12:58:07,195 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:58:14,810 - Epoch: [176][  100/  500]    Overall Loss 0.237072    Objective Loss 0.237072                                        LR 0.000125    Time 0.076092    
2024-02-17 12:58:21,860 - Epoch: [176][  200/  500]    Overall Loss 0.234331    Objective Loss 0.234331                                        LR 0.000125    Time 0.073278    
2024-02-17 12:58:29,367 - Epoch: [176][  300/  500]    Overall Loss 0.234554    Objective Loss 0.234554                                        LR 0.000125    Time 0.073859    
2024-02-17 12:58:36,800 - Epoch: [176][  400/  500]    Overall Loss 0.234573    Objective Loss 0.234573                                        LR 0.000125    Time 0.073967    
2024-02-17 12:58:43,991 - Epoch: [176][  500/  500]    Overall Loss 0.234733    Objective Loss 0.234733    Top1 96.500000    Top5 100.000000    LR 0.000125    Time 0.073548    
2024-02-17 12:58:44,133 - --- validate (epoch=176)-----------
2024-02-17 12:58:44,134 - 10000 samples (100 per mini-batch)
2024-02-17 12:58:47,260 - Epoch: [176][  100/  100]    Loss 1.951452    Top1 59.990000    Top5 85.820000    
2024-02-17 12:58:47,365 - ==> Top1: 59.990    Top5: 85.820    Loss: 1.951

2024-02-17 12:58:47,372 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:58:47,372 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:58:47,429 - 

2024-02-17 12:58:47,430 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:58:55,186 - Epoch: [177][  100/  500]    Overall Loss 0.229778    Objective Loss 0.229778                                        LR 0.000125    Time 0.077504    
2024-02-17 12:59:02,538 - Epoch: [177][  200/  500]    Overall Loss 0.233496    Objective Loss 0.233496                                        LR 0.000125    Time 0.075492    
2024-02-17 12:59:09,982 - Epoch: [177][  300/  500]    Overall Loss 0.234882    Objective Loss 0.234882                                        LR 0.000125    Time 0.075128    
2024-02-17 12:59:17,503 - Epoch: [177][  400/  500]    Overall Loss 0.234475    Objective Loss 0.234475                                        LR 0.000125    Time 0.075137    
2024-02-17 12:59:24,716 - Epoch: [177][  500/  500]    Overall Loss 0.233947    Objective Loss 0.233947    Top1 97.000000    Top5 100.000000    LR 0.000125    Time 0.074526    
2024-02-17 12:59:24,816 - --- validate (epoch=177)-----------
2024-02-17 12:59:24,817 - 10000 samples (100 per mini-batch)
2024-02-17 12:59:27,744 - Epoch: [177][  100/  100]    Loss 1.948968    Top1 59.860000    Top5 85.470000    
2024-02-17 12:59:27,937 - ==> Top1: 59.860    Top5: 85.470    Loss: 1.949

2024-02-17 12:59:27,948 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 12:59:27,948 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 12:59:28,012 - 

2024-02-17 12:59:28,012 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 12:59:35,850 - Epoch: [178][  100/  500]    Overall Loss 0.229757    Objective Loss 0.229757                                        LR 0.000125    Time 0.078322    
2024-02-17 12:59:42,943 - Epoch: [178][  200/  500]    Overall Loss 0.227889    Objective Loss 0.227889                                        LR 0.000125    Time 0.074604    
2024-02-17 12:59:50,063 - Epoch: [178][  300/  500]    Overall Loss 0.233380    Objective Loss 0.233380                                        LR 0.000125    Time 0.073454    
2024-02-17 12:59:57,168 - Epoch: [178][  400/  500]    Overall Loss 0.233113    Objective Loss 0.233113                                        LR 0.000125    Time 0.072842    
2024-02-17 13:00:04,424 - Epoch: [178][  500/  500]    Overall Loss 0.232737    Objective Loss 0.232737    Top1 91.000000    Top5 100.000000    LR 0.000125    Time 0.072776    
2024-02-17 13:00:04,539 - --- validate (epoch=178)-----------
2024-02-17 13:00:04,541 - 10000 samples (100 per mini-batch)
2024-02-17 13:00:07,448 - Epoch: [178][  100/  100]    Loss 1.957101    Top1 60.500000    Top5 85.740000    
2024-02-17 13:00:07,559 - ==> Top1: 60.500    Top5: 85.740    Loss: 1.957

2024-02-17 13:00:07,571 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:00:07,571 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:00:07,635 - 

2024-02-17 13:00:07,635 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:00:15,182 - Epoch: [179][  100/  500]    Overall Loss 0.225508    Objective Loss 0.225508                                        LR 0.000125    Time 0.075419    
2024-02-17 13:00:22,148 - Epoch: [179][  200/  500]    Overall Loss 0.226666    Objective Loss 0.226666                                        LR 0.000125    Time 0.072518    
2024-02-17 13:00:29,567 - Epoch: [179][  300/  500]    Overall Loss 0.228379    Objective Loss 0.228379                                        LR 0.000125    Time 0.073062    
2024-02-17 13:00:37,036 - Epoch: [179][  400/  500]    Overall Loss 0.229011    Objective Loss 0.229011                                        LR 0.000125    Time 0.073457    
2024-02-17 13:00:44,415 - Epoch: [179][  500/  500]    Overall Loss 0.229008    Objective Loss 0.229008    Top1 94.000000    Top5 100.000000    LR 0.000125    Time 0.073516    
2024-02-17 13:00:44,538 - --- validate (epoch=179)-----------
2024-02-17 13:00:44,539 - 10000 samples (100 per mini-batch)
2024-02-17 13:00:47,342 - Epoch: [179][  100/  100]    Loss 1.946505    Top1 60.290000    Top5 85.820000    
2024-02-17 13:00:47,443 - ==> Top1: 60.290    Top5: 85.820    Loss: 1.947

2024-02-17 13:00:47,450 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:00:47,450 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:00:47,506 - 

2024-02-17 13:00:47,506 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:00:55,304 - Epoch: [180][  100/  500]    Overall Loss 0.226473    Objective Loss 0.226473                                        LR 0.000125    Time 0.077929    
2024-02-17 13:01:02,243 - Epoch: [180][  200/  500]    Overall Loss 0.226837    Objective Loss 0.226837                                        LR 0.000125    Time 0.073640    
2024-02-17 13:01:09,216 - Epoch: [180][  300/  500]    Overall Loss 0.230913    Objective Loss 0.230913                                        LR 0.000125    Time 0.072322    
2024-02-17 13:01:16,051 - Epoch: [180][  400/  500]    Overall Loss 0.231669    Objective Loss 0.231669                                        LR 0.000125    Time 0.071322    
2024-02-17 13:01:22,881 - Epoch: [180][  500/  500]    Overall Loss 0.230858    Objective Loss 0.230858    Top1 91.000000    Top5 100.000000    LR 0.000125    Time 0.070711    
2024-02-17 13:01:23,014 - --- validate (epoch=180)-----------
2024-02-17 13:01:23,015 - 10000 samples (100 per mini-batch)
2024-02-17 13:01:25,910 - Epoch: [180][  100/  100]    Loss 1.952863    Top1 60.010000    Top5 85.820000    
2024-02-17 13:01:26,037 - ==> Top1: 60.010    Top5: 85.820    Loss: 1.953

2024-02-17 13:01:26,048 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:01:26,049 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:01:26,112 - 

2024-02-17 13:01:26,112 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:01:33,730 - Epoch: [181][  100/  500]    Overall Loss 0.221967    Objective Loss 0.221967                                        LR 0.000125    Time 0.076125    
2024-02-17 13:01:40,773 - Epoch: [181][  200/  500]    Overall Loss 0.225882    Objective Loss 0.225882                                        LR 0.000125    Time 0.073259    
2024-02-17 13:01:47,815 - Epoch: [181][  300/  500]    Overall Loss 0.229935    Objective Loss 0.229935                                        LR 0.000125    Time 0.072297    
2024-02-17 13:01:54,811 - Epoch: [181][  400/  500]    Overall Loss 0.229989    Objective Loss 0.229989                                        LR 0.000125    Time 0.071701    
2024-02-17 13:02:01,777 - Epoch: [181][  500/  500]    Overall Loss 0.229479    Objective Loss 0.229479    Top1 94.500000    Top5 100.000000    LR 0.000125    Time 0.071286    
2024-02-17 13:02:01,889 - --- validate (epoch=181)-----------
2024-02-17 13:02:01,891 - 10000 samples (100 per mini-batch)
2024-02-17 13:02:04,955 - Epoch: [181][  100/  100]    Loss 1.967669    Top1 59.920000    Top5 85.520000    
2024-02-17 13:02:05,072 - ==> Top1: 59.920    Top5: 85.520    Loss: 1.968

2024-02-17 13:02:05,083 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:02:05,084 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:02:05,146 - 

2024-02-17 13:02:05,146 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:02:12,638 - Epoch: [182][  100/  500]    Overall Loss 0.228324    Objective Loss 0.228324                                        LR 0.000125    Time 0.074858    
2024-02-17 13:02:19,776 - Epoch: [182][  200/  500]    Overall Loss 0.222645    Objective Loss 0.222645                                        LR 0.000125    Time 0.073101    
2024-02-17 13:02:26,973 - Epoch: [182][  300/  500]    Overall Loss 0.223732    Objective Loss 0.223732                                        LR 0.000125    Time 0.072708    
2024-02-17 13:02:34,098 - Epoch: [182][  400/  500]    Overall Loss 0.227632    Objective Loss 0.227632                                        LR 0.000125    Time 0.072334    
2024-02-17 13:02:41,261 - Epoch: [182][  500/  500]    Overall Loss 0.229066    Objective Loss 0.229066    Top1 92.500000    Top5 99.500000    LR 0.000125    Time 0.072184    
2024-02-17 13:02:41,368 - --- validate (epoch=182)-----------
2024-02-17 13:02:41,369 - 10000 samples (100 per mini-batch)
2024-02-17 13:02:44,644 - Epoch: [182][  100/  100]    Loss 1.974157    Top1 59.970000    Top5 85.750000    
2024-02-17 13:02:44,765 - ==> Top1: 59.970    Top5: 85.750    Loss: 1.974

2024-02-17 13:02:44,778 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:02:44,779 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:02:44,833 - 

2024-02-17 13:02:44,833 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:02:52,748 - Epoch: [183][  100/  500]    Overall Loss 0.215227    Objective Loss 0.215227                                        LR 0.000125    Time 0.079090    
2024-02-17 13:02:59,850 - Epoch: [183][  200/  500]    Overall Loss 0.222679    Objective Loss 0.222679                                        LR 0.000125    Time 0.075035    
2024-02-17 13:03:06,917 - Epoch: [183][  300/  500]    Overall Loss 0.226788    Objective Loss 0.226788                                        LR 0.000125    Time 0.073566    
2024-02-17 13:03:14,010 - Epoch: [183][  400/  500]    Overall Loss 0.227187    Objective Loss 0.227187                                        LR 0.000125    Time 0.072897    
2024-02-17 13:03:21,163 - Epoch: [183][  500/  500]    Overall Loss 0.227088    Objective Loss 0.227088    Top1 94.000000    Top5 99.000000    LR 0.000125    Time 0.072615    
2024-02-17 13:03:21,296 - --- validate (epoch=183)-----------
2024-02-17 13:03:21,297 - 10000 samples (100 per mini-batch)
2024-02-17 13:03:24,269 - Epoch: [183][  100/  100]    Loss 1.957040    Top1 59.830000    Top5 85.790000    
2024-02-17 13:03:24,369 - ==> Top1: 59.830    Top5: 85.790    Loss: 1.957

2024-02-17 13:03:24,381 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:03:24,382 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:03:24,443 - 

2024-02-17 13:03:24,444 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:03:31,862 - Epoch: [184][  100/  500]    Overall Loss 0.219644    Objective Loss 0.219644                                        LR 0.000125    Time 0.074131    
2024-02-17 13:03:38,843 - Epoch: [184][  200/  500]    Overall Loss 0.224112    Objective Loss 0.224112                                        LR 0.000125    Time 0.071952    
2024-02-17 13:03:45,573 - Epoch: [184][  300/  500]    Overall Loss 0.224402    Objective Loss 0.224402                                        LR 0.000125    Time 0.070387    
2024-02-17 13:03:52,289 - Epoch: [184][  400/  500]    Overall Loss 0.227515    Objective Loss 0.227515                                        LR 0.000125    Time 0.069573    
2024-02-17 13:03:59,047 - Epoch: [184][  500/  500]    Overall Loss 0.229596    Objective Loss 0.229596    Top1 93.500000    Top5 99.500000    LR 0.000125    Time 0.069169    
2024-02-17 13:03:59,163 - --- validate (epoch=184)-----------
2024-02-17 13:03:59,164 - 10000 samples (100 per mini-batch)
2024-02-17 13:04:02,384 - Epoch: [184][  100/  100]    Loss 1.966017    Top1 60.000000    Top5 85.630000    
2024-02-17 13:04:02,498 - ==> Top1: 60.000    Top5: 85.630    Loss: 1.966

2024-02-17 13:04:02,510 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:04:02,510 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:04:02,573 - 

2024-02-17 13:04:02,574 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:04:09,757 - Epoch: [185][  100/  500]    Overall Loss 0.227796    Objective Loss 0.227796                                        LR 0.000125    Time 0.071786    
2024-02-17 13:04:16,479 - Epoch: [185][  200/  500]    Overall Loss 0.223938    Objective Loss 0.223938                                        LR 0.000125    Time 0.069488    
2024-02-17 13:04:23,492 - Epoch: [185][  300/  500]    Overall Loss 0.226002    Objective Loss 0.226002                                        LR 0.000125    Time 0.069690    
2024-02-17 13:04:30,423 - Epoch: [185][  400/  500]    Overall Loss 0.225594    Objective Loss 0.225594                                        LR 0.000125    Time 0.069585    
2024-02-17 13:04:37,296 - Epoch: [185][  500/  500]    Overall Loss 0.225943    Objective Loss 0.225943    Top1 93.500000    Top5 100.000000    LR 0.000125    Time 0.069406    
2024-02-17 13:04:37,431 - --- validate (epoch=185)-----------
2024-02-17 13:04:37,432 - 10000 samples (100 per mini-batch)
2024-02-17 13:04:40,430 - Epoch: [185][  100/  100]    Loss 1.967303    Top1 59.800000    Top5 85.760000    
2024-02-17 13:04:40,525 - ==> Top1: 59.800    Top5: 85.760    Loss: 1.967

2024-02-17 13:04:40,535 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:04:40,536 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:04:40,596 - 

2024-02-17 13:04:40,596 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:04:48,370 - Epoch: [186][  100/  500]    Overall Loss 0.210687    Objective Loss 0.210687                                        LR 0.000125    Time 0.077691    
2024-02-17 13:04:55,410 - Epoch: [186][  200/  500]    Overall Loss 0.218706    Objective Loss 0.218706                                        LR 0.000125    Time 0.074023    
2024-02-17 13:05:02,517 - Epoch: [186][  300/  500]    Overall Loss 0.220617    Objective Loss 0.220617                                        LR 0.000125    Time 0.073025    
2024-02-17 13:05:09,615 - Epoch: [186][  400/  500]    Overall Loss 0.221219    Objective Loss 0.221219                                        LR 0.000125    Time 0.072504    
2024-02-17 13:05:16,785 - Epoch: [186][  500/  500]    Overall Loss 0.224767    Objective Loss 0.224767    Top1 93.000000    Top5 99.500000    LR 0.000125    Time 0.072335    
2024-02-17 13:05:16,919 - --- validate (epoch=186)-----------
2024-02-17 13:05:16,920 - 10000 samples (100 per mini-batch)
2024-02-17 13:05:19,929 - Epoch: [186][  100/  100]    Loss 1.972442    Top1 59.790000    Top5 85.870000    
2024-02-17 13:05:20,027 - ==> Top1: 59.790    Top5: 85.870    Loss: 1.972

2024-02-17 13:05:20,034 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:05:20,034 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:05:20,095 - 

2024-02-17 13:05:20,095 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:05:27,672 - Epoch: [187][  100/  500]    Overall Loss 0.222190    Objective Loss 0.222190                                        LR 0.000125    Time 0.075711    
2024-02-17 13:05:34,729 - Epoch: [187][  200/  500]    Overall Loss 0.221114    Objective Loss 0.221114                                        LR 0.000125    Time 0.073120    
2024-02-17 13:05:41,830 - Epoch: [187][  300/  500]    Overall Loss 0.221460    Objective Loss 0.221460                                        LR 0.000125    Time 0.072402    
2024-02-17 13:05:49,004 - Epoch: [187][  400/  500]    Overall Loss 0.224399    Objective Loss 0.224399                                        LR 0.000125    Time 0.072226    
2024-02-17 13:05:56,104 - Epoch: [187][  500/  500]    Overall Loss 0.226769    Objective Loss 0.226769    Top1 93.000000    Top5 100.000000    LR 0.000125    Time 0.071973    
2024-02-17 13:05:56,234 - --- validate (epoch=187)-----------
2024-02-17 13:05:56,234 - 10000 samples (100 per mini-batch)
2024-02-17 13:05:59,234 - Epoch: [187][  100/  100]    Loss 1.984545    Top1 59.840000    Top5 85.680000    
2024-02-17 13:05:59,357 - ==> Top1: 59.840    Top5: 85.680    Loss: 1.985

2024-02-17 13:05:59,368 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:05:59,369 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:05:59,432 - 

2024-02-17 13:05:59,432 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:06:07,192 - Epoch: [188][  100/  500]    Overall Loss 0.213649    Objective Loss 0.213649                                        LR 0.000125    Time 0.077540    
2024-02-17 13:06:14,271 - Epoch: [188][  200/  500]    Overall Loss 0.218540    Objective Loss 0.218540                                        LR 0.000125    Time 0.074146    
2024-02-17 13:06:21,314 - Epoch: [188][  300/  500]    Overall Loss 0.218849    Objective Loss 0.218849                                        LR 0.000125    Time 0.072895    
2024-02-17 13:06:28,564 - Epoch: [188][  400/  500]    Overall Loss 0.223136    Objective Loss 0.223136                                        LR 0.000125    Time 0.072786    
2024-02-17 13:06:35,897 - Epoch: [188][  500/  500]    Overall Loss 0.225520    Objective Loss 0.225520    Top1 90.000000    Top5 100.000000    LR 0.000125    Time 0.072887    
2024-02-17 13:06:36,007 - --- validate (epoch=188)-----------
2024-02-17 13:06:36,007 - 10000 samples (100 per mini-batch)
2024-02-17 13:06:39,029 - Epoch: [188][  100/  100]    Loss 1.987643    Top1 59.360000    Top5 85.380000    
2024-02-17 13:06:39,123 - ==> Top1: 59.360    Top5: 85.380    Loss: 1.988

2024-02-17 13:06:39,135 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:06:39,135 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:06:39,199 - 

2024-02-17 13:06:39,199 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:06:46,873 - Epoch: [189][  100/  500]    Overall Loss 0.225949    Objective Loss 0.225949                                        LR 0.000125    Time 0.076684    
2024-02-17 13:06:53,824 - Epoch: [189][  200/  500]    Overall Loss 0.222795    Objective Loss 0.222795                                        LR 0.000125    Time 0.073068    
2024-02-17 13:07:00,885 - Epoch: [189][  300/  500]    Overall Loss 0.224683    Objective Loss 0.224683                                        LR 0.000125    Time 0.072237    
2024-02-17 13:07:07,974 - Epoch: [189][  400/  500]    Overall Loss 0.223539    Objective Loss 0.223539                                        LR 0.000125    Time 0.071888    
2024-02-17 13:07:15,140 - Epoch: [189][  500/  500]    Overall Loss 0.226418    Objective Loss 0.226418    Top1 93.000000    Top5 98.500000    LR 0.000125    Time 0.071835    
2024-02-17 13:07:15,269 - --- validate (epoch=189)-----------
2024-02-17 13:07:15,270 - 10000 samples (100 per mini-batch)
2024-02-17 13:07:18,235 - Epoch: [189][  100/  100]    Loss 1.986064    Top1 60.010000    Top5 85.360000    
2024-02-17 13:07:18,349 - ==> Top1: 60.010    Top5: 85.360    Loss: 1.986

2024-02-17 13:07:18,361 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:07:18,361 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:07:18,422 - 

2024-02-17 13:07:18,422 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:07:26,170 - Epoch: [190][  100/  500]    Overall Loss 0.211893    Objective Loss 0.211893                                        LR 0.000125    Time 0.077420    
2024-02-17 13:07:33,148 - Epoch: [190][  200/  500]    Overall Loss 0.216724    Objective Loss 0.216724                                        LR 0.000125    Time 0.073578    
2024-02-17 13:07:40,161 - Epoch: [190][  300/  500]    Overall Loss 0.217644    Objective Loss 0.217644                                        LR 0.000125    Time 0.072415    
2024-02-17 13:07:47,244 - Epoch: [190][  400/  500]    Overall Loss 0.220819    Objective Loss 0.220819                                        LR 0.000125    Time 0.072010    
2024-02-17 13:07:54,475 - Epoch: [190][  500/  500]    Overall Loss 0.222200    Objective Loss 0.222200    Top1 94.500000    Top5 99.500000    LR 0.000125    Time 0.072061    
2024-02-17 13:07:54,603 - --- validate (epoch=190)-----------
2024-02-17 13:07:54,604 - 10000 samples (100 per mini-batch)
2024-02-17 13:07:57,964 - Epoch: [190][  100/  100]    Loss 1.970894    Top1 59.700000    Top5 85.680000    
2024-02-17 13:07:58,066 - ==> Top1: 59.700    Top5: 85.680    Loss: 1.971

2024-02-17 13:07:58,078 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:07:58,078 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:07:58,139 - 

2024-02-17 13:07:58,139 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:08:05,655 - Epoch: [191][  100/  500]    Overall Loss 0.219249    Objective Loss 0.219249                                        LR 0.000125    Time 0.075101    
2024-02-17 13:08:12,685 - Epoch: [191][  200/  500]    Overall Loss 0.219000    Objective Loss 0.219000                                        LR 0.000125    Time 0.072682    
2024-02-17 13:08:19,779 - Epoch: [191][  300/  500]    Overall Loss 0.219732    Objective Loss 0.219732                                        LR 0.000125    Time 0.072084    
2024-02-17 13:08:26,873 - Epoch: [191][  400/  500]    Overall Loss 0.221630    Objective Loss 0.221630                                        LR 0.000125    Time 0.071788    
2024-02-17 13:08:34,003 - Epoch: [191][  500/  500]    Overall Loss 0.222612    Objective Loss 0.222612    Top1 95.000000    Top5 100.000000    LR 0.000125    Time 0.071683    
2024-02-17 13:08:34,167 - --- validate (epoch=191)-----------
2024-02-17 13:08:34,168 - 10000 samples (100 per mini-batch)
2024-02-17 13:08:37,355 - Epoch: [191][  100/  100]    Loss 1.991651    Top1 59.920000    Top5 85.580000    
2024-02-17 13:08:37,477 - ==> Top1: 59.920    Top5: 85.580    Loss: 1.992

2024-02-17 13:08:37,484 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:08:37,484 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:08:37,545 - 

2024-02-17 13:08:37,546 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:08:45,464 - Epoch: [192][  100/  500]    Overall Loss 0.218386    Objective Loss 0.218386                                        LR 0.000125    Time 0.079124    
2024-02-17 13:08:52,554 - Epoch: [192][  200/  500]    Overall Loss 0.217557    Objective Loss 0.217557                                        LR 0.000125    Time 0.074991    
2024-02-17 13:08:59,386 - Epoch: [192][  300/  500]    Overall Loss 0.215115    Objective Loss 0.215115                                        LR 0.000125    Time 0.072755    
2024-02-17 13:09:06,556 - Epoch: [192][  400/  500]    Overall Loss 0.217083    Objective Loss 0.217083                                        LR 0.000125    Time 0.072480    
2024-02-17 13:09:13,656 - Epoch: [192][  500/  500]    Overall Loss 0.219166    Objective Loss 0.219166    Top1 97.000000    Top5 100.000000    LR 0.000125    Time 0.072174    
2024-02-17 13:09:13,789 - --- validate (epoch=192)-----------
2024-02-17 13:09:13,790 - 10000 samples (100 per mini-batch)
2024-02-17 13:09:17,215 - Epoch: [192][  100/  100]    Loss 1.972215    Top1 59.890000    Top5 85.600000    
2024-02-17 13:09:17,330 - ==> Top1: 59.890    Top5: 85.600    Loss: 1.972

2024-02-17 13:09:17,337 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:09:17,338 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:09:17,397 - 

2024-02-17 13:09:17,398 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:09:25,051 - Epoch: [193][  100/  500]    Overall Loss 0.213690    Objective Loss 0.213690                                        LR 0.000125    Time 0.076471    
2024-02-17 13:09:32,047 - Epoch: [193][  200/  500]    Overall Loss 0.216133    Objective Loss 0.216133                                        LR 0.000125    Time 0.073193    
2024-02-17 13:09:39,122 - Epoch: [193][  300/  500]    Overall Loss 0.217282    Objective Loss 0.217282                                        LR 0.000125    Time 0.072363    
2024-02-17 13:09:46,284 - Epoch: [193][  400/  500]    Overall Loss 0.218157    Objective Loss 0.218157                                        LR 0.000125    Time 0.072168    
2024-02-17 13:09:53,414 - Epoch: [193][  500/  500]    Overall Loss 0.217944    Objective Loss 0.217944    Top1 95.000000    Top5 100.000000    LR 0.000125    Time 0.071984    
2024-02-17 13:09:53,593 - --- validate (epoch=193)-----------
2024-02-17 13:09:53,594 - 10000 samples (100 per mini-batch)
2024-02-17 13:09:56,493 - Epoch: [193][  100/  100]    Loss 1.982611    Top1 59.880000    Top5 85.910000    
2024-02-17 13:09:56,657 - ==> Top1: 59.880    Top5: 85.910    Loss: 1.983

2024-02-17 13:09:56,671 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:09:56,671 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:09:56,736 - 

2024-02-17 13:09:56,737 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:10:04,673 - Epoch: [194][  100/  500]    Overall Loss 0.203038    Objective Loss 0.203038                                        LR 0.000125    Time 0.079306    
2024-02-17 13:10:11,797 - Epoch: [194][  200/  500]    Overall Loss 0.209588    Objective Loss 0.209588                                        LR 0.000125    Time 0.075252    
2024-02-17 13:10:18,706 - Epoch: [194][  300/  500]    Overall Loss 0.212824    Objective Loss 0.212824                                        LR 0.000125    Time 0.073187    
2024-02-17 13:10:25,680 - Epoch: [194][  400/  500]    Overall Loss 0.214587    Objective Loss 0.214587                                        LR 0.000125    Time 0.072315    
2024-02-17 13:10:32,960 - Epoch: [194][  500/  500]    Overall Loss 0.216752    Objective Loss 0.216752    Top1 92.000000    Top5 99.500000    LR 0.000125    Time 0.072404    
2024-02-17 13:10:33,092 - --- validate (epoch=194)-----------
2024-02-17 13:10:33,093 - 10000 samples (100 per mini-batch)
2024-02-17 13:10:36,284 - Epoch: [194][  100/  100]    Loss 2.009548    Top1 59.420000    Top5 85.450000    
2024-02-17 13:10:36,445 - ==> Top1: 59.420    Top5: 85.450    Loss: 2.010

2024-02-17 13:10:36,456 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:10:36,456 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:10:36,524 - 

2024-02-17 13:10:36,524 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:10:44,332 - Epoch: [195][  100/  500]    Overall Loss 0.218217    Objective Loss 0.218217                                        LR 0.000125    Time 0.078008    
2024-02-17 13:10:51,670 - Epoch: [195][  200/  500]    Overall Loss 0.218245    Objective Loss 0.218245                                        LR 0.000125    Time 0.075673    
2024-02-17 13:10:58,971 - Epoch: [195][  300/  500]    Overall Loss 0.219620    Objective Loss 0.219620                                        LR 0.000125    Time 0.074771    
2024-02-17 13:11:06,293 - Epoch: [195][  400/  500]    Overall Loss 0.220032    Objective Loss 0.220032                                        LR 0.000125    Time 0.074375    
2024-02-17 13:11:13,463 - Epoch: [195][  500/  500]    Overall Loss 0.220848    Objective Loss 0.220848    Top1 93.000000    Top5 100.000000    LR 0.000125    Time 0.073832    
2024-02-17 13:11:13,608 - --- validate (epoch=195)-----------
2024-02-17 13:11:13,609 - 10000 samples (100 per mini-batch)
2024-02-17 13:11:16,895 - Epoch: [195][  100/  100]    Loss 1.998598    Top1 59.870000    Top5 85.750000    
2024-02-17 13:11:16,996 - ==> Top1: 59.870    Top5: 85.750    Loss: 1.999

2024-02-17 13:11:17,001 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:11:17,001 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:11:17,056 - 

2024-02-17 13:11:17,056 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:11:24,771 - Epoch: [196][  100/  500]    Overall Loss 0.215504    Objective Loss 0.215504                                        LR 0.000125    Time 0.077092    
2024-02-17 13:11:31,933 - Epoch: [196][  200/  500]    Overall Loss 0.215083    Objective Loss 0.215083                                        LR 0.000125    Time 0.074334    
2024-02-17 13:11:39,040 - Epoch: [196][  300/  500]    Overall Loss 0.214971    Objective Loss 0.214971                                        LR 0.000125    Time 0.073231    
2024-02-17 13:11:46,254 - Epoch: [196][  400/  500]    Overall Loss 0.215812    Objective Loss 0.215812                                        LR 0.000125    Time 0.072950    
2024-02-17 13:11:53,396 - Epoch: [196][  500/  500]    Overall Loss 0.216520    Objective Loss 0.216520    Top1 95.500000    Top5 100.000000    LR 0.000125    Time 0.072634    
2024-02-17 13:11:53,533 - --- validate (epoch=196)-----------
2024-02-17 13:11:53,535 - 10000 samples (100 per mini-batch)
2024-02-17 13:11:56,813 - Epoch: [196][  100/  100]    Loss 2.017509    Top1 59.410000    Top5 85.360000    
2024-02-17 13:11:56,913 - ==> Top1: 59.410    Top5: 85.360    Loss: 2.018

2024-02-17 13:11:56,920 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:11:56,920 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:11:56,978 - 

2024-02-17 13:11:56,979 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:12:04,634 - Epoch: [197][  100/  500]    Overall Loss 0.199093    Objective Loss 0.199093                                        LR 0.000125    Time 0.076493    
2024-02-17 13:12:11,509 - Epoch: [197][  200/  500]    Overall Loss 0.210532    Objective Loss 0.210532                                        LR 0.000125    Time 0.072606    
2024-02-17 13:12:18,585 - Epoch: [197][  300/  500]    Overall Loss 0.213149    Objective Loss 0.213149                                        LR 0.000125    Time 0.071976    
2024-02-17 13:12:25,646 - Epoch: [197][  400/  500]    Overall Loss 0.213956    Objective Loss 0.213956                                        LR 0.000125    Time 0.071625    
2024-02-17 13:12:32,825 - Epoch: [197][  500/  500]    Overall Loss 0.215248    Objective Loss 0.215248    Top1 93.000000    Top5 100.000000    LR 0.000125    Time 0.071649    
2024-02-17 13:12:32,990 - --- validate (epoch=197)-----------
2024-02-17 13:12:32,991 - 10000 samples (100 per mini-batch)
2024-02-17 13:12:36,301 - Epoch: [197][  100/  100]    Loss 1.980144    Top1 60.340000    Top5 85.590000    
2024-02-17 13:12:36,409 - ==> Top1: 60.340    Top5: 85.590    Loss: 1.980

2024-02-17 13:12:36,420 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:12:36,420 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:12:36,488 - 

2024-02-17 13:12:36,489 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:12:44,349 - Epoch: [198][  100/  500]    Overall Loss 0.212729    Objective Loss 0.212729                                        LR 0.000125    Time 0.078551    
2024-02-17 13:12:51,351 - Epoch: [198][  200/  500]    Overall Loss 0.210995    Objective Loss 0.210995                                        LR 0.000125    Time 0.074262    
2024-02-17 13:12:58,299 - Epoch: [198][  300/  500]    Overall Loss 0.211916    Objective Loss 0.211916                                        LR 0.000125    Time 0.072656    
2024-02-17 13:13:05,391 - Epoch: [198][  400/  500]    Overall Loss 0.213681    Objective Loss 0.213681                                        LR 0.000125    Time 0.072210    
2024-02-17 13:13:12,496 - Epoch: [198][  500/  500]    Overall Loss 0.216552    Objective Loss 0.216552    Top1 91.500000    Top5 100.000000    LR 0.000125    Time 0.071970    
2024-02-17 13:13:12,618 - --- validate (epoch=198)-----------
2024-02-17 13:13:12,618 - 10000 samples (100 per mini-batch)
2024-02-17 13:13:15,701 - Epoch: [198][  100/  100]    Loss 2.016967    Top1 59.390000    Top5 85.560000    
2024-02-17 13:13:15,817 - ==> Top1: 59.390    Top5: 85.560    Loss: 2.017

2024-02-17 13:13:15,823 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:13:15,824 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:13:15,880 - 

2024-02-17 13:13:15,880 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:13:23,452 - Epoch: [199][  100/  500]    Overall Loss 0.208800    Objective Loss 0.208800                                        LR 0.000125    Time 0.075667    
2024-02-17 13:13:30,457 - Epoch: [199][  200/  500]    Overall Loss 0.209824    Objective Loss 0.209824                                        LR 0.000125    Time 0.072839    
2024-02-17 13:13:37,503 - Epoch: [199][  300/  500]    Overall Loss 0.211934    Objective Loss 0.211934                                        LR 0.000125    Time 0.072031    
2024-02-17 13:13:44,551 - Epoch: [199][  400/  500]    Overall Loss 0.214977    Objective Loss 0.214977                                        LR 0.000125    Time 0.071634    
2024-02-17 13:13:51,600 - Epoch: [199][  500/  500]    Overall Loss 0.216143    Objective Loss 0.216143    Top1 93.500000    Top5 100.000000    LR 0.000125    Time 0.071397    
2024-02-17 13:13:51,728 - --- validate (epoch=199)-----------
2024-02-17 13:13:51,729 - 10000 samples (100 per mini-batch)
2024-02-17 13:13:54,670 - Epoch: [199][  100/  100]    Loss 1.993669    Top1 59.600000    Top5 85.480000    
2024-02-17 13:13:54,781 - ==> Top1: 59.600    Top5: 85.480    Loss: 1.994

2024-02-17 13:13:54,792 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:13:54,792 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:13:54,853 - 

2024-02-17 13:13:54,853 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:14:02,642 - Epoch: [200][  100/  500]    Overall Loss 0.203186    Objective Loss 0.203186                                        LR 0.000063    Time 0.077832    
2024-02-17 13:14:09,505 - Epoch: [200][  200/  500]    Overall Loss 0.200658    Objective Loss 0.200658                                        LR 0.000063    Time 0.073212    
2024-02-17 13:14:16,219 - Epoch: [200][  300/  500]    Overall Loss 0.199368    Objective Loss 0.199368                                        LR 0.000063    Time 0.071177    
2024-02-17 13:14:22,972 - Epoch: [200][  400/  500]    Overall Loss 0.198770    Objective Loss 0.198770                                        LR 0.000063    Time 0.070257    
2024-02-17 13:14:30,005 - Epoch: [200][  500/  500]    Overall Loss 0.200164    Objective Loss 0.200164    Top1 94.500000    Top5 100.000000    LR 0.000063    Time 0.070263    
2024-02-17 13:14:30,118 - --- validate (epoch=200)-----------
2024-02-17 13:14:30,119 - 10000 samples (100 per mini-batch)
2024-02-17 13:14:33,281 - Epoch: [200][  100/  100]    Loss 1.965052    Top1 59.960000    Top5 85.850000    
2024-02-17 13:14:33,406 - ==> Top1: 59.960    Top5: 85.850    Loss: 1.965

2024-02-17 13:14:33,418 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:14:33,418 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:14:33,477 - 

2024-02-17 13:14:33,477 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:14:41,115 - Epoch: [201][  100/  500]    Overall Loss 0.194777    Objective Loss 0.194777                                        LR 0.000063    Time 0.076318    
2024-02-17 13:14:48,532 - Epoch: [201][  200/  500]    Overall Loss 0.195865    Objective Loss 0.195865                                        LR 0.000063    Time 0.075224    
2024-02-17 13:14:55,857 - Epoch: [201][  300/  500]    Overall Loss 0.197402    Objective Loss 0.197402                                        LR 0.000063    Time 0.074553    
2024-02-17 13:15:02,631 - Epoch: [201][  400/  500]    Overall Loss 0.197737    Objective Loss 0.197737                                        LR 0.000063    Time 0.072841    
2024-02-17 13:15:09,755 - Epoch: [201][  500/  500]    Overall Loss 0.198817    Objective Loss 0.198817    Top1 92.000000    Top5 99.500000    LR 0.000063    Time 0.072513    
2024-02-17 13:15:09,866 - --- validate (epoch=201)-----------
2024-02-17 13:15:09,867 - 10000 samples (100 per mini-batch)
2024-02-17 13:15:12,897 - Epoch: [201][  100/  100]    Loss 1.979687    Top1 60.100000    Top5 85.590000    
2024-02-17 13:15:13,056 - ==> Top1: 60.100    Top5: 85.590    Loss: 1.980

2024-02-17 13:15:13,062 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:15:13,063 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:15:13,121 - 

2024-02-17 13:15:13,121 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:15:21,018 - Epoch: [202][  100/  500]    Overall Loss 0.196987    Objective Loss 0.196987                                        LR 0.000063    Time 0.078914    
2024-02-17 13:15:28,031 - Epoch: [202][  200/  500]    Overall Loss 0.198782    Objective Loss 0.198782                                        LR 0.000063    Time 0.074501    
2024-02-17 13:15:35,070 - Epoch: [202][  300/  500]    Overall Loss 0.199579    Objective Loss 0.199579                                        LR 0.000063    Time 0.073117    
2024-02-17 13:15:42,142 - Epoch: [202][  400/  500]    Overall Loss 0.202158    Objective Loss 0.202158                                        LR 0.000063    Time 0.072507    
2024-02-17 13:15:49,155 - Epoch: [202][  500/  500]    Overall Loss 0.202946    Objective Loss 0.202946    Top1 94.000000    Top5 100.000000    LR 0.000063    Time 0.072024    
2024-02-17 13:15:49,334 - --- validate (epoch=202)-----------
2024-02-17 13:15:49,335 - 10000 samples (100 per mini-batch)
2024-02-17 13:15:52,448 - Epoch: [202][  100/  100]    Loss 1.980951    Top1 60.160000    Top5 85.550000    
2024-02-17 13:15:52,593 - ==> Top1: 60.160    Top5: 85.550    Loss: 1.981

2024-02-17 13:15:52,600 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:15:52,600 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:15:52,665 - 

2024-02-17 13:15:52,666 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:16:00,118 - Epoch: [203][  100/  500]    Overall Loss 0.198696    Objective Loss 0.198696                                        LR 0.000063    Time 0.074470    
2024-02-17 13:16:07,180 - Epoch: [203][  200/  500]    Overall Loss 0.202378    Objective Loss 0.202378                                        LR 0.000063    Time 0.072524    
2024-02-17 13:16:14,021 - Epoch: [203][  300/  500]    Overall Loss 0.199438    Objective Loss 0.199438                                        LR 0.000063    Time 0.071142    
2024-02-17 13:16:21,039 - Epoch: [203][  400/  500]    Overall Loss 0.200565    Objective Loss 0.200565                                        LR 0.000063    Time 0.070891    
2024-02-17 13:16:28,206 - Epoch: [203][  500/  500]    Overall Loss 0.198712    Objective Loss 0.198712    Top1 93.500000    Top5 100.000000    LR 0.000063    Time 0.071038    
2024-02-17 13:16:28,305 - --- validate (epoch=203)-----------
2024-02-17 13:16:28,306 - 10000 samples (100 per mini-batch)
2024-02-17 13:16:31,193 - Epoch: [203][  100/  100]    Loss 1.983417    Top1 60.080000    Top5 85.490000    
2024-02-17 13:16:31,313 - ==> Top1: 60.080    Top5: 85.490    Loss: 1.983

2024-02-17 13:16:31,325 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:16:31,326 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:16:31,388 - 

2024-02-17 13:16:31,388 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:16:39,357 - Epoch: [204][  100/  500]    Overall Loss 0.194937    Objective Loss 0.194937                                        LR 0.000063    Time 0.079627    
2024-02-17 13:16:46,459 - Epoch: [204][  200/  500]    Overall Loss 0.194596    Objective Loss 0.194596                                        LR 0.000063    Time 0.075303    
2024-02-17 13:16:53,467 - Epoch: [204][  300/  500]    Overall Loss 0.192799    Objective Loss 0.192799                                        LR 0.000063    Time 0.073549    
2024-02-17 13:17:00,400 - Epoch: [204][  400/  500]    Overall Loss 0.195310    Objective Loss 0.195310                                        LR 0.000063    Time 0.072484    
2024-02-17 13:17:07,249 - Epoch: [204][  500/  500]    Overall Loss 0.196354    Objective Loss 0.196354    Top1 93.500000    Top5 99.500000    LR 0.000063    Time 0.071678    
2024-02-17 13:17:07,355 - --- validate (epoch=204)-----------
2024-02-17 13:17:07,356 - 10000 samples (100 per mini-batch)
2024-02-17 13:17:10,196 - Epoch: [204][  100/  100]    Loss 1.986956    Top1 60.240000    Top5 85.410000    
2024-02-17 13:17:10,352 - ==> Top1: 60.240    Top5: 85.410    Loss: 1.987

2024-02-17 13:17:10,365 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:17:10,365 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:17:10,430 - 

2024-02-17 13:17:10,431 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:17:18,071 - Epoch: [205][  100/  500]    Overall Loss 0.195522    Objective Loss 0.195522                                        LR 0.000063    Time 0.076344    
2024-02-17 13:17:25,153 - Epoch: [205][  200/  500]    Overall Loss 0.197703    Objective Loss 0.197703                                        LR 0.000063    Time 0.073562    
2024-02-17 13:17:32,161 - Epoch: [205][  300/  500]    Overall Loss 0.197991    Objective Loss 0.197991                                        LR 0.000063    Time 0.072388    
2024-02-17 13:17:39,159 - Epoch: [205][  400/  500]    Overall Loss 0.197787    Objective Loss 0.197787                                        LR 0.000063    Time 0.071775    
2024-02-17 13:17:46,233 - Epoch: [205][  500/  500]    Overall Loss 0.198011    Objective Loss 0.198011    Top1 96.000000    Top5 100.000000    LR 0.000063    Time 0.071560    
2024-02-17 13:17:46,334 - --- validate (epoch=205)-----------
2024-02-17 13:17:46,335 - 10000 samples (100 per mini-batch)
2024-02-17 13:17:49,341 - Epoch: [205][  100/  100]    Loss 1.993326    Top1 59.860000    Top5 85.530000    
2024-02-17 13:17:49,440 - ==> Top1: 59.860    Top5: 85.530    Loss: 1.993

2024-02-17 13:17:49,447 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:17:49,447 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:17:49,507 - 

2024-02-17 13:17:49,508 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:17:57,132 - Epoch: [206][  100/  500]    Overall Loss 0.187737    Objective Loss 0.187737                                        LR 0.000063    Time 0.076187    
2024-02-17 13:18:04,422 - Epoch: [206][  200/  500]    Overall Loss 0.190697    Objective Loss 0.190697                                        LR 0.000063    Time 0.074521    
2024-02-17 13:18:11,699 - Epoch: [206][  300/  500]    Overall Loss 0.191348    Objective Loss 0.191348                                        LR 0.000063    Time 0.073925    
2024-02-17 13:18:18,907 - Epoch: [206][  400/  500]    Overall Loss 0.193628    Objective Loss 0.193628                                        LR 0.000063    Time 0.073454    
2024-02-17 13:18:26,174 - Epoch: [206][  500/  500]    Overall Loss 0.194200    Objective Loss 0.194200    Top1 91.000000    Top5 100.000000    LR 0.000063    Time 0.073288    
2024-02-17 13:18:26,296 - --- validate (epoch=206)-----------
2024-02-17 13:18:26,297 - 10000 samples (100 per mini-batch)
2024-02-17 13:18:29,451 - Epoch: [206][  100/  100]    Loss 1.985267    Top1 59.780000    Top5 85.580000    
2024-02-17 13:18:29,549 - ==> Top1: 59.780    Top5: 85.580    Loss: 1.985

2024-02-17 13:18:29,561 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:18:29,562 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:18:29,623 - 

2024-02-17 13:18:29,624 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:18:37,231 - Epoch: [207][  100/  500]    Overall Loss 0.194213    Objective Loss 0.194213                                        LR 0.000063    Time 0.076017    
2024-02-17 13:18:44,426 - Epoch: [207][  200/  500]    Overall Loss 0.199908    Objective Loss 0.199908                                        LR 0.000063    Time 0.073963    
2024-02-17 13:18:51,714 - Epoch: [207][  300/  500]    Overall Loss 0.195590    Objective Loss 0.195590                                        LR 0.000063    Time 0.073587    
2024-02-17 13:18:58,862 - Epoch: [207][  400/  500]    Overall Loss 0.196481    Objective Loss 0.196481                                        LR 0.000063    Time 0.073050    
2024-02-17 13:19:05,791 - Epoch: [207][  500/  500]    Overall Loss 0.196540    Objective Loss 0.196540    Top1 96.500000    Top5 100.000000    LR 0.000063    Time 0.072289    
2024-02-17 13:19:05,924 - --- validate (epoch=207)-----------
2024-02-17 13:19:05,925 - 10000 samples (100 per mini-batch)
2024-02-17 13:19:08,951 - Epoch: [207][  100/  100]    Loss 1.995921    Top1 60.050000    Top5 85.430000    
2024-02-17 13:19:09,093 - ==> Top1: 60.050    Top5: 85.430    Loss: 1.996

2024-02-17 13:19:09,106 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:19:09,106 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:19:09,169 - 

2024-02-17 13:19:09,170 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:19:16,700 - Epoch: [208][  100/  500]    Overall Loss 0.186295    Objective Loss 0.186295                                        LR 0.000063    Time 0.075249    
2024-02-17 13:19:23,886 - Epoch: [208][  200/  500]    Overall Loss 0.191939    Objective Loss 0.191939                                        LR 0.000063    Time 0.073532    
2024-02-17 13:19:31,034 - Epoch: [208][  300/  500]    Overall Loss 0.191132    Objective Loss 0.191132                                        LR 0.000063    Time 0.072834    
2024-02-17 13:19:38,124 - Epoch: [208][  400/  500]    Overall Loss 0.191322    Objective Loss 0.191322                                        LR 0.000063    Time 0.072342    
2024-02-17 13:19:45,010 - Epoch: [208][  500/  500]    Overall Loss 0.191751    Objective Loss 0.191751    Top1 94.500000    Top5 99.500000    LR 0.000063    Time 0.071637    
2024-02-17 13:19:45,114 - --- validate (epoch=208)-----------
2024-02-17 13:19:45,115 - 10000 samples (100 per mini-batch)
2024-02-17 13:19:48,168 - Epoch: [208][  100/  100]    Loss 1.989587    Top1 59.850000    Top5 85.630000    
2024-02-17 13:19:48,248 - ==> Top1: 59.850    Top5: 85.630    Loss: 1.990

2024-02-17 13:19:48,258 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:19:48,258 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:19:48,310 - 

2024-02-17 13:19:48,310 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:19:55,690 - Epoch: [209][  100/  500]    Overall Loss 0.182289    Objective Loss 0.182289                                        LR 0.000063    Time 0.073754    
2024-02-17 13:20:02,753 - Epoch: [209][  200/  500]    Overall Loss 0.184634    Objective Loss 0.184634                                        LR 0.000063    Time 0.072169    
2024-02-17 13:20:09,859 - Epoch: [209][  300/  500]    Overall Loss 0.189662    Objective Loss 0.189662                                        LR 0.000063    Time 0.071786    
2024-02-17 13:20:16,862 - Epoch: [209][  400/  500]    Overall Loss 0.189371    Objective Loss 0.189371                                        LR 0.000063    Time 0.071338    
2024-02-17 13:20:23,867 - Epoch: [209][  500/  500]    Overall Loss 0.191692    Objective Loss 0.191692    Top1 97.500000    Top5 100.000000    LR 0.000063    Time 0.071069    
2024-02-17 13:20:24,003 - --- validate (epoch=209)-----------
2024-02-17 13:20:24,004 - 10000 samples (100 per mini-batch)
2024-02-17 13:20:26,766 - Epoch: [209][  100/  100]    Loss 1.987464    Top1 60.260000    Top5 85.570000    
2024-02-17 13:20:26,927 - ==> Top1: 60.260    Top5: 85.570    Loss: 1.987

2024-02-17 13:20:26,940 - ==> Best [Top1: 60.580   Top5: 85.860   Sparsity:0.00   Params: 753952 on epoch: 151]
2024-02-17 13:20:26,940 - Saving checkpoint to: logs/2024.02.17-110158/checkpoint.pth.tar
2024-02-17 13:20:27,002 - 

2024-02-17 13:20:27,002 - Initiating quantization aware training (QAT)...
2024-02-17 13:20:27,066 - 

2024-02-17 13:20:27,066 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:20:36,449 - Epoch: [210][  100/  500]    Overall Loss 6.124247    Objective Loss 6.124247                                        LR 0.000063    Time 0.093783    
2024-02-17 13:20:45,037 - Epoch: [210][  200/  500]    Overall Loss 5.222038    Objective Loss 5.222038                                        LR 0.000063    Time 0.089814    
2024-02-17 13:20:53,565 - Epoch: [210][  300/  500]    Overall Loss 4.866382    Objective Loss 4.866382                                        LR 0.000063    Time 0.088292    
2024-02-17 13:21:01,707 - Epoch: [210][  400/  500]    Overall Loss 4.664867    Objective Loss 4.664867                                        LR 0.000063    Time 0.086566    
2024-02-17 13:21:10,410 - Epoch: [210][  500/  500]    Overall Loss 4.521233    Objective Loss 4.521233    Top1 9.500000    Top5 27.500000    LR 0.000063    Time 0.086653    
2024-02-17 13:21:10,588 - --- validate (epoch=210)-----------
2024-02-17 13:21:10,589 - 10000 samples (100 per mini-batch)
2024-02-17 13:21:16,150 - Epoch: [210][  100/  100]    Loss 3.897221    Top1 10.110000    Top5 30.700000    
2024-02-17 13:21:16,256 - ==> Top1: 10.110    Top5: 30.700    Loss: 3.897

2024-02-17 13:21:16,266 - ==> Best [Top1: 10.110   Top5: 30.700   Sparsity:0.00   Params: 753952 on epoch: 210]
2024-02-17 13:21:16,266 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:21:16,320 - 

2024-02-17 13:21:16,321 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:21:25,580 - Epoch: [211][  100/  500]    Overall Loss 3.870322    Objective Loss 3.870322                                        LR 0.000063    Time 0.092531    
2024-02-17 13:21:34,087 - Epoch: [211][  200/  500]    Overall Loss 3.837896    Objective Loss 3.837896                                        LR 0.000063    Time 0.088786    
2024-02-17 13:21:42,615 - Epoch: [211][  300/  500]    Overall Loss 3.815821    Objective Loss 3.815821                                        LR 0.000063    Time 0.087606    
2024-02-17 13:21:51,221 - Epoch: [211][  400/  500]    Overall Loss 3.793396    Objective Loss 3.793396                                        LR 0.000063    Time 0.087211    
2024-02-17 13:22:00,098 - Epoch: [211][  500/  500]    Overall Loss 3.772180    Objective Loss 3.772180    Top1 10.500000    Top5 35.500000    LR 0.000063    Time 0.087516    
2024-02-17 13:22:00,216 - --- validate (epoch=211)-----------
2024-02-17 13:22:00,217 - 10000 samples (100 per mini-batch)
2024-02-17 13:22:05,685 - Epoch: [211][  100/  100]    Loss 3.569264    Top1 15.320000    Top5 40.300000    
2024-02-17 13:22:05,813 - ==> Top1: 15.320    Top5: 40.300    Loss: 3.569

2024-02-17 13:22:05,823 - ==> Best [Top1: 15.320   Top5: 40.300   Sparsity:0.00   Params: 753952 on epoch: 211]
2024-02-17 13:22:05,823 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:22:05,888 - 

2024-02-17 13:22:05,888 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:22:15,567 - Epoch: [212][  100/  500]    Overall Loss 3.644640    Objective Loss 3.644640                                        LR 0.000063    Time 0.096732    
2024-02-17 13:22:24,578 - Epoch: [212][  200/  500]    Overall Loss 3.628560    Objective Loss 3.628560                                        LR 0.000063    Time 0.093403    
2024-02-17 13:22:32,805 - Epoch: [212][  300/  500]    Overall Loss 3.616452    Objective Loss 3.616452                                        LR 0.000063    Time 0.089683    
2024-02-17 13:22:41,452 - Epoch: [212][  400/  500]    Overall Loss 3.602706    Objective Loss 3.602706                                        LR 0.000063    Time 0.088871    
2024-02-17 13:22:50,223 - Epoch: [212][  500/  500]    Overall Loss 3.583621    Objective Loss 3.583621    Top1 15.500000    Top5 44.000000    LR 0.000063    Time 0.088631    
2024-02-17 13:22:50,325 - --- validate (epoch=212)-----------
2024-02-17 13:22:50,327 - 10000 samples (100 per mini-batch)
2024-02-17 13:22:56,078 - Epoch: [212][  100/  100]    Loss 3.440256    Top1 18.090000    Top5 44.020000    
2024-02-17 13:22:56,236 - ==> Top1: 18.090    Top5: 44.020    Loss: 3.440

2024-02-17 13:22:56,246 - ==> Best [Top1: 18.090   Top5: 44.020   Sparsity:0.00   Params: 753952 on epoch: 212]
2024-02-17 13:22:56,246 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:22:56,313 - 

2024-02-17 13:22:56,313 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:23:05,902 - Epoch: [213][  100/  500]    Overall Loss 3.472034    Objective Loss 3.472034                                        LR 0.000063    Time 0.095829    
2024-02-17 13:23:14,961 - Epoch: [213][  200/  500]    Overall Loss 3.471718    Objective Loss 3.471718                                        LR 0.000063    Time 0.093188    
2024-02-17 13:23:23,738 - Epoch: [213][  300/  500]    Overall Loss 3.466396    Objective Loss 3.466396                                        LR 0.000063    Time 0.091372    
2024-02-17 13:23:32,498 - Epoch: [213][  400/  500]    Overall Loss 3.449883    Objective Loss 3.449883                                        LR 0.000063    Time 0.090418    
2024-02-17 13:23:41,227 - Epoch: [213][  500/  500]    Overall Loss 3.436295    Objective Loss 3.436295    Top1 17.000000    Top5 48.000000    LR 0.000063    Time 0.089785    
2024-02-17 13:23:41,353 - --- validate (epoch=213)-----------
2024-02-17 13:23:41,354 - 10000 samples (100 per mini-batch)
2024-02-17 13:23:46,793 - Epoch: [213][  100/  100]    Loss 3.259679    Top1 21.010000    Top5 48.240000    
2024-02-17 13:23:46,900 - ==> Top1: 21.010    Top5: 48.240    Loss: 3.260

2024-02-17 13:23:46,909 - ==> Best [Top1: 21.010   Top5: 48.240   Sparsity:0.00   Params: 753952 on epoch: 213]
2024-02-17 13:23:46,909 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:23:46,980 - 

2024-02-17 13:23:46,980 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:23:56,476 - Epoch: [214][  100/  500]    Overall Loss 3.334383    Objective Loss 3.334383                                        LR 0.000063    Time 0.094897    
2024-02-17 13:24:05,188 - Epoch: [214][  200/  500]    Overall Loss 3.342141    Objective Loss 3.342141                                        LR 0.000063    Time 0.090991    
2024-02-17 13:24:13,816 - Epoch: [214][  300/  500]    Overall Loss 3.334294    Objective Loss 3.334294                                        LR 0.000063    Time 0.089409    
2024-02-17 13:24:22,440 - Epoch: [214][  400/  500]    Overall Loss 3.327660    Objective Loss 3.327660                                        LR 0.000063    Time 0.088607    
2024-02-17 13:24:30,749 - Epoch: [214][  500/  500]    Overall Loss 3.321653    Objective Loss 3.321653    Top1 15.500000    Top5 44.000000    LR 0.000063    Time 0.087497    
2024-02-17 13:24:30,853 - --- validate (epoch=214)-----------
2024-02-17 13:24:30,854 - 10000 samples (100 per mini-batch)
2024-02-17 13:24:36,119 - Epoch: [214][  100/  100]    Loss 3.190542    Top1 21.250000    Top5 50.350000    
2024-02-17 13:24:36,213 - ==> Top1: 21.250    Top5: 50.350    Loss: 3.191

2024-02-17 13:24:36,223 - ==> Best [Top1: 21.250   Top5: 50.350   Sparsity:0.00   Params: 753952 on epoch: 214]
2024-02-17 13:24:36,223 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:24:36,291 - 

2024-02-17 13:24:36,291 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:24:45,646 - Epoch: [215][  100/  500]    Overall Loss 3.279288    Objective Loss 3.279288                                        LR 0.000063    Time 0.093499    
2024-02-17 13:24:54,421 - Epoch: [215][  200/  500]    Overall Loss 3.255095    Objective Loss 3.255095                                        LR 0.000063    Time 0.090605    
2024-02-17 13:25:03,107 - Epoch: [215][  300/  500]    Overall Loss 3.254737    Objective Loss 3.254737                                        LR 0.000063    Time 0.089345    
2024-02-17 13:25:11,847 - Epoch: [215][  400/  500]    Overall Loss 3.238196    Objective Loss 3.238196                                        LR 0.000063    Time 0.088849    
2024-02-17 13:25:20,637 - Epoch: [215][  500/  500]    Overall Loss 3.232759    Objective Loss 3.232759    Top1 27.000000    Top5 49.500000    LR 0.000063    Time 0.088654    
2024-02-17 13:25:20,747 - --- validate (epoch=215)-----------
2024-02-17 13:25:20,748 - 10000 samples (100 per mini-batch)
2024-02-17 13:25:25,959 - Epoch: [215][  100/  100]    Loss 3.149867    Top1 23.310000    Top5 51.210000    
2024-02-17 13:25:26,072 - ==> Top1: 23.310    Top5: 51.210    Loss: 3.150

2024-02-17 13:25:26,081 - ==> Best [Top1: 23.310   Top5: 51.210   Sparsity:0.00   Params: 753952 on epoch: 215]
2024-02-17 13:25:26,081 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:25:26,139 - 

2024-02-17 13:25:26,140 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:25:35,291 - Epoch: [216][  100/  500]    Overall Loss 3.212523    Objective Loss 3.212523                                        LR 0.000063    Time 0.091468    
2024-02-17 13:25:43,762 - Epoch: [216][  200/  500]    Overall Loss 3.196538    Objective Loss 3.196538                                        LR 0.000063    Time 0.088071    
2024-02-17 13:25:52,370 - Epoch: [216][  300/  500]    Overall Loss 3.178787    Objective Loss 3.178787                                        LR 0.000063    Time 0.087397    
2024-02-17 13:26:00,937 - Epoch: [216][  400/  500]    Overall Loss 3.162316    Objective Loss 3.162316                                        LR 0.000063    Time 0.086956    
2024-02-17 13:26:09,628 - Epoch: [216][  500/  500]    Overall Loss 3.152336    Objective Loss 3.152336    Top1 30.000000    Top5 57.000000    LR 0.000063    Time 0.086940    
2024-02-17 13:26:09,805 - --- validate (epoch=216)-----------
2024-02-17 13:26:09,806 - 10000 samples (100 per mini-batch)
2024-02-17 13:26:15,262 - Epoch: [216][  100/  100]    Loss 3.101714    Top1 23.810000    Top5 52.380000    
2024-02-17 13:26:15,383 - ==> Top1: 23.810    Top5: 52.380    Loss: 3.102

2024-02-17 13:26:15,392 - ==> Best [Top1: 23.810   Top5: 52.380   Sparsity:0.00   Params: 753952 on epoch: 216]
2024-02-17 13:26:15,393 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:26:15,456 - 

2024-02-17 13:26:15,457 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:26:24,915 - Epoch: [217][  100/  500]    Overall Loss 3.117277    Objective Loss 3.117277                                        LR 0.000063    Time 0.094535    
2024-02-17 13:26:33,652 - Epoch: [217][  200/  500]    Overall Loss 3.091770    Objective Loss 3.091770                                        LR 0.000063    Time 0.090930    
2024-02-17 13:26:42,594 - Epoch: [217][  300/  500]    Overall Loss 3.084095    Objective Loss 3.084095                                        LR 0.000063    Time 0.090414    
2024-02-17 13:26:51,645 - Epoch: [217][  400/  500]    Overall Loss 3.083109    Objective Loss 3.083109                                        LR 0.000063    Time 0.090428    
2024-02-17 13:27:00,571 - Epoch: [217][  500/  500]    Overall Loss 3.079040    Objective Loss 3.079040    Top1 24.000000    Top5 55.500000    LR 0.000063    Time 0.090187    
2024-02-17 13:27:00,709 - --- validate (epoch=217)-----------
2024-02-17 13:27:00,710 - 10000 samples (100 per mini-batch)
2024-02-17 13:27:06,594 - Epoch: [217][  100/  100]    Loss 2.977503    Top1 25.780000    Top5 55.950000    
2024-02-17 13:27:06,693 - ==> Top1: 25.780    Top5: 55.950    Loss: 2.978

2024-02-17 13:27:06,703 - ==> Best [Top1: 25.780   Top5: 55.950   Sparsity:0.00   Params: 753952 on epoch: 217]
2024-02-17 13:27:06,704 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:27:06,776 - 

2024-02-17 13:27:06,776 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:27:16,213 - Epoch: [218][  100/  500]    Overall Loss 3.050107    Objective Loss 3.050107                                        LR 0.000063    Time 0.094310    
2024-02-17 13:27:25,159 - Epoch: [218][  200/  500]    Overall Loss 3.034533    Objective Loss 3.034533                                        LR 0.000063    Time 0.091867    
2024-02-17 13:27:34,193 - Epoch: [218][  300/  500]    Overall Loss 3.031471    Objective Loss 3.031471                                        LR 0.000063    Time 0.091344    
2024-02-17 13:27:43,025 - Epoch: [218][  400/  500]    Overall Loss 3.029593    Objective Loss 3.029593                                        LR 0.000063    Time 0.090579    
2024-02-17 13:27:51,811 - Epoch: [218][  500/  500]    Overall Loss 3.022184    Objective Loss 3.022184    Top1 24.000000    Top5 53.500000    LR 0.000063    Time 0.090027    
2024-02-17 13:27:51,955 - --- validate (epoch=218)-----------
2024-02-17 13:27:51,955 - 10000 samples (100 per mini-batch)
2024-02-17 13:27:57,335 - Epoch: [218][  100/  100]    Loss 2.935082    Top1 26.950000    Top5 56.540000    
2024-02-17 13:27:57,421 - ==> Top1: 26.950    Top5: 56.540    Loss: 2.935

2024-02-17 13:27:57,426 - ==> Best [Top1: 26.950   Top5: 56.540   Sparsity:0.00   Params: 753952 on epoch: 218]
2024-02-17 13:27:57,426 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:27:57,480 - 

2024-02-17 13:27:57,480 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:28:06,496 - Epoch: [219][  100/  500]    Overall Loss 2.950139    Objective Loss 2.950139                                        LR 0.000063    Time 0.090112    
2024-02-17 13:28:15,636 - Epoch: [219][  200/  500]    Overall Loss 2.985613    Objective Loss 2.985613                                        LR 0.000063    Time 0.090738    
2024-02-17 13:28:24,087 - Epoch: [219][  300/  500]    Overall Loss 2.961757    Objective Loss 2.961757                                        LR 0.000063    Time 0.088651    
2024-02-17 13:28:32,755 - Epoch: [219][  400/  500]    Overall Loss 2.959490    Objective Loss 2.959490                                        LR 0.000063    Time 0.088151    
2024-02-17 13:28:40,921 - Epoch: [219][  500/  500]    Overall Loss 2.956137    Objective Loss 2.956137    Top1 29.000000    Top5 59.500000    LR 0.000063    Time 0.086847    
2024-02-17 13:28:41,054 - --- validate (epoch=219)-----------
2024-02-17 13:28:41,055 - 10000 samples (100 per mini-batch)
2024-02-17 13:28:46,563 - Epoch: [219][  100/  100]    Loss 2.833788    Top1 28.710000    Top5 58.980000    
2024-02-17 13:28:46,717 - ==> Top1: 28.710    Top5: 58.980    Loss: 2.834

2024-02-17 13:28:46,727 - ==> Best [Top1: 28.710   Top5: 58.980   Sparsity:0.00   Params: 753952 on epoch: 219]
2024-02-17 13:28:46,727 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:28:46,793 - 

2024-02-17 13:28:46,793 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:28:56,544 - Epoch: [220][  100/  500]    Overall Loss 2.897346    Objective Loss 2.897346                                        LR 0.000063    Time 0.097456    
2024-02-17 13:29:05,383 - Epoch: [220][  200/  500]    Overall Loss 2.898250    Objective Loss 2.898250                                        LR 0.000063    Time 0.092901    
2024-02-17 13:29:13,970 - Epoch: [220][  300/  500]    Overall Loss 2.889440    Objective Loss 2.889440                                        LR 0.000063    Time 0.090548    
2024-02-17 13:29:22,800 - Epoch: [220][  400/  500]    Overall Loss 2.884036    Objective Loss 2.884036                                        LR 0.000063    Time 0.089976    
2024-02-17 13:29:31,719 - Epoch: [220][  500/  500]    Overall Loss 2.880362    Objective Loss 2.880362    Top1 34.500000    Top5 59.000000    LR 0.000063    Time 0.089811    
2024-02-17 13:29:31,849 - --- validate (epoch=220)-----------
2024-02-17 13:29:31,850 - 10000 samples (100 per mini-batch)
2024-02-17 13:29:37,345 - Epoch: [220][  100/  100]    Loss 2.830918    Top1 28.340000    Top5 59.430000    
2024-02-17 13:29:37,459 - ==> Top1: 28.340    Top5: 59.430    Loss: 2.831

2024-02-17 13:29:37,468 - ==> Best [Top1: 28.710   Top5: 58.980   Sparsity:0.00   Params: 753952 on epoch: 219]
2024-02-17 13:29:37,468 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:29:37,515 - 

2024-02-17 13:29:37,516 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:29:46,845 - Epoch: [221][  100/  500]    Overall Loss 2.854455    Objective Loss 2.854455                                        LR 0.000063    Time 0.093239    
2024-02-17 13:29:55,525 - Epoch: [221][  200/  500]    Overall Loss 2.840783    Objective Loss 2.840783                                        LR 0.000063    Time 0.090003    
2024-02-17 13:30:04,008 - Epoch: [221][  300/  500]    Overall Loss 2.847887    Objective Loss 2.847887                                        LR 0.000063    Time 0.088268    
2024-02-17 13:30:12,768 - Epoch: [221][  400/  500]    Overall Loss 2.841815    Objective Loss 2.841815                                        LR 0.000063    Time 0.088093    
2024-02-17 13:30:21,806 - Epoch: [221][  500/  500]    Overall Loss 2.838715    Objective Loss 2.838715    Top1 30.500000    Top5 59.500000    LR 0.000063    Time 0.088543    
2024-02-17 13:30:21,963 - --- validate (epoch=221)-----------
2024-02-17 13:30:21,964 - 10000 samples (100 per mini-batch)
2024-02-17 13:30:27,640 - Epoch: [221][  100/  100]    Loss 2.846238    Top1 28.750000    Top5 59.780000    
2024-02-17 13:30:27,735 - ==> Top1: 28.750    Top5: 59.780    Loss: 2.846

2024-02-17 13:30:27,744 - ==> Best [Top1: 28.750   Top5: 59.780   Sparsity:0.00   Params: 753952 on epoch: 221]
2024-02-17 13:30:27,745 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:30:27,811 - 

2024-02-17 13:30:27,811 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:30:37,196 - Epoch: [222][  100/  500]    Overall Loss 2.824057    Objective Loss 2.824057                                        LR 0.000063    Time 0.093799    
2024-02-17 13:30:46,089 - Epoch: [222][  200/  500]    Overall Loss 2.802206    Objective Loss 2.802206                                        LR 0.000063    Time 0.091342    
2024-02-17 13:30:54,814 - Epoch: [222][  300/  500]    Overall Loss 2.793029    Objective Loss 2.793029                                        LR 0.000063    Time 0.089966    
2024-02-17 13:31:03,316 - Epoch: [222][  400/  500]    Overall Loss 2.794786    Objective Loss 2.794786                                        LR 0.000063    Time 0.088721    
2024-02-17 13:31:11,912 - Epoch: [222][  500/  500]    Overall Loss 2.793255    Objective Loss 2.793255    Top1 24.000000    Top5 55.000000    LR 0.000063    Time 0.088160    
2024-02-17 13:31:12,066 - --- validate (epoch=222)-----------
2024-02-17 13:31:12,067 - 10000 samples (100 per mini-batch)
2024-02-17 13:31:17,550 - Epoch: [222][  100/  100]    Loss 2.795253    Top1 29.690000    Top5 60.190000    
2024-02-17 13:31:17,646 - ==> Top1: 29.690    Top5: 60.190    Loss: 2.795

2024-02-17 13:31:17,651 - ==> Best [Top1: 29.690   Top5: 60.190   Sparsity:0.00   Params: 753952 on epoch: 222]
2024-02-17 13:31:17,651 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:31:17,711 - 

2024-02-17 13:31:17,711 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:31:26,933 - Epoch: [223][  100/  500]    Overall Loss 2.769440    Objective Loss 2.769440                                        LR 0.000063    Time 0.092169    
2024-02-17 13:31:35,735 - Epoch: [223][  200/  500]    Overall Loss 2.740412    Objective Loss 2.740412                                        LR 0.000063    Time 0.090072    
2024-02-17 13:31:44,537 - Epoch: [223][  300/  500]    Overall Loss 2.732600    Objective Loss 2.732600                                        LR 0.000063    Time 0.089377    
2024-02-17 13:31:53,121 - Epoch: [223][  400/  500]    Overall Loss 2.737406    Objective Loss 2.737406                                        LR 0.000063    Time 0.088484    
2024-02-17 13:32:01,507 - Epoch: [223][  500/  500]    Overall Loss 2.733703    Objective Loss 2.733703    Top1 29.000000    Top5 58.500000    LR 0.000063    Time 0.087553    
2024-02-17 13:32:01,618 - --- validate (epoch=223)-----------
2024-02-17 13:32:01,619 - 10000 samples (100 per mini-batch)
2024-02-17 13:32:07,018 - Epoch: [223][  100/  100]    Loss 2.767389    Top1 30.720000    Top5 61.640000    
2024-02-17 13:32:07,098 - ==> Top1: 30.720    Top5: 61.640    Loss: 2.767

2024-02-17 13:32:07,106 - ==> Best [Top1: 30.720   Top5: 61.640   Sparsity:0.00   Params: 753952 on epoch: 223]
2024-02-17 13:32:07,107 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:32:07,164 - 

2024-02-17 13:32:07,165 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:32:16,575 - Epoch: [224][  100/  500]    Overall Loss 2.675517    Objective Loss 2.675517                                        LR 0.000063    Time 0.094051    
2024-02-17 13:32:25,545 - Epoch: [224][  200/  500]    Overall Loss 2.679488    Objective Loss 2.679488                                        LR 0.000063    Time 0.091854    
2024-02-17 13:32:34,607 - Epoch: [224][  300/  500]    Overall Loss 2.691833    Objective Loss 2.691833                                        LR 0.000063    Time 0.091430    
2024-02-17 13:32:43,514 - Epoch: [224][  400/  500]    Overall Loss 2.695079    Objective Loss 2.695079                                        LR 0.000063    Time 0.090830    
2024-02-17 13:32:52,244 - Epoch: [224][  500/  500]    Overall Loss 2.695522    Objective Loss 2.695522    Top1 29.000000    Top5 63.500000    LR 0.000063    Time 0.090117    
2024-02-17 13:32:52,343 - --- validate (epoch=224)-----------
2024-02-17 13:32:52,344 - 10000 samples (100 per mini-batch)
2024-02-17 13:32:57,536 - Epoch: [224][  100/  100]    Loss 2.708027    Top1 31.760000    Top5 62.600000    
2024-02-17 13:32:57,715 - ==> Top1: 31.760    Top5: 62.600    Loss: 2.708

2024-02-17 13:32:57,725 - ==> Best [Top1: 31.760   Top5: 62.600   Sparsity:0.00   Params: 753952 on epoch: 224]
2024-02-17 13:32:57,726 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:32:57,987 - 

2024-02-17 13:32:57,988 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:33:07,202 - Epoch: [225][  100/  500]    Overall Loss 2.692502    Objective Loss 2.692502                                        LR 0.000063    Time 0.092094    
2024-02-17 13:33:16,202 - Epoch: [225][  200/  500]    Overall Loss 2.705546    Objective Loss 2.705546                                        LR 0.000063    Time 0.091026    
2024-02-17 13:33:25,342 - Epoch: [225][  300/  500]    Overall Loss 2.701614    Objective Loss 2.701614                                        LR 0.000063    Time 0.091137    
2024-02-17 13:33:34,419 - Epoch: [225][  400/  500]    Overall Loss 2.705496    Objective Loss 2.705496                                        LR 0.000063    Time 0.091035    
2024-02-17 13:33:43,480 - Epoch: [225][  500/  500]    Overall Loss 2.705715    Objective Loss 2.705715    Top1 31.000000    Top5 63.500000    LR 0.000063    Time 0.090944    
2024-02-17 13:33:43,599 - --- validate (epoch=225)-----------
2024-02-17 13:33:43,600 - 10000 samples (100 per mini-batch)
2024-02-17 13:33:49,045 - Epoch: [225][  100/  100]    Loss 2.582200    Top1 33.730000    Top5 65.360000    
2024-02-17 13:33:49,142 - ==> Top1: 33.730    Top5: 65.360    Loss: 2.582

2024-02-17 13:33:49,149 - ==> Best [Top1: 33.730   Top5: 65.360   Sparsity:0.00   Params: 753952 on epoch: 225]
2024-02-17 13:33:49,150 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:33:49,209 - 

2024-02-17 13:33:49,209 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:33:58,851 - Epoch: [226][  100/  500]    Overall Loss 2.623650    Objective Loss 2.623650                                        LR 0.000063    Time 0.096374    
2024-02-17 13:34:07,619 - Epoch: [226][  200/  500]    Overall Loss 2.665326    Objective Loss 2.665326                                        LR 0.000063    Time 0.092005    
2024-02-17 13:34:16,316 - Epoch: [226][  300/  500]    Overall Loss 2.655271    Objective Loss 2.655271                                        LR 0.000063    Time 0.090314    
2024-02-17 13:34:24,739 - Epoch: [226][  400/  500]    Overall Loss 2.650780    Objective Loss 2.650780                                        LR 0.000063    Time 0.088787    
2024-02-17 13:34:33,481 - Epoch: [226][  500/  500]    Overall Loss 2.647632    Objective Loss 2.647632    Top1 33.000000    Top5 65.500000    LR 0.000063    Time 0.088505    
2024-02-17 13:34:33,627 - --- validate (epoch=226)-----------
2024-02-17 13:34:33,627 - 10000 samples (100 per mini-batch)
2024-02-17 13:34:39,250 - Epoch: [226][  100/  100]    Loss 2.678545    Top1 31.570000    Top5 63.290000    
2024-02-17 13:34:39,397 - ==> Top1: 31.570    Top5: 63.290    Loss: 2.679

2024-02-17 13:34:39,407 - ==> Best [Top1: 33.730   Top5: 65.360   Sparsity:0.00   Params: 753952 on epoch: 225]
2024-02-17 13:34:39,408 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:34:39,459 - 

2024-02-17 13:34:39,460 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:34:48,752 - Epoch: [227][  100/  500]    Overall Loss 2.625645    Objective Loss 2.625645                                        LR 0.000063    Time 0.092868    
2024-02-17 13:34:57,476 - Epoch: [227][  200/  500]    Overall Loss 2.622561    Objective Loss 2.622561                                        LR 0.000063    Time 0.090034    
2024-02-17 13:35:06,239 - Epoch: [227][  300/  500]    Overall Loss 2.629373    Objective Loss 2.629373                                        LR 0.000063    Time 0.089220    
2024-02-17 13:35:14,824 - Epoch: [227][  400/  500]    Overall Loss 2.636234    Objective Loss 2.636234                                        LR 0.000063    Time 0.088370    
2024-02-17 13:35:23,610 - Epoch: [227][  500/  500]    Overall Loss 2.637061    Objective Loss 2.637061    Top1 30.500000    Top5 61.000000    LR 0.000063    Time 0.088262    
2024-02-17 13:35:23,717 - --- validate (epoch=227)-----------
2024-02-17 13:35:23,718 - 10000 samples (100 per mini-batch)
2024-02-17 13:35:28,980 - Epoch: [227][  100/  100]    Loss 2.601317    Top1 34.130000    Top5 64.990000    
2024-02-17 13:35:29,147 - ==> Top1: 34.130    Top5: 64.990    Loss: 2.601

2024-02-17 13:35:29,157 - ==> Best [Top1: 34.130   Top5: 64.990   Sparsity:0.00   Params: 753952 on epoch: 227]
2024-02-17 13:35:29,158 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:35:29,225 - 

2024-02-17 13:35:29,225 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:35:38,993 - Epoch: [228][  100/  500]    Overall Loss 2.559295    Objective Loss 2.559295                                        LR 0.000063    Time 0.097625    
2024-02-17 13:35:47,625 - Epoch: [228][  200/  500]    Overall Loss 2.566209    Objective Loss 2.566209                                        LR 0.000063    Time 0.091956    
2024-02-17 13:35:56,241 - Epoch: [228][  300/  500]    Overall Loss 2.580945    Objective Loss 2.580945                                        LR 0.000063    Time 0.090013    
2024-02-17 13:36:04,846 - Epoch: [228][  400/  500]    Overall Loss 2.592973    Objective Loss 2.592973                                        LR 0.000063    Time 0.089013    
2024-02-17 13:36:13,620 - Epoch: [228][  500/  500]    Overall Loss 2.596183    Objective Loss 2.596183    Top1 32.500000    Top5 66.500000    LR 0.000063    Time 0.088752    
2024-02-17 13:36:13,744 - --- validate (epoch=228)-----------
2024-02-17 13:36:13,744 - 10000 samples (100 per mini-batch)
2024-02-17 13:36:18,983 - Epoch: [228][  100/  100]    Loss 2.557874    Top1 34.350000    Top5 66.180000    
2024-02-17 13:36:19,148 - ==> Top1: 34.350    Top5: 66.180    Loss: 2.558

2024-02-17 13:36:19,157 - ==> Best [Top1: 34.350   Top5: 66.180   Sparsity:0.00   Params: 753952 on epoch: 228]
2024-02-17 13:36:19,158 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:36:19,225 - 

2024-02-17 13:36:19,225 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:36:28,426 - Epoch: [229][  100/  500]    Overall Loss 2.596295    Objective Loss 2.596295                                        LR 0.000063    Time 0.091960    
2024-02-17 13:36:37,072 - Epoch: [229][  200/  500]    Overall Loss 2.569832    Objective Loss 2.569832                                        LR 0.000063    Time 0.089191    
2024-02-17 13:36:45,768 - Epoch: [229][  300/  500]    Overall Loss 2.562545    Objective Loss 2.562545                                        LR 0.000063    Time 0.088435    
2024-02-17 13:36:54,111 - Epoch: [229][  400/  500]    Overall Loss 2.572572    Objective Loss 2.572572                                        LR 0.000063    Time 0.087175    
2024-02-17 13:37:02,799 - Epoch: [229][  500/  500]    Overall Loss 2.579789    Objective Loss 2.579789    Top1 28.500000    Top5 62.000000    LR 0.000063    Time 0.087109    
2024-02-17 13:37:02,901 - --- validate (epoch=229)-----------
2024-02-17 13:37:02,901 - 10000 samples (100 per mini-batch)
2024-02-17 13:37:08,455 - Epoch: [229][  100/  100]    Loss 2.578526    Top1 34.160000    Top5 65.360000    
2024-02-17 13:37:08,561 - ==> Top1: 34.160    Top5: 65.360    Loss: 2.579

2024-02-17 13:37:08,572 - ==> Best [Top1: 34.350   Top5: 66.180   Sparsity:0.00   Params: 753952 on epoch: 228]
2024-02-17 13:37:08,573 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:37:08,624 - 

2024-02-17 13:37:08,624 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:37:17,741 - Epoch: [230][  100/  500]    Overall Loss 2.607138    Objective Loss 2.607138                                        LR 0.000063    Time 0.091119    
2024-02-17 13:37:26,387 - Epoch: [230][  200/  500]    Overall Loss 2.598622    Objective Loss 2.598622                                        LR 0.000063    Time 0.088772    
2024-02-17 13:37:34,839 - Epoch: [230][  300/  500]    Overall Loss 2.602235    Objective Loss 2.602235                                        LR 0.000063    Time 0.087343    
2024-02-17 13:37:43,431 - Epoch: [230][  400/  500]    Overall Loss 2.601095    Objective Loss 2.601095                                        LR 0.000063    Time 0.086980    
2024-02-17 13:37:52,128 - Epoch: [230][  500/  500]    Overall Loss 2.603389    Objective Loss 2.603389    Top1 32.500000    Top5 64.000000    LR 0.000063    Time 0.086970    
2024-02-17 13:37:52,295 - --- validate (epoch=230)-----------
2024-02-17 13:37:52,297 - 10000 samples (100 per mini-batch)
2024-02-17 13:37:57,714 - Epoch: [230][  100/  100]    Loss 2.550128    Top1 34.280000    Top5 66.160000    
2024-02-17 13:37:57,813 - ==> Top1: 34.280    Top5: 66.160    Loss: 2.550

2024-02-17 13:37:57,824 - ==> Best [Top1: 34.350   Top5: 66.180   Sparsity:0.00   Params: 753952 on epoch: 228]
2024-02-17 13:37:57,824 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:37:57,876 - 

2024-02-17 13:37:57,876 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:38:07,073 - Epoch: [231][  100/  500]    Overall Loss 2.600825    Objective Loss 2.600825                                        LR 0.000063    Time 0.091924    
2024-02-17 13:38:16,000 - Epoch: [231][  200/  500]    Overall Loss 2.594801    Objective Loss 2.594801                                        LR 0.000063    Time 0.090579    
2024-02-17 13:38:25,023 - Epoch: [231][  300/  500]    Overall Loss 2.589484    Objective Loss 2.589484                                        LR 0.000063    Time 0.090450    
2024-02-17 13:38:33,373 - Epoch: [231][  400/  500]    Overall Loss 2.579918    Objective Loss 2.579918                                        LR 0.000063    Time 0.088704    
2024-02-17 13:38:41,815 - Epoch: [231][  500/  500]    Overall Loss 2.571555    Objective Loss 2.571555    Top1 40.500000    Top5 67.000000    LR 0.000063    Time 0.087840    
2024-02-17 13:38:41,980 - --- validate (epoch=231)-----------
2024-02-17 13:38:41,981 - 10000 samples (100 per mini-batch)
2024-02-17 13:38:47,423 - Epoch: [231][  100/  100]    Loss 2.474818    Top1 36.320000    Top5 67.690000    
2024-02-17 13:38:47,571 - ==> Top1: 36.320    Top5: 67.690    Loss: 2.475

2024-02-17 13:38:47,576 - ==> Best [Top1: 36.320   Top5: 67.690   Sparsity:0.00   Params: 753952 on epoch: 231]
2024-02-17 13:38:47,577 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:38:47,644 - 

2024-02-17 13:38:47,644 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:38:57,131 - Epoch: [232][  100/  500]    Overall Loss 2.557359    Objective Loss 2.557359                                        LR 0.000063    Time 0.094810    
2024-02-17 13:39:05,889 - Epoch: [232][  200/  500]    Overall Loss 2.545042    Objective Loss 2.545042                                        LR 0.000063    Time 0.091178    
2024-02-17 13:39:14,497 - Epoch: [232][  300/  500]    Overall Loss 2.538489    Objective Loss 2.538489                                        LR 0.000063    Time 0.089467    
2024-02-17 13:39:23,347 - Epoch: [232][  400/  500]    Overall Loss 2.549675    Objective Loss 2.549675                                        LR 0.000063    Time 0.089218    
2024-02-17 13:39:32,012 - Epoch: [232][  500/  500]    Overall Loss 2.552214    Objective Loss 2.552214    Top1 29.500000    Top5 62.000000    LR 0.000063    Time 0.088696    
2024-02-17 13:39:32,121 - --- validate (epoch=232)-----------
2024-02-17 13:39:32,122 - 10000 samples (100 per mini-batch)
2024-02-17 13:39:38,171 - Epoch: [232][  100/  100]    Loss 2.492502    Top1 35.650000    Top5 67.530000    
2024-02-17 13:39:38,355 - ==> Top1: 35.650    Top5: 67.530    Loss: 2.493

2024-02-17 13:39:38,366 - ==> Best [Top1: 36.320   Top5: 67.690   Sparsity:0.00   Params: 753952 on epoch: 231]
2024-02-17 13:39:38,367 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:39:38,418 - 

2024-02-17 13:39:38,418 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:39:47,495 - Epoch: [233][  100/  500]    Overall Loss 2.548686    Objective Loss 2.548686                                        LR 0.000063    Time 0.090721    
2024-02-17 13:39:56,013 - Epoch: [233][  200/  500]    Overall Loss 2.542747    Objective Loss 2.542747                                        LR 0.000063    Time 0.087935    
2024-02-17 13:40:04,509 - Epoch: [233][  300/  500]    Overall Loss 2.540472    Objective Loss 2.540472                                        LR 0.000063    Time 0.086932    
2024-02-17 13:40:13,048 - Epoch: [233][  400/  500]    Overall Loss 2.532915    Objective Loss 2.532915                                        LR 0.000063    Time 0.086536    
2024-02-17 13:40:21,813 - Epoch: [233][  500/  500]    Overall Loss 2.532188    Objective Loss 2.532188    Top1 30.000000    Top5 67.000000    LR 0.000063    Time 0.086753    
2024-02-17 13:40:21,941 - --- validate (epoch=233)-----------
2024-02-17 13:40:21,942 - 10000 samples (100 per mini-batch)
2024-02-17 13:40:27,501 - Epoch: [233][  100/  100]    Loss 2.459256    Top1 36.030000    Top5 68.420000    
2024-02-17 13:40:27,635 - ==> Top1: 36.030    Top5: 68.420    Loss: 2.459

2024-02-17 13:40:27,646 - ==> Best [Top1: 36.320   Top5: 67.690   Sparsity:0.00   Params: 753952 on epoch: 231]
2024-02-17 13:40:27,647 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:40:27,701 - 

2024-02-17 13:40:27,702 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:40:37,142 - Epoch: [234][  100/  500]    Overall Loss 2.515917    Objective Loss 2.515917                                        LR 0.000063    Time 0.094329    
2024-02-17 13:40:45,943 - Epoch: [234][  200/  500]    Overall Loss 2.508522    Objective Loss 2.508522                                        LR 0.000063    Time 0.091153    
2024-02-17 13:40:54,717 - Epoch: [234][  300/  500]    Overall Loss 2.497913    Objective Loss 2.497913                                        LR 0.000063    Time 0.090001    
2024-02-17 13:41:03,389 - Epoch: [234][  400/  500]    Overall Loss 2.503450    Objective Loss 2.503450                                        LR 0.000063    Time 0.089174    
2024-02-17 13:41:11,923 - Epoch: [234][  500/  500]    Overall Loss 2.499891    Objective Loss 2.499891    Top1 36.000000    Top5 68.500000    LR 0.000063    Time 0.088400    
2024-02-17 13:41:12,027 - --- validate (epoch=234)-----------
2024-02-17 13:41:12,028 - 10000 samples (100 per mini-batch)
2024-02-17 13:41:17,711 - Epoch: [234][  100/  100]    Loss 2.522018    Top1 34.830000    Top5 67.540000    
2024-02-17 13:41:17,869 - ==> Top1: 34.830    Top5: 67.540    Loss: 2.522

2024-02-17 13:41:17,877 - ==> Best [Top1: 36.320   Top5: 67.690   Sparsity:0.00   Params: 753952 on epoch: 231]
2024-02-17 13:41:17,877 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:41:17,935 - 

2024-02-17 13:41:17,935 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:41:27,120 - Epoch: [235][  100/  500]    Overall Loss 2.447158    Objective Loss 2.447158                                        LR 0.000063    Time 0.091790    
2024-02-17 13:41:35,989 - Epoch: [235][  200/  500]    Overall Loss 2.469948    Objective Loss 2.469948                                        LR 0.000063    Time 0.090219    
2024-02-17 13:41:45,032 - Epoch: [235][  300/  500]    Overall Loss 2.472091    Objective Loss 2.472091                                        LR 0.000063    Time 0.090278    
2024-02-17 13:41:54,083 - Epoch: [235][  400/  500]    Overall Loss 2.480523    Objective Loss 2.480523                                        LR 0.000063    Time 0.090326    
2024-02-17 13:42:02,902 - Epoch: [235][  500/  500]    Overall Loss 2.471618    Objective Loss 2.471618    Top1 35.000000    Top5 67.500000    LR 0.000063    Time 0.089891    
2024-02-17 13:42:03,039 - --- validate (epoch=235)-----------
2024-02-17 13:42:03,040 - 10000 samples (100 per mini-batch)
2024-02-17 13:42:07,881 - Epoch: [235][  100/  100]    Loss 2.518049    Top1 35.540000    Top5 67.060000    
2024-02-17 13:42:07,984 - ==> Top1: 35.540    Top5: 67.060    Loss: 2.518

2024-02-17 13:42:07,993 - ==> Best [Top1: 36.320   Top5: 67.690   Sparsity:0.00   Params: 753952 on epoch: 231]
2024-02-17 13:42:07,993 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:42:08,038 - 

2024-02-17 13:42:08,039 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:42:17,526 - Epoch: [236][  100/  500]    Overall Loss 2.476405    Objective Loss 2.476405                                        LR 0.000063    Time 0.094807    
2024-02-17 13:42:26,416 - Epoch: [236][  200/  500]    Overall Loss 2.472039    Objective Loss 2.472039                                        LR 0.000063    Time 0.091831    
2024-02-17 13:42:35,149 - Epoch: [236][  300/  500]    Overall Loss 2.457736    Objective Loss 2.457736                                        LR 0.000063    Time 0.090319    
2024-02-17 13:42:43,813 - Epoch: [236][  400/  500]    Overall Loss 2.459223    Objective Loss 2.459223                                        LR 0.000063    Time 0.089391    
2024-02-17 13:42:52,499 - Epoch: [236][  500/  500]    Overall Loss 2.459777    Objective Loss 2.459777    Top1 36.000000    Top5 70.500000    LR 0.000063    Time 0.088878    
2024-02-17 13:42:52,668 - --- validate (epoch=236)-----------
2024-02-17 13:42:52,669 - 10000 samples (100 per mini-batch)
2024-02-17 13:42:58,220 - Epoch: [236][  100/  100]    Loss 2.422213    Top1 36.920000    Top5 69.030000    
2024-02-17 13:42:58,319 - ==> Top1: 36.920    Top5: 69.030    Loss: 2.422

2024-02-17 13:42:58,330 - ==> Best [Top1: 36.920   Top5: 69.030   Sparsity:0.00   Params: 753952 on epoch: 236]
2024-02-17 13:42:58,331 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:42:58,397 - 

2024-02-17 13:42:58,397 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:43:07,574 - Epoch: [237][  100/  500]    Overall Loss 2.419037    Objective Loss 2.419037                                        LR 0.000063    Time 0.091717    
2024-02-17 13:43:16,427 - Epoch: [237][  200/  500]    Overall Loss 2.431138    Objective Loss 2.431138                                        LR 0.000063    Time 0.090104    
2024-02-17 13:43:25,178 - Epoch: [237][  300/  500]    Overall Loss 2.431429    Objective Loss 2.431429                                        LR 0.000063    Time 0.089227    
2024-02-17 13:43:33,811 - Epoch: [237][  400/  500]    Overall Loss 2.426584    Objective Loss 2.426584                                        LR 0.000063    Time 0.088495    
2024-02-17 13:43:42,451 - Epoch: [237][  500/  500]    Overall Loss 2.421631    Objective Loss 2.421631    Top1 37.000000    Top5 69.000000    LR 0.000063    Time 0.088068    
2024-02-17 13:43:42,593 - --- validate (epoch=237)-----------
2024-02-17 13:43:42,593 - 10000 samples (100 per mini-batch)
2024-02-17 13:43:48,043 - Epoch: [237][  100/  100]    Loss 2.397421    Top1 37.840000    Top5 69.690000    
2024-02-17 13:43:48,182 - ==> Top1: 37.840    Top5: 69.690    Loss: 2.397

2024-02-17 13:43:48,188 - ==> Best [Top1: 37.840   Top5: 69.690   Sparsity:0.00   Params: 753952 on epoch: 237]
2024-02-17 13:43:48,188 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:43:48,247 - 

2024-02-17 13:43:48,247 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:43:58,000 - Epoch: [238][  100/  500]    Overall Loss 2.367696    Objective Loss 2.367696                                        LR 0.000063    Time 0.097482    
2024-02-17 13:44:06,645 - Epoch: [238][  200/  500]    Overall Loss 2.379557    Objective Loss 2.379557                                        LR 0.000063    Time 0.091945    
2024-02-17 13:44:15,436 - Epoch: [238][  300/  500]    Overall Loss 2.392677    Objective Loss 2.392677                                        LR 0.000063    Time 0.090588    
2024-02-17 13:44:24,113 - Epoch: [238][  400/  500]    Overall Loss 2.408943    Objective Loss 2.408943                                        LR 0.000063    Time 0.089623    
2024-02-17 13:44:32,862 - Epoch: [238][  500/  500]    Overall Loss 2.408731    Objective Loss 2.408731    Top1 39.500000    Top5 67.000000    LR 0.000063    Time 0.089189    
2024-02-17 13:44:33,035 - --- validate (epoch=238)-----------
2024-02-17 13:44:33,036 - 10000 samples (100 per mini-batch)
2024-02-17 13:44:38,530 - Epoch: [238][  100/  100]    Loss 2.499392    Top1 36.670000    Top5 67.600000    
2024-02-17 13:44:38,658 - ==> Top1: 36.670    Top5: 67.600    Loss: 2.499

2024-02-17 13:44:38,668 - ==> Best [Top1: 37.840   Top5: 69.690   Sparsity:0.00   Params: 753952 on epoch: 237]
2024-02-17 13:44:38,668 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:44:38,721 - 

2024-02-17 13:44:38,721 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:44:47,915 - Epoch: [239][  100/  500]    Overall Loss 2.401597    Objective Loss 2.401597                                        LR 0.000063    Time 0.091871    
2024-02-17 13:44:56,832 - Epoch: [239][  200/  500]    Overall Loss 2.407482    Objective Loss 2.407482                                        LR 0.000063    Time 0.090498    
2024-02-17 13:45:05,544 - Epoch: [239][  300/  500]    Overall Loss 2.399262    Objective Loss 2.399262                                        LR 0.000063    Time 0.089362    
2024-02-17 13:45:14,087 - Epoch: [239][  400/  500]    Overall Loss 2.396563    Objective Loss 2.396563                                        LR 0.000063    Time 0.088368    
2024-02-17 13:45:22,822 - Epoch: [239][  500/  500]    Overall Loss 2.395257    Objective Loss 2.395257    Top1 33.000000    Top5 69.000000    LR 0.000063    Time 0.088158    
2024-02-17 13:45:22,933 - --- validate (epoch=239)-----------
2024-02-17 13:45:22,934 - 10000 samples (100 per mini-batch)
2024-02-17 13:45:28,109 - Epoch: [239][  100/  100]    Loss 2.421040    Top1 36.910000    Top5 68.520000    
2024-02-17 13:45:28,205 - ==> Top1: 36.910    Top5: 68.520    Loss: 2.421

2024-02-17 13:45:28,209 - ==> Best [Top1: 37.840   Top5: 69.690   Sparsity:0.00   Params: 753952 on epoch: 237]
2024-02-17 13:45:28,210 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:45:28,254 - 

2024-02-17 13:45:28,254 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:45:36,717 - Epoch: [240][  100/  500]    Overall Loss 2.374790    Objective Loss 2.374790                                        LR 0.000063    Time 0.084589    
2024-02-17 13:45:45,453 - Epoch: [240][  200/  500]    Overall Loss 2.381319    Objective Loss 2.381319                                        LR 0.000063    Time 0.085960    
2024-02-17 13:45:54,217 - Epoch: [240][  300/  500]    Overall Loss 2.394164    Objective Loss 2.394164                                        LR 0.000063    Time 0.086506    
2024-02-17 13:46:03,052 - Epoch: [240][  400/  500]    Overall Loss 2.377730    Objective Loss 2.377730                                        LR 0.000063    Time 0.086957    
2024-02-17 13:46:11,513 - Epoch: [240][  500/  500]    Overall Loss 2.371153    Objective Loss 2.371153    Top1 39.000000    Top5 71.000000    LR 0.000063    Time 0.086482    
2024-02-17 13:46:11,662 - --- validate (epoch=240)-----------
2024-02-17 13:46:11,663 - 10000 samples (100 per mini-batch)
2024-02-17 13:46:16,992 - Epoch: [240][  100/  100]    Loss 2.397709    Top1 38.080000    Top5 69.180000    
2024-02-17 13:46:17,097 - ==> Top1: 38.080    Top5: 69.180    Loss: 2.398

2024-02-17 13:46:17,103 - ==> Best [Top1: 38.080   Top5: 69.180   Sparsity:0.00   Params: 753952 on epoch: 240]
2024-02-17 13:46:17,103 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:46:17,170 - 

2024-02-17 13:46:17,171 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:46:26,589 - Epoch: [241][  100/  500]    Overall Loss 2.361147    Objective Loss 2.361147                                        LR 0.000063    Time 0.094136    
2024-02-17 13:46:35,670 - Epoch: [241][  200/  500]    Overall Loss 2.360164    Objective Loss 2.360164                                        LR 0.000063    Time 0.092450    
2024-02-17 13:46:44,429 - Epoch: [241][  300/  500]    Overall Loss 2.370113    Objective Loss 2.370113                                        LR 0.000063    Time 0.090819    
2024-02-17 13:46:53,052 - Epoch: [241][  400/  500]    Overall Loss 2.367796    Objective Loss 2.367796                                        LR 0.000063    Time 0.089662    
2024-02-17 13:47:01,353 - Epoch: [241][  500/  500]    Overall Loss 2.360716    Objective Loss 2.360716    Top1 38.000000    Top5 68.000000    LR 0.000063    Time 0.088325    
2024-02-17 13:47:01,490 - --- validate (epoch=241)-----------
2024-02-17 13:47:01,491 - 10000 samples (100 per mini-batch)
2024-02-17 13:47:07,206 - Epoch: [241][  100/  100]    Loss 2.450356    Top1 37.130000    Top5 68.900000    
2024-02-17 13:47:07,305 - ==> Top1: 37.130    Top5: 68.900    Loss: 2.450

2024-02-17 13:47:07,313 - ==> Best [Top1: 38.080   Top5: 69.180   Sparsity:0.00   Params: 753952 on epoch: 240]
2024-02-17 13:47:07,313 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:47:07,361 - 

2024-02-17 13:47:07,361 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:47:16,656 - Epoch: [242][  100/  500]    Overall Loss 2.353883    Objective Loss 2.353883                                        LR 0.000063    Time 0.092904    
2024-02-17 13:47:25,349 - Epoch: [242][  200/  500]    Overall Loss 2.349539    Objective Loss 2.349539                                        LR 0.000063    Time 0.089893    
2024-02-17 13:47:34,118 - Epoch: [242][  300/  500]    Overall Loss 2.338220    Objective Loss 2.338220                                        LR 0.000063    Time 0.089146    
2024-02-17 13:47:43,306 - Epoch: [242][  400/  500]    Overall Loss 2.350491    Objective Loss 2.350491                                        LR 0.000063    Time 0.089821    
2024-02-17 13:47:52,449 - Epoch: [242][  500/  500]    Overall Loss 2.351637    Objective Loss 2.351637    Top1 43.500000    Top5 72.000000    LR 0.000063    Time 0.090134    
2024-02-17 13:47:52,607 - --- validate (epoch=242)-----------
2024-02-17 13:47:52,607 - 10000 samples (100 per mini-batch)
2024-02-17 13:47:58,279 - Epoch: [242][  100/  100]    Loss 2.417374    Top1 37.550000    Top5 69.260000    
2024-02-17 13:47:58,401 - ==> Top1: 37.550    Top5: 69.260    Loss: 2.417

2024-02-17 13:47:58,410 - ==> Best [Top1: 38.080   Top5: 69.180   Sparsity:0.00   Params: 753952 on epoch: 240]
2024-02-17 13:47:58,411 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:47:58,464 - 

2024-02-17 13:47:58,464 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:48:07,950 - Epoch: [243][  100/  500]    Overall Loss 2.325546    Objective Loss 2.325546                                        LR 0.000063    Time 0.094812    
2024-02-17 13:48:16,857 - Epoch: [243][  200/  500]    Overall Loss 2.321882    Objective Loss 2.321882                                        LR 0.000063    Time 0.091920    
2024-02-17 13:48:25,733 - Epoch: [243][  300/  500]    Overall Loss 2.317373    Objective Loss 2.317373                                        LR 0.000063    Time 0.090855    
2024-02-17 13:48:34,624 - Epoch: [243][  400/  500]    Overall Loss 2.323195    Objective Loss 2.323195                                        LR 0.000063    Time 0.090359    
2024-02-17 13:48:43,821 - Epoch: [243][  500/  500]    Overall Loss 2.323330    Objective Loss 2.323330    Top1 36.500000    Top5 67.000000    LR 0.000063    Time 0.090673    
2024-02-17 13:48:43,989 - --- validate (epoch=243)-----------
2024-02-17 13:48:43,990 - 10000 samples (100 per mini-batch)
2024-02-17 13:48:49,646 - Epoch: [243][  100/  100]    Loss 2.365726    Top1 38.820000    Top5 70.170000    
2024-02-17 13:48:49,794 - ==> Top1: 38.820    Top5: 70.170    Loss: 2.366

2024-02-17 13:48:49,804 - ==> Best [Top1: 38.820   Top5: 70.170   Sparsity:0.00   Params: 753952 on epoch: 243]
2024-02-17 13:48:49,805 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:48:49,869 - 

2024-02-17 13:48:49,870 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:48:59,757 - Epoch: [244][  100/  500]    Overall Loss 2.287903    Objective Loss 2.287903                                        LR 0.000063    Time 0.098816    
2024-02-17 13:49:08,287 - Epoch: [244][  200/  500]    Overall Loss 2.299662    Objective Loss 2.299662                                        LR 0.000063    Time 0.092041    
2024-02-17 13:49:16,964 - Epoch: [244][  300/  500]    Overall Loss 2.294865    Objective Loss 2.294865                                        LR 0.000063    Time 0.090273    
2024-02-17 13:49:25,631 - Epoch: [244][  400/  500]    Overall Loss 2.287316    Objective Loss 2.287316                                        LR 0.000063    Time 0.089363    
2024-02-17 13:49:34,505 - Epoch: [244][  500/  500]    Overall Loss 2.284600    Objective Loss 2.284600    Top1 40.000000    Top5 71.500000    LR 0.000063    Time 0.089230    
2024-02-17 13:49:34,633 - --- validate (epoch=244)-----------
2024-02-17 13:49:34,635 - 10000 samples (100 per mini-batch)
2024-02-17 13:49:39,952 - Epoch: [244][  100/  100]    Loss 2.315674    Top1 39.490000    Top5 71.070000    
2024-02-17 13:49:40,055 - ==> Top1: 39.490    Top5: 71.070    Loss: 2.316

2024-02-17 13:49:40,066 - ==> Best [Top1: 39.490   Top5: 71.070   Sparsity:0.00   Params: 753952 on epoch: 244]
2024-02-17 13:49:40,067 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:49:40,133 - 

2024-02-17 13:49:40,133 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:49:49,481 - Epoch: [245][  100/  500]    Overall Loss 2.309362    Objective Loss 2.309362                                        LR 0.000063    Time 0.093422    
2024-02-17 13:49:58,311 - Epoch: [245][  200/  500]    Overall Loss 2.303912    Objective Loss 2.303912                                        LR 0.000063    Time 0.090838    
2024-02-17 13:50:07,129 - Epoch: [245][  300/  500]    Overall Loss 2.298763    Objective Loss 2.298763                                        LR 0.000063    Time 0.089941    
2024-02-17 13:50:15,866 - Epoch: [245][  400/  500]    Overall Loss 2.294253    Objective Loss 2.294253                                        LR 0.000063    Time 0.089287    
2024-02-17 13:50:24,662 - Epoch: [245][  500/  500]    Overall Loss 2.290268    Objective Loss 2.290268    Top1 41.000000    Top5 75.500000    LR 0.000063    Time 0.089015    
2024-02-17 13:50:24,799 - --- validate (epoch=245)-----------
2024-02-17 13:50:24,800 - 10000 samples (100 per mini-batch)
2024-02-17 13:50:30,243 - Epoch: [245][  100/  100]    Loss 2.297431    Top1 39.820000    Top5 71.390000    
2024-02-17 13:50:30,359 - ==> Top1: 39.820    Top5: 71.390    Loss: 2.297

2024-02-17 13:50:30,368 - ==> Best [Top1: 39.820   Top5: 71.390   Sparsity:0.00   Params: 753952 on epoch: 245]
2024-02-17 13:50:30,368 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:50:30,428 - 

2024-02-17 13:50:30,428 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:50:40,159 - Epoch: [246][  100/  500]    Overall Loss 2.254400    Objective Loss 2.254400                                        LR 0.000063    Time 0.097259    
2024-02-17 13:50:48,948 - Epoch: [246][  200/  500]    Overall Loss 2.270139    Objective Loss 2.270139                                        LR 0.000063    Time 0.092552    
2024-02-17 13:50:57,818 - Epoch: [246][  300/  500]    Overall Loss 2.261126    Objective Loss 2.261126                                        LR 0.000063    Time 0.091256    
2024-02-17 13:51:06,581 - Epoch: [246][  400/  500]    Overall Loss 2.258625    Objective Loss 2.258625                                        LR 0.000063    Time 0.090340    
2024-02-17 13:51:15,167 - Epoch: [246][  500/  500]    Overall Loss 2.251746    Objective Loss 2.251746    Top1 45.500000    Top5 73.500000    LR 0.000063    Time 0.089437    
2024-02-17 13:51:15,331 - --- validate (epoch=246)-----------
2024-02-17 13:51:15,332 - 10000 samples (100 per mini-batch)
2024-02-17 13:51:21,033 - Epoch: [246][  100/  100]    Loss 2.346335    Top1 39.190000    Top5 70.970000    
2024-02-17 13:51:21,131 - ==> Top1: 39.190    Top5: 70.970    Loss: 2.346

2024-02-17 13:51:21,140 - ==> Best [Top1: 39.820   Top5: 71.390   Sparsity:0.00   Params: 753952 on epoch: 245]
2024-02-17 13:51:21,141 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:51:21,193 - 

2024-02-17 13:51:21,194 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:51:30,267 - Epoch: [247][  100/  500]    Overall Loss 2.246758    Objective Loss 2.246758                                        LR 0.000063    Time 0.090681    
2024-02-17 13:51:39,143 - Epoch: [247][  200/  500]    Overall Loss 2.236609    Objective Loss 2.236609                                        LR 0.000063    Time 0.089702    
2024-02-17 13:51:47,681 - Epoch: [247][  300/  500]    Overall Loss 2.252938    Objective Loss 2.252938                                        LR 0.000063    Time 0.088252    
2024-02-17 13:51:56,037 - Epoch: [247][  400/  500]    Overall Loss 2.259002    Objective Loss 2.259002                                        LR 0.000063    Time 0.087071    
2024-02-17 13:52:04,346 - Epoch: [247][  500/  500]    Overall Loss 2.269183    Objective Loss 2.269183    Top1 37.000000    Top5 73.000000    LR 0.000063    Time 0.086270    
2024-02-17 13:52:04,523 - --- validate (epoch=247)-----------
2024-02-17 13:52:04,523 - 10000 samples (100 per mini-batch)
2024-02-17 13:52:09,810 - Epoch: [247][  100/  100]    Loss 2.301812    Top1 40.170000    Top5 71.610000    
2024-02-17 13:52:09,999 - ==> Top1: 40.170    Top5: 71.610    Loss: 2.302

2024-02-17 13:52:10,009 - ==> Best [Top1: 40.170   Top5: 71.610   Sparsity:0.00   Params: 753952 on epoch: 247]
2024-02-17 13:52:10,009 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:52:10,076 - 

2024-02-17 13:52:10,077 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:52:19,198 - Epoch: [248][  100/  500]    Overall Loss 2.256521    Objective Loss 2.256521                                        LR 0.000063    Time 0.091167    
2024-02-17 13:52:27,928 - Epoch: [248][  200/  500]    Overall Loss 2.260415    Objective Loss 2.260415                                        LR 0.000063    Time 0.089215    
2024-02-17 13:52:36,703 - Epoch: [248][  300/  500]    Overall Loss 2.263146    Objective Loss 2.263146                                        LR 0.000063    Time 0.088714    
2024-02-17 13:52:45,500 - Epoch: [248][  400/  500]    Overall Loss 2.265552    Objective Loss 2.265552                                        LR 0.000063    Time 0.088519    
2024-02-17 13:52:54,158 - Epoch: [248][  500/  500]    Overall Loss 2.271593    Objective Loss 2.271593    Top1 33.500000    Top5 70.000000    LR 0.000063    Time 0.088124    
2024-02-17 13:52:54,250 - --- validate (epoch=248)-----------
2024-02-17 13:52:54,250 - 10000 samples (100 per mini-batch)
2024-02-17 13:52:59,282 - Epoch: [248][  100/  100]    Loss 2.322254    Top1 39.160000    Top5 71.050000    
2024-02-17 13:52:59,378 - ==> Top1: 39.160    Top5: 71.050    Loss: 2.322

2024-02-17 13:52:59,387 - ==> Best [Top1: 40.170   Top5: 71.610   Sparsity:0.00   Params: 753952 on epoch: 247]
2024-02-17 13:52:59,387 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:52:59,431 - 

2024-02-17 13:52:59,431 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:53:07,677 - Epoch: [249][  100/  500]    Overall Loss 2.248873    Objective Loss 2.248873                                        LR 0.000063    Time 0.082430    
2024-02-17 13:53:16,481 - Epoch: [249][  200/  500]    Overall Loss 2.264794    Objective Loss 2.264794                                        LR 0.000063    Time 0.085213    
2024-02-17 13:53:25,090 - Epoch: [249][  300/  500]    Overall Loss 2.256314    Objective Loss 2.256314                                        LR 0.000063    Time 0.085495    
2024-02-17 13:53:33,900 - Epoch: [249][  400/  500]    Overall Loss 2.260539    Objective Loss 2.260539                                        LR 0.000063    Time 0.086137    
2024-02-17 13:53:42,694 - Epoch: [249][  500/  500]    Overall Loss 2.263734    Objective Loss 2.263734    Top1 41.000000    Top5 75.500000    LR 0.000063    Time 0.086489    
2024-02-17 13:53:42,832 - --- validate (epoch=249)-----------
2024-02-17 13:53:42,832 - 10000 samples (100 per mini-batch)
2024-02-17 13:53:48,370 - Epoch: [249][  100/  100]    Loss 2.226227    Top1 41.140000    Top5 73.040000    
2024-02-17 13:53:48,477 - ==> Top1: 41.140    Top5: 73.040    Loss: 2.226

2024-02-17 13:53:48,483 - ==> Best [Top1: 41.140   Top5: 73.040   Sparsity:0.00   Params: 753952 on epoch: 249]
2024-02-17 13:53:48,483 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:53:48,544 - 

2024-02-17 13:53:48,545 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:53:57,957 - Epoch: [250][  100/  500]    Overall Loss 2.233623    Objective Loss 2.233623                                        LR 0.000063    Time 0.094071    
2024-02-17 13:54:06,573 - Epoch: [250][  200/  500]    Overall Loss 2.249098    Objective Loss 2.249098                                        LR 0.000063    Time 0.090100    
2024-02-17 13:54:15,436 - Epoch: [250][  300/  500]    Overall Loss 2.236392    Objective Loss 2.236392                                        LR 0.000063    Time 0.089595    
2024-02-17 13:54:24,180 - Epoch: [250][  400/  500]    Overall Loss 2.234276    Objective Loss 2.234276                                        LR 0.000063    Time 0.089049    
2024-02-17 13:54:33,339 - Epoch: [250][  500/  500]    Overall Loss 2.234496    Objective Loss 2.234496    Top1 35.000000    Top5 73.500000    LR 0.000063    Time 0.089548    
2024-02-17 13:54:33,456 - --- validate (epoch=250)-----------
2024-02-17 13:54:33,457 - 10000 samples (100 per mini-batch)
2024-02-17 13:54:38,734 - Epoch: [250][  100/  100]    Loss 2.221465    Top1 41.560000    Top5 73.400000    
2024-02-17 13:54:38,833 - ==> Top1: 41.560    Top5: 73.400    Loss: 2.221

2024-02-17 13:54:38,843 - ==> Best [Top1: 41.560   Top5: 73.400   Sparsity:0.00   Params: 753952 on epoch: 250]
2024-02-17 13:54:38,843 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:54:38,903 - 

2024-02-17 13:54:38,904 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:54:48,247 - Epoch: [251][  100/  500]    Overall Loss 2.270556    Objective Loss 2.270556                                        LR 0.000063    Time 0.093353    
2024-02-17 13:54:57,066 - Epoch: [251][  200/  500]    Overall Loss 2.263551    Objective Loss 2.263551                                        LR 0.000063    Time 0.090753    
2024-02-17 13:55:06,146 - Epoch: [251][  300/  500]    Overall Loss 2.259489    Objective Loss 2.259489                                        LR 0.000063    Time 0.090756    
2024-02-17 13:55:14,891 - Epoch: [251][  400/  500]    Overall Loss 2.255256    Objective Loss 2.255256                                        LR 0.000063    Time 0.089919    
2024-02-17 13:55:23,815 - Epoch: [251][  500/  500]    Overall Loss 2.253279    Objective Loss 2.253279    Top1 43.000000    Top5 77.000000    LR 0.000063    Time 0.089775    
2024-02-17 13:55:23,959 - --- validate (epoch=251)-----------
2024-02-17 13:55:23,960 - 10000 samples (100 per mini-batch)
2024-02-17 13:55:29,098 - Epoch: [251][  100/  100]    Loss 2.208035    Top1 42.000000    Top5 73.250000    
2024-02-17 13:55:29,179 - ==> Top1: 42.000    Top5: 73.250    Loss: 2.208

2024-02-17 13:55:29,183 - ==> Best [Top1: 42.000   Top5: 73.250   Sparsity:0.00   Params: 753952 on epoch: 251]
2024-02-17 13:55:29,183 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:55:29,235 - 

2024-02-17 13:55:29,235 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:55:38,088 - Epoch: [252][  100/  500]    Overall Loss 2.197070    Objective Loss 2.197070                                        LR 0.000063    Time 0.088492    
2024-02-17 13:55:46,832 - Epoch: [252][  200/  500]    Overall Loss 2.204098    Objective Loss 2.204098                                        LR 0.000063    Time 0.087949    
2024-02-17 13:55:55,781 - Epoch: [252][  300/  500]    Overall Loss 2.217528    Objective Loss 2.217528                                        LR 0.000063    Time 0.088448    
2024-02-17 13:56:04,567 - Epoch: [252][  400/  500]    Overall Loss 2.209774    Objective Loss 2.209774                                        LR 0.000063    Time 0.088292    
2024-02-17 13:56:13,459 - Epoch: [252][  500/  500]    Overall Loss 2.220253    Objective Loss 2.220253    Top1 44.000000    Top5 73.500000    LR 0.000063    Time 0.088410    
2024-02-17 13:56:13,578 - --- validate (epoch=252)-----------
2024-02-17 13:56:13,578 - 10000 samples (100 per mini-batch)
2024-02-17 13:56:19,204 - Epoch: [252][  100/  100]    Loss 2.211537    Top1 41.660000    Top5 73.220000    
2024-02-17 13:56:19,307 - ==> Top1: 41.660    Top5: 73.220    Loss: 2.212

2024-02-17 13:56:19,318 - ==> Best [Top1: 42.000   Top5: 73.250   Sparsity:0.00   Params: 753952 on epoch: 251]
2024-02-17 13:56:19,319 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:56:19,369 - 

2024-02-17 13:56:19,369 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:56:28,479 - Epoch: [253][  100/  500]    Overall Loss 2.185051    Objective Loss 2.185051                                        LR 0.000063    Time 0.091039    
2024-02-17 13:56:36,961 - Epoch: [253][  200/  500]    Overall Loss 2.196653    Objective Loss 2.196653                                        LR 0.000063    Time 0.087916    
2024-02-17 13:56:45,796 - Epoch: [253][  300/  500]    Overall Loss 2.205903    Objective Loss 2.205903                                        LR 0.000063    Time 0.088046    
2024-02-17 13:56:54,621 - Epoch: [253][  400/  500]    Overall Loss 2.206177    Objective Loss 2.206177                                        LR 0.000063    Time 0.088087    
2024-02-17 13:57:03,203 - Epoch: [253][  500/  500]    Overall Loss 2.202462    Objective Loss 2.202462    Top1 38.500000    Top5 69.500000    LR 0.000063    Time 0.087628    
2024-02-17 13:57:03,324 - --- validate (epoch=253)-----------
2024-02-17 13:57:03,325 - 10000 samples (100 per mini-batch)
2024-02-17 13:57:09,090 - Epoch: [253][  100/  100]    Loss 2.236711    Top1 41.220000    Top5 72.760000    
2024-02-17 13:57:09,198 - ==> Top1: 41.220    Top5: 72.760    Loss: 2.237

2024-02-17 13:57:09,209 - ==> Best [Top1: 42.000   Top5: 73.250   Sparsity:0.00   Params: 753952 on epoch: 251]
2024-02-17 13:57:09,209 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:57:09,262 - 

2024-02-17 13:57:09,262 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:57:18,726 - Epoch: [254][  100/  500]    Overall Loss 2.190460    Objective Loss 2.190460                                        LR 0.000063    Time 0.094586    
2024-02-17 13:57:27,407 - Epoch: [254][  200/  500]    Overall Loss 2.200871    Objective Loss 2.200871                                        LR 0.000063    Time 0.090680    
2024-02-17 13:57:36,184 - Epoch: [254][  300/  500]    Overall Loss 2.193950    Objective Loss 2.193950                                        LR 0.000063    Time 0.089697    
2024-02-17 13:57:45,057 - Epoch: [254][  400/  500]    Overall Loss 2.201229    Objective Loss 2.201229                                        LR 0.000063    Time 0.089445    
2024-02-17 13:57:54,072 - Epoch: [254][  500/  500]    Overall Loss 2.205392    Objective Loss 2.205392    Top1 45.500000    Top5 78.000000    LR 0.000063    Time 0.089579    
2024-02-17 13:57:54,179 - --- validate (epoch=254)-----------
2024-02-17 13:57:54,179 - 10000 samples (100 per mini-batch)
2024-02-17 13:57:59,410 - Epoch: [254][  100/  100]    Loss 2.240257    Top1 41.920000    Top5 72.690000    
2024-02-17 13:57:59,542 - ==> Top1: 41.920    Top5: 72.690    Loss: 2.240

2024-02-17 13:57:59,556 - ==> Best [Top1: 42.000   Top5: 73.250   Sparsity:0.00   Params: 753952 on epoch: 251]
2024-02-17 13:57:59,557 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:57:59,617 - 

2024-02-17 13:57:59,617 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:58:08,972 - Epoch: [255][  100/  500]    Overall Loss 2.230432    Objective Loss 2.230432                                        LR 0.000063    Time 0.093476    
2024-02-17 13:58:17,649 - Epoch: [255][  200/  500]    Overall Loss 2.218818    Objective Loss 2.218818                                        LR 0.000063    Time 0.090101    
2024-02-17 13:58:26,268 - Epoch: [255][  300/  500]    Overall Loss 2.213787    Objective Loss 2.213787                                        LR 0.000063    Time 0.088785    
2024-02-17 13:58:34,773 - Epoch: [255][  400/  500]    Overall Loss 2.208968    Objective Loss 2.208968                                        LR 0.000063    Time 0.087845    
2024-02-17 13:58:43,490 - Epoch: [255][  500/  500]    Overall Loss 2.213801    Objective Loss 2.213801    Top1 42.000000    Top5 77.000000    LR 0.000063    Time 0.087702    
2024-02-17 13:58:43,610 - --- validate (epoch=255)-----------
2024-02-17 13:58:43,611 - 10000 samples (100 per mini-batch)
2024-02-17 13:58:48,721 - Epoch: [255][  100/  100]    Loss 2.176179    Top1 42.140000    Top5 74.290000    
2024-02-17 13:58:48,827 - ==> Top1: 42.140    Top5: 74.290    Loss: 2.176

2024-02-17 13:58:48,837 - ==> Best [Top1: 42.140   Top5: 74.290   Sparsity:0.00   Params: 753952 on epoch: 255]
2024-02-17 13:58:48,838 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:58:48,895 - 

2024-02-17 13:58:48,896 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:58:58,728 - Epoch: [256][  100/  500]    Overall Loss 2.182423    Objective Loss 2.182423                                        LR 0.000063    Time 0.098271    
2024-02-17 13:59:07,439 - Epoch: [256][  200/  500]    Overall Loss 2.196352    Objective Loss 2.196352                                        LR 0.000063    Time 0.092671    
2024-02-17 13:59:16,142 - Epoch: [256][  300/  500]    Overall Loss 2.206843    Objective Loss 2.206843                                        LR 0.000063    Time 0.090777    
2024-02-17 13:59:25,057 - Epoch: [256][  400/  500]    Overall Loss 2.205498    Objective Loss 2.205498                                        LR 0.000063    Time 0.090361    
2024-02-17 13:59:33,888 - Epoch: [256][  500/  500]    Overall Loss 2.203204    Objective Loss 2.203204    Top1 41.500000    Top5 69.000000    LR 0.000063    Time 0.089942    
2024-02-17 13:59:34,027 - --- validate (epoch=256)-----------
2024-02-17 13:59:34,028 - 10000 samples (100 per mini-batch)
2024-02-17 13:59:39,354 - Epoch: [256][  100/  100]    Loss 2.232372    Top1 42.090000    Top5 72.950000    
2024-02-17 13:59:39,448 - ==> Top1: 42.090    Top5: 72.950    Loss: 2.232

2024-02-17 13:59:39,460 - ==> Best [Top1: 42.140   Top5: 74.290   Sparsity:0.00   Params: 753952 on epoch: 255]
2024-02-17 13:59:39,461 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 13:59:39,512 - 

2024-02-17 13:59:39,512 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 13:59:48,509 - Epoch: [257][  100/  500]    Overall Loss 2.174540    Objective Loss 2.174540                                        LR 0.000063    Time 0.089921    
2024-02-17 13:59:57,130 - Epoch: [257][  200/  500]    Overall Loss 2.175278    Objective Loss 2.175278                                        LR 0.000063    Time 0.088049    
2024-02-17 14:00:05,899 - Epoch: [257][  300/  500]    Overall Loss 2.180852    Objective Loss 2.180852                                        LR 0.000063    Time 0.087914    
2024-02-17 14:00:14,563 - Epoch: [257][  400/  500]    Overall Loss 2.190930    Objective Loss 2.190930                                        LR 0.000063    Time 0.087589    
2024-02-17 14:00:23,287 - Epoch: [257][  500/  500]    Overall Loss 2.192346    Objective Loss 2.192346    Top1 45.000000    Top5 76.000000    LR 0.000063    Time 0.087510    
2024-02-17 14:00:23,442 - --- validate (epoch=257)-----------
2024-02-17 14:00:23,443 - 10000 samples (100 per mini-batch)
2024-02-17 14:00:29,228 - Epoch: [257][  100/  100]    Loss 2.178770    Top1 43.060000    Top5 73.710000    
2024-02-17 14:00:29,322 - ==> Top1: 43.060    Top5: 73.710    Loss: 2.179

2024-02-17 14:00:29,329 - ==> Best [Top1: 43.060   Top5: 73.710   Sparsity:0.00   Params: 753952 on epoch: 257]
2024-02-17 14:00:29,330 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:00:29,392 - 

2024-02-17 14:00:29,392 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:00:39,141 - Epoch: [258][  100/  500]    Overall Loss 2.210727    Objective Loss 2.210727                                        LR 0.000063    Time 0.097447    
2024-02-17 14:00:48,090 - Epoch: [258][  200/  500]    Overall Loss 2.184806    Objective Loss 2.184806                                        LR 0.000063    Time 0.093450    
2024-02-17 14:00:56,887 - Epoch: [258][  300/  500]    Overall Loss 2.179673    Objective Loss 2.179673                                        LR 0.000063    Time 0.091608    
2024-02-17 14:01:05,674 - Epoch: [258][  400/  500]    Overall Loss 2.182009    Objective Loss 2.182009                                        LR 0.000063    Time 0.090663    
2024-02-17 14:01:14,590 - Epoch: [258][  500/  500]    Overall Loss 2.184866    Objective Loss 2.184866    Top1 35.500000    Top5 76.000000    LR 0.000063    Time 0.090355    
2024-02-17 14:01:14,718 - --- validate (epoch=258)-----------
2024-02-17 14:01:14,719 - 10000 samples (100 per mini-batch)
2024-02-17 14:01:20,250 - Epoch: [258][  100/  100]    Loss 2.258817    Top1 40.560000    Top5 72.980000    
2024-02-17 14:01:20,385 - ==> Top1: 40.560    Top5: 72.980    Loss: 2.259

2024-02-17 14:01:20,397 - ==> Best [Top1: 43.060   Top5: 73.710   Sparsity:0.00   Params: 753952 on epoch: 257]
2024-02-17 14:01:20,398 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:01:20,450 - 

2024-02-17 14:01:20,451 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:01:29,690 - Epoch: [259][  100/  500]    Overall Loss 2.185365    Objective Loss 2.185365                                        LR 0.000063    Time 0.092325    
2024-02-17 14:01:38,352 - Epoch: [259][  200/  500]    Overall Loss 2.201992    Objective Loss 2.201992                                        LR 0.000063    Time 0.089453    
2024-02-17 14:01:47,025 - Epoch: [259][  300/  500]    Overall Loss 2.199300    Objective Loss 2.199300                                        LR 0.000063    Time 0.088532    
2024-02-17 14:01:55,854 - Epoch: [259][  400/  500]    Overall Loss 2.192847    Objective Loss 2.192847                                        LR 0.000063    Time 0.088462    
2024-02-17 14:02:04,684 - Epoch: [259][  500/  500]    Overall Loss 2.203419    Objective Loss 2.203419    Top1 45.000000    Top5 76.000000    LR 0.000063    Time 0.088421    
2024-02-17 14:02:04,804 - --- validate (epoch=259)-----------
2024-02-17 14:02:04,805 - 10000 samples (100 per mini-batch)
2024-02-17 14:02:10,187 - Epoch: [259][  100/  100]    Loss 2.311023    Top1 40.470000    Top5 71.630000    
2024-02-17 14:02:10,290 - ==> Top1: 40.470    Top5: 71.630    Loss: 2.311

2024-02-17 14:02:10,298 - ==> Best [Top1: 43.060   Top5: 73.710   Sparsity:0.00   Params: 753952 on epoch: 257]
2024-02-17 14:02:10,298 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:02:10,353 - 

2024-02-17 14:02:10,353 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:02:19,683 - Epoch: [260][  100/  500]    Overall Loss 2.208992    Objective Loss 2.208992                                        LR 0.000063    Time 0.093243    
2024-02-17 14:02:28,426 - Epoch: [260][  200/  500]    Overall Loss 2.209148    Objective Loss 2.209148                                        LR 0.000063    Time 0.090318    
2024-02-17 14:02:37,203 - Epoch: [260][  300/  500]    Overall Loss 2.185787    Objective Loss 2.185787                                        LR 0.000063    Time 0.089457    
2024-02-17 14:02:45,987 - Epoch: [260][  400/  500]    Overall Loss 2.187339    Objective Loss 2.187339                                        LR 0.000063    Time 0.089043    
2024-02-17 14:02:54,812 - Epoch: [260][  500/  500]    Overall Loss 2.183569    Objective Loss 2.183569    Top1 45.000000    Top5 76.500000    LR 0.000063    Time 0.088876    
2024-02-17 14:02:54,951 - --- validate (epoch=260)-----------
2024-02-17 14:02:54,952 - 10000 samples (100 per mini-batch)
2024-02-17 14:03:00,512 - Epoch: [260][  100/  100]    Loss 2.208175    Top1 42.840000    Top5 73.240000    
2024-02-17 14:03:00,647 - ==> Top1: 42.840    Top5: 73.240    Loss: 2.208

2024-02-17 14:03:00,657 - ==> Best [Top1: 43.060   Top5: 73.710   Sparsity:0.00   Params: 753952 on epoch: 257]
2024-02-17 14:03:00,657 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:03:00,936 - 

2024-02-17 14:03:00,937 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:03:10,211 - Epoch: [261][  100/  500]    Overall Loss 2.194621    Objective Loss 2.194621                                        LR 0.000063    Time 0.092694    
2024-02-17 14:03:18,988 - Epoch: [261][  200/  500]    Overall Loss 2.183655    Objective Loss 2.183655                                        LR 0.000063    Time 0.090211    
2024-02-17 14:03:27,708 - Epoch: [261][  300/  500]    Overall Loss 2.168440    Objective Loss 2.168440                                        LR 0.000063    Time 0.089195    
2024-02-17 14:03:36,371 - Epoch: [261][  400/  500]    Overall Loss 2.169451    Objective Loss 2.169451                                        LR 0.000063    Time 0.088544    
2024-02-17 14:03:45,200 - Epoch: [261][  500/  500]    Overall Loss 2.167181    Objective Loss 2.167181    Top1 39.500000    Top5 70.000000    LR 0.000063    Time 0.088486    
2024-02-17 14:03:45,341 - --- validate (epoch=261)-----------
2024-02-17 14:03:45,341 - 10000 samples (100 per mini-batch)
2024-02-17 14:03:51,051 - Epoch: [261][  100/  100]    Loss 2.238618    Top1 41.060000    Top5 72.410000    
2024-02-17 14:03:51,169 - ==> Top1: 41.060    Top5: 72.410    Loss: 2.239

2024-02-17 14:03:51,175 - ==> Best [Top1: 43.060   Top5: 73.710   Sparsity:0.00   Params: 753952 on epoch: 257]
2024-02-17 14:03:51,175 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:03:51,229 - 

2024-02-17 14:03:51,230 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:04:00,951 - Epoch: [262][  100/  500]    Overall Loss 2.168337    Objective Loss 2.168337                                        LR 0.000063    Time 0.097162    
2024-02-17 14:04:09,756 - Epoch: [262][  200/  500]    Overall Loss 2.154068    Objective Loss 2.154068                                        LR 0.000063    Time 0.092589    
2024-02-17 14:04:18,549 - Epoch: [262][  300/  500]    Overall Loss 2.160811    Objective Loss 2.160811                                        LR 0.000063    Time 0.091024    
2024-02-17 14:04:27,438 - Epoch: [262][  400/  500]    Overall Loss 2.158111    Objective Loss 2.158111                                        LR 0.000063    Time 0.090480    
2024-02-17 14:04:36,460 - Epoch: [262][  500/  500]    Overall Loss 2.158472    Objective Loss 2.158472    Top1 36.500000    Top5 70.000000    LR 0.000063    Time 0.090421    
2024-02-17 14:04:36,581 - --- validate (epoch=262)-----------
2024-02-17 14:04:36,582 - 10000 samples (100 per mini-batch)
2024-02-17 14:04:42,055 - Epoch: [262][  100/  100]    Loss 2.242483    Top1 41.130000    Top5 73.220000    
2024-02-17 14:04:42,148 - ==> Top1: 41.130    Top5: 73.220    Loss: 2.242

2024-02-17 14:04:42,155 - ==> Best [Top1: 43.060   Top5: 73.710   Sparsity:0.00   Params: 753952 on epoch: 257]
2024-02-17 14:04:42,155 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:04:42,201 - 

2024-02-17 14:04:42,201 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:04:51,468 - Epoch: [263][  100/  500]    Overall Loss 2.177899    Objective Loss 2.177899                                        LR 0.000063    Time 0.092617    
2024-02-17 14:05:00,529 - Epoch: [263][  200/  500]    Overall Loss 2.160254    Objective Loss 2.160254                                        LR 0.000063    Time 0.091596    
2024-02-17 14:05:09,525 - Epoch: [263][  300/  500]    Overall Loss 2.156779    Objective Loss 2.156779                                        LR 0.000063    Time 0.091036    
2024-02-17 14:05:18,421 - Epoch: [263][  400/  500]    Overall Loss 2.163987    Objective Loss 2.163987                                        LR 0.000063    Time 0.090509    
2024-02-17 14:05:27,328 - Epoch: [263][  500/  500]    Overall Loss 2.152740    Objective Loss 2.152740    Top1 48.000000    Top5 75.000000    LR 0.000063    Time 0.090213    
2024-02-17 14:05:27,434 - --- validate (epoch=263)-----------
2024-02-17 14:05:27,435 - 10000 samples (100 per mini-batch)
2024-02-17 14:05:32,900 - Epoch: [263][  100/  100]    Loss 2.264680    Top1 41.540000    Top5 72.540000    
2024-02-17 14:05:33,033 - ==> Top1: 41.540    Top5: 72.540    Loss: 2.265

2024-02-17 14:05:33,039 - ==> Best [Top1: 43.060   Top5: 73.710   Sparsity:0.00   Params: 753952 on epoch: 257]
2024-02-17 14:05:33,040 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:05:33,088 - 

2024-02-17 14:05:33,088 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:05:42,024 - Epoch: [264][  100/  500]    Overall Loss 2.133004    Objective Loss 2.133004                                        LR 0.000063    Time 0.089315    
2024-02-17 14:05:50,792 - Epoch: [264][  200/  500]    Overall Loss 2.142457    Objective Loss 2.142457                                        LR 0.000063    Time 0.088482    
2024-02-17 14:05:59,332 - Epoch: [264][  300/  500]    Overall Loss 2.151430    Objective Loss 2.151430                                        LR 0.000063    Time 0.087443    
2024-02-17 14:06:08,102 - Epoch: [264][  400/  500]    Overall Loss 2.142896    Objective Loss 2.142896                                        LR 0.000063    Time 0.087498    
2024-02-17 14:06:16,666 - Epoch: [264][  500/  500]    Overall Loss 2.151601    Objective Loss 2.151601    Top1 41.500000    Top5 77.500000    LR 0.000063    Time 0.087120    
2024-02-17 14:06:16,823 - --- validate (epoch=264)-----------
2024-02-17 14:06:16,824 - 10000 samples (100 per mini-batch)
2024-02-17 14:06:22,033 - Epoch: [264][  100/  100]    Loss 2.423862    Top1 38.350000    Top5 69.220000    
2024-02-17 14:06:22,184 - ==> Top1: 38.350    Top5: 69.220    Loss: 2.424

2024-02-17 14:06:22,194 - ==> Best [Top1: 43.060   Top5: 73.710   Sparsity:0.00   Params: 753952 on epoch: 257]
2024-02-17 14:06:22,194 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:06:22,239 - 

2024-02-17 14:06:22,239 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:06:31,302 - Epoch: [265][  100/  500]    Overall Loss 2.155465    Objective Loss 2.155465                                        LR 0.000063    Time 0.090588    
2024-02-17 14:06:40,134 - Epoch: [265][  200/  500]    Overall Loss 2.149830    Objective Loss 2.149830                                        LR 0.000063    Time 0.089433    
2024-02-17 14:06:48,613 - Epoch: [265][  300/  500]    Overall Loss 2.147542    Objective Loss 2.147542                                        LR 0.000063    Time 0.087876    
2024-02-17 14:06:57,031 - Epoch: [265][  400/  500]    Overall Loss 2.152617    Objective Loss 2.152617                                        LR 0.000063    Time 0.086944    
2024-02-17 14:07:05,862 - Epoch: [265][  500/  500]    Overall Loss 2.152129    Objective Loss 2.152129    Top1 48.000000    Top5 80.000000    LR 0.000063    Time 0.087210    
2024-02-17 14:07:05,970 - --- validate (epoch=265)-----------
2024-02-17 14:07:05,972 - 10000 samples (100 per mini-batch)
2024-02-17 14:07:11,461 - Epoch: [265][  100/  100]    Loss 2.125487    Top1 43.910000    Top5 74.820000    
2024-02-17 14:07:11,564 - ==> Top1: 43.910    Top5: 74.820    Loss: 2.125

2024-02-17 14:07:11,570 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:07:11,571 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:07:11,631 - 

2024-02-17 14:07:11,631 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:07:20,850 - Epoch: [266][  100/  500]    Overall Loss 2.131550    Objective Loss 2.131550                                        LR 0.000063    Time 0.092139    
2024-02-17 14:07:29,660 - Epoch: [266][  200/  500]    Overall Loss 2.125173    Objective Loss 2.125173                                        LR 0.000063    Time 0.090099    
2024-02-17 14:07:38,252 - Epoch: [266][  300/  500]    Overall Loss 2.128565    Objective Loss 2.128565                                        LR 0.000063    Time 0.088695    
2024-02-17 14:07:46,968 - Epoch: [266][  400/  500]    Overall Loss 2.147767    Objective Loss 2.147767                                        LR 0.000063    Time 0.088301    
2024-02-17 14:07:55,732 - Epoch: [266][  500/  500]    Overall Loss 2.150353    Objective Loss 2.150353    Top1 45.500000    Top5 73.000000    LR 0.000063    Time 0.088160    
2024-02-17 14:07:55,857 - --- validate (epoch=266)-----------
2024-02-17 14:07:55,858 - 10000 samples (100 per mini-batch)
2024-02-17 14:08:01,031 - Epoch: [266][  100/  100]    Loss 2.237626    Top1 41.790000    Top5 72.670000    
2024-02-17 14:08:01,198 - ==> Top1: 41.790    Top5: 72.670    Loss: 2.238

2024-02-17 14:08:01,208 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:08:01,208 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:08:01,381 - 

2024-02-17 14:08:01,381 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:08:10,337 - Epoch: [267][  100/  500]    Overall Loss 2.144825    Objective Loss 2.144825                                        LR 0.000063    Time 0.089489    
2024-02-17 14:08:18,761 - Epoch: [267][  200/  500]    Overall Loss 2.138710    Objective Loss 2.138710                                        LR 0.000063    Time 0.086847    
2024-02-17 14:08:27,287 - Epoch: [267][  300/  500]    Overall Loss 2.154993    Objective Loss 2.154993                                        LR 0.000063    Time 0.086307    
2024-02-17 14:08:35,880 - Epoch: [267][  400/  500]    Overall Loss 2.164753    Objective Loss 2.164753                                        LR 0.000063    Time 0.086203    
2024-02-17 14:08:44,697 - Epoch: [267][  500/  500]    Overall Loss 2.157887    Objective Loss 2.157887    Top1 35.000000    Top5 71.000000    LR 0.000063    Time 0.086588    
2024-02-17 14:08:44,833 - --- validate (epoch=267)-----------
2024-02-17 14:08:44,834 - 10000 samples (100 per mini-batch)
2024-02-17 14:08:50,044 - Epoch: [267][  100/  100]    Loss 2.388749    Top1 38.520000    Top5 69.940000    
2024-02-17 14:08:50,136 - ==> Top1: 38.520    Top5: 69.940    Loss: 2.389

2024-02-17 14:08:50,142 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:08:50,142 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:08:50,186 - 

2024-02-17 14:08:50,187 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:08:59,620 - Epoch: [268][  100/  500]    Overall Loss 2.154771    Objective Loss 2.154771                                        LR 0.000063    Time 0.094283    
2024-02-17 14:09:08,482 - Epoch: [268][  200/  500]    Overall Loss 2.154213    Objective Loss 2.154213                                        LR 0.000063    Time 0.091434    
2024-02-17 14:09:17,179 - Epoch: [268][  300/  500]    Overall Loss 2.172957    Objective Loss 2.172957                                        LR 0.000063    Time 0.089933    
2024-02-17 14:09:26,201 - Epoch: [268][  400/  500]    Overall Loss 2.164715    Objective Loss 2.164715                                        LR 0.000063    Time 0.089995    
2024-02-17 14:09:35,101 - Epoch: [268][  500/  500]    Overall Loss 2.159847    Objective Loss 2.159847    Top1 44.000000    Top5 73.500000    LR 0.000063    Time 0.089788    
2024-02-17 14:09:35,228 - --- validate (epoch=268)-----------
2024-02-17 14:09:35,229 - 10000 samples (100 per mini-batch)
2024-02-17 14:09:40,446 - Epoch: [268][  100/  100]    Loss 2.184608    Top1 42.440000    Top5 73.790000    
2024-02-17 14:09:40,562 - ==> Top1: 42.440    Top5: 73.790    Loss: 2.185

2024-02-17 14:09:40,572 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:09:40,572 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:09:40,624 - 

2024-02-17 14:09:40,624 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:09:49,701 - Epoch: [269][  100/  500]    Overall Loss 2.127832    Objective Loss 2.127832                                        LR 0.000063    Time 0.090721    
2024-02-17 14:09:58,362 - Epoch: [269][  200/  500]    Overall Loss 2.117665    Objective Loss 2.117665                                        LR 0.000063    Time 0.088646    
2024-02-17 14:10:06,933 - Epoch: [269][  300/  500]    Overall Loss 2.120866    Objective Loss 2.120866                                        LR 0.000063    Time 0.087656    
2024-02-17 14:10:15,537 - Epoch: [269][  400/  500]    Overall Loss 2.129320    Objective Loss 2.129320                                        LR 0.000063    Time 0.087242    
2024-02-17 14:10:24,175 - Epoch: [269][  500/  500]    Overall Loss 2.129926    Objective Loss 2.129926    Top1 44.000000    Top5 74.000000    LR 0.000063    Time 0.087062    
2024-02-17 14:10:24,319 - --- validate (epoch=269)-----------
2024-02-17 14:10:24,319 - 10000 samples (100 per mini-batch)
2024-02-17 14:10:30,037 - Epoch: [269][  100/  100]    Loss 2.184111    Top1 42.830000    Top5 74.220000    
2024-02-17 14:10:30,156 - ==> Top1: 42.830    Top5: 74.220    Loss: 2.184

2024-02-17 14:10:30,166 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:10:30,166 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:10:30,218 - 

2024-02-17 14:10:30,218 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:10:39,841 - Epoch: [270][  100/  500]    Overall Loss 2.175100    Objective Loss 2.175100                                        LR 0.000063    Time 0.096176    
2024-02-17 14:10:48,866 - Epoch: [270][  200/  500]    Overall Loss 2.169400    Objective Loss 2.169400                                        LR 0.000063    Time 0.093190    
2024-02-17 14:10:58,030 - Epoch: [270][  300/  500]    Overall Loss 2.161396    Objective Loss 2.161396                                        LR 0.000063    Time 0.092661    
2024-02-17 14:11:06,939 - Epoch: [270][  400/  500]    Overall Loss 2.165426    Objective Loss 2.165426                                        LR 0.000063    Time 0.091759    
2024-02-17 14:11:15,697 - Epoch: [270][  500/  500]    Overall Loss 2.157991    Objective Loss 2.157991    Top1 45.000000    Top5 75.500000    LR 0.000063    Time 0.090916    
2024-02-17 14:11:15,791 - --- validate (epoch=270)-----------
2024-02-17 14:11:15,792 - 10000 samples (100 per mini-batch)
2024-02-17 14:11:21,206 - Epoch: [270][  100/  100]    Loss 2.157843    Top1 42.870000    Top5 74.500000    
2024-02-17 14:11:21,325 - ==> Top1: 42.870    Top5: 74.500    Loss: 2.158

2024-02-17 14:11:21,336 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:11:21,336 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:11:21,388 - 

2024-02-17 14:11:21,388 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:11:30,507 - Epoch: [271][  100/  500]    Overall Loss 2.151652    Objective Loss 2.151652                                        LR 0.000063    Time 0.091137    
2024-02-17 14:11:39,202 - Epoch: [271][  200/  500]    Overall Loss 2.153075    Objective Loss 2.153075                                        LR 0.000063    Time 0.089023    
2024-02-17 14:11:47,899 - Epoch: [271][  300/  500]    Overall Loss 2.152750    Objective Loss 2.152750                                        LR 0.000063    Time 0.088328    
2024-02-17 14:11:57,053 - Epoch: [271][  400/  500]    Overall Loss 2.151083    Objective Loss 2.151083                                        LR 0.000063    Time 0.089120    
2024-02-17 14:12:05,806 - Epoch: [271][  500/  500]    Overall Loss 2.156670    Objective Loss 2.156670    Top1 37.500000    Top5 73.000000    LR 0.000063    Time 0.088796    
2024-02-17 14:12:05,968 - --- validate (epoch=271)-----------
2024-02-17 14:12:05,969 - 10000 samples (100 per mini-batch)
2024-02-17 14:12:11,032 - Epoch: [271][  100/  100]    Loss 2.271672    Top1 40.370000    Top5 72.450000    
2024-02-17 14:12:11,172 - ==> Top1: 40.370    Top5: 72.450    Loss: 2.272

2024-02-17 14:12:11,178 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:12:11,178 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:12:11,226 - 

2024-02-17 14:12:11,226 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:12:20,129 - Epoch: [272][  100/  500]    Overall Loss 2.153240    Objective Loss 2.153240                                        LR 0.000063    Time 0.088979    
2024-02-17 14:12:28,827 - Epoch: [272][  200/  500]    Overall Loss 2.144262    Objective Loss 2.144262                                        LR 0.000063    Time 0.087960    
2024-02-17 14:12:37,661 - Epoch: [272][  300/  500]    Overall Loss 2.147243    Objective Loss 2.147243                                        LR 0.000063    Time 0.088074    
2024-02-17 14:12:46,011 - Epoch: [272][  400/  500]    Overall Loss 2.140966    Objective Loss 2.140966                                        LR 0.000063    Time 0.086922    
2024-02-17 14:12:54,641 - Epoch: [272][  500/  500]    Overall Loss 2.145879    Objective Loss 2.145879    Top1 43.500000    Top5 77.000000    LR 0.000063    Time 0.086790    
2024-02-17 14:12:54,770 - --- validate (epoch=272)-----------
2024-02-17 14:12:54,771 - 10000 samples (100 per mini-batch)
2024-02-17 14:13:00,343 - Epoch: [272][  100/  100]    Loss 2.210991    Top1 42.570000    Top5 73.580000    
2024-02-17 14:13:00,436 - ==> Top1: 42.570    Top5: 73.580    Loss: 2.211

2024-02-17 14:13:00,448 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:13:00,449 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:13:00,695 - 

2024-02-17 14:13:00,695 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:13:09,735 - Epoch: [273][  100/  500]    Overall Loss 2.140114    Objective Loss 2.140114                                        LR 0.000063    Time 0.090350    
2024-02-17 14:13:18,340 - Epoch: [273][  200/  500]    Overall Loss 2.141673    Objective Loss 2.141673                                        LR 0.000063    Time 0.088181    
2024-02-17 14:13:27,191 - Epoch: [273][  300/  500]    Overall Loss 2.143946    Objective Loss 2.143946                                        LR 0.000063    Time 0.088281    
2024-02-17 14:13:35,824 - Epoch: [273][  400/  500]    Overall Loss 2.147834    Objective Loss 2.147834                                        LR 0.000063    Time 0.087784    
2024-02-17 14:13:44,464 - Epoch: [273][  500/  500]    Overall Loss 2.145273    Objective Loss 2.145273    Top1 46.500000    Top5 77.500000    LR 0.000063    Time 0.087500    
2024-02-17 14:13:44,590 - --- validate (epoch=273)-----------
2024-02-17 14:13:44,591 - 10000 samples (100 per mini-batch)
2024-02-17 14:13:49,971 - Epoch: [273][  100/  100]    Loss 2.260716    Top1 40.960000    Top5 72.070000    
2024-02-17 14:13:50,072 - ==> Top1: 40.960    Top5: 72.070    Loss: 2.261

2024-02-17 14:13:50,081 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:13:50,081 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:13:50,130 - 

2024-02-17 14:13:50,130 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:13:59,469 - Epoch: [274][  100/  500]    Overall Loss 2.125419    Objective Loss 2.125419                                        LR 0.000063    Time 0.093317    
2024-02-17 14:14:08,052 - Epoch: [274][  200/  500]    Overall Loss 2.138930    Objective Loss 2.138930                                        LR 0.000063    Time 0.089555    
2024-02-17 14:14:16,568 - Epoch: [274][  300/  500]    Overall Loss 2.137808    Objective Loss 2.137808                                        LR 0.000063    Time 0.088077    
2024-02-17 14:14:25,340 - Epoch: [274][  400/  500]    Overall Loss 2.138662    Objective Loss 2.138662                                        LR 0.000063    Time 0.087981    
2024-02-17 14:14:33,884 - Epoch: [274][  500/  500]    Overall Loss 2.139882    Objective Loss 2.139882    Top1 55.000000    Top5 79.500000    LR 0.000063    Time 0.087464    
2024-02-17 14:14:33,991 - --- validate (epoch=274)-----------
2024-02-17 14:14:33,991 - 10000 samples (100 per mini-batch)
2024-02-17 14:14:38,833 - Epoch: [274][  100/  100]    Loss 2.275514    Top1 41.000000    Top5 71.330000    
2024-02-17 14:14:38,926 - ==> Top1: 41.000    Top5: 71.330    Loss: 2.276

2024-02-17 14:14:38,936 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:14:38,936 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:14:38,981 - 

2024-02-17 14:14:38,981 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:14:46,890 - Epoch: [275][  100/  500]    Overall Loss 2.117724    Objective Loss 2.117724                                        LR 0.000063    Time 0.079056    
2024-02-17 14:14:55,238 - Epoch: [275][  200/  500]    Overall Loss 2.108041    Objective Loss 2.108041                                        LR 0.000063    Time 0.081250    
2024-02-17 14:15:03,932 - Epoch: [275][  300/  500]    Overall Loss 2.109100    Objective Loss 2.109100                                        LR 0.000063    Time 0.083136    
2024-02-17 14:15:12,860 - Epoch: [275][  400/  500]    Overall Loss 2.122688    Objective Loss 2.122688                                        LR 0.000063    Time 0.084660    
2024-02-17 14:15:21,596 - Epoch: [275][  500/  500]    Overall Loss 2.126925    Objective Loss 2.126925    Top1 44.500000    Top5 76.000000    LR 0.000063    Time 0.085193    
2024-02-17 14:15:21,717 - --- validate (epoch=275)-----------
2024-02-17 14:15:21,718 - 10000 samples (100 per mini-batch)
2024-02-17 14:15:27,014 - Epoch: [275][  100/  100]    Loss 2.303068    Top1 40.320000    Top5 71.660000    
2024-02-17 14:15:27,180 - ==> Top1: 40.320    Top5: 71.660    Loss: 2.303

2024-02-17 14:15:27,190 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:15:27,190 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:15:27,241 - 

2024-02-17 14:15:27,241 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:15:36,644 - Epoch: [276][  100/  500]    Overall Loss 2.149656    Objective Loss 2.149656                                        LR 0.000063    Time 0.093977    
2024-02-17 14:15:45,499 - Epoch: [276][  200/  500]    Overall Loss 2.146620    Objective Loss 2.146620                                        LR 0.000063    Time 0.091245    
2024-02-17 14:15:54,125 - Epoch: [276][  300/  500]    Overall Loss 2.136029    Objective Loss 2.136029                                        LR 0.000063    Time 0.089574    
2024-02-17 14:16:02,989 - Epoch: [276][  400/  500]    Overall Loss 2.126817    Objective Loss 2.126817                                        LR 0.000063    Time 0.089331    
2024-02-17 14:16:11,781 - Epoch: [276][  500/  500]    Overall Loss 2.130168    Objective Loss 2.130168    Top1 37.500000    Top5 69.500000    LR 0.000063    Time 0.089041    
2024-02-17 14:16:11,903 - --- validate (epoch=276)-----------
2024-02-17 14:16:11,904 - 10000 samples (100 per mini-batch)
2024-02-17 14:16:17,531 - Epoch: [276][  100/  100]    Loss 2.278068    Top1 41.050000    Top5 72.110000    
2024-02-17 14:16:17,634 - ==> Top1: 41.050    Top5: 72.110    Loss: 2.278

2024-02-17 14:16:17,647 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:16:17,647 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:16:17,700 - 

2024-02-17 14:16:17,700 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:16:26,823 - Epoch: [277][  100/  500]    Overall Loss 2.135106    Objective Loss 2.135106                                        LR 0.000063    Time 0.091171    
2024-02-17 14:16:35,481 - Epoch: [277][  200/  500]    Overall Loss 2.150452    Objective Loss 2.150452                                        LR 0.000063    Time 0.088858    
2024-02-17 14:16:44,145 - Epoch: [277][  300/  500]    Overall Loss 2.151206    Objective Loss 2.151206                                        LR 0.000063    Time 0.088106    
2024-02-17 14:16:52,933 - Epoch: [277][  400/  500]    Overall Loss 2.152959    Objective Loss 2.152959                                        LR 0.000063    Time 0.088041    
2024-02-17 14:17:01,711 - Epoch: [277][  500/  500]    Overall Loss 2.145595    Objective Loss 2.145595    Top1 44.500000    Top5 70.000000    LR 0.000063    Time 0.087980    
2024-02-17 14:17:01,863 - --- validate (epoch=277)-----------
2024-02-17 14:17:01,864 - 10000 samples (100 per mini-batch)
2024-02-17 14:17:07,271 - Epoch: [277][  100/  100]    Loss 2.326511    Top1 40.290000    Top5 71.810000    
2024-02-17 14:17:07,369 - ==> Top1: 40.290    Top5: 71.810    Loss: 2.327

2024-02-17 14:17:07,375 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:17:07,375 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:17:07,424 - 

2024-02-17 14:17:07,424 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:17:16,807 - Epoch: [278][  100/  500]    Overall Loss 2.140789    Objective Loss 2.140789                                        LR 0.000063    Time 0.093767    
2024-02-17 14:17:25,785 - Epoch: [278][  200/  500]    Overall Loss 2.126093    Objective Loss 2.126093                                        LR 0.000063    Time 0.091756    
2024-02-17 14:17:34,672 - Epoch: [278][  300/  500]    Overall Loss 2.122441    Objective Loss 2.122441                                        LR 0.000063    Time 0.090783    
2024-02-17 14:17:43,579 - Epoch: [278][  400/  500]    Overall Loss 2.121852    Objective Loss 2.121852                                        LR 0.000063    Time 0.090345    
2024-02-17 14:17:52,581 - Epoch: [278][  500/  500]    Overall Loss 2.123404    Objective Loss 2.123404    Top1 44.500000    Top5 71.500000    LR 0.000063    Time 0.090271    
2024-02-17 14:17:52,728 - --- validate (epoch=278)-----------
2024-02-17 14:17:52,730 - 10000 samples (100 per mini-batch)
2024-02-17 14:17:58,419 - Epoch: [278][  100/  100]    Loss 2.198513    Top1 42.250000    Top5 73.710000    
2024-02-17 14:17:58,573 - ==> Top1: 42.250    Top5: 73.710    Loss: 2.199

2024-02-17 14:17:58,579 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:17:58,580 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:17:58,850 - 

2024-02-17 14:17:58,850 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:18:08,404 - Epoch: [279][  100/  500]    Overall Loss 2.102781    Objective Loss 2.102781                                        LR 0.000063    Time 0.095484    
2024-02-17 14:18:17,373 - Epoch: [279][  200/  500]    Overall Loss 2.118776    Objective Loss 2.118776                                        LR 0.000063    Time 0.092567    
2024-02-17 14:18:26,384 - Epoch: [279][  300/  500]    Overall Loss 2.108472    Objective Loss 2.108472                                        LR 0.000063    Time 0.091734    
2024-02-17 14:18:35,195 - Epoch: [279][  400/  500]    Overall Loss 2.113646    Objective Loss 2.113646                                        LR 0.000063    Time 0.090819    
2024-02-17 14:18:44,074 - Epoch: [279][  500/  500]    Overall Loss 2.110661    Objective Loss 2.110661    Top1 46.000000    Top5 74.000000    LR 0.000063    Time 0.090406    
2024-02-17 14:18:44,204 - --- validate (epoch=279)-----------
2024-02-17 14:18:44,204 - 10000 samples (100 per mini-batch)
2024-02-17 14:18:49,011 - Epoch: [279][  100/  100]    Loss 2.186593    Top1 42.700000    Top5 73.620000    
2024-02-17 14:18:49,115 - ==> Top1: 42.700    Top5: 73.620    Loss: 2.187

2024-02-17 14:18:49,124 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:18:49,124 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:18:49,168 - 

2024-02-17 14:18:49,168 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:18:58,438 - Epoch: [280][  100/  500]    Overall Loss 2.090610    Objective Loss 2.090610                                        LR 0.000063    Time 0.092652    
2024-02-17 14:19:07,236 - Epoch: [280][  200/  500]    Overall Loss 2.112515    Objective Loss 2.112515                                        LR 0.000063    Time 0.090299    
2024-02-17 14:19:15,685 - Epoch: [280][  300/  500]    Overall Loss 2.111154    Objective Loss 2.111154                                        LR 0.000063    Time 0.088351    
2024-02-17 14:19:24,551 - Epoch: [280][  400/  500]    Overall Loss 2.116053    Objective Loss 2.116053                                        LR 0.000063    Time 0.088417    
2024-02-17 14:19:33,558 - Epoch: [280][  500/  500]    Overall Loss 2.114819    Objective Loss 2.114819    Top1 39.000000    Top5 72.000000    LR 0.000063    Time 0.088740    
2024-02-17 14:19:33,693 - --- validate (epoch=280)-----------
2024-02-17 14:19:33,694 - 10000 samples (100 per mini-batch)
2024-02-17 14:19:39,166 - Epoch: [280][  100/  100]    Loss 2.278164    Top1 40.320000    Top5 72.030000    
2024-02-17 14:19:39,279 - ==> Top1: 40.320    Top5: 72.030    Loss: 2.278

2024-02-17 14:19:39,285 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:19:39,285 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:19:39,332 - 

2024-02-17 14:19:39,333 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:19:48,408 - Epoch: [281][  100/  500]    Overall Loss 2.078336    Objective Loss 2.078336                                        LR 0.000063    Time 0.090706    
2024-02-17 14:19:56,972 - Epoch: [281][  200/  500]    Overall Loss 2.085491    Objective Loss 2.085491                                        LR 0.000063    Time 0.088156    
2024-02-17 14:20:05,579 - Epoch: [281][  300/  500]    Overall Loss 2.098836    Objective Loss 2.098836                                        LR 0.000063    Time 0.087449    
2024-02-17 14:20:13,958 - Epoch: [281][  400/  500]    Overall Loss 2.096208    Objective Loss 2.096208                                        LR 0.000063    Time 0.086525    
2024-02-17 14:20:22,736 - Epoch: [281][  500/  500]    Overall Loss 2.094447    Objective Loss 2.094447    Top1 44.500000    Top5 71.500000    LR 0.000063    Time 0.086769    
2024-02-17 14:20:22,910 - --- validate (epoch=281)-----------
2024-02-17 14:20:22,911 - 10000 samples (100 per mini-batch)
2024-02-17 14:20:28,463 - Epoch: [281][  100/  100]    Loss 2.186947    Top1 42.720000    Top5 73.780000    
2024-02-17 14:20:28,593 - ==> Top1: 42.720    Top5: 73.780    Loss: 2.187

2024-02-17 14:20:28,604 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:20:28,604 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:20:28,657 - 

2024-02-17 14:20:28,657 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:20:38,280 - Epoch: [282][  100/  500]    Overall Loss 2.046980    Objective Loss 2.046980                                        LR 0.000063    Time 0.096175    
2024-02-17 14:20:47,212 - Epoch: [282][  200/  500]    Overall Loss 2.080130    Objective Loss 2.080130                                        LR 0.000063    Time 0.092727    
2024-02-17 14:20:56,200 - Epoch: [282][  300/  500]    Overall Loss 2.068814    Objective Loss 2.068814                                        LR 0.000063    Time 0.091768    
2024-02-17 14:21:04,869 - Epoch: [282][  400/  500]    Overall Loss 2.077296    Objective Loss 2.077296                                        LR 0.000063    Time 0.090489    
2024-02-17 14:21:13,660 - Epoch: [282][  500/  500]    Overall Loss 2.080907    Objective Loss 2.080907    Top1 48.000000    Top5 79.500000    LR 0.000063    Time 0.089964    
2024-02-17 14:21:13,787 - --- validate (epoch=282)-----------
2024-02-17 14:21:13,788 - 10000 samples (100 per mini-batch)
2024-02-17 14:21:19,774 - Epoch: [282][  100/  100]    Loss 2.199990    Top1 42.080000    Top5 73.030000    
2024-02-17 14:21:19,957 - ==> Top1: 42.080    Top5: 73.030    Loss: 2.200

2024-02-17 14:21:19,967 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:21:19,967 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:21:20,018 - 

2024-02-17 14:21:20,019 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:21:29,225 - Epoch: [283][  100/  500]    Overall Loss 2.093683    Objective Loss 2.093683                                        LR 0.000063    Time 0.092018    
2024-02-17 14:21:37,813 - Epoch: [283][  200/  500]    Overall Loss 2.084748    Objective Loss 2.084748                                        LR 0.000063    Time 0.088929    
2024-02-17 14:21:46,105 - Epoch: [283][  300/  500]    Overall Loss 2.091106    Objective Loss 2.091106                                        LR 0.000063    Time 0.086916    
2024-02-17 14:21:54,730 - Epoch: [283][  400/  500]    Overall Loss 2.090898    Objective Loss 2.090898                                        LR 0.000063    Time 0.086740    
2024-02-17 14:22:03,638 - Epoch: [283][  500/  500]    Overall Loss 2.087785    Objective Loss 2.087785    Top1 50.500000    Top5 81.500000    LR 0.000063    Time 0.087200    
2024-02-17 14:22:03,774 - --- validate (epoch=283)-----------
2024-02-17 14:22:03,774 - 10000 samples (100 per mini-batch)
2024-02-17 14:22:09,227 - Epoch: [283][  100/  100]    Loss 2.206103    Top1 42.150000    Top5 73.560000    
2024-02-17 14:22:09,336 - ==> Top1: 42.150    Top5: 73.560    Loss: 2.206

2024-02-17 14:22:09,346 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:22:09,347 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:22:09,398 - 

2024-02-17 14:22:09,399 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:22:18,749 - Epoch: [284][  100/  500]    Overall Loss 2.076457    Objective Loss 2.076457                                        LR 0.000063    Time 0.093443    
2024-02-17 14:22:27,416 - Epoch: [284][  200/  500]    Overall Loss 2.087476    Objective Loss 2.087476                                        LR 0.000063    Time 0.090036    
2024-02-17 14:22:36,058 - Epoch: [284][  300/  500]    Overall Loss 2.076148    Objective Loss 2.076148                                        LR 0.000063    Time 0.088820    
2024-02-17 14:22:44,261 - Epoch: [284][  400/  500]    Overall Loss 2.075921    Objective Loss 2.075921                                        LR 0.000063    Time 0.087115    
2024-02-17 14:22:53,098 - Epoch: [284][  500/  500]    Overall Loss 2.083419    Objective Loss 2.083419    Top1 48.000000    Top5 79.000000    LR 0.000063    Time 0.087358    
2024-02-17 14:22:53,224 - --- validate (epoch=284)-----------
2024-02-17 14:22:53,225 - 10000 samples (100 per mini-batch)
2024-02-17 14:22:59,371 - Epoch: [284][  100/  100]    Loss 2.209395    Top1 42.130000    Top5 73.400000    
2024-02-17 14:22:59,540 - ==> Top1: 42.130    Top5: 73.400    Loss: 2.209

2024-02-17 14:22:59,549 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:22:59,550 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:22:59,601 - 

2024-02-17 14:22:59,602 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:23:08,830 - Epoch: [285][  100/  500]    Overall Loss 2.105450    Objective Loss 2.105450                                        LR 0.000063    Time 0.092229    
2024-02-17 14:23:17,538 - Epoch: [285][  200/  500]    Overall Loss 2.103318    Objective Loss 2.103318                                        LR 0.000063    Time 0.089634    
2024-02-17 14:23:26,194 - Epoch: [285][  300/  500]    Overall Loss 2.092561    Objective Loss 2.092561                                        LR 0.000063    Time 0.088599    
2024-02-17 14:23:34,820 - Epoch: [285][  400/  500]    Overall Loss 2.096942    Objective Loss 2.096942                                        LR 0.000063    Time 0.088005    
2024-02-17 14:23:43,359 - Epoch: [285][  500/  500]    Overall Loss 2.095800    Objective Loss 2.095800    Top1 48.500000    Top5 73.000000    LR 0.000063    Time 0.087474    
2024-02-17 14:23:43,516 - --- validate (epoch=285)-----------
2024-02-17 14:23:43,517 - 10000 samples (100 per mini-batch)
2024-02-17 14:23:48,886 - Epoch: [285][  100/  100]    Loss 2.203385    Top1 42.300000    Top5 73.720000    
2024-02-17 14:23:48,983 - ==> Top1: 42.300    Top5: 73.720    Loss: 2.203

2024-02-17 14:23:48,993 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:23:48,994 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:23:49,047 - 

2024-02-17 14:23:49,047 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:23:58,710 - Epoch: [286][  100/  500]    Overall Loss 2.058983    Objective Loss 2.058983                                        LR 0.000063    Time 0.096582    
2024-02-17 14:24:07,184 - Epoch: [286][  200/  500]    Overall Loss 2.051390    Objective Loss 2.051390                                        LR 0.000063    Time 0.090643    
2024-02-17 14:24:15,495 - Epoch: [286][  300/  500]    Overall Loss 2.072247    Objective Loss 2.072247                                        LR 0.000063    Time 0.088122    
2024-02-17 14:24:24,730 - Epoch: [286][  400/  500]    Overall Loss 2.083443    Objective Loss 2.083443                                        LR 0.000063    Time 0.089169    
2024-02-17 14:24:33,470 - Epoch: [286][  500/  500]    Overall Loss 2.085801    Objective Loss 2.085801    Top1 40.500000    Top5 71.000000    LR 0.000063    Time 0.088807    
2024-02-17 14:24:33,610 - --- validate (epoch=286)-----------
2024-02-17 14:24:33,611 - 10000 samples (100 per mini-batch)
2024-02-17 14:24:38,550 - Epoch: [286][  100/  100]    Loss 2.171307    Top1 43.290000    Top5 74.010000    
2024-02-17 14:24:38,653 - ==> Top1: 43.290    Top5: 74.010    Loss: 2.171

2024-02-17 14:24:38,657 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:24:38,657 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:24:38,697 - 

2024-02-17 14:24:38,697 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:24:47,264 - Epoch: [287][  100/  500]    Overall Loss 2.047228    Objective Loss 2.047228                                        LR 0.000063    Time 0.085631    
2024-02-17 14:24:55,873 - Epoch: [287][  200/  500]    Overall Loss 2.056393    Objective Loss 2.056393                                        LR 0.000063    Time 0.085843    
2024-02-17 14:25:04,567 - Epoch: [287][  300/  500]    Overall Loss 2.057687    Objective Loss 2.057687                                        LR 0.000063    Time 0.086196    
2024-02-17 14:25:13,345 - Epoch: [287][  400/  500]    Overall Loss 2.063433    Objective Loss 2.063433                                        LR 0.000063    Time 0.086582    
2024-02-17 14:25:22,141 - Epoch: [287][  500/  500]    Overall Loss 2.067854    Objective Loss 2.067854    Top1 45.000000    Top5 78.500000    LR 0.000063    Time 0.086850    
2024-02-17 14:25:22,266 - --- validate (epoch=287)-----------
2024-02-17 14:25:22,267 - 10000 samples (100 per mini-batch)
2024-02-17 14:25:27,724 - Epoch: [287][  100/  100]    Loss 2.163256    Top1 43.640000    Top5 74.620000    
2024-02-17 14:25:27,832 - ==> Top1: 43.640    Top5: 74.620    Loss: 2.163

2024-02-17 14:25:27,837 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:25:27,837 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:25:27,882 - 

2024-02-17 14:25:27,883 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:25:37,435 - Epoch: [288][  100/  500]    Overall Loss 2.077317    Objective Loss 2.077317                                        LR 0.000063    Time 0.095471    
2024-02-17 14:25:46,034 - Epoch: [288][  200/  500]    Overall Loss 2.070616    Objective Loss 2.070616                                        LR 0.000063    Time 0.090716    
2024-02-17 14:25:54,755 - Epoch: [288][  300/  500]    Overall Loss 2.081491    Objective Loss 2.081491                                        LR 0.000063    Time 0.089534    
2024-02-17 14:26:03,369 - Epoch: [288][  400/  500]    Overall Loss 2.091767    Objective Loss 2.091767                                        LR 0.000063    Time 0.088677    
2024-02-17 14:26:12,189 - Epoch: [288][  500/  500]    Overall Loss 2.094265    Objective Loss 2.094265    Top1 42.500000    Top5 72.000000    LR 0.000063    Time 0.088574    
2024-02-17 14:26:12,306 - --- validate (epoch=288)-----------
2024-02-17 14:26:12,307 - 10000 samples (100 per mini-batch)
2024-02-17 14:26:17,707 - Epoch: [288][  100/  100]    Loss 2.216333    Top1 42.770000    Top5 73.190000    
2024-02-17 14:26:17,851 - ==> Top1: 42.770    Top5: 73.190    Loss: 2.216

2024-02-17 14:26:17,863 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:26:17,864 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:26:17,923 - 

2024-02-17 14:26:17,924 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:26:27,142 - Epoch: [289][  100/  500]    Overall Loss 2.082443    Objective Loss 2.082443                                        LR 0.000063    Time 0.092130    
2024-02-17 14:26:35,281 - Epoch: [289][  200/  500]    Overall Loss 2.081229    Objective Loss 2.081229                                        LR 0.000063    Time 0.086746    
2024-02-17 14:26:43,910 - Epoch: [289][  300/  500]    Overall Loss 2.098335    Objective Loss 2.098335                                        LR 0.000063    Time 0.086583    
2024-02-17 14:26:52,880 - Epoch: [289][  400/  500]    Overall Loss 2.088637    Objective Loss 2.088637                                        LR 0.000063    Time 0.087353    
2024-02-17 14:27:01,939 - Epoch: [289][  500/  500]    Overall Loss 2.095599    Objective Loss 2.095599    Top1 48.000000    Top5 81.500000    LR 0.000063    Time 0.087992    
2024-02-17 14:27:02,045 - --- validate (epoch=289)-----------
2024-02-17 14:27:02,047 - 10000 samples (100 per mini-batch)
2024-02-17 14:27:07,831 - Epoch: [289][  100/  100]    Loss 2.161097    Top1 43.160000    Top5 73.840000    
2024-02-17 14:27:07,949 - ==> Top1: 43.160    Top5: 73.840    Loss: 2.161

2024-02-17 14:27:07,959 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:27:07,959 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:27:08,014 - 

2024-02-17 14:27:08,014 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:27:17,273 - Epoch: [290][  100/  500]    Overall Loss 2.055351    Objective Loss 2.055351                                        LR 0.000063    Time 0.092530    
2024-02-17 14:27:25,509 - Epoch: [290][  200/  500]    Overall Loss 2.061499    Objective Loss 2.061499                                        LR 0.000063    Time 0.087433    
2024-02-17 14:27:34,242 - Epoch: [290][  300/  500]    Overall Loss 2.083611    Objective Loss 2.083611                                        LR 0.000063    Time 0.087387    
2024-02-17 14:27:42,656 - Epoch: [290][  400/  500]    Overall Loss 2.099399    Objective Loss 2.099399                                        LR 0.000063    Time 0.086565    
2024-02-17 14:27:51,278 - Epoch: [290][  500/  500]    Overall Loss 2.097252    Objective Loss 2.097252    Top1 45.000000    Top5 80.000000    LR 0.000063    Time 0.086490    
2024-02-17 14:27:51,389 - --- validate (epoch=290)-----------
2024-02-17 14:27:51,390 - 10000 samples (100 per mini-batch)
2024-02-17 14:27:57,024 - Epoch: [290][  100/  100]    Loss 2.172009    Top1 43.200000    Top5 73.840000    
2024-02-17 14:27:57,130 - ==> Top1: 43.200    Top5: 73.840    Loss: 2.172

2024-02-17 14:27:57,140 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:27:57,140 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:27:57,193 - 

2024-02-17 14:27:57,193 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:28:06,475 - Epoch: [291][  100/  500]    Overall Loss 2.071871    Objective Loss 2.071871                                        LR 0.000063    Time 0.092745    
2024-02-17 14:28:14,964 - Epoch: [291][  200/  500]    Overall Loss 2.085288    Objective Loss 2.085288                                        LR 0.000063    Time 0.088798    
2024-02-17 14:28:23,389 - Epoch: [291][  300/  500]    Overall Loss 2.081577    Objective Loss 2.081577                                        LR 0.000063    Time 0.087273    
2024-02-17 14:28:31,858 - Epoch: [291][  400/  500]    Overall Loss 2.095577    Objective Loss 2.095577                                        LR 0.000063    Time 0.086618    
2024-02-17 14:28:40,456 - Epoch: [291][  500/  500]    Overall Loss 2.094852    Objective Loss 2.094852    Top1 44.500000    Top5 73.000000    LR 0.000063    Time 0.086483    
2024-02-17 14:28:40,558 - --- validate (epoch=291)-----------
2024-02-17 14:28:40,559 - 10000 samples (100 per mini-batch)
2024-02-17 14:28:45,872 - Epoch: [291][  100/  100]    Loss 2.165038    Top1 42.940000    Top5 74.130000    
2024-02-17 14:28:45,982 - ==> Top1: 42.940    Top5: 74.130    Loss: 2.165

2024-02-17 14:28:45,994 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:28:45,995 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:28:46,047 - 

2024-02-17 14:28:46,047 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:28:55,408 - Epoch: [292][  100/  500]    Overall Loss 2.098251    Objective Loss 2.098251                                        LR 0.000063    Time 0.093562    
2024-02-17 14:29:04,108 - Epoch: [292][  200/  500]    Overall Loss 2.087743    Objective Loss 2.087743                                        LR 0.000063    Time 0.090265    
2024-02-17 14:29:12,694 - Epoch: [292][  300/  500]    Overall Loss 2.083152    Objective Loss 2.083152                                        LR 0.000063    Time 0.088783    
2024-02-17 14:29:21,167 - Epoch: [292][  400/  500]    Overall Loss 2.081525    Objective Loss 2.081525                                        LR 0.000063    Time 0.087762    
2024-02-17 14:29:30,016 - Epoch: [292][  500/  500]    Overall Loss 2.080256    Objective Loss 2.080256    Top1 48.000000    Top5 76.000000    LR 0.000063    Time 0.087899    
2024-02-17 14:29:30,139 - --- validate (epoch=292)-----------
2024-02-17 14:29:30,139 - 10000 samples (100 per mini-batch)
2024-02-17 14:29:35,792 - Epoch: [292][  100/  100]    Loss 2.287329    Top1 41.850000    Top5 71.920000    
2024-02-17 14:29:35,890 - ==> Top1: 41.850    Top5: 71.920    Loss: 2.287

2024-02-17 14:29:35,896 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:29:35,896 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:29:35,943 - 

2024-02-17 14:29:35,944 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:29:45,497 - Epoch: [293][  100/  500]    Overall Loss 2.063275    Objective Loss 2.063275                                        LR 0.000063    Time 0.095475    
2024-02-17 14:29:54,326 - Epoch: [293][  200/  500]    Overall Loss 2.072953    Objective Loss 2.072953                                        LR 0.000063    Time 0.091863    
2024-02-17 14:30:03,098 - Epoch: [293][  300/  500]    Overall Loss 2.081630    Objective Loss 2.081630                                        LR 0.000063    Time 0.090471    
2024-02-17 14:30:11,794 - Epoch: [293][  400/  500]    Overall Loss 2.094186    Objective Loss 2.094186                                        LR 0.000063    Time 0.089582    
2024-02-17 14:30:20,839 - Epoch: [293][  500/  500]    Overall Loss 2.104630    Objective Loss 2.104630    Top1 50.500000    Top5 80.000000    LR 0.000063    Time 0.089748    
2024-02-17 14:30:20,989 - --- validate (epoch=293)-----------
2024-02-17 14:30:20,991 - 10000 samples (100 per mini-batch)
2024-02-17 14:30:26,795 - Epoch: [293][  100/  100]    Loss 2.252237    Top1 41.820000    Top5 72.310000    
2024-02-17 14:30:26,925 - ==> Top1: 41.820    Top5: 72.310    Loss: 2.252

2024-02-17 14:30:26,935 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:30:26,936 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:30:26,988 - 

2024-02-17 14:30:26,988 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:30:36,319 - Epoch: [294][  100/  500]    Overall Loss 2.089961    Objective Loss 2.089961                                        LR 0.000063    Time 0.093257    
2024-02-17 14:30:45,111 - Epoch: [294][  200/  500]    Overall Loss 2.100130    Objective Loss 2.100130                                        LR 0.000063    Time 0.090568    
2024-02-17 14:30:53,792 - Epoch: [294][  300/  500]    Overall Loss 2.099754    Objective Loss 2.099754                                        LR 0.000063    Time 0.089304    
2024-02-17 14:31:02,356 - Epoch: [294][  400/  500]    Overall Loss 2.088566    Objective Loss 2.088566                                        LR 0.000063    Time 0.088379    
2024-02-17 14:31:11,201 - Epoch: [294][  500/  500]    Overall Loss 2.093900    Objective Loss 2.093900    Top1 43.000000    Top5 74.500000    LR 0.000063    Time 0.088385    
2024-02-17 14:31:11,326 - --- validate (epoch=294)-----------
2024-02-17 14:31:11,327 - 10000 samples (100 per mini-batch)
2024-02-17 14:31:16,891 - Epoch: [294][  100/  100]    Loss 2.142497    Top1 43.720000    Top5 74.290000    
2024-02-17 14:31:17,010 - ==> Top1: 43.720    Top5: 74.290    Loss: 2.142

2024-02-17 14:31:17,021 - ==> Best [Top1: 43.910   Top5: 74.820   Sparsity:0.00   Params: 753952 on epoch: 265]
2024-02-17 14:31:17,022 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:31:17,073 - 

2024-02-17 14:31:17,073 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:31:26,383 - Epoch: [295][  100/  500]    Overall Loss 2.062862    Objective Loss 2.062862                                        LR 0.000063    Time 0.093039    
2024-02-17 14:31:35,387 - Epoch: [295][  200/  500]    Overall Loss 2.064526    Objective Loss 2.064526                                        LR 0.000063    Time 0.091520    
2024-02-17 14:31:44,635 - Epoch: [295][  300/  500]    Overall Loss 2.060591    Objective Loss 2.060591                                        LR 0.000063    Time 0.091829    
2024-02-17 14:31:53,706 - Epoch: [295][  400/  500]    Overall Loss 2.059907    Objective Loss 2.059907                                        LR 0.000063    Time 0.091540    
2024-02-17 14:32:02,733 - Epoch: [295][  500/  500]    Overall Loss 2.063492    Objective Loss 2.063492    Top1 45.000000    Top5 75.500000    LR 0.000063    Time 0.091277    
2024-02-17 14:32:02,925 - --- validate (epoch=295)-----------
2024-02-17 14:32:02,926 - 10000 samples (100 per mini-batch)
2024-02-17 14:32:08,526 - Epoch: [295][  100/  100]    Loss 2.088158    Top1 44.730000    Top5 76.460000    
2024-02-17 14:32:08,670 - ==> Top1: 44.730    Top5: 76.460    Loss: 2.088

2024-02-17 14:32:08,677 - ==> Best [Top1: 44.730   Top5: 76.460   Sparsity:0.00   Params: 753952 on epoch: 295]
2024-02-17 14:32:08,677 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:32:08,738 - 

2024-02-17 14:32:08,739 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:32:18,162 - Epoch: [296][  100/  500]    Overall Loss 2.040429    Objective Loss 2.040429                                        LR 0.000063    Time 0.094186    
2024-02-17 14:32:26,814 - Epoch: [296][  200/  500]    Overall Loss 2.066411    Objective Loss 2.066411                                        LR 0.000063    Time 0.090335    
2024-02-17 14:32:35,527 - Epoch: [296][  300/  500]    Overall Loss 2.077455    Objective Loss 2.077455                                        LR 0.000063    Time 0.089256    
2024-02-17 14:32:44,206 - Epoch: [296][  400/  500]    Overall Loss 2.076386    Objective Loss 2.076386                                        LR 0.000063    Time 0.088629    
2024-02-17 14:32:52,859 - Epoch: [296][  500/  500]    Overall Loss 2.078478    Objective Loss 2.078478    Top1 50.000000    Top5 81.500000    LR 0.000063    Time 0.088203    
2024-02-17 14:32:52,978 - --- validate (epoch=296)-----------
2024-02-17 14:32:52,978 - 10000 samples (100 per mini-batch)
2024-02-17 14:32:58,523 - Epoch: [296][  100/  100]    Loss 2.193960    Top1 42.490000    Top5 73.270000    
2024-02-17 14:32:58,650 - ==> Top1: 42.490    Top5: 73.270    Loss: 2.194

2024-02-17 14:32:58,661 - ==> Best [Top1: 44.730   Top5: 76.460   Sparsity:0.00   Params: 753952 on epoch: 295]
2024-02-17 14:32:58,662 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:32:58,714 - 

2024-02-17 14:32:58,714 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:33:07,934 - Epoch: [297][  100/  500]    Overall Loss 2.071955    Objective Loss 2.071955                                        LR 0.000063    Time 0.092149    
2024-02-17 14:33:16,731 - Epoch: [297][  200/  500]    Overall Loss 2.073477    Objective Loss 2.073477                                        LR 0.000063    Time 0.090039    
2024-02-17 14:33:25,508 - Epoch: [297][  300/  500]    Overall Loss 2.086286    Objective Loss 2.086286                                        LR 0.000063    Time 0.089270    
2024-02-17 14:33:34,147 - Epoch: [297][  400/  500]    Overall Loss 2.079679    Objective Loss 2.079679                                        LR 0.000063    Time 0.088541    
2024-02-17 14:33:42,653 - Epoch: [297][  500/  500]    Overall Loss 2.078435    Objective Loss 2.078435    Top1 44.500000    Top5 77.500000    LR 0.000063    Time 0.087838    
2024-02-17 14:33:42,787 - --- validate (epoch=297)-----------
2024-02-17 14:33:42,788 - 10000 samples (100 per mini-batch)
2024-02-17 14:33:48,491 - Epoch: [297][  100/  100]    Loss 2.127887    Top1 43.670000    Top5 74.840000    
2024-02-17 14:33:48,597 - ==> Top1: 43.670    Top5: 74.840    Loss: 2.128

2024-02-17 14:33:48,607 - ==> Best [Top1: 44.730   Top5: 76.460   Sparsity:0.00   Params: 753952 on epoch: 295]
2024-02-17 14:33:48,608 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:33:48,658 - 

2024-02-17 14:33:48,658 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:33:58,114 - Epoch: [298][  100/  500]    Overall Loss 2.060903    Objective Loss 2.060903                                        LR 0.000063    Time 0.094511    
2024-02-17 14:34:06,915 - Epoch: [298][  200/  500]    Overall Loss 2.064544    Objective Loss 2.064544                                        LR 0.000063    Time 0.091241    
2024-02-17 14:34:15,717 - Epoch: [298][  300/  500]    Overall Loss 2.072139    Objective Loss 2.072139                                        LR 0.000063    Time 0.090153    
2024-02-17 14:34:24,476 - Epoch: [298][  400/  500]    Overall Loss 2.078642    Objective Loss 2.078642                                        LR 0.000063    Time 0.089502    
2024-02-17 14:34:33,171 - Epoch: [298][  500/  500]    Overall Loss 2.086951    Objective Loss 2.086951    Top1 42.000000    Top5 70.000000    LR 0.000063    Time 0.088985    
2024-02-17 14:34:33,303 - --- validate (epoch=298)-----------
2024-02-17 14:34:33,304 - 10000 samples (100 per mini-batch)
2024-02-17 14:34:38,887 - Epoch: [298][  100/  100]    Loss 2.099522    Top1 44.390000    Top5 75.420000    
2024-02-17 14:34:39,052 - ==> Top1: 44.390    Top5: 75.420    Loss: 2.100

2024-02-17 14:34:39,062 - ==> Best [Top1: 44.730   Top5: 76.460   Sparsity:0.00   Params: 753952 on epoch: 295]
2024-02-17 14:34:39,062 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:34:39,112 - 

2024-02-17 14:34:39,113 - Training epoch: 50000 samples (100 per mini-batch)
2024-02-17 14:34:48,291 - Epoch: [299][  100/  500]    Overall Loss 2.053952    Objective Loss 2.053952                                        LR 0.000063    Time 0.091727    
2024-02-17 14:34:57,005 - Epoch: [299][  200/  500]    Overall Loss 2.076318    Objective Loss 2.076318                                        LR 0.000063    Time 0.089418    
2024-02-17 14:35:05,579 - Epoch: [299][  300/  500]    Overall Loss 2.082993    Objective Loss 2.082993                                        LR 0.000063    Time 0.088180    
2024-02-17 14:35:13,977 - Epoch: [299][  400/  500]    Overall Loss 2.087819    Objective Loss 2.087819                                        LR 0.000063    Time 0.087123    
2024-02-17 14:35:22,844 - Epoch: [299][  500/  500]    Overall Loss 2.076680    Objective Loss 2.076680    Top1 43.000000    Top5 80.000000    LR 0.000063    Time 0.087425    
2024-02-17 14:35:22,964 - --- validate (epoch=299)-----------
2024-02-17 14:35:22,965 - 10000 samples (100 per mini-batch)
2024-02-17 14:35:28,399 - Epoch: [299][  100/  100]    Loss 2.162741    Top1 43.690000    Top5 74.540000    
2024-02-17 14:35:28,498 - ==> Top1: 43.690    Top5: 74.540    Loss: 2.163

2024-02-17 14:35:28,510 - ==> Best [Top1: 44.730   Top5: 76.460   Sparsity:0.00   Params: 753952 on epoch: 295]
2024-02-17 14:35:28,511 - Saving checkpoint to: logs/2024.02.17-110158/qat_checkpoint.pth.tar
2024-02-17 14:35:28,561 - --- test ---------------------
2024-02-17 14:35:28,562 - 10000 samples (100 per mini-batch)
2024-02-17 14:35:34,070 - Test: [  100/  100]    Loss 2.162741    Top1 43.690000    Top5 74.540000    
2024-02-17 14:35:34,168 - ==> Top1: 43.690    Top5: 74.540    Loss: 2.163

2024-02-17 14:35:34,177 - 
2024-02-17 14:35:34,178 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.02.17-110158/2024.02.17-110158.log
