2024-02-17 10:52:30,742 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.02.17-105230/2024.02.17-105230.log
2024-02-17 10:52:35,455 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2024-02-17 10:52:35,455 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}
2024-02-17 10:52:37,198 - Dataset sizes:
	training=50000
	validation=10000
	test=10000
2024-02-17 10:52:37,199 - Reading compression schedule from: policies/schedule-cifar100-mobilenetv2.yaml
2024-02-17 10:52:37,215 - 

2024-02-17 10:52:37,215 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:52:46,777 - Epoch: [0][  100/  391]    Overall Loss 4.416315    Objective Loss 4.416315                                        LR 0.100000    Time 0.095561    
2024-02-17 10:52:55,625 - Epoch: [0][  200/  391]    Overall Loss 4.286143    Objective Loss 4.286143                                        LR 0.100000    Time 0.091997    
2024-02-17 10:53:04,655 - Epoch: [0][  300/  391]    Overall Loss 4.195390    Objective Loss 4.195390                                        LR 0.100000    Time 0.091413    
2024-02-17 10:53:12,935 - Epoch: [0][  391/  391]    Overall Loss 4.128643    Objective Loss 4.128643    Top1 8.173077    Top5 30.769231    LR 0.100000    Time 0.091303    
2024-02-17 10:53:13,096 - --- validate (epoch=0)-----------
2024-02-17 10:53:13,097 - 10000 samples (128 per mini-batch)
2024-02-17 10:53:16,013 - Epoch: [0][   79/   79]    Loss 4.294045    Top1 5.940000    Top5 21.100000    
2024-02-17 10:53:16,194 - ==> Top1: 5.940    Top5: 21.100    Loss: 4.294

2024-02-17 10:53:16,464 - ==> Best [Top1: 5.940   Top5: 21.100   Sparsity:0.00   Params: 1341960 on epoch: 0]
2024-02-17 10:53:16,464 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 10:53:16,540 - 

2024-02-17 10:53:16,540 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:53:26,214 - Epoch: [1][  100/  391]    Overall Loss 3.804707    Objective Loss 3.804707                                        LR 0.100000    Time 0.096664    
2024-02-17 10:53:35,610 - Epoch: [1][  200/  391]    Overall Loss 3.776933    Objective Loss 3.776933                                        LR 0.100000    Time 0.095290    
2024-02-17 10:53:45,011 - Epoch: [1][  300/  391]    Overall Loss 3.746371    Objective Loss 3.746371                                        LR 0.100000    Time 0.094845    
2024-02-17 10:53:53,539 - Epoch: [1][  391/  391]    Overall Loss 3.722566    Objective Loss 3.722566    Top1 11.538462    Top5 36.538462    LR 0.100000    Time 0.094572    
2024-02-17 10:53:53,651 - --- validate (epoch=1)-----------
2024-02-17 10:53:53,652 - 10000 samples (128 per mini-batch)
2024-02-17 10:53:56,501 - Epoch: [1][   79/   79]    Loss 3.612136    Top1 11.770000    Top5 36.110000    
2024-02-17 10:53:56,627 - ==> Top1: 11.770    Top5: 36.110    Loss: 3.612

2024-02-17 10:53:56,639 - ==> Best [Top1: 11.770   Top5: 36.110   Sparsity:0.00   Params: 1341960 on epoch: 1]
2024-02-17 10:53:56,639 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 10:53:56,727 - 

2024-02-17 10:53:56,728 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:54:06,668 - Epoch: [2][  100/  391]    Overall Loss 3.567376    Objective Loss 3.567376                                        LR 0.100000    Time 0.099338    
2024-02-17 10:54:16,064 - Epoch: [2][  200/  391]    Overall Loss 3.529924    Objective Loss 3.529924                                        LR 0.100000    Time 0.096626    
2024-02-17 10:54:25,798 - Epoch: [2][  300/  391]    Overall Loss 3.513421    Objective Loss 3.513421                                        LR 0.100000    Time 0.096847    
2024-02-17 10:54:34,635 - Epoch: [2][  391/  391]    Overall Loss 3.484502    Objective Loss 3.484502    Top1 13.461538    Top5 45.192308    LR 0.100000    Time 0.096899    
2024-02-17 10:54:34,810 - --- validate (epoch=2)-----------
2024-02-17 10:54:34,811 - 10000 samples (128 per mini-batch)
2024-02-17 10:54:37,646 - Epoch: [2][   79/   79]    Loss 3.470122    Top1 14.090000    Top5 42.420000    
2024-02-17 10:54:37,747 - ==> Top1: 14.090    Top5: 42.420    Loss: 3.470

2024-02-17 10:54:37,763 - ==> Best [Top1: 14.090   Top5: 42.420   Sparsity:0.00   Params: 1341960 on epoch: 2]
2024-02-17 10:54:37,763 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 10:54:37,863 - 

2024-02-17 10:54:37,864 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:54:47,318 - Epoch: [3][  100/  391]    Overall Loss 3.324654    Objective Loss 3.324654                                        LR 0.100000    Time 0.094467    
2024-02-17 10:54:56,190 - Epoch: [3][  200/  391]    Overall Loss 3.292986    Objective Loss 3.292986                                        LR 0.100000    Time 0.091574    
2024-02-17 10:55:04,860 - Epoch: [3][  300/  391]    Overall Loss 3.274662    Objective Loss 3.274662                                        LR 0.100000    Time 0.089934    
2024-02-17 10:55:13,192 - Epoch: [3][  391/  391]    Overall Loss 3.251404    Objective Loss 3.251404    Top1 17.788462    Top5 50.000000    LR 0.100000    Time 0.090302    
2024-02-17 10:55:13,315 - --- validate (epoch=3)-----------
2024-02-17 10:55:13,316 - 10000 samples (128 per mini-batch)
2024-02-17 10:55:15,840 - Epoch: [3][   79/   79]    Loss 3.512974    Top1 16.070000    Top5 42.120000    
2024-02-17 10:55:15,954 - ==> Top1: 16.070    Top5: 42.120    Loss: 3.513

2024-02-17 10:55:15,972 - ==> Best [Top1: 16.070   Top5: 42.120   Sparsity:0.00   Params: 1341960 on epoch: 3]
2024-02-17 10:55:15,972 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 10:55:16,081 - 

2024-02-17 10:55:16,081 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:55:25,662 - Epoch: [4][  100/  391]    Overall Loss 3.115228    Objective Loss 3.115228                                        LR 0.100000    Time 0.095732    
2024-02-17 10:55:34,951 - Epoch: [4][  200/  391]    Overall Loss 3.098125    Objective Loss 3.098125                                        LR 0.100000    Time 0.094286    
2024-02-17 10:55:44,255 - Epoch: [4][  300/  391]    Overall Loss 3.082714    Objective Loss 3.082714                                        LR 0.100000    Time 0.093855    
2024-02-17 10:55:52,712 - Epoch: [4][  391/  391]    Overall Loss 3.057796    Objective Loss 3.057796    Top1 21.153846    Top5 56.730769    LR 0.100000    Time 0.093631    
2024-02-17 10:55:52,882 - --- validate (epoch=4)-----------
2024-02-17 10:55:52,883 - 10000 samples (128 per mini-batch)
2024-02-17 10:55:55,420 - Epoch: [4][   79/   79]    Loss 3.680597    Top1 15.720000    Top5 42.760000    
2024-02-17 10:55:55,554 - ==> Top1: 15.720    Top5: 42.760    Loss: 3.681

2024-02-17 10:55:55,574 - ==> Best [Top1: 16.070   Top5: 42.120   Sparsity:0.00   Params: 1341960 on epoch: 3]
2024-02-17 10:55:55,574 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 10:55:55,862 - 

2024-02-17 10:55:55,862 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:56:05,317 - Epoch: [5][  100/  391]    Overall Loss 2.921600    Objective Loss 2.921600                                        LR 0.100000    Time 0.094472    
2024-02-17 10:56:14,747 - Epoch: [5][  200/  391]    Overall Loss 2.925484    Objective Loss 2.925484                                        LR 0.100000    Time 0.094359    
2024-02-17 10:56:24,193 - Epoch: [5][  300/  391]    Overall Loss 2.903268    Objective Loss 2.903268                                        LR 0.100000    Time 0.094378    
2024-02-17 10:56:32,800 - Epoch: [5][  391/  391]    Overall Loss 2.888552    Objective Loss 2.888552    Top1 24.038462    Top5 55.769231    LR 0.100000    Time 0.094416    
2024-02-17 10:56:32,919 - --- validate (epoch=5)-----------
2024-02-17 10:56:32,920 - 10000 samples (128 per mini-batch)
2024-02-17 10:56:35,478 - Epoch: [5][   79/   79]    Loss 3.252491    Top1 20.500000    Top5 51.040000    
2024-02-17 10:56:35,651 - ==> Top1: 20.500    Top5: 51.040    Loss: 3.252

2024-02-17 10:56:35,669 - ==> Best [Top1: 20.500   Top5: 51.040   Sparsity:0.00   Params: 1341960 on epoch: 5]
2024-02-17 10:56:35,670 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 10:56:35,771 - 

2024-02-17 10:56:35,771 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:56:45,654 - Epoch: [6][  100/  391]    Overall Loss 2.782903    Objective Loss 2.782903                                        LR 0.100000    Time 0.098759    
2024-02-17 10:56:54,604 - Epoch: [6][  200/  391]    Overall Loss 2.783424    Objective Loss 2.783424                                        LR 0.100000    Time 0.094104    
2024-02-17 10:57:03,626 - Epoch: [6][  300/  391]    Overall Loss 2.762626    Objective Loss 2.762626                                        LR 0.100000    Time 0.092796    
2024-02-17 10:57:11,879 - Epoch: [6][  391/  391]    Overall Loss 2.749103    Objective Loss 2.749103    Top1 28.365385    Top5 61.057692    LR 0.100000    Time 0.092296    
2024-02-17 10:57:12,052 - --- validate (epoch=6)-----------
2024-02-17 10:57:12,053 - 10000 samples (128 per mini-batch)
2024-02-17 10:57:14,519 - Epoch: [6][   79/   79]    Loss 2.988540    Top1 23.640000    Top5 56.320000    
2024-02-17 10:57:14,625 - ==> Top1: 23.640    Top5: 56.320    Loss: 2.989

2024-02-17 10:57:14,645 - ==> Best [Top1: 23.640   Top5: 56.320   Sparsity:0.00   Params: 1341960 on epoch: 6]
2024-02-17 10:57:14,645 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 10:57:14,736 - 

2024-02-17 10:57:14,736 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:57:24,783 - Epoch: [7][  100/  391]    Overall Loss 2.656388    Objective Loss 2.656388                                        LR 0.100000    Time 0.100390    
2024-02-17 10:57:33,820 - Epoch: [7][  200/  391]    Overall Loss 2.651310    Objective Loss 2.651310                                        LR 0.100000    Time 0.095357    
2024-02-17 10:57:42,819 - Epoch: [7][  300/  391]    Overall Loss 2.638051    Objective Loss 2.638051                                        LR 0.100000    Time 0.093554    
2024-02-17 10:57:51,052 - Epoch: [7][  391/  391]    Overall Loss 2.628637    Objective Loss 2.628637    Top1 29.807692    Top5 65.865385    LR 0.100000    Time 0.092828    
2024-02-17 10:57:51,161 - --- validate (epoch=7)-----------
2024-02-17 10:57:51,162 - 10000 samples (128 per mini-batch)
2024-02-17 10:57:53,739 - Epoch: [7][   79/   79]    Loss 2.928358    Top1 26.420000    Top5 58.360000    
2024-02-17 10:57:53,845 - ==> Top1: 26.420    Top5: 58.360    Loss: 2.928

2024-02-17 10:57:53,864 - ==> Best [Top1: 26.420   Top5: 58.360   Sparsity:0.00   Params: 1341960 on epoch: 7]
2024-02-17 10:57:53,864 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 10:57:53,955 - 

2024-02-17 10:57:53,955 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:58:04,036 - Epoch: [8][  100/  391]    Overall Loss 2.557369    Objective Loss 2.557369                                        LR 0.100000    Time 0.100738    
2024-02-17 10:58:13,281 - Epoch: [8][  200/  391]    Overall Loss 2.552592    Objective Loss 2.552592                                        LR 0.100000    Time 0.096568    
2024-02-17 10:58:22,797 - Epoch: [8][  300/  391]    Overall Loss 2.547581    Objective Loss 2.547581                                        LR 0.100000    Time 0.096082    
2024-02-17 10:58:31,873 - Epoch: [8][  391/  391]    Overall Loss 2.535819    Objective Loss 2.535819    Top1 30.769231    Top5 67.788462    LR 0.100000    Time 0.096922    
2024-02-17 10:58:31,999 - --- validate (epoch=8)-----------
2024-02-17 10:58:32,000 - 10000 samples (128 per mini-batch)
2024-02-17 10:58:34,520 - Epoch: [8][   79/   79]    Loss 2.706972    Top1 29.560000    Top5 62.900000    
2024-02-17 10:58:34,640 - ==> Top1: 29.560    Top5: 62.900    Loss: 2.707

2024-02-17 10:58:34,658 - ==> Best [Top1: 29.560   Top5: 62.900   Sparsity:0.00   Params: 1341960 on epoch: 8]
2024-02-17 10:58:34,659 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 10:58:34,750 - 

2024-02-17 10:58:34,751 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:58:44,627 - Epoch: [9][  100/  391]    Overall Loss 2.443809    Objective Loss 2.443809                                        LR 0.100000    Time 0.098683    
2024-02-17 10:58:53,787 - Epoch: [9][  200/  391]    Overall Loss 2.461339    Objective Loss 2.461339                                        LR 0.100000    Time 0.095119    
2024-02-17 10:59:02,925 - Epoch: [9][  300/  391]    Overall Loss 2.454683    Objective Loss 2.454683                                        LR 0.100000    Time 0.093860    
2024-02-17 10:59:11,275 - Epoch: [9][  391/  391]    Overall Loss 2.455477    Objective Loss 2.455477    Top1 37.019231    Top5 63.942308    LR 0.100000    Time 0.093358    
2024-02-17 10:59:11,390 - --- validate (epoch=9)-----------
2024-02-17 10:59:11,391 - 10000 samples (128 per mini-batch)
2024-02-17 10:59:13,933 - Epoch: [9][   79/   79]    Loss 3.159537    Top1 24.220000    Top5 55.490000    
2024-02-17 10:59:14,025 - ==> Top1: 24.220    Top5: 55.490    Loss: 3.160

2024-02-17 10:59:14,044 - ==> Best [Top1: 29.560   Top5: 62.900   Sparsity:0.00   Params: 1341960 on epoch: 8]
2024-02-17 10:59:14,044 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 10:59:14,119 - 

2024-02-17 10:59:14,120 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:59:24,291 - Epoch: [10][  100/  391]    Overall Loss 2.413701    Objective Loss 2.413701                                        LR 0.100000    Time 0.101646    
2024-02-17 10:59:33,183 - Epoch: [10][  200/  391]    Overall Loss 2.394479    Objective Loss 2.394479                                        LR 0.100000    Time 0.095259    
2024-02-17 10:59:42,344 - Epoch: [10][  300/  391]    Overall Loss 2.385760    Objective Loss 2.385760                                        LR 0.100000    Time 0.094029    
2024-02-17 10:59:50,945 - Epoch: [10][  391/  391]    Overall Loss 2.385589    Objective Loss 2.385589    Top1 31.250000    Top5 69.711538    LR 0.100000    Time 0.094130    
2024-02-17 10:59:51,058 - --- validate (epoch=10)-----------
2024-02-17 10:59:51,058 - 10000 samples (128 per mini-batch)
2024-02-17 10:59:53,593 - Epoch: [10][   79/   79]    Loss 2.457716    Top1 33.660000    Top5 68.120000    
2024-02-17 10:59:53,739 - ==> Top1: 33.660    Top5: 68.120    Loss: 2.458

2024-02-17 10:59:53,759 - ==> Best [Top1: 33.660   Top5: 68.120   Sparsity:0.00   Params: 1341960 on epoch: 10]
2024-02-17 10:59:53,760 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 10:59:53,851 - 

2024-02-17 10:59:53,851 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:00:03,571 - Epoch: [11][  100/  391]    Overall Loss 2.344371    Objective Loss 2.344371                                        LR 0.100000    Time 0.097130    
2024-02-17 11:00:12,818 - Epoch: [11][  200/  391]    Overall Loss 2.329532    Objective Loss 2.329532                                        LR 0.100000    Time 0.094778    
2024-02-17 11:00:22,205 - Epoch: [11][  300/  391]    Overall Loss 2.320764    Objective Loss 2.320764                                        LR 0.100000    Time 0.094457    
2024-02-17 11:00:30,703 - Epoch: [11][  391/  391]    Overall Loss 2.318187    Objective Loss 2.318187    Top1 40.384615    Top5 67.307692    LR 0.100000    Time 0.094197    
2024-02-17 11:00:30,864 - --- validate (epoch=11)-----------
2024-02-17 11:00:30,864 - 10000 samples (128 per mini-batch)
2024-02-17 11:00:33,362 - Epoch: [11][   79/   79]    Loss 2.570210    Top1 32.130000    Top5 66.130000    
2024-02-17 11:00:33,466 - ==> Top1: 32.130    Top5: 66.130    Loss: 2.570

2024-02-17 11:00:33,484 - ==> Best [Top1: 33.660   Top5: 68.120   Sparsity:0.00   Params: 1341960 on epoch: 10]
2024-02-17 11:00:33,485 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:00:33,568 - 

2024-02-17 11:00:33,568 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:00:43,368 - Epoch: [12][  100/  391]    Overall Loss 2.279788    Objective Loss 2.279788                                        LR 0.100000    Time 0.097935    
2024-02-17 11:00:52,638 - Epoch: [12][  200/  391]    Overall Loss 2.269952    Objective Loss 2.269952                                        LR 0.100000    Time 0.095293    
2024-02-17 11:01:01,870 - Epoch: [12][  300/  391]    Overall Loss 2.270224    Objective Loss 2.270224                                        LR 0.100000    Time 0.094287    
2024-02-17 11:01:10,399 - Epoch: [12][  391/  391]    Overall Loss 2.271273    Objective Loss 2.271273    Top1 36.057692    Top5 73.076923    LR 0.100000    Time 0.094145    
2024-02-17 11:01:10,543 - --- validate (epoch=12)-----------
2024-02-17 11:01:10,544 - 10000 samples (128 per mini-batch)
2024-02-17 11:01:13,382 - Epoch: [12][   79/   79]    Loss 2.388325    Top1 36.210000    Top5 69.180000    
2024-02-17 11:01:13,559 - ==> Top1: 36.210    Top5: 69.180    Loss: 2.388

2024-02-17 11:01:13,577 - ==> Best [Top1: 36.210   Top5: 69.180   Sparsity:0.00   Params: 1341960 on epoch: 12]
2024-02-17 11:01:13,578 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:01:13,676 - 

2024-02-17 11:01:13,676 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:01:22,860 - Epoch: [13][  100/  391]    Overall Loss 2.193745    Objective Loss 2.193745                                        LR 0.100000    Time 0.091760    
2024-02-17 11:01:31,177 - Epoch: [13][  200/  391]    Overall Loss 2.201809    Objective Loss 2.201809                                        LR 0.100000    Time 0.087443    
2024-02-17 11:01:39,497 - Epoch: [13][  300/  391]    Overall Loss 2.204183    Objective Loss 2.204183                                        LR 0.100000    Time 0.086014    
2024-02-17 11:01:48,237 - Epoch: [13][  391/  391]    Overall Loss 2.202856    Objective Loss 2.202856    Top1 39.903846    Top5 68.750000    LR 0.100000    Time 0.088337    
2024-02-17 11:01:48,372 - --- validate (epoch=13)-----------
2024-02-17 11:01:48,373 - 10000 samples (128 per mini-batch)
2024-02-17 11:01:51,037 - Epoch: [13][   79/   79]    Loss 3.709286    Top1 22.120000    Top5 47.120000    
2024-02-17 11:01:51,199 - ==> Top1: 22.120    Top5: 47.120    Loss: 3.709

2024-02-17 11:01:51,209 - ==> Best [Top1: 36.210   Top5: 69.180   Sparsity:0.00   Params: 1341960 on epoch: 12]
2024-02-17 11:01:51,209 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:01:51,279 - 

2024-02-17 11:01:51,279 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:02:01,318 - Epoch: [14][  100/  391]    Overall Loss 2.149988    Objective Loss 2.149988                                        LR 0.100000    Time 0.100316    
2024-02-17 11:02:10,782 - Epoch: [14][  200/  391]    Overall Loss 2.156148    Objective Loss 2.156148                                        LR 0.100000    Time 0.097451    
2024-02-17 11:02:20,127 - Epoch: [14][  300/  391]    Overall Loss 2.161825    Objective Loss 2.161825                                        LR 0.100000    Time 0.096100    
2024-02-17 11:02:28,227 - Epoch: [14][  391/  391]    Overall Loss 2.157114    Objective Loss 2.157114    Top1 41.826923    Top5 75.000000    LR 0.100000    Time 0.094440    
2024-02-17 11:02:28,374 - --- validate (epoch=14)-----------
2024-02-17 11:02:28,375 - 10000 samples (128 per mini-batch)
2024-02-17 11:02:30,576 - Epoch: [14][   79/   79]    Loss 2.429733    Top1 36.470000    Top5 68.480000    
2024-02-17 11:02:30,739 - ==> Top1: 36.470    Top5: 68.480    Loss: 2.430

2024-02-17 11:02:30,755 - ==> Best [Top1: 36.470   Top5: 68.480   Sparsity:0.00   Params: 1341960 on epoch: 14]
2024-02-17 11:02:30,755 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:02:30,849 - 

2024-02-17 11:02:30,849 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:02:40,675 - Epoch: [15][  100/  391]    Overall Loss 2.120016    Objective Loss 2.120016                                        LR 0.100000    Time 0.098177    
2024-02-17 11:02:50,136 - Epoch: [15][  200/  391]    Overall Loss 2.116726    Objective Loss 2.116726                                        LR 0.100000    Time 0.096368    
2024-02-17 11:02:59,521 - Epoch: [15][  300/  391]    Overall Loss 2.114391    Objective Loss 2.114391                                        LR 0.100000    Time 0.095510    
2024-02-17 11:03:08,207 - Epoch: [15][  391/  391]    Overall Loss 2.116584    Objective Loss 2.116584    Top1 47.596154    Top5 78.846154    LR 0.100000    Time 0.095485    
2024-02-17 11:03:08,333 - --- validate (epoch=15)-----------
2024-02-17 11:03:08,334 - 10000 samples (128 per mini-batch)
2024-02-17 11:03:11,096 - Epoch: [15][   79/   79]    Loss 2.365633    Top1 37.020000    Top5 70.350000    
2024-02-17 11:03:11,208 - ==> Top1: 37.020    Top5: 70.350    Loss: 2.366

2024-02-17 11:03:11,221 - ==> Best [Top1: 37.020   Top5: 70.350   Sparsity:0.00   Params: 1341960 on epoch: 15]
2024-02-17 11:03:11,221 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:03:11,324 - 

2024-02-17 11:03:11,325 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:03:21,728 - Epoch: [16][  100/  391]    Overall Loss 2.047925    Objective Loss 2.047925                                        LR 0.100000    Time 0.103954    
2024-02-17 11:03:31,165 - Epoch: [16][  200/  391]    Overall Loss 2.072812    Objective Loss 2.072812                                        LR 0.100000    Time 0.099138    
2024-02-17 11:03:39,895 - Epoch: [16][  300/  391]    Overall Loss 2.064370    Objective Loss 2.064370                                        LR 0.100000    Time 0.095176    
2024-02-17 11:03:46,301 - Epoch: [16][  391/  391]    Overall Loss 2.064254    Objective Loss 2.064254    Top1 44.230769    Top5 77.403846    LR 0.100000    Time 0.089401    
2024-02-17 11:03:46,426 - --- validate (epoch=16)-----------
2024-02-17 11:03:46,426 - 10000 samples (128 per mini-batch)
2024-02-17 11:03:49,047 - Epoch: [16][   79/   79]    Loss 2.320833    Top1 38.740000    Top5 71.180000    
2024-02-17 11:03:49,195 - ==> Top1: 38.740    Top5: 71.180    Loss: 2.321

2024-02-17 11:03:49,213 - ==> Best [Top1: 38.740   Top5: 71.180   Sparsity:0.00   Params: 1341960 on epoch: 16]
2024-02-17 11:03:49,214 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:03:49,324 - 

2024-02-17 11:03:49,325 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:03:58,527 - Epoch: [17][  100/  391]    Overall Loss 2.010597    Objective Loss 2.010597                                        LR 0.100000    Time 0.091955    
2024-02-17 11:04:07,190 - Epoch: [17][  200/  391]    Overall Loss 2.013927    Objective Loss 2.013927                                        LR 0.100000    Time 0.089264    
2024-02-17 11:04:16,474 - Epoch: [17][  300/  391]    Overall Loss 2.022046    Objective Loss 2.022046                                        LR 0.100000    Time 0.090441    
2024-02-17 11:04:25,160 - Epoch: [17][  391/  391]    Overall Loss 2.025805    Objective Loss 2.025805    Top1 44.230769    Top5 77.403846    LR 0.100000    Time 0.091596    
2024-02-17 11:04:25,259 - --- validate (epoch=17)-----------
2024-02-17 11:04:25,260 - 10000 samples (128 per mini-batch)
2024-02-17 11:04:27,991 - Epoch: [17][   79/   79]    Loss 2.210307    Top1 40.720000    Top5 73.640000    
2024-02-17 11:04:28,171 - ==> Top1: 40.720    Top5: 73.640    Loss: 2.210

2024-02-17 11:04:28,189 - ==> Best [Top1: 40.720   Top5: 73.640   Sparsity:0.00   Params: 1341960 on epoch: 17]
2024-02-17 11:04:28,190 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:04:28,286 - 

2024-02-17 11:04:28,286 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:04:38,759 - Epoch: [18][  100/  391]    Overall Loss 2.002099    Objective Loss 2.002099                                        LR 0.100000    Time 0.104654    
2024-02-17 11:04:48,255 - Epoch: [18][  200/  391]    Overall Loss 1.996806    Objective Loss 1.996806                                        LR 0.100000    Time 0.099782    
2024-02-17 11:04:57,517 - Epoch: [18][  300/  391]    Overall Loss 2.003348    Objective Loss 2.003348                                        LR 0.100000    Time 0.097376    
2024-02-17 11:05:06,013 - Epoch: [18][  391/  391]    Overall Loss 2.002153    Objective Loss 2.002153    Top1 42.788462    Top5 75.000000    LR 0.100000    Time 0.096431    
2024-02-17 11:05:06,143 - --- validate (epoch=18)-----------
2024-02-17 11:05:06,144 - 10000 samples (128 per mini-batch)
2024-02-17 11:05:08,713 - Epoch: [18][   79/   79]    Loss 2.192672    Top1 41.320000    Top5 74.030000    
2024-02-17 11:05:08,874 - ==> Top1: 41.320    Top5: 74.030    Loss: 2.193

2024-02-17 11:05:08,892 - ==> Best [Top1: 41.320   Top5: 74.030   Sparsity:0.00   Params: 1341960 on epoch: 18]
2024-02-17 11:05:08,893 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:05:08,988 - 

2024-02-17 11:05:08,988 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:05:18,845 - Epoch: [19][  100/  391]    Overall Loss 1.957174    Objective Loss 1.957174                                        LR 0.100000    Time 0.098489    
2024-02-17 11:05:28,077 - Epoch: [19][  200/  391]    Overall Loss 1.959424    Objective Loss 1.959424                                        LR 0.100000    Time 0.095376    
2024-02-17 11:05:37,371 - Epoch: [19][  300/  391]    Overall Loss 1.953811    Objective Loss 1.953811                                        LR 0.100000    Time 0.094549    
2024-02-17 11:05:45,963 - Epoch: [19][  391/  391]    Overall Loss 1.957145    Objective Loss 1.957145    Top1 46.634615    Top5 79.807692    LR 0.100000    Time 0.094506    
2024-02-17 11:05:46,083 - --- validate (epoch=19)-----------
2024-02-17 11:05:46,083 - 10000 samples (128 per mini-batch)
2024-02-17 11:05:48,716 - Epoch: [19][   79/   79]    Loss 2.076991    Top1 42.880000    Top5 75.840000    
2024-02-17 11:05:48,848 - ==> Top1: 42.880    Top5: 75.840    Loss: 2.077

2024-02-17 11:05:48,865 - ==> Best [Top1: 42.880   Top5: 75.840   Sparsity:0.00   Params: 1341960 on epoch: 19]
2024-02-17 11:05:48,865 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:05:48,957 - 

2024-02-17 11:05:48,957 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:05:57,700 - Epoch: [20][  100/  391]    Overall Loss 1.917232    Objective Loss 1.917232                                        LR 0.100000    Time 0.087359    
2024-02-17 11:06:05,600 - Epoch: [20][  200/  391]    Overall Loss 1.918055    Objective Loss 1.918055                                        LR 0.100000    Time 0.083159    
2024-02-17 11:06:15,515 - Epoch: [20][  300/  391]    Overall Loss 1.924079    Objective Loss 1.924079                                        LR 0.100000    Time 0.088471    
2024-02-17 11:06:24,404 - Epoch: [20][  391/  391]    Overall Loss 1.920631    Objective Loss 1.920631    Top1 50.480769    Top5 82.692308    LR 0.100000    Time 0.090604    
2024-02-17 11:06:24,582 - --- validate (epoch=20)-----------
2024-02-17 11:06:24,583 - 10000 samples (128 per mini-batch)
2024-02-17 11:06:27,556 - Epoch: [20][   79/   79]    Loss 2.159016    Top1 40.930000    Top5 75.230000    
2024-02-17 11:06:27,704 - ==> Top1: 40.930    Top5: 75.230    Loss: 2.159

2024-02-17 11:06:27,722 - ==> Best [Top1: 42.880   Top5: 75.840   Sparsity:0.00   Params: 1341960 on epoch: 19]
2024-02-17 11:06:27,722 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:06:27,797 - 

2024-02-17 11:06:27,797 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:06:37,537 - Epoch: [21][  100/  391]    Overall Loss 1.908032    Objective Loss 1.908032                                        LR 0.100000    Time 0.097327    
2024-02-17 11:06:46,683 - Epoch: [21][  200/  391]    Overall Loss 1.902803    Objective Loss 1.902803                                        LR 0.100000    Time 0.094365    
2024-02-17 11:06:55,822 - Epoch: [21][  300/  391]    Overall Loss 1.902629    Objective Loss 1.902629                                        LR 0.100000    Time 0.093356    
2024-02-17 11:07:03,689 - Epoch: [21][  391/  391]    Overall Loss 1.899149    Objective Loss 1.899149    Top1 44.230769    Top5 75.961538    LR 0.100000    Time 0.091739    
2024-02-17 11:07:03,822 - --- validate (epoch=21)-----------
2024-02-17 11:07:03,823 - 10000 samples (128 per mini-batch)
2024-02-17 11:07:06,519 - Epoch: [21][   79/   79]    Loss 2.036169    Top1 44.570000    Top5 76.830000    
2024-02-17 11:07:06,633 - ==> Top1: 44.570    Top5: 76.830    Loss: 2.036

2024-02-17 11:07:06,653 - ==> Best [Top1: 44.570   Top5: 76.830   Sparsity:0.00   Params: 1341960 on epoch: 21]
2024-02-17 11:07:06,654 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:07:06,746 - 

2024-02-17 11:07:06,747 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:07:16,629 - Epoch: [22][  100/  391]    Overall Loss 1.867678    Objective Loss 1.867678                                        LR 0.100000    Time 0.098751    
2024-02-17 11:07:25,916 - Epoch: [22][  200/  391]    Overall Loss 1.858907    Objective Loss 1.858907                                        LR 0.100000    Time 0.095782    
2024-02-17 11:07:35,347 - Epoch: [22][  300/  391]    Overall Loss 1.868168    Objective Loss 1.868168                                        LR 0.100000    Time 0.095275    
2024-02-17 11:07:44,056 - Epoch: [22][  391/  391]    Overall Loss 1.867814    Objective Loss 1.867814    Top1 46.634615    Top5 80.288462    LR 0.100000    Time 0.095363    
2024-02-17 11:07:44,188 - --- validate (epoch=22)-----------
2024-02-17 11:07:44,188 - 10000 samples (128 per mini-batch)
2024-02-17 11:07:46,986 - Epoch: [22][   79/   79]    Loss 2.015076    Top1 44.960000    Top5 77.410000    
2024-02-17 11:07:47,086 - ==> Top1: 44.960    Top5: 77.410    Loss: 2.015

2024-02-17 11:07:47,103 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:07:47,103 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:07:47,207 - 

2024-02-17 11:07:47,208 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:07:56,932 - Epoch: [23][  100/  391]    Overall Loss 1.836784    Objective Loss 1.836784                                        LR 0.100000    Time 0.097167    
2024-02-17 11:08:05,990 - Epoch: [23][  200/  391]    Overall Loss 1.845296    Objective Loss 1.845296                                        LR 0.100000    Time 0.093850    
2024-02-17 11:08:14,849 - Epoch: [23][  300/  391]    Overall Loss 1.845424    Objective Loss 1.845424                                        LR 0.100000    Time 0.092081    
2024-02-17 11:08:23,332 - Epoch: [23][  391/  391]    Overall Loss 1.846385    Objective Loss 1.846385    Top1 42.788462    Top5 74.519231    LR 0.100000    Time 0.092336    
2024-02-17 11:08:23,455 - --- validate (epoch=23)-----------
2024-02-17 11:08:23,456 - 10000 samples (128 per mini-batch)
2024-02-17 11:08:26,112 - Epoch: [23][   79/   79]    Loss 2.083110    Top1 44.040000    Top5 76.330000    
2024-02-17 11:08:26,290 - ==> Top1: 44.040    Top5: 76.330    Loss: 2.083

2024-02-17 11:08:26,309 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:08:26,309 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:08:26,386 - 

2024-02-17 11:08:26,386 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:08:36,635 - Epoch: [24][  100/  391]    Overall Loss 1.795736    Objective Loss 1.795736                                        LR 0.100000    Time 0.102406    
2024-02-17 11:08:45,892 - Epoch: [24][  200/  391]    Overall Loss 1.800785    Objective Loss 1.800785                                        LR 0.100000    Time 0.097467    
2024-02-17 11:08:55,139 - Epoch: [24][  300/  391]    Overall Loss 1.814031    Objective Loss 1.814031                                        LR 0.100000    Time 0.095783    
2024-02-17 11:09:03,884 - Epoch: [24][  391/  391]    Overall Loss 1.819678    Objective Loss 1.819678    Top1 53.365385    Top5 83.653846    LR 0.100000    Time 0.095847    
2024-02-17 11:09:04,076 - --- validate (epoch=24)-----------
2024-02-17 11:09:04,076 - 10000 samples (128 per mini-batch)
2024-02-17 11:09:06,620 - Epoch: [24][   79/   79]    Loss 2.628696    Top1 35.390000    Top5 68.330000    
2024-02-17 11:09:06,739 - ==> Top1: 35.390    Top5: 68.330    Loss: 2.629

2024-02-17 11:09:06,754 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:09:06,754 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:09:06,833 - 

2024-02-17 11:09:06,833 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:09:16,671 - Epoch: [25][  100/  391]    Overall Loss 1.772238    Objective Loss 1.772238                                        LR 0.100000    Time 0.098310    
2024-02-17 11:09:25,606 - Epoch: [25][  200/  391]    Overall Loss 1.775472    Objective Loss 1.775472                                        LR 0.100000    Time 0.093804    
2024-02-17 11:09:35,157 - Epoch: [25][  300/  391]    Overall Loss 1.788799    Objective Loss 1.788799                                        LR 0.100000    Time 0.094356    
2024-02-17 11:09:43,808 - Epoch: [25][  391/  391]    Overall Loss 1.791222    Objective Loss 1.791222    Top1 48.076923    Top5 81.730769    LR 0.100000    Time 0.094510    
2024-02-17 11:09:43,944 - --- validate (epoch=25)-----------
2024-02-17 11:09:43,945 - 10000 samples (128 per mini-batch)
2024-02-17 11:09:46,527 - Epoch: [25][   79/   79]    Loss 2.358221    Top1 39.670000    Top5 71.360000    
2024-02-17 11:09:46,699 - ==> Top1: 39.670    Top5: 71.360    Loss: 2.358

2024-02-17 11:09:46,717 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:09:46,717 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:09:46,793 - 

2024-02-17 11:09:46,794 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:09:56,982 - Epoch: [26][  100/  391]    Overall Loss 1.733931    Objective Loss 1.733931                                        LR 0.100000    Time 0.101809    
2024-02-17 11:10:06,027 - Epoch: [26][  200/  391]    Overall Loss 1.755922    Objective Loss 1.755922                                        LR 0.100000    Time 0.096105    
2024-02-17 11:10:15,973 - Epoch: [26][  300/  391]    Overall Loss 1.762424    Objective Loss 1.762424                                        LR 0.100000    Time 0.097207    
2024-02-17 11:10:24,651 - Epoch: [26][  391/  391]    Overall Loss 1.766247    Objective Loss 1.766247    Top1 51.442308    Top5 82.692308    LR 0.100000    Time 0.096768    
2024-02-17 11:10:24,763 - --- validate (epoch=26)-----------
2024-02-17 11:10:24,764 - 10000 samples (128 per mini-batch)
2024-02-17 11:10:27,385 - Epoch: [26][   79/   79]    Loss 2.096929    Top1 43.990000    Top5 76.130000    
2024-02-17 11:10:27,495 - ==> Top1: 43.990    Top5: 76.130    Loss: 2.097

2024-02-17 11:10:27,506 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:10:27,507 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:10:27,584 - 

2024-02-17 11:10:27,584 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:10:37,702 - Epoch: [27][  100/  391]    Overall Loss 1.701968    Objective Loss 1.701968                                        LR 0.100000    Time 0.101098    
2024-02-17 11:10:46,602 - Epoch: [27][  200/  391]    Overall Loss 1.725346    Objective Loss 1.725346                                        LR 0.100000    Time 0.095027    
2024-02-17 11:10:56,143 - Epoch: [27][  300/  391]    Overall Loss 1.735624    Objective Loss 1.735624                                        LR 0.100000    Time 0.095139    
2024-02-17 11:11:04,887 - Epoch: [27][  391/  391]    Overall Loss 1.742071    Objective Loss 1.742071    Top1 48.076923    Top5 81.250000    LR 0.100000    Time 0.095347    
2024-02-17 11:11:05,012 - --- validate (epoch=27)-----------
2024-02-17 11:11:05,013 - 10000 samples (128 per mini-batch)
2024-02-17 11:11:07,633 - Epoch: [27][   79/   79]    Loss 2.224197    Top1 41.210000    Top5 73.400000    
2024-02-17 11:11:07,739 - ==> Top1: 41.210    Top5: 73.400    Loss: 2.224

2024-02-17 11:11:07,757 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:11:07,757 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:11:07,834 - 

2024-02-17 11:11:07,835 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:11:18,098 - Epoch: [28][  100/  391]    Overall Loss 1.722163    Objective Loss 1.722163                                        LR 0.100000    Time 0.102556    
2024-02-17 11:11:27,170 - Epoch: [28][  200/  391]    Overall Loss 1.715269    Objective Loss 1.715269                                        LR 0.100000    Time 0.096612    
2024-02-17 11:11:36,645 - Epoch: [28][  300/  391]    Overall Loss 1.721193    Objective Loss 1.721193                                        LR 0.100000    Time 0.095977    
2024-02-17 11:11:45,348 - Epoch: [28][  391/  391]    Overall Loss 1.723180    Objective Loss 1.723180    Top1 50.961538    Top5 80.769231    LR 0.100000    Time 0.095883    
2024-02-17 11:11:45,468 - --- validate (epoch=28)-----------
2024-02-17 11:11:45,469 - 10000 samples (128 per mini-batch)
2024-02-17 11:11:48,214 - Epoch: [28][   79/   79]    Loss 2.306765    Top1 40.140000    Top5 72.850000    
2024-02-17 11:11:48,334 - ==> Top1: 40.140    Top5: 72.850    Loss: 2.307

2024-02-17 11:11:48,346 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:11:48,346 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:11:48,421 - 

2024-02-17 11:11:48,421 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:11:58,396 - Epoch: [29][  100/  391]    Overall Loss 1.684613    Objective Loss 1.684613                                        LR 0.100000    Time 0.099674    
2024-02-17 11:12:06,595 - Epoch: [29][  200/  391]    Overall Loss 1.698510    Objective Loss 1.698510                                        LR 0.100000    Time 0.090810    
2024-02-17 11:12:13,872 - Epoch: [29][  300/  391]    Overall Loss 1.710720    Objective Loss 1.710720                                        LR 0.100000    Time 0.084781    
2024-02-17 11:12:20,398 - Epoch: [29][  391/  391]    Overall Loss 1.714815    Objective Loss 1.714815    Top1 50.000000    Top5 83.173077    LR 0.100000    Time 0.081731    
2024-02-17 11:12:20,579 - --- validate (epoch=29)-----------
2024-02-17 11:12:20,580 - 10000 samples (128 per mini-batch)
2024-02-17 11:12:23,212 - Epoch: [29][   79/   79]    Loss 2.234177    Top1 42.070000    Top5 73.000000    
2024-02-17 11:12:23,324 - ==> Top1: 42.070    Top5: 73.000    Loss: 2.234

2024-02-17 11:12:23,342 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:12:23,342 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:12:23,417 - 

2024-02-17 11:12:23,417 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:12:32,511 - Epoch: [30][  100/  391]    Overall Loss 1.644164    Objective Loss 1.644164                                        LR 0.100000    Time 0.090869    
2024-02-17 11:12:41,806 - Epoch: [30][  200/  391]    Overall Loss 1.668109    Objective Loss 1.668109                                        LR 0.100000    Time 0.091884    
2024-02-17 11:12:51,043 - Epoch: [30][  300/  391]    Overall Loss 1.679802    Objective Loss 1.679802                                        LR 0.100000    Time 0.092028    
2024-02-17 11:12:59,291 - Epoch: [30][  391/  391]    Overall Loss 1.683561    Objective Loss 1.683561    Top1 51.923077    Top5 85.096154    LR 0.100000    Time 0.091694    
2024-02-17 11:12:59,430 - --- validate (epoch=30)-----------
2024-02-17 11:12:59,430 - 10000 samples (128 per mini-batch)
2024-02-17 11:13:02,053 - Epoch: [30][   79/   79]    Loss 2.379153    Top1 39.670000    Top5 71.900000    
2024-02-17 11:13:02,170 - ==> Top1: 39.670    Top5: 71.900    Loss: 2.379

2024-02-17 11:13:02,188 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:13:02,189 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:13:02,265 - 

2024-02-17 11:13:02,265 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:13:12,445 - Epoch: [31][  100/  391]    Overall Loss 1.661652    Objective Loss 1.661652                                        LR 0.100000    Time 0.101729    
2024-02-17 11:13:21,717 - Epoch: [31][  200/  391]    Overall Loss 1.659721    Objective Loss 1.659721                                        LR 0.100000    Time 0.097200    
2024-02-17 11:13:31,122 - Epoch: [31][  300/  391]    Overall Loss 1.663134    Objective Loss 1.663134                                        LR 0.100000    Time 0.096133    
2024-02-17 11:13:39,915 - Epoch: [31][  391/  391]    Overall Loss 1.668387    Objective Loss 1.668387    Top1 56.730769    Top5 82.692308    LR 0.100000    Time 0.096238    
2024-02-17 11:13:40,074 - --- validate (epoch=31)-----------
2024-02-17 11:13:40,075 - 10000 samples (128 per mini-batch)
2024-02-17 11:13:42,790 - Epoch: [31][   79/   79]    Loss 1.868021    Top1 48.600000    Top5 80.400000    
2024-02-17 11:13:42,901 - ==> Top1: 48.600    Top5: 80.400    Loss: 1.868

2024-02-17 11:13:42,919 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:13:42,920 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:13:43,013 - 

2024-02-17 11:13:43,013 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:13:52,957 - Epoch: [32][  100/  391]    Overall Loss 1.627926    Objective Loss 1.627926                                        LR 0.100000    Time 0.099359    
2024-02-17 11:14:02,640 - Epoch: [32][  200/  391]    Overall Loss 1.643109    Objective Loss 1.643109                                        LR 0.100000    Time 0.098070    
2024-02-17 11:14:11,715 - Epoch: [32][  300/  391]    Overall Loss 1.644081    Objective Loss 1.644081                                        LR 0.100000    Time 0.095615    
2024-02-17 11:14:20,225 - Epoch: [32][  391/  391]    Overall Loss 1.646557    Objective Loss 1.646557    Top1 48.076923    Top5 83.653846    LR 0.100000    Time 0.095114    
2024-02-17 11:14:20,404 - --- validate (epoch=32)-----------
2024-02-17 11:14:20,405 - 10000 samples (128 per mini-batch)
2024-02-17 11:14:23,248 - Epoch: [32][   79/   79]    Loss 1.976561    Top1 45.680000    Top5 78.180000    
2024-02-17 11:14:23,376 - ==> Top1: 45.680    Top5: 78.180    Loss: 1.977

2024-02-17 11:14:23,393 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:14:23,393 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:14:23,470 - 

2024-02-17 11:14:23,471 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:14:33,656 - Epoch: [33][  100/  391]    Overall Loss 1.595683    Objective Loss 1.595683                                        LR 0.100000    Time 0.101778    
2024-02-17 11:14:42,777 - Epoch: [33][  200/  391]    Overall Loss 1.606158    Objective Loss 1.606158                                        LR 0.100000    Time 0.096470    
2024-02-17 11:14:52,526 - Epoch: [33][  300/  391]    Overall Loss 1.621585    Objective Loss 1.621585                                        LR 0.100000    Time 0.096793    
2024-02-17 11:15:00,993 - Epoch: [33][  391/  391]    Overall Loss 1.629625    Objective Loss 1.629625    Top1 54.807692    Top5 82.211538    LR 0.100000    Time 0.095909    
2024-02-17 11:15:01,163 - --- validate (epoch=33)-----------
2024-02-17 11:15:01,163 - 10000 samples (128 per mini-batch)
2024-02-17 11:15:03,726 - Epoch: [33][   79/   79]    Loss 1.999365    Top1 46.460000    Top5 78.210000    
2024-02-17 11:15:03,821 - ==> Top1: 46.460    Top5: 78.210    Loss: 1.999

2024-02-17 11:15:03,832 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:15:03,832 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:15:03,907 - 

2024-02-17 11:15:03,908 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:15:14,270 - Epoch: [34][  100/  391]    Overall Loss 1.594835    Objective Loss 1.594835                                        LR 0.100000    Time 0.103552    
2024-02-17 11:15:23,709 - Epoch: [34][  200/  391]    Overall Loss 1.604233    Objective Loss 1.604233                                        LR 0.100000    Time 0.098945    
2024-02-17 11:15:33,020 - Epoch: [34][  300/  391]    Overall Loss 1.605438    Objective Loss 1.605438                                        LR 0.100000    Time 0.096984    
2024-02-17 11:15:41,974 - Epoch: [34][  391/  391]    Overall Loss 1.612352    Objective Loss 1.612352    Top1 54.807692    Top5 87.500000    LR 0.100000    Time 0.097301    
2024-02-17 11:15:42,152 - --- validate (epoch=34)-----------
2024-02-17 11:15:42,153 - 10000 samples (128 per mini-batch)
2024-02-17 11:15:44,793 - Epoch: [34][   79/   79]    Loss 2.018327    Top1 46.060000    Top5 77.830000    
2024-02-17 11:15:44,978 - ==> Top1: 46.060    Top5: 77.830    Loss: 2.018

2024-02-17 11:15:44,997 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:15:44,998 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:15:45,074 - 

2024-02-17 11:15:45,074 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:15:54,316 - Epoch: [35][  100/  391]    Overall Loss 1.575489    Objective Loss 1.575489                                        LR 0.100000    Time 0.092343    
2024-02-17 11:16:03,666 - Epoch: [35][  200/  391]    Overall Loss 1.586312    Objective Loss 1.586312                                        LR 0.100000    Time 0.092900    
2024-02-17 11:16:12,279 - Epoch: [35][  300/  391]    Overall Loss 1.602771    Objective Loss 1.602771                                        LR 0.100000    Time 0.090625    
2024-02-17 11:16:21,058 - Epoch: [35][  391/  391]    Overall Loss 1.606899    Objective Loss 1.606899    Top1 51.442308    Top5 86.057692    LR 0.100000    Time 0.091974    
2024-02-17 11:16:21,206 - --- validate (epoch=35)-----------
2024-02-17 11:16:21,207 - 10000 samples (128 per mini-batch)
2024-02-17 11:16:23,841 - Epoch: [35][   79/   79]    Loss 1.989054    Top1 45.770000    Top5 77.170000    
2024-02-17 11:16:23,941 - ==> Top1: 45.770    Top5: 77.170    Loss: 1.989

2024-02-17 11:16:23,960 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:16:23,960 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:16:24,038 - 

2024-02-17 11:16:24,038 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:16:34,304 - Epoch: [36][  100/  391]    Overall Loss 1.539836    Objective Loss 1.539836                                        LR 0.100000    Time 0.102583    
2024-02-17 11:16:43,749 - Epoch: [36][  200/  391]    Overall Loss 1.562597    Objective Loss 1.562597                                        LR 0.100000    Time 0.098492    
2024-02-17 11:16:53,038 - Epoch: [36][  300/  391]    Overall Loss 1.576128    Objective Loss 1.576128                                        LR 0.100000    Time 0.096608    
2024-02-17 11:17:01,761 - Epoch: [36][  391/  391]    Overall Loss 1.586168    Objective Loss 1.586168    Top1 52.403846    Top5 86.057692    LR 0.100000    Time 0.096421    
2024-02-17 11:17:01,935 - --- validate (epoch=36)-----------
2024-02-17 11:17:01,936 - 10000 samples (128 per mini-batch)
2024-02-17 11:17:04,470 - Epoch: [36][   79/   79]    Loss 1.807605    Top1 50.040000    Top5 81.320000    
2024-02-17 11:17:04,653 - ==> Top1: 50.040    Top5: 81.320    Loss: 1.808

2024-02-17 11:17:04,672 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:17:04,673 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:17:04,768 - 

2024-02-17 11:17:04,768 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:17:14,633 - Epoch: [37][  100/  391]    Overall Loss 1.568262    Objective Loss 1.568262                                        LR 0.100000    Time 0.098579    
2024-02-17 11:17:24,009 - Epoch: [37][  200/  391]    Overall Loss 1.577324    Objective Loss 1.577324                                        LR 0.100000    Time 0.096142    
2024-02-17 11:17:33,260 - Epoch: [37][  300/  391]    Overall Loss 1.573395    Objective Loss 1.573395                                        LR 0.100000    Time 0.094916    
2024-02-17 11:17:42,026 - Epoch: [37][  391/  391]    Overall Loss 1.579745    Objective Loss 1.579745    Top1 52.884615    Top5 84.134615    LR 0.100000    Time 0.095234    
2024-02-17 11:17:42,168 - --- validate (epoch=37)-----------
2024-02-17 11:17:42,169 - 10000 samples (128 per mini-batch)
2024-02-17 11:17:44,806 - Epoch: [37][   79/   79]    Loss 2.064832    Top1 45.690000    Top5 76.750000    
2024-02-17 11:17:44,965 - ==> Top1: 45.690    Top5: 76.750    Loss: 2.065

2024-02-17 11:17:44,984 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:17:44,985 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:17:45,060 - 

2024-02-17 11:17:45,060 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:17:55,238 - Epoch: [38][  100/  391]    Overall Loss 1.527588    Objective Loss 1.527588                                        LR 0.100000    Time 0.101704    
2024-02-17 11:18:04,476 - Epoch: [38][  200/  391]    Overall Loss 1.537939    Objective Loss 1.537939                                        LR 0.100000    Time 0.097018    
2024-02-17 11:18:13,983 - Epoch: [38][  300/  391]    Overall Loss 1.550885    Objective Loss 1.550885                                        LR 0.100000    Time 0.096355    
2024-02-17 11:18:22,858 - Epoch: [38][  391/  391]    Overall Loss 1.551893    Objective Loss 1.551893    Top1 51.923077    Top5 85.096154    LR 0.100000    Time 0.096615    
2024-02-17 11:18:22,990 - --- validate (epoch=38)-----------
2024-02-17 11:18:22,991 - 10000 samples (128 per mini-batch)
2024-02-17 11:18:25,677 - Epoch: [38][   79/   79]    Loss 1.979522    Top1 47.710000    Top5 78.950000    
2024-02-17 11:18:25,811 - ==> Top1: 47.710    Top5: 78.950    Loss: 1.980

2024-02-17 11:18:25,829 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:18:25,829 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:18:25,904 - 

2024-02-17 11:18:25,905 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:18:36,093 - Epoch: [39][  100/  391]    Overall Loss 1.531712    Objective Loss 1.531712                                        LR 0.100000    Time 0.101807    
2024-02-17 11:18:45,199 - Epoch: [39][  200/  391]    Overall Loss 1.542680    Objective Loss 1.542680                                        LR 0.100000    Time 0.096409    
2024-02-17 11:18:53,862 - Epoch: [39][  300/  391]    Overall Loss 1.548814    Objective Loss 1.548814                                        LR 0.100000    Time 0.093136    
2024-02-17 11:19:00,584 - Epoch: [39][  391/  391]    Overall Loss 1.550180    Objective Loss 1.550180    Top1 59.134615    Top5 87.019231    LR 0.100000    Time 0.088641    
2024-02-17 11:19:00,709 - --- validate (epoch=39)-----------
2024-02-17 11:19:00,710 - 10000 samples (128 per mini-batch)
2024-02-17 11:19:03,280 - Epoch: [39][   79/   79]    Loss 1.908074    Top1 47.730000    Top5 79.360000    
2024-02-17 11:19:03,401 - ==> Top1: 47.730    Top5: 79.360    Loss: 1.908

2024-02-17 11:19:03,418 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:19:03,419 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:19:03,493 - 

2024-02-17 11:19:03,493 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:19:13,784 - Epoch: [40][  100/  391]    Overall Loss 1.499845    Objective Loss 1.499845                                        LR 0.100000    Time 0.102837    
2024-02-17 11:19:22,701 - Epoch: [40][  200/  391]    Overall Loss 1.531605    Objective Loss 1.531605                                        LR 0.100000    Time 0.095977    
2024-02-17 11:19:32,073 - Epoch: [40][  300/  391]    Overall Loss 1.545265    Objective Loss 1.545265                                        LR 0.100000    Time 0.095211    
2024-02-17 11:19:39,014 - Epoch: [40][  391/  391]    Overall Loss 1.539392    Objective Loss 1.539392    Top1 58.173077    Top5 89.903846    LR 0.100000    Time 0.090795    
2024-02-17 11:19:39,152 - --- validate (epoch=40)-----------
2024-02-17 11:19:39,153 - 10000 samples (128 per mini-batch)
2024-02-17 11:19:41,902 - Epoch: [40][   79/   79]    Loss 1.845986    Top1 48.800000    Top5 80.410000    
2024-02-17 11:19:42,027 - ==> Top1: 48.800    Top5: 80.410    Loss: 1.846

2024-02-17 11:19:42,039 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:19:42,039 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:19:42,118 - 

2024-02-17 11:19:42,118 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:19:52,067 - Epoch: [41][  100/  391]    Overall Loss 1.485648    Objective Loss 1.485648                                        LR 0.100000    Time 0.099402    
2024-02-17 11:20:01,234 - Epoch: [41][  200/  391]    Overall Loss 1.491010    Objective Loss 1.491010                                        LR 0.100000    Time 0.095511    
2024-02-17 11:20:10,391 - Epoch: [41][  300/  391]    Overall Loss 1.509338    Objective Loss 1.509338                                        LR 0.100000    Time 0.094182    
2024-02-17 11:20:19,084 - Epoch: [41][  391/  391]    Overall Loss 1.520214    Objective Loss 1.520214    Top1 59.134615    Top5 88.942308    LR 0.100000    Time 0.094482    
2024-02-17 11:20:19,260 - --- validate (epoch=41)-----------
2024-02-17 11:20:19,261 - 10000 samples (128 per mini-batch)
2024-02-17 11:20:21,957 - Epoch: [41][   79/   79]    Loss 1.984574    Top1 46.930000    Top5 77.890000    
2024-02-17 11:20:22,063 - ==> Top1: 46.930    Top5: 77.890    Loss: 1.985

2024-02-17 11:20:22,081 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:20:22,082 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:20:22,157 - 

2024-02-17 11:20:22,157 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:20:32,175 - Epoch: [42][  100/  391]    Overall Loss 1.489325    Objective Loss 1.489325                                        LR 0.100000    Time 0.100103    
2024-02-17 11:20:41,763 - Epoch: [42][  200/  391]    Overall Loss 1.483331    Objective Loss 1.483331                                        LR 0.100000    Time 0.097963    
2024-02-17 11:20:50,926 - Epoch: [42][  300/  391]    Overall Loss 1.500461    Objective Loss 1.500461                                        LR 0.100000    Time 0.095837    
2024-02-17 11:20:59,593 - Epoch: [42][  391/  391]    Overall Loss 1.510962    Objective Loss 1.510962    Top1 56.250000    Top5 87.019231    LR 0.100000    Time 0.095686    
2024-02-17 11:20:59,727 - --- validate (epoch=42)-----------
2024-02-17 11:20:59,728 - 10000 samples (128 per mini-batch)
2024-02-17 11:21:02,462 - Epoch: [42][   79/   79]    Loss 1.952823    Top1 47.340000    Top5 78.560000    
2024-02-17 11:21:02,573 - ==> Top1: 47.340    Top5: 78.560    Loss: 1.953

2024-02-17 11:21:02,591 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:21:02,591 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:21:02,860 - 

2024-02-17 11:21:02,860 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:21:12,848 - Epoch: [43][  100/  391]    Overall Loss 1.457503    Objective Loss 1.457503                                        LR 0.100000    Time 0.099804    
2024-02-17 11:21:22,233 - Epoch: [43][  200/  391]    Overall Loss 1.475390    Objective Loss 1.475390                                        LR 0.100000    Time 0.096799    
2024-02-17 11:21:31,507 - Epoch: [43][  300/  391]    Overall Loss 1.492215    Objective Loss 1.492215                                        LR 0.100000    Time 0.095430    
2024-02-17 11:21:39,888 - Epoch: [43][  391/  391]    Overall Loss 1.502405    Objective Loss 1.502405    Top1 54.807692    Top5 82.211538    LR 0.100000    Time 0.094643    
2024-02-17 11:21:40,058 - --- validate (epoch=43)-----------
2024-02-17 11:21:40,059 - 10000 samples (128 per mini-batch)
2024-02-17 11:21:42,741 - Epoch: [43][   79/   79]    Loss 1.839245    Top1 48.860000    Top5 81.150000    
2024-02-17 11:21:42,839 - ==> Top1: 48.860    Top5: 81.150    Loss: 1.839

2024-02-17 11:21:42,850 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:21:42,850 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:21:42,923 - 

2024-02-17 11:21:42,923 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:21:53,064 - Epoch: [44][  100/  391]    Overall Loss 1.465794    Objective Loss 1.465794                                        LR 0.100000    Time 0.101333    
2024-02-17 11:22:00,347 - Epoch: [44][  200/  391]    Overall Loss 1.475357    Objective Loss 1.475357                                        LR 0.100000    Time 0.087061    
2024-02-17 11:22:09,321 - Epoch: [44][  300/  391]    Overall Loss 1.490347    Objective Loss 1.490347                                        LR 0.100000    Time 0.087936    
2024-02-17 11:22:18,036 - Epoch: [44][  391/  391]    Overall Loss 1.492887    Objective Loss 1.492887    Top1 54.807692    Top5 79.326923    LR 0.100000    Time 0.089749    
2024-02-17 11:22:18,152 - --- validate (epoch=44)-----------
2024-02-17 11:22:18,152 - 10000 samples (128 per mini-batch)
2024-02-17 11:22:20,797 - Epoch: [44][   79/   79]    Loss 1.917619    Top1 48.810000    Top5 79.840000    
2024-02-17 11:22:20,926 - ==> Top1: 48.810    Top5: 79.840    Loss: 1.918

2024-02-17 11:22:20,949 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:22:20,949 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:22:21,026 - 

2024-02-17 11:22:21,027 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:22:31,014 - Epoch: [45][  100/  391]    Overall Loss 1.433537    Objective Loss 1.433537                                        LR 0.100000    Time 0.099796    
2024-02-17 11:22:40,523 - Epoch: [45][  200/  391]    Overall Loss 1.449299    Objective Loss 1.449299                                        LR 0.100000    Time 0.097420    
2024-02-17 11:22:49,693 - Epoch: [45][  300/  391]    Overall Loss 1.465140    Objective Loss 1.465140                                        LR 0.100000    Time 0.095495    
2024-02-17 11:22:57,938 - Epoch: [45][  391/  391]    Overall Loss 1.475613    Objective Loss 1.475613    Top1 58.173077    Top5 87.019231    LR 0.100000    Time 0.094346    
2024-02-17 11:22:58,101 - --- validate (epoch=45)-----------
2024-02-17 11:22:58,102 - 10000 samples (128 per mini-batch)
2024-02-17 11:23:00,650 - Epoch: [45][   79/   79]    Loss 1.833996    Top1 49.530000    Top5 80.410000    
2024-02-17 11:23:00,818 - ==> Top1: 49.530    Top5: 80.410    Loss: 1.834

2024-02-17 11:23:00,838 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:23:00,838 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:23:00,912 - 

2024-02-17 11:23:00,913 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:23:10,965 - Epoch: [46][  100/  391]    Overall Loss 1.404987    Objective Loss 1.404987                                        LR 0.100000    Time 0.100444    
2024-02-17 11:23:20,467 - Epoch: [46][  200/  391]    Overall Loss 1.428167    Objective Loss 1.428167                                        LR 0.100000    Time 0.097708    
2024-02-17 11:23:29,206 - Epoch: [46][  300/  391]    Overall Loss 1.445878    Objective Loss 1.445878                                        LR 0.100000    Time 0.094253    
2024-02-17 11:23:37,569 - Epoch: [46][  391/  391]    Overall Loss 1.459151    Objective Loss 1.459151    Top1 62.019231    Top5 93.269231    LR 0.100000    Time 0.093696    
2024-02-17 11:23:37,713 - --- validate (epoch=46)-----------
2024-02-17 11:23:37,714 - 10000 samples (128 per mini-batch)
2024-02-17 11:23:40,535 - Epoch: [46][   79/   79]    Loss 1.885629    Top1 50.330000    Top5 80.910000    
2024-02-17 11:23:40,637 - ==> Top1: 50.330    Top5: 80.910    Loss: 1.886

2024-02-17 11:23:40,648 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:23:40,648 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:23:40,747 - 

2024-02-17 11:23:40,748 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:23:50,044 - Epoch: [47][  100/  391]    Overall Loss 1.427855    Objective Loss 1.427855                                        LR 0.100000    Time 0.092889    
2024-02-17 11:23:59,212 - Epoch: [47][  200/  391]    Overall Loss 1.431328    Objective Loss 1.431328                                        LR 0.100000    Time 0.092260    
2024-02-17 11:24:08,618 - Epoch: [47][  300/  391]    Overall Loss 1.449821    Objective Loss 1.449821                                        LR 0.100000    Time 0.092844    
2024-02-17 11:24:17,018 - Epoch: [47][  391/  391]    Overall Loss 1.456628    Objective Loss 1.456628    Top1 54.807692    Top5 90.384615    LR 0.100000    Time 0.092707    
2024-02-17 11:24:17,141 - --- validate (epoch=47)-----------
2024-02-17 11:24:17,142 - 10000 samples (128 per mini-batch)
2024-02-17 11:24:19,866 - Epoch: [47][   79/   79]    Loss 2.009425    Top1 47.200000    Top5 77.860000    
2024-02-17 11:24:19,973 - ==> Top1: 47.200    Top5: 77.860    Loss: 2.009

2024-02-17 11:24:19,990 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:24:19,991 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:24:20,084 - 

2024-02-17 11:24:20,084 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:24:29,912 - Epoch: [48][  100/  391]    Overall Loss 1.426738    Objective Loss 1.426738                                        LR 0.100000    Time 0.098202    
2024-02-17 11:24:39,499 - Epoch: [48][  200/  391]    Overall Loss 1.439196    Objective Loss 1.439196                                        LR 0.100000    Time 0.097009    
2024-02-17 11:24:48,888 - Epoch: [48][  300/  391]    Overall Loss 1.440461    Objective Loss 1.440461                                        LR 0.100000    Time 0.095954    
2024-02-17 11:24:57,222 - Epoch: [48][  391/  391]    Overall Loss 1.447062    Objective Loss 1.447062    Top1 54.807692    Top5 85.096154    LR 0.100000    Time 0.094925    
2024-02-17 11:24:57,406 - --- validate (epoch=48)-----------
2024-02-17 11:24:57,407 - 10000 samples (128 per mini-batch)
2024-02-17 11:25:00,481 - Epoch: [48][   79/   79]    Loss 1.940782    Top1 48.330000    Top5 79.390000    
2024-02-17 11:25:00,606 - ==> Top1: 48.330    Top5: 79.390    Loss: 1.941

2024-02-17 11:25:00,625 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:25:00,625 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:25:00,706 - 

2024-02-17 11:25:00,707 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:25:09,341 - Epoch: [49][  100/  391]    Overall Loss 1.422912    Objective Loss 1.422912                                        LR 0.100000    Time 0.086266    
2024-02-17 11:25:18,681 - Epoch: [49][  200/  391]    Overall Loss 1.418644    Objective Loss 1.418644                                        LR 0.100000    Time 0.089810    
2024-02-17 11:25:27,942 - Epoch: [49][  300/  391]    Overall Loss 1.426263    Objective Loss 1.426263                                        LR 0.100000    Time 0.090726    
2024-02-17 11:25:36,206 - Epoch: [49][  391/  391]    Overall Loss 1.433422    Objective Loss 1.433422    Top1 62.019231    Top5 87.019231    LR 0.100000    Time 0.090736    
2024-02-17 11:25:36,342 - --- validate (epoch=49)-----------
2024-02-17 11:25:36,343 - 10000 samples (128 per mini-batch)
2024-02-17 11:25:39,147 - Epoch: [49][   79/   79]    Loss 2.283702    Top1 42.440000    Top5 73.860000    
2024-02-17 11:25:39,250 - ==> Top1: 42.440    Top5: 73.860    Loss: 2.284

2024-02-17 11:25:39,261 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:25:39,262 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:25:39,337 - 

2024-02-17 11:25:39,338 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:25:49,501 - Epoch: [50][  100/  391]    Overall Loss 1.403427    Objective Loss 1.403427                                        LR 0.100000    Time 0.101563    
2024-02-17 11:25:58,898 - Epoch: [50][  200/  391]    Overall Loss 1.417837    Objective Loss 1.417837                                        LR 0.100000    Time 0.097742    
2024-02-17 11:26:08,476 - Epoch: [50][  300/  391]    Overall Loss 1.419865    Objective Loss 1.419865                                        LR 0.100000    Time 0.097071    
2024-02-17 11:26:16,919 - Epoch: [50][  391/  391]    Overall Loss 1.429393    Objective Loss 1.429393    Top1 57.211538    Top5 84.615385    LR 0.100000    Time 0.096059    
2024-02-17 11:26:17,050 - --- validate (epoch=50)-----------
2024-02-17 11:26:17,051 - 10000 samples (128 per mini-batch)
2024-02-17 11:26:19,637 - Epoch: [50][   79/   79]    Loss 1.889087    Top1 50.290000    Top5 79.830000    
2024-02-17 11:26:19,739 - ==> Top1: 50.290    Top5: 79.830    Loss: 1.889

2024-02-17 11:26:19,758 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:26:19,758 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:26:19,835 - 

2024-02-17 11:26:19,836 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:26:29,817 - Epoch: [51][  100/  391]    Overall Loss 1.368844    Objective Loss 1.368844                                        LR 0.100000    Time 0.099733    
2024-02-17 11:26:39,257 - Epoch: [51][  200/  391]    Overall Loss 1.400591    Objective Loss 1.400591                                        LR 0.100000    Time 0.097045    
2024-02-17 11:26:48,749 - Epoch: [51][  300/  391]    Overall Loss 1.412388    Objective Loss 1.412388                                        LR 0.100000    Time 0.096318    
2024-02-17 11:26:57,286 - Epoch: [51][  391/  391]    Overall Loss 1.425627    Objective Loss 1.425627    Top1 52.403846    Top5 84.615385    LR 0.100000    Time 0.095725    
2024-02-17 11:26:57,420 - --- validate (epoch=51)-----------
2024-02-17 11:26:57,421 - 10000 samples (128 per mini-batch)
2024-02-17 11:27:00,191 - Epoch: [51][   79/   79]    Loss 1.779124    Top1 51.110000    Top5 81.550000    
2024-02-17 11:27:00,305 - ==> Top1: 51.110    Top5: 81.550    Loss: 1.779

2024-02-17 11:27:00,323 - ==> Best [Top1: 51.110   Top5: 81.550   Sparsity:0.00   Params: 1341960 on epoch: 51]
2024-02-17 11:27:00,323 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:27:00,416 - 

2024-02-17 11:27:00,417 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:27:10,412 - Epoch: [52][  100/  391]    Overall Loss 1.356018    Objective Loss 1.356018                                        LR 0.100000    Time 0.099877    
2024-02-17 11:27:19,815 - Epoch: [52][  200/  391]    Overall Loss 1.368702    Objective Loss 1.368702                                        LR 0.100000    Time 0.096929    
2024-02-17 11:27:29,368 - Epoch: [52][  300/  391]    Overall Loss 1.390229    Objective Loss 1.390229                                        LR 0.100000    Time 0.096445    
2024-02-17 11:27:36,802 - Epoch: [52][  391/  391]    Overall Loss 1.396170    Objective Loss 1.396170    Top1 62.980769    Top5 86.538462    LR 0.100000    Time 0.093004    
2024-02-17 11:27:36,918 - --- validate (epoch=52)-----------
2024-02-17 11:27:36,919 - 10000 samples (128 per mini-batch)
2024-02-17 11:27:39,750 - Epoch: [52][   79/   79]    Loss 1.752732    Top1 51.550000    Top5 81.970000    
2024-02-17 11:27:39,868 - ==> Top1: 51.550    Top5: 81.970    Loss: 1.753

2024-02-17 11:27:39,886 - ==> Best [Top1: 51.550   Top5: 81.970   Sparsity:0.00   Params: 1341960 on epoch: 52]
2024-02-17 11:27:39,887 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:27:39,986 - 

2024-02-17 11:27:39,987 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:27:49,877 - Epoch: [53][  100/  391]    Overall Loss 1.352209    Objective Loss 1.352209                                        LR 0.100000    Time 0.098819    
2024-02-17 11:27:58,945 - Epoch: [53][  200/  391]    Overall Loss 1.385189    Objective Loss 1.385189                                        LR 0.100000    Time 0.094723    
2024-02-17 11:28:08,656 - Epoch: [53][  300/  391]    Overall Loss 1.395419    Objective Loss 1.395419                                        LR 0.100000    Time 0.095502    
2024-02-17 11:28:16,971 - Epoch: [53][  391/  391]    Overall Loss 1.400876    Objective Loss 1.400876    Top1 61.057692    Top5 88.461538    LR 0.100000    Time 0.094529    
2024-02-17 11:28:17,103 - --- validate (epoch=53)-----------
2024-02-17 11:28:17,104 - 10000 samples (128 per mini-batch)
2024-02-17 11:28:19,737 - Epoch: [53][   79/   79]    Loss 1.756776    Top1 52.330000    Top5 82.320000    
2024-02-17 11:28:19,857 - ==> Top1: 52.330    Top5: 82.320    Loss: 1.757

2024-02-17 11:28:19,868 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:28:19,868 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:28:19,965 - 

2024-02-17 11:28:19,965 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:28:30,205 - Epoch: [54][  100/  391]    Overall Loss 1.365076    Objective Loss 1.365076                                        LR 0.100000    Time 0.102320    
2024-02-17 11:28:39,141 - Epoch: [54][  200/  391]    Overall Loss 1.377045    Objective Loss 1.377045                                        LR 0.100000    Time 0.095817    
2024-02-17 11:28:47,909 - Epoch: [54][  300/  391]    Overall Loss 1.386387    Objective Loss 1.386387                                        LR 0.100000    Time 0.093089    
2024-02-17 11:28:55,324 - Epoch: [54][  391/  391]    Overall Loss 1.388969    Objective Loss 1.388969    Top1 61.057692    Top5 86.538462    LR 0.100000    Time 0.090378    
2024-02-17 11:28:55,446 - --- validate (epoch=54)-----------
2024-02-17 11:28:55,447 - 10000 samples (128 per mini-batch)
2024-02-17 11:28:58,396 - Epoch: [54][   79/   79]    Loss 1.958111    Top1 48.140000    Top5 78.360000    
2024-02-17 11:28:58,503 - ==> Top1: 48.140    Top5: 78.360    Loss: 1.958

2024-02-17 11:28:58,522 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:28:58,523 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:28:58,602 - 

2024-02-17 11:28:58,602 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:29:08,799 - Epoch: [55][  100/  391]    Overall Loss 1.354112    Objective Loss 1.354112                                        LR 0.100000    Time 0.101888    
2024-02-17 11:29:18,120 - Epoch: [55][  200/  391]    Overall Loss 1.363370    Objective Loss 1.363370                                        LR 0.100000    Time 0.097529    
2024-02-17 11:29:27,465 - Epoch: [55][  300/  391]    Overall Loss 1.373081    Objective Loss 1.373081                                        LR 0.100000    Time 0.096151    
2024-02-17 11:29:36,103 - Epoch: [55][  391/  391]    Overall Loss 1.380583    Objective Loss 1.380583    Top1 61.538462    Top5 89.903846    LR 0.100000    Time 0.095855    
2024-02-17 11:29:36,269 - --- validate (epoch=55)-----------
2024-02-17 11:29:36,269 - 10000 samples (128 per mini-batch)
2024-02-17 11:29:39,089 - Epoch: [55][   79/   79]    Loss 1.765769    Top1 51.630000    Top5 82.110000    
2024-02-17 11:29:39,241 - ==> Top1: 51.630    Top5: 82.110    Loss: 1.766

2024-02-17 11:29:39,257 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:29:39,257 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:29:39,331 - 

2024-02-17 11:29:39,331 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:29:49,316 - Epoch: [56][  100/  391]    Overall Loss 1.326227    Objective Loss 1.326227                                        LR 0.100000    Time 0.099765    
2024-02-17 11:29:58,313 - Epoch: [56][  200/  391]    Overall Loss 1.348147    Objective Loss 1.348147                                        LR 0.100000    Time 0.094849    
2024-02-17 11:30:05,402 - Epoch: [56][  300/  391]    Overall Loss 1.367657    Objective Loss 1.367657                                        LR 0.100000    Time 0.086850    
2024-02-17 11:30:13,793 - Epoch: [56][  391/  391]    Overall Loss 1.379064    Objective Loss 1.379064    Top1 57.692308    Top5 86.538462    LR 0.100000    Time 0.088085    
2024-02-17 11:30:13,912 - --- validate (epoch=56)-----------
2024-02-17 11:30:13,912 - 10000 samples (128 per mini-batch)
2024-02-17 11:30:16,666 - Epoch: [56][   79/   79]    Loss 1.785426    Top1 51.600000    Top5 81.980000    
2024-02-17 11:30:16,768 - ==> Top1: 51.600    Top5: 81.980    Loss: 1.785

2024-02-17 11:30:16,790 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:30:16,790 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:30:16,883 - 

2024-02-17 11:30:16,884 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:30:26,832 - Epoch: [57][  100/  391]    Overall Loss 1.349260    Objective Loss 1.349260                                        LR 0.100000    Time 0.099404    
2024-02-17 11:30:35,979 - Epoch: [57][  200/  391]    Overall Loss 1.363172    Objective Loss 1.363172                                        LR 0.100000    Time 0.095415    
2024-02-17 11:30:45,264 - Epoch: [57][  300/  391]    Overall Loss 1.365457    Objective Loss 1.365457                                        LR 0.100000    Time 0.094545    
2024-02-17 11:30:53,913 - Epoch: [57][  391/  391]    Overall Loss 1.366706    Objective Loss 1.366706    Top1 62.019231    Top5 88.942308    LR 0.100000    Time 0.094649    
2024-02-17 11:30:54,050 - --- validate (epoch=57)-----------
2024-02-17 11:30:54,051 - 10000 samples (128 per mini-batch)
2024-02-17 11:30:56,818 - Epoch: [57][   79/   79]    Loss 1.894377    Top1 49.460000    Top5 80.050000    
2024-02-17 11:30:56,943 - ==> Top1: 49.460    Top5: 80.050    Loss: 1.894

2024-02-17 11:30:56,955 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:30:56,955 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:30:57,035 - 

2024-02-17 11:30:57,035 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:31:07,065 - Epoch: [58][  100/  391]    Overall Loss 1.325014    Objective Loss 1.325014                                        LR 0.100000    Time 0.100219    
2024-02-17 11:31:15,910 - Epoch: [58][  200/  391]    Overall Loss 1.345946    Objective Loss 1.345946                                        LR 0.100000    Time 0.094313    
2024-02-17 11:31:25,404 - Epoch: [58][  300/  391]    Overall Loss 1.349931    Objective Loss 1.349931                                        LR 0.100000    Time 0.094504    
2024-02-17 11:31:33,209 - Epoch: [58][  391/  391]    Overall Loss 1.360621    Objective Loss 1.360621    Top1 62.980769    Top5 87.019231    LR 0.100000    Time 0.092461    
2024-02-17 11:31:33,338 - --- validate (epoch=58)-----------
2024-02-17 11:31:33,339 - 10000 samples (128 per mini-batch)
2024-02-17 11:31:36,312 - Epoch: [58][   79/   79]    Loss 1.667708    Top1 54.190000    Top5 83.330000    
2024-02-17 11:31:36,422 - ==> Top1: 54.190    Top5: 83.330    Loss: 1.668

2024-02-17 11:31:36,436 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:31:36,436 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:31:36,535 - 

2024-02-17 11:31:36,536 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:31:46,040 - Epoch: [59][  100/  391]    Overall Loss 1.308791    Objective Loss 1.308791                                        LR 0.100000    Time 0.094968    
2024-02-17 11:31:55,386 - Epoch: [59][  200/  391]    Overall Loss 1.326215    Objective Loss 1.326215                                        LR 0.100000    Time 0.094192    
2024-02-17 11:32:04,860 - Epoch: [59][  300/  391]    Overall Loss 1.341077    Objective Loss 1.341077                                        LR 0.100000    Time 0.094359    
2024-02-17 11:32:13,552 - Epoch: [59][  391/  391]    Overall Loss 1.348947    Objective Loss 1.348947    Top1 62.500000    Top5 88.942308    LR 0.100000    Time 0.094616    
2024-02-17 11:32:13,722 - --- validate (epoch=59)-----------
2024-02-17 11:32:13,723 - 10000 samples (128 per mini-batch)
2024-02-17 11:32:16,283 - Epoch: [59][   79/   79]    Loss 1.826966    Top1 50.750000    Top5 81.150000    
2024-02-17 11:32:16,386 - ==> Top1: 50.750    Top5: 81.150    Loss: 1.827

2024-02-17 11:32:16,407 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:32:16,407 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:32:16,504 - 

2024-02-17 11:32:16,505 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:32:26,469 - Epoch: [60][  100/  391]    Overall Loss 1.305665    Objective Loss 1.305665                                        LR 0.100000    Time 0.099508    
2024-02-17 11:32:35,696 - Epoch: [60][  200/  391]    Overall Loss 1.311440    Objective Loss 1.311440                                        LR 0.100000    Time 0.095864    
2024-02-17 11:32:45,111 - Epoch: [60][  300/  391]    Overall Loss 1.328394    Objective Loss 1.328394                                        LR 0.100000    Time 0.095277    
2024-02-17 11:32:53,722 - Epoch: [60][  391/  391]    Overall Loss 1.339107    Objective Loss 1.339107    Top1 63.942308    Top5 88.461538    LR 0.100000    Time 0.095113    
2024-02-17 11:32:53,851 - --- validate (epoch=60)-----------
2024-02-17 11:32:53,852 - 10000 samples (128 per mini-batch)
2024-02-17 11:32:56,463 - Epoch: [60][   79/   79]    Loss 1.691084    Top1 52.590000    Top5 83.400000    
2024-02-17 11:32:56,573 - ==> Top1: 52.590    Top5: 83.400    Loss: 1.691

2024-02-17 11:32:56,592 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:32:56,593 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:32:56,668 - 

2024-02-17 11:32:56,668 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:33:06,559 - Epoch: [61][  100/  391]    Overall Loss 1.296610    Objective Loss 1.296610                                        LR 0.100000    Time 0.098828    
2024-02-17 11:33:15,811 - Epoch: [61][  200/  391]    Overall Loss 1.326394    Objective Loss 1.326394                                        LR 0.100000    Time 0.095652    
2024-02-17 11:33:25,309 - Epoch: [61][  300/  391]    Overall Loss 1.333381    Objective Loss 1.333381                                        LR 0.100000    Time 0.095410    
2024-02-17 11:33:33,236 - Epoch: [61][  391/  391]    Overall Loss 1.339205    Objective Loss 1.339205    Top1 61.538462    Top5 86.538462    LR 0.100000    Time 0.093469    
2024-02-17 11:33:33,348 - --- validate (epoch=61)-----------
2024-02-17 11:33:33,349 - 10000 samples (128 per mini-batch)
2024-02-17 11:33:35,972 - Epoch: [61][   79/   79]    Loss 1.818900    Top1 50.560000    Top5 81.340000    
2024-02-17 11:33:36,155 - ==> Top1: 50.560    Top5: 81.340    Loss: 1.819

2024-02-17 11:33:36,174 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:33:36,175 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:33:36,260 - 

2024-02-17 11:33:36,261 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:33:46,045 - Epoch: [62][  100/  391]    Overall Loss 1.288146    Objective Loss 1.288146                                        LR 0.100000    Time 0.097728    
2024-02-17 11:33:55,330 - Epoch: [62][  200/  391]    Overall Loss 1.302987    Objective Loss 1.302987                                        LR 0.100000    Time 0.095267    
2024-02-17 11:34:04,550 - Epoch: [62][  300/  391]    Overall Loss 1.328770    Objective Loss 1.328770                                        LR 0.100000    Time 0.094229    
2024-02-17 11:34:12,806 - Epoch: [62][  391/  391]    Overall Loss 1.332459    Objective Loss 1.332459    Top1 58.173077    Top5 86.538462    LR 0.100000    Time 0.093403    
2024-02-17 11:34:12,948 - --- validate (epoch=62)-----------
2024-02-17 11:34:12,949 - 10000 samples (128 per mini-batch)
2024-02-17 11:34:15,631 - Epoch: [62][   79/   79]    Loss 1.716351    Top1 52.900000    Top5 82.810000    
2024-02-17 11:34:15,798 - ==> Top1: 52.900    Top5: 82.810    Loss: 1.716

2024-02-17 11:34:15,816 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:34:15,816 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:34:15,889 - 

2024-02-17 11:34:15,890 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:34:25,698 - Epoch: [63][  100/  391]    Overall Loss 1.285306    Objective Loss 1.285306                                        LR 0.100000    Time 0.098012    
2024-02-17 11:34:35,302 - Epoch: [63][  200/  391]    Overall Loss 1.292782    Objective Loss 1.292782                                        LR 0.100000    Time 0.096997    
2024-02-17 11:34:43,987 - Epoch: [63][  300/  391]    Overall Loss 1.312682    Objective Loss 1.312682                                        LR 0.100000    Time 0.093603    
2024-02-17 11:34:50,455 - Epoch: [63][  391/  391]    Overall Loss 1.317608    Objective Loss 1.317608    Top1 61.057692    Top5 87.980769    LR 0.100000    Time 0.088350    
2024-02-17 11:34:50,639 - --- validate (epoch=63)-----------
2024-02-17 11:34:50,640 - 10000 samples (128 per mini-batch)
2024-02-17 11:34:53,362 - Epoch: [63][   79/   79]    Loss 1.956995    Top1 48.160000    Top5 78.860000    
2024-02-17 11:34:53,467 - ==> Top1: 48.160    Top5: 78.860    Loss: 1.957

2024-02-17 11:34:53,485 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:34:53,485 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:34:53,561 - 

2024-02-17 11:34:53,562 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:35:03,716 - Epoch: [64][  100/  391]    Overall Loss 1.297899    Objective Loss 1.297899                                        LR 0.100000    Time 0.101469    
2024-02-17 11:35:12,543 - Epoch: [64][  200/  391]    Overall Loss 1.311936    Objective Loss 1.311936                                        LR 0.100000    Time 0.094846    
2024-02-17 11:35:22,024 - Epoch: [64][  300/  391]    Overall Loss 1.316976    Objective Loss 1.316976                                        LR 0.100000    Time 0.094816    
2024-02-17 11:35:30,638 - Epoch: [64][  391/  391]    Overall Loss 1.320537    Objective Loss 1.320537    Top1 63.461538    Top5 92.307692    LR 0.100000    Time 0.094767    
2024-02-17 11:35:30,800 - --- validate (epoch=64)-----------
2024-02-17 11:35:30,801 - 10000 samples (128 per mini-batch)
2024-02-17 11:35:33,453 - Epoch: [64][   79/   79]    Loss 1.750812    Top1 51.930000    Top5 82.150000    
2024-02-17 11:35:33,623 - ==> Top1: 51.930    Top5: 82.150    Loss: 1.751

2024-02-17 11:35:33,640 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:35:33,640 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:35:33,714 - 

2024-02-17 11:35:33,715 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:35:43,436 - Epoch: [65][  100/  391]    Overall Loss 1.261279    Objective Loss 1.261279                                        LR 0.100000    Time 0.097143    
2024-02-17 11:35:52,353 - Epoch: [65][  200/  391]    Overall Loss 1.285149    Objective Loss 1.285149                                        LR 0.100000    Time 0.093129    
2024-02-17 11:36:01,945 - Epoch: [65][  300/  391]    Overall Loss 1.301239    Objective Loss 1.301239                                        LR 0.100000    Time 0.094042    
2024-02-17 11:36:10,294 - Epoch: [65][  391/  391]    Overall Loss 1.310330    Objective Loss 1.310330    Top1 60.096154    Top5 87.019231    LR 0.100000    Time 0.093498    
2024-02-17 11:36:10,414 - --- validate (epoch=65)-----------
2024-02-17 11:36:10,414 - 10000 samples (128 per mini-batch)
2024-02-17 11:36:12,928 - Epoch: [65][   79/   79]    Loss 1.815695    Top1 51.080000    Top5 80.620000    
2024-02-17 11:36:13,038 - ==> Top1: 51.080    Top5: 80.620    Loss: 1.816

2024-02-17 11:36:13,057 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:36:13,058 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:36:13,132 - 

2024-02-17 11:36:13,133 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:36:23,208 - Epoch: [66][  100/  391]    Overall Loss 1.250810    Objective Loss 1.250810                                        LR 0.100000    Time 0.100681    
2024-02-17 11:36:32,163 - Epoch: [66][  200/  391]    Overall Loss 1.267830    Objective Loss 1.267830                                        LR 0.100000    Time 0.095088    
2024-02-17 11:36:41,744 - Epoch: [66][  300/  391]    Overall Loss 1.283463    Objective Loss 1.283463                                        LR 0.100000    Time 0.095312    
2024-02-17 11:36:50,177 - Epoch: [66][  391/  391]    Overall Loss 1.292746    Objective Loss 1.292746    Top1 49.519231    Top5 80.769231    LR 0.100000    Time 0.094687    
2024-02-17 11:36:50,298 - --- validate (epoch=66)-----------
2024-02-17 11:36:50,299 - 10000 samples (128 per mini-batch)
2024-02-17 11:36:52,867 - Epoch: [66][   79/   79]    Loss 1.934755    Top1 49.240000    Top5 80.590000    
2024-02-17 11:36:53,028 - ==> Top1: 49.240    Top5: 80.590    Loss: 1.935

2024-02-17 11:36:53,037 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:36:53,038 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:36:53,119 - 

2024-02-17 11:36:53,119 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:37:02,885 - Epoch: [67][  100/  391]    Overall Loss 1.269191    Objective Loss 1.269191                                        LR 0.100000    Time 0.097582    
2024-02-17 11:37:11,657 - Epoch: [67][  200/  391]    Overall Loss 1.276007    Objective Loss 1.276007                                        LR 0.100000    Time 0.092625    
2024-02-17 11:37:21,034 - Epoch: [67][  300/  391]    Overall Loss 1.285233    Objective Loss 1.285233                                        LR 0.100000    Time 0.092990    
2024-02-17 11:37:29,821 - Epoch: [67][  391/  391]    Overall Loss 1.298565    Objective Loss 1.298565    Top1 66.346154    Top5 89.423077    LR 0.100000    Time 0.093810    
2024-02-17 11:37:30,013 - --- validate (epoch=67)-----------
2024-02-17 11:37:30,014 - 10000 samples (128 per mini-batch)
2024-02-17 11:37:32,647 - Epoch: [67][   79/   79]    Loss 1.758080    Top1 52.600000    Top5 82.260000    
2024-02-17 11:37:32,756 - ==> Top1: 52.600    Top5: 82.260    Loss: 1.758

2024-02-17 11:37:32,774 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:37:32,775 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:37:32,859 - 

2024-02-17 11:37:32,860 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:37:42,871 - Epoch: [68][  100/  391]    Overall Loss 1.250863    Objective Loss 1.250863                                        LR 0.100000    Time 0.100034    
2024-02-17 11:37:51,922 - Epoch: [68][  200/  391]    Overall Loss 1.268339    Objective Loss 1.268339                                        LR 0.100000    Time 0.095250    
2024-02-17 11:38:01,282 - Epoch: [68][  300/  391]    Overall Loss 1.273696    Objective Loss 1.273696                                        LR 0.100000    Time 0.094682    
2024-02-17 11:38:09,317 - Epoch: [68][  391/  391]    Overall Loss 1.285888    Objective Loss 1.285888    Top1 61.538462    Top5 90.865385    LR 0.100000    Time 0.093185    
2024-02-17 11:38:09,456 - --- validate (epoch=68)-----------
2024-02-17 11:38:09,457 - 10000 samples (128 per mini-batch)
2024-02-17 11:38:12,077 - Epoch: [68][   79/   79]    Loss 1.862298    Top1 50.420000    Top5 80.730000    
2024-02-17 11:38:12,193 - ==> Top1: 50.420    Top5: 80.730    Loss: 1.862

2024-02-17 11:38:12,211 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:38:12,211 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:38:12,494 - 

2024-02-17 11:38:12,495 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:38:22,392 - Epoch: [69][  100/  391]    Overall Loss 1.257213    Objective Loss 1.257213                                        LR 0.100000    Time 0.098891    
2024-02-17 11:38:31,443 - Epoch: [69][  200/  391]    Overall Loss 1.259300    Objective Loss 1.259300                                        LR 0.100000    Time 0.094679    
2024-02-17 11:38:40,588 - Epoch: [69][  300/  391]    Overall Loss 1.275271    Objective Loss 1.275271                                        LR 0.100000    Time 0.093583    
2024-02-17 11:38:49,189 - Epoch: [69][  391/  391]    Overall Loss 1.284235    Objective Loss 1.284235    Top1 63.942308    Top5 87.980769    LR 0.100000    Time 0.093791    
2024-02-17 11:38:49,379 - --- validate (epoch=69)-----------
2024-02-17 11:38:49,380 - 10000 samples (128 per mini-batch)
2024-02-17 11:38:51,953 - Epoch: [69][   79/   79]    Loss 1.776082    Top1 51.820000    Top5 82.050000    
2024-02-17 11:38:52,149 - ==> Top1: 51.820    Top5: 82.050    Loss: 1.776

2024-02-17 11:38:52,166 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:38:52,166 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:38:52,243 - 

2024-02-17 11:38:52,243 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:39:02,683 - Epoch: [70][  100/  391]    Overall Loss 1.246878    Objective Loss 1.246878                                        LR 0.100000    Time 0.104317    
2024-02-17 11:39:11,858 - Epoch: [70][  200/  391]    Overall Loss 1.248874    Objective Loss 1.248874                                        LR 0.100000    Time 0.098010    
2024-02-17 11:39:20,731 - Epoch: [70][  300/  391]    Overall Loss 1.265239    Objective Loss 1.265239                                        LR 0.100000    Time 0.094903    
2024-02-17 11:39:27,127 - Epoch: [70][  391/  391]    Overall Loss 1.268964    Objective Loss 1.268964    Top1 63.461538    Top5 88.461538    LR 0.100000    Time 0.089166    
2024-02-17 11:39:27,254 - --- validate (epoch=70)-----------
2024-02-17 11:39:27,254 - 10000 samples (128 per mini-batch)
2024-02-17 11:39:29,819 - Epoch: [70][   79/   79]    Loss 1.651100    Top1 54.200000    Top5 84.270000    
2024-02-17 11:39:29,927 - ==> Top1: 54.200    Top5: 84.270    Loss: 1.651

2024-02-17 11:39:29,945 - ==> Best [Top1: 54.200   Top5: 84.270   Sparsity:0.00   Params: 1341960 on epoch: 70]
2024-02-17 11:39:29,945 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:39:30,037 - 

2024-02-17 11:39:30,037 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:39:39,983 - Epoch: [71][  100/  391]    Overall Loss 1.214183    Objective Loss 1.214183                                        LR 0.100000    Time 0.099375    
2024-02-17 11:39:47,263 - Epoch: [71][  200/  391]    Overall Loss 1.237423    Objective Loss 1.237423                                        LR 0.100000    Time 0.086068    
2024-02-17 11:39:56,493 - Epoch: [71][  300/  391]    Overall Loss 1.252054    Objective Loss 1.252054                                        LR 0.100000    Time 0.088129    
2024-02-17 11:40:05,051 - Epoch: [71][  391/  391]    Overall Loss 1.269942    Objective Loss 1.269942    Top1 62.500000    Top5 92.307692    LR 0.100000    Time 0.089493    
2024-02-17 11:40:05,181 - --- validate (epoch=71)-----------
2024-02-17 11:40:05,181 - 10000 samples (128 per mini-batch)
2024-02-17 11:40:07,864 - Epoch: [71][   79/   79]    Loss 1.664598    Top1 54.580000    Top5 83.680000    
2024-02-17 11:40:08,011 - ==> Top1: 54.580    Top5: 83.680    Loss: 1.665

2024-02-17 11:40:08,031 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:40:08,031 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:40:08,125 - 

2024-02-17 11:40:08,126 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:40:18,360 - Epoch: [72][  100/  391]    Overall Loss 1.228445    Objective Loss 1.228445                                        LR 0.100000    Time 0.102266    
2024-02-17 11:40:27,835 - Epoch: [72][  200/  391]    Overall Loss 1.231338    Objective Loss 1.231338                                        LR 0.100000    Time 0.098481    
2024-02-17 11:40:37,041 - Epoch: [72][  300/  391]    Overall Loss 1.256302    Objective Loss 1.256302                                        LR 0.100000    Time 0.096324    
2024-02-17 11:40:45,681 - Epoch: [72][  391/  391]    Overall Loss 1.258400    Objective Loss 1.258400    Top1 60.576923    Top5 88.461538    LR 0.100000    Time 0.095990    
2024-02-17 11:40:45,848 - --- validate (epoch=72)-----------
2024-02-17 11:40:45,849 - 10000 samples (128 per mini-batch)
2024-02-17 11:40:48,525 - Epoch: [72][   79/   79]    Loss 1.766127    Top1 52.280000    Top5 81.960000    
2024-02-17 11:40:48,665 - ==> Top1: 52.280    Top5: 81.960    Loss: 1.766

2024-02-17 11:40:48,685 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:40:48,685 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:40:48,761 - 

2024-02-17 11:40:48,762 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:40:58,799 - Epoch: [73][  100/  391]    Overall Loss 1.235244    Objective Loss 1.235244                                        LR 0.100000    Time 0.100289    
2024-02-17 11:41:08,156 - Epoch: [73][  200/  391]    Overall Loss 1.225271    Objective Loss 1.225271                                        LR 0.100000    Time 0.096900    
2024-02-17 11:41:17,516 - Epoch: [73][  300/  391]    Overall Loss 1.248749    Objective Loss 1.248749                                        LR 0.100000    Time 0.095786    
2024-02-17 11:41:25,921 - Epoch: [73][  391/  391]    Overall Loss 1.258203    Objective Loss 1.258203    Top1 62.019231    Top5 92.307692    LR 0.100000    Time 0.094977    
2024-02-17 11:41:26,109 - --- validate (epoch=73)-----------
2024-02-17 11:41:26,109 - 10000 samples (128 per mini-batch)
2024-02-17 11:41:28,766 - Epoch: [73][   79/   79]    Loss 1.792526    Top1 52.330000    Top5 81.370000    
2024-02-17 11:41:28,943 - ==> Top1: 52.330    Top5: 81.370    Loss: 1.793

2024-02-17 11:41:28,966 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:41:28,966 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:41:29,042 - 

2024-02-17 11:41:29,043 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:41:37,836 - Epoch: [74][  100/  391]    Overall Loss 1.213928    Objective Loss 1.213928                                        LR 0.100000    Time 0.087858    
2024-02-17 11:41:47,301 - Epoch: [74][  200/  391]    Overall Loss 1.237431    Objective Loss 1.237431                                        LR 0.100000    Time 0.091229    
2024-02-17 11:41:56,492 - Epoch: [74][  300/  391]    Overall Loss 1.250356    Objective Loss 1.250356                                        LR 0.100000    Time 0.091441    
2024-02-17 11:42:04,807 - Epoch: [74][  391/  391]    Overall Loss 1.257349    Objective Loss 1.257349    Top1 62.980769    Top5 90.865385    LR 0.100000    Time 0.091414    
2024-02-17 11:42:04,927 - --- validate (epoch=74)-----------
2024-02-17 11:42:04,927 - 10000 samples (128 per mini-batch)
2024-02-17 11:42:07,702 - Epoch: [74][   79/   79]    Loss 1.893712    Top1 50.480000    Top5 80.250000    
2024-02-17 11:42:07,871 - ==> Top1: 50.480    Top5: 80.250    Loss: 1.894

2024-02-17 11:42:07,889 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:42:07,890 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:42:07,966 - 

2024-02-17 11:42:07,967 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:42:18,263 - Epoch: [75][  100/  391]    Overall Loss 1.223472    Objective Loss 1.223472                                        LR 0.100000    Time 0.102891    
2024-02-17 11:42:27,139 - Epoch: [75][  200/  391]    Overall Loss 1.242622    Objective Loss 1.242622                                        LR 0.100000    Time 0.095799    
2024-02-17 11:42:35,136 - Epoch: [75][  300/  391]    Overall Loss 1.249421    Objective Loss 1.249421                                        LR 0.100000    Time 0.090511    
2024-02-17 11:42:43,663 - Epoch: [75][  391/  391]    Overall Loss 1.251632    Objective Loss 1.251632    Top1 62.019231    Top5 87.500000    LR 0.100000    Time 0.091240    
2024-02-17 11:42:43,828 - --- validate (epoch=75)-----------
2024-02-17 11:42:43,829 - 10000 samples (128 per mini-batch)
2024-02-17 11:42:46,681 - Epoch: [75][   79/   79]    Loss 1.653270    Top1 54.870000    Top5 83.890000    
2024-02-17 11:42:46,849 - ==> Top1: 54.870    Top5: 83.890    Loss: 1.653

2024-02-17 11:42:46,870 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:42:46,870 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:42:46,964 - 

2024-02-17 11:42:46,964 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:42:56,957 - Epoch: [76][  100/  391]    Overall Loss 1.170750    Objective Loss 1.170750                                        LR 0.100000    Time 0.099844    
2024-02-17 11:43:06,241 - Epoch: [76][  200/  391]    Overall Loss 1.213522    Objective Loss 1.213522                                        LR 0.100000    Time 0.096317    
2024-02-17 11:43:15,802 - Epoch: [76][  300/  391]    Overall Loss 1.231686    Objective Loss 1.231686                                        LR 0.100000    Time 0.096065    
2024-02-17 11:43:24,194 - Epoch: [76][  391/  391]    Overall Loss 1.240826    Objective Loss 1.240826    Top1 67.788462    Top5 90.865385    LR 0.100000    Time 0.095159    
2024-02-17 11:43:24,310 - --- validate (epoch=76)-----------
2024-02-17 11:43:24,311 - 10000 samples (128 per mini-batch)
2024-02-17 11:43:27,109 - Epoch: [76][   79/   79]    Loss 1.814758    Top1 51.530000    Top5 81.440000    
2024-02-17 11:43:27,227 - ==> Top1: 51.530    Top5: 81.440    Loss: 1.815

2024-02-17 11:43:27,246 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:43:27,246 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:43:27,321 - 

2024-02-17 11:43:27,321 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:43:37,292 - Epoch: [77][  100/  391]    Overall Loss 1.180127    Objective Loss 1.180127                                        LR 0.100000    Time 0.099636    
2024-02-17 11:43:46,349 - Epoch: [77][  200/  391]    Overall Loss 1.209763    Objective Loss 1.209763                                        LR 0.100000    Time 0.095076    
2024-02-17 11:43:55,432 - Epoch: [77][  300/  391]    Overall Loss 1.223902    Objective Loss 1.223902                                        LR 0.100000    Time 0.093646    
2024-02-17 11:44:03,526 - Epoch: [77][  391/  391]    Overall Loss 1.233894    Objective Loss 1.233894    Top1 61.538462    Top5 92.307692    LR 0.100000    Time 0.092542    
2024-02-17 11:44:03,671 - --- validate (epoch=77)-----------
2024-02-17 11:44:03,671 - 10000 samples (128 per mini-batch)
2024-02-17 11:44:06,317 - Epoch: [77][   79/   79]    Loss 1.657645    Top1 54.100000    Top5 83.660000    
2024-02-17 11:44:06,511 - ==> Top1: 54.100    Top5: 83.660    Loss: 1.658

2024-02-17 11:44:06,535 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:44:06,536 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:44:06,623 - 

2024-02-17 11:44:06,624 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:44:14,803 - Epoch: [78][  100/  391]    Overall Loss 1.201644    Objective Loss 1.201644                                        LR 0.100000    Time 0.081726    
2024-02-17 11:44:22,140 - Epoch: [78][  200/  391]    Overall Loss 1.203288    Objective Loss 1.203288                                        LR 0.100000    Time 0.077527    
2024-02-17 11:44:31,048 - Epoch: [78][  300/  391]    Overall Loss 1.218297    Objective Loss 1.218297                                        LR 0.100000    Time 0.081362    
2024-02-17 11:44:39,257 - Epoch: [78][  391/  391]    Overall Loss 1.222866    Objective Loss 1.222866    Top1 61.057692    Top5 92.307692    LR 0.100000    Time 0.083409    
2024-02-17 11:44:39,383 - --- validate (epoch=78)-----------
2024-02-17 11:44:39,384 - 10000 samples (128 per mini-batch)
2024-02-17 11:44:42,121 - Epoch: [78][   79/   79]    Loss 1.797887    Top1 52.410000    Top5 81.490000    
2024-02-17 11:44:42,236 - ==> Top1: 52.410    Top5: 81.490    Loss: 1.798

2024-02-17 11:44:42,260 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:44:42,260 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:44:42,342 - 

2024-02-17 11:44:42,343 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:44:52,375 - Epoch: [79][  100/  391]    Overall Loss 1.185781    Objective Loss 1.185781                                        LR 0.100000    Time 0.100227    
2024-02-17 11:45:01,768 - Epoch: [79][  200/  391]    Overall Loss 1.206895    Objective Loss 1.206895                                        LR 0.100000    Time 0.097057    
2024-02-17 11:45:10,773 - Epoch: [79][  300/  391]    Overall Loss 1.223145    Objective Loss 1.223145                                        LR 0.100000    Time 0.094706    
2024-02-17 11:45:18,897 - Epoch: [79][  391/  391]    Overall Loss 1.228430    Objective Loss 1.228430    Top1 60.576923    Top5 89.903846    LR 0.100000    Time 0.093430    
2024-02-17 11:45:19,085 - --- validate (epoch=79)-----------
2024-02-17 11:45:19,086 - 10000 samples (128 per mini-batch)
2024-02-17 11:45:21,631 - Epoch: [79][   79/   79]    Loss 1.761844    Top1 52.140000    Top5 82.710000    
2024-02-17 11:45:21,742 - ==> Top1: 52.140    Top5: 82.710    Loss: 1.762

2024-02-17 11:45:21,760 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:45:21,761 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:45:21,837 - 

2024-02-17 11:45:21,838 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:45:31,970 - Epoch: [80][  100/  391]    Overall Loss 1.183672    Objective Loss 1.183672                                        LR 0.100000    Time 0.101246    
2024-02-17 11:45:41,371 - Epoch: [80][  200/  391]    Overall Loss 1.188290    Objective Loss 1.188290                                        LR 0.100000    Time 0.097606    
2024-02-17 11:45:50,647 - Epoch: [80][  300/  391]    Overall Loss 1.207868    Objective Loss 1.207868                                        LR 0.100000    Time 0.095973    
2024-02-17 11:45:58,255 - Epoch: [80][  391/  391]    Overall Loss 1.216519    Objective Loss 1.216519    Top1 59.615385    Top5 87.019231    LR 0.100000    Time 0.093082    
2024-02-17 11:45:58,426 - --- validate (epoch=80)-----------
2024-02-17 11:45:58,427 - 10000 samples (128 per mini-batch)
2024-02-17 11:46:00,965 - Epoch: [80][   79/   79]    Loss 1.780596    Top1 51.910000    Top5 82.270000    
2024-02-17 11:46:01,076 - ==> Top1: 51.910    Top5: 82.270    Loss: 1.781

2024-02-17 11:46:01,094 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:46:01,094 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:46:01,168 - 

2024-02-17 11:46:01,169 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:46:11,157 - Epoch: [81][  100/  391]    Overall Loss 1.154308    Objective Loss 1.154308                                        LR 0.100000    Time 0.099802    
2024-02-17 11:46:20,658 - Epoch: [81][  200/  391]    Overall Loss 1.192567    Objective Loss 1.192567                                        LR 0.100000    Time 0.097384    
2024-02-17 11:46:30,164 - Epoch: [81][  300/  391]    Overall Loss 1.212697    Objective Loss 1.212697                                        LR 0.100000    Time 0.096592    
2024-02-17 11:46:38,525 - Epoch: [81][  391/  391]    Overall Loss 1.216576    Objective Loss 1.216576    Top1 65.865385    Top5 88.461538    LR 0.100000    Time 0.095483    
2024-02-17 11:46:38,643 - --- validate (epoch=81)-----------
2024-02-17 11:46:38,644 - 10000 samples (128 per mini-batch)
2024-02-17 11:46:41,244 - Epoch: [81][   79/   79]    Loss 1.901255    Top1 50.050000    Top5 80.380000    
2024-02-17 11:46:41,356 - ==> Top1: 50.050    Top5: 80.380    Loss: 1.901

2024-02-17 11:46:41,374 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:46:41,375 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:46:41,450 - 

2024-02-17 11:46:41,451 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:46:51,696 - Epoch: [82][  100/  391]    Overall Loss 1.181685    Objective Loss 1.181685                                        LR 0.100000    Time 0.102377    
2024-02-17 11:47:01,249 - Epoch: [82][  200/  391]    Overall Loss 1.191637    Objective Loss 1.191637                                        LR 0.100000    Time 0.098929    
2024-02-17 11:47:10,707 - Epoch: [82][  300/  391]    Overall Loss 1.195987    Objective Loss 1.195987                                        LR 0.100000    Time 0.097462    
2024-02-17 11:47:18,992 - Epoch: [82][  391/  391]    Overall Loss 1.205439    Objective Loss 1.205439    Top1 65.384615    Top5 88.461538    LR 0.100000    Time 0.095957    
2024-02-17 11:47:19,123 - --- validate (epoch=82)-----------
2024-02-17 11:47:19,124 - 10000 samples (128 per mini-batch)
2024-02-17 11:47:21,750 - Epoch: [82][   79/   79]    Loss 1.780336    Top1 52.640000    Top5 82.950000    
2024-02-17 11:47:21,872 - ==> Top1: 52.640    Top5: 82.950    Loss: 1.780

2024-02-17 11:47:21,891 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:47:21,891 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:47:21,966 - 

2024-02-17 11:47:21,966 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:47:31,900 - Epoch: [83][  100/  391]    Overall Loss 1.189221    Objective Loss 1.189221                                        LR 0.100000    Time 0.099260    
2024-02-17 11:47:41,319 - Epoch: [83][  200/  391]    Overall Loss 1.189462    Objective Loss 1.189462                                        LR 0.100000    Time 0.096701    
2024-02-17 11:47:50,799 - Epoch: [83][  300/  391]    Overall Loss 1.210404    Objective Loss 1.210404                                        LR 0.100000    Time 0.096053    
2024-02-17 11:47:59,163 - Epoch: [83][  391/  391]    Overall Loss 1.212851    Objective Loss 1.212851    Top1 72.596154    Top5 90.865385    LR 0.100000    Time 0.095079    
2024-02-17 11:47:59,284 - --- validate (epoch=83)-----------
2024-02-17 11:47:59,285 - 10000 samples (128 per mini-batch)
2024-02-17 11:48:02,003 - Epoch: [83][   79/   79]    Loss 1.912212    Top1 49.810000    Top5 80.350000    
2024-02-17 11:48:02,121 - ==> Top1: 49.810    Top5: 80.350    Loss: 1.912

2024-02-17 11:48:02,140 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:48:02,140 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:48:02,215 - 

2024-02-17 11:48:02,216 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:48:12,430 - Epoch: [84][  100/  391]    Overall Loss 1.144011    Objective Loss 1.144011                                        LR 0.100000    Time 0.102067    
2024-02-17 11:48:21,883 - Epoch: [84][  200/  391]    Overall Loss 1.156792    Objective Loss 1.156792                                        LR 0.100000    Time 0.098273    
2024-02-17 11:48:31,389 - Epoch: [84][  300/  391]    Overall Loss 1.189619    Objective Loss 1.189619                                        LR 0.100000    Time 0.097188    
2024-02-17 11:48:39,730 - Epoch: [84][  391/  391]    Overall Loss 1.205060    Objective Loss 1.205060    Top1 65.384615    Top5 91.346154    LR 0.100000    Time 0.095890    
2024-02-17 11:48:39,852 - --- validate (epoch=84)-----------
2024-02-17 11:48:39,853 - 10000 samples (128 per mini-batch)
2024-02-17 11:48:42,690 - Epoch: [84][   79/   79]    Loss 1.722928    Top1 53.200000    Top5 83.280000    
2024-02-17 11:48:42,800 - ==> Top1: 53.200    Top5: 83.280    Loss: 1.723

2024-02-17 11:48:42,823 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:48:42,823 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:48:42,913 - 

2024-02-17 11:48:42,914 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:48:52,889 - Epoch: [85][  100/  391]    Overall Loss 1.142040    Objective Loss 1.142040                                        LR 0.100000    Time 0.099679    
2024-02-17 11:49:02,112 - Epoch: [85][  200/  391]    Overall Loss 1.177139    Objective Loss 1.177139                                        LR 0.100000    Time 0.095925    
2024-02-17 11:49:11,401 - Epoch: [85][  300/  391]    Overall Loss 1.182821    Objective Loss 1.182821                                        LR 0.100000    Time 0.094898    
2024-02-17 11:49:19,827 - Epoch: [85][  391/  391]    Overall Loss 1.191620    Objective Loss 1.191620    Top1 67.788462    Top5 89.423077    LR 0.100000    Time 0.094351    
2024-02-17 11:49:19,989 - --- validate (epoch=85)-----------
2024-02-17 11:49:19,990 - 10000 samples (128 per mini-batch)
2024-02-17 11:49:22,756 - Epoch: [85][   79/   79]    Loss 1.687304    Top1 54.170000    Top5 83.350000    
2024-02-17 11:49:22,871 - ==> Top1: 54.170    Top5: 83.350    Loss: 1.687

2024-02-17 11:49:22,885 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:49:22,886 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:49:22,963 - 

2024-02-17 11:49:22,964 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:49:32,751 - Epoch: [86][  100/  391]    Overall Loss 1.130520    Objective Loss 1.130520                                        LR 0.100000    Time 0.097800    
2024-02-17 11:49:42,344 - Epoch: [86][  200/  391]    Overall Loss 1.160144    Objective Loss 1.160144                                        LR 0.100000    Time 0.096835    
2024-02-17 11:49:51,872 - Epoch: [86][  300/  391]    Overall Loss 1.178367    Objective Loss 1.178367                                        LR 0.100000    Time 0.096300    
2024-02-17 11:50:00,732 - Epoch: [86][  391/  391]    Overall Loss 1.190103    Objective Loss 1.190103    Top1 63.942308    Top5 87.980769    LR 0.100000    Time 0.096537    
2024-02-17 11:50:00,887 - --- validate (epoch=86)-----------
2024-02-17 11:50:00,888 - 10000 samples (128 per mini-batch)
2024-02-17 11:50:04,113 - Epoch: [86][   79/   79]    Loss 1.617547    Top1 55.120000    Top5 84.970000    
2024-02-17 11:50:04,226 - ==> Top1: 55.120    Top5: 84.970    Loss: 1.618

2024-02-17 11:50:04,245 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:50:04,246 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:50:04,340 - 

2024-02-17 11:50:04,340 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:50:14,589 - Epoch: [87][  100/  391]    Overall Loss 1.147955    Objective Loss 1.147955                                        LR 0.100000    Time 0.102412    
2024-02-17 11:50:24,172 - Epoch: [87][  200/  391]    Overall Loss 1.160226    Objective Loss 1.160226                                        LR 0.100000    Time 0.099093    
2024-02-17 11:50:33,330 - Epoch: [87][  300/  391]    Overall Loss 1.174703    Objective Loss 1.174703                                        LR 0.100000    Time 0.096571    
2024-02-17 11:50:41,327 - Epoch: [87][  391/  391]    Overall Loss 1.181049    Objective Loss 1.181049    Top1 60.096154    Top5 89.423077    LR 0.100000    Time 0.094539    
2024-02-17 11:50:41,451 - --- validate (epoch=87)-----------
2024-02-17 11:50:41,452 - 10000 samples (128 per mini-batch)
2024-02-17 11:50:44,169 - Epoch: [87][   79/   79]    Loss 1.951335    Top1 49.510000    Top5 80.040000    
2024-02-17 11:50:44,281 - ==> Top1: 49.510    Top5: 80.040    Loss: 1.951

2024-02-17 11:50:44,299 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:50:44,299 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:50:44,375 - 

2024-02-17 11:50:44,375 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:50:54,500 - Epoch: [88][  100/  391]    Overall Loss 1.139977    Objective Loss 1.139977                                        LR 0.100000    Time 0.101175    
2024-02-17 11:51:03,944 - Epoch: [88][  200/  391]    Overall Loss 1.153389    Objective Loss 1.153389                                        LR 0.100000    Time 0.097780    
2024-02-17 11:51:13,521 - Epoch: [88][  300/  391]    Overall Loss 1.167938    Objective Loss 1.167938                                        LR 0.100000    Time 0.097094    
2024-02-17 11:51:22,210 - Epoch: [88][  391/  391]    Overall Loss 1.181036    Objective Loss 1.181036    Top1 69.230769    Top5 93.269231    LR 0.100000    Time 0.096709    
2024-02-17 11:51:22,336 - --- validate (epoch=88)-----------
2024-02-17 11:51:22,337 - 10000 samples (128 per mini-batch)
2024-02-17 11:51:25,119 - Epoch: [88][   79/   79]    Loss 1.785244    Top1 52.210000    Top5 82.760000    
2024-02-17 11:51:25,273 - ==> Top1: 52.210    Top5: 82.760    Loss: 1.785

2024-02-17 11:51:25,292 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:51:25,292 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:51:25,368 - 

2024-02-17 11:51:25,368 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:51:35,309 - Epoch: [89][  100/  391]    Overall Loss 1.127520    Objective Loss 1.127520                                        LR 0.100000    Time 0.099331    
2024-02-17 11:51:44,613 - Epoch: [89][  200/  391]    Overall Loss 1.147720    Objective Loss 1.147720                                        LR 0.100000    Time 0.096158    
2024-02-17 11:51:53,719 - Epoch: [89][  300/  391]    Overall Loss 1.166428    Objective Loss 1.166428                                        LR 0.100000    Time 0.094445    
2024-02-17 11:52:01,999 - Epoch: [89][  391/  391]    Overall Loss 1.177647    Objective Loss 1.177647    Top1 65.384615    Top5 89.423077    LR 0.100000    Time 0.093628    
2024-02-17 11:52:02,158 - --- validate (epoch=89)-----------
2024-02-17 11:52:02,160 - 10000 samples (128 per mini-batch)
2024-02-17 11:52:04,994 - Epoch: [89][   79/   79]    Loss 1.973575    Top1 50.370000    Top5 78.920000    
2024-02-17 11:52:05,115 - ==> Top1: 50.370    Top5: 78.920    Loss: 1.974

2024-02-17 11:52:05,132 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:52:05,133 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:52:05,209 - 

2024-02-17 11:52:05,209 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:52:15,523 - Epoch: [90][  100/  391]    Overall Loss 1.121073    Objective Loss 1.121073                                        LR 0.100000    Time 0.103063    
2024-02-17 11:52:25,289 - Epoch: [90][  200/  391]    Overall Loss 1.145805    Objective Loss 1.145805                                        LR 0.100000    Time 0.100341    
2024-02-17 11:52:34,563 - Epoch: [90][  300/  391]    Overall Loss 1.156484    Objective Loss 1.156484                                        LR 0.100000    Time 0.097792    
2024-02-17 11:52:43,093 - Epoch: [90][  391/  391]    Overall Loss 1.164850    Objective Loss 1.164850    Top1 64.423077    Top5 89.423077    LR 0.100000    Time 0.096834    
2024-02-17 11:52:43,220 - --- validate (epoch=90)-----------
2024-02-17 11:52:43,222 - 10000 samples (128 per mini-batch)
2024-02-17 11:52:45,989 - Epoch: [90][   79/   79]    Loss 1.671971    Top1 54.660000    Top5 83.970000    
2024-02-17 11:52:46,103 - ==> Top1: 54.660    Top5: 83.970    Loss: 1.672

2024-02-17 11:52:46,130 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:52:46,131 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:52:46,220 - 

2024-02-17 11:52:46,220 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:52:56,012 - Epoch: [91][  100/  391]    Overall Loss 1.106952    Objective Loss 1.106952                                        LR 0.100000    Time 0.097837    
2024-02-17 11:53:05,660 - Epoch: [91][  200/  391]    Overall Loss 1.141084    Objective Loss 1.141084                                        LR 0.100000    Time 0.097133    
2024-02-17 11:53:14,913 - Epoch: [91][  300/  391]    Overall Loss 1.156907    Objective Loss 1.156907                                        LR 0.100000    Time 0.095582    
2024-02-17 11:53:23,760 - Epoch: [91][  391/  391]    Overall Loss 1.162098    Objective Loss 1.162098    Top1 61.538462    Top5 86.538462    LR 0.100000    Time 0.095952    
2024-02-17 11:53:23,949 - --- validate (epoch=91)-----------
2024-02-17 11:53:23,950 - 10000 samples (128 per mini-batch)
2024-02-17 11:53:26,775 - Epoch: [91][   79/   79]    Loss 1.832772    Top1 52.330000    Top5 81.380000    
2024-02-17 11:53:26,927 - ==> Top1: 52.330    Top5: 81.380    Loss: 1.833

2024-02-17 11:53:26,940 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:53:26,940 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:53:27,018 - 

2024-02-17 11:53:27,019 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:53:37,085 - Epoch: [92][  100/  391]    Overall Loss 1.128679    Objective Loss 1.128679                                        LR 0.100000    Time 0.100588    
2024-02-17 11:53:46,602 - Epoch: [92][  200/  391]    Overall Loss 1.136649    Objective Loss 1.136649                                        LR 0.100000    Time 0.097852    
2024-02-17 11:53:55,835 - Epoch: [92][  300/  391]    Overall Loss 1.152592    Objective Loss 1.152592                                        LR 0.100000    Time 0.095994    
2024-02-17 11:54:04,532 - Epoch: [92][  391/  391]    Overall Loss 1.165580    Objective Loss 1.165580    Top1 61.057692    Top5 88.461538    LR 0.100000    Time 0.095885    
2024-02-17 11:54:04,666 - --- validate (epoch=92)-----------
2024-02-17 11:54:04,667 - 10000 samples (128 per mini-batch)
2024-02-17 11:54:07,316 - Epoch: [92][   79/   79]    Loss 1.663866    Top1 55.390000    Top5 84.270000    
2024-02-17 11:54:07,508 - ==> Top1: 55.390    Top5: 84.270    Loss: 1.664

2024-02-17 11:54:07,518 - ==> Best [Top1: 55.390   Top5: 84.270   Sparsity:0.00   Params: 1341960 on epoch: 92]
2024-02-17 11:54:07,519 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:54:07,612 - 

2024-02-17 11:54:07,612 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:54:17,494 - Epoch: [93][  100/  391]    Overall Loss 1.132603    Objective Loss 1.132603                                        LR 0.100000    Time 0.098743    
2024-02-17 11:54:27,000 - Epoch: [93][  200/  391]    Overall Loss 1.157660    Objective Loss 1.157660                                        LR 0.100000    Time 0.096874    
2024-02-17 11:54:36,030 - Epoch: [93][  300/  391]    Overall Loss 1.163294    Objective Loss 1.163294                                        LR 0.100000    Time 0.094666    
2024-02-17 11:54:44,542 - Epoch: [93][  391/  391]    Overall Loss 1.168071    Objective Loss 1.168071    Top1 64.903846    Top5 92.307692    LR 0.100000    Time 0.094392    
2024-02-17 11:54:44,685 - --- validate (epoch=93)-----------
2024-02-17 11:54:44,686 - 10000 samples (128 per mini-batch)
2024-02-17 11:54:47,360 - Epoch: [93][   79/   79]    Loss 1.778660    Top1 53.460000    Top5 82.040000    
2024-02-17 11:54:47,487 - ==> Top1: 53.460    Top5: 82.040    Loss: 1.779

2024-02-17 11:54:47,506 - ==> Best [Top1: 55.390   Top5: 84.270   Sparsity:0.00   Params: 1341960 on epoch: 92]
2024-02-17 11:54:47,506 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:54:47,587 - 

2024-02-17 11:54:47,587 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:54:56,422 - Epoch: [94][  100/  391]    Overall Loss 1.124040    Objective Loss 1.124040                                        LR 0.100000    Time 0.088277    
2024-02-17 11:55:03,732 - Epoch: [94][  200/  391]    Overall Loss 1.130566    Objective Loss 1.130566                                        LR 0.100000    Time 0.080668    
2024-02-17 11:55:11,381 - Epoch: [94][  300/  391]    Overall Loss 1.136517    Objective Loss 1.136517                                        LR 0.100000    Time 0.079262    
2024-02-17 11:55:19,821 - Epoch: [94][  391/  391]    Overall Loss 1.153137    Objective Loss 1.153137    Top1 62.019231    Top5 90.865385    LR 0.100000    Time 0.082391    
2024-02-17 11:55:19,984 - --- validate (epoch=94)-----------
2024-02-17 11:55:19,985 - 10000 samples (128 per mini-batch)
2024-02-17 11:55:22,666 - Epoch: [94][   79/   79]    Loss 1.611510    Top1 55.400000    Top5 84.510000    
2024-02-17 11:55:22,805 - ==> Top1: 55.400    Top5: 84.510    Loss: 1.612

2024-02-17 11:55:22,816 - ==> Best [Top1: 55.400   Top5: 84.510   Sparsity:0.00   Params: 1341960 on epoch: 94]
2024-02-17 11:55:22,817 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:55:22,915 - 

2024-02-17 11:55:22,916 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:55:32,909 - Epoch: [95][  100/  391]    Overall Loss 1.102432    Objective Loss 1.102432                                        LR 0.100000    Time 0.099860    
2024-02-17 11:55:42,748 - Epoch: [95][  200/  391]    Overall Loss 1.122185    Objective Loss 1.122185                                        LR 0.100000    Time 0.099096    
2024-02-17 11:55:52,040 - Epoch: [95][  300/  391]    Overall Loss 1.136620    Objective Loss 1.136620                                        LR 0.100000    Time 0.097022    
2024-02-17 11:56:00,403 - Epoch: [95][  391/  391]    Overall Loss 1.147500    Objective Loss 1.147500    Top1 66.826923    Top5 92.307692    LR 0.100000    Time 0.095821    
2024-02-17 11:56:00,582 - --- validate (epoch=95)-----------
2024-02-17 11:56:00,583 - 10000 samples (128 per mini-batch)
2024-02-17 11:56:03,354 - Epoch: [95][   79/   79]    Loss 1.925001    Top1 49.310000    Top5 79.670000    
2024-02-17 11:56:03,508 - ==> Top1: 49.310    Top5: 79.670    Loss: 1.925

2024-02-17 11:56:03,531 - ==> Best [Top1: 55.400   Top5: 84.510   Sparsity:0.00   Params: 1341960 on epoch: 94]
2024-02-17 11:56:03,532 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:56:03,618 - 

2024-02-17 11:56:03,618 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:56:11,921 - Epoch: [96][  100/  391]    Overall Loss 1.119518    Objective Loss 1.119518                                        LR 0.100000    Time 0.082962    
2024-02-17 11:56:21,502 - Epoch: [96][  200/  391]    Overall Loss 1.122402    Objective Loss 1.122402                                        LR 0.100000    Time 0.089360    
2024-02-17 11:56:30,806 - Epoch: [96][  300/  391]    Overall Loss 1.134191    Objective Loss 1.134191                                        LR 0.100000    Time 0.090570    
2024-02-17 11:56:39,108 - Epoch: [96][  391/  391]    Overall Loss 1.143472    Objective Loss 1.143472    Top1 58.653846    Top5 87.980769    LR 0.100000    Time 0.090713    
2024-02-17 11:56:39,217 - --- validate (epoch=96)-----------
2024-02-17 11:56:39,218 - 10000 samples (128 per mini-batch)
2024-02-17 11:56:42,339 - Epoch: [96][   79/   79]    Loss 1.566625    Top1 57.290000    Top5 85.610000    
2024-02-17 11:56:42,465 - ==> Top1: 57.290    Top5: 85.610    Loss: 1.567

2024-02-17 11:56:42,485 - ==> Best [Top1: 57.290   Top5: 85.610   Sparsity:0.00   Params: 1341960 on epoch: 96]
2024-02-17 11:56:42,485 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:56:42,580 - 

2024-02-17 11:56:42,581 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:56:52,409 - Epoch: [97][  100/  391]    Overall Loss 1.097953    Objective Loss 1.097953                                        LR 0.100000    Time 0.098206    
2024-02-17 11:57:01,506 - Epoch: [97][  200/  391]    Overall Loss 1.116361    Objective Loss 1.116361                                        LR 0.100000    Time 0.094561    
2024-02-17 11:57:10,820 - Epoch: [97][  300/  391]    Overall Loss 1.132750    Objective Loss 1.132750                                        LR 0.100000    Time 0.094072    
2024-02-17 11:57:19,300 - Epoch: [97][  391/  391]    Overall Loss 1.143397    Objective Loss 1.143397    Top1 67.788462    Top5 91.826923    LR 0.100000    Time 0.093854    
2024-02-17 11:57:19,442 - --- validate (epoch=97)-----------
2024-02-17 11:57:19,443 - 10000 samples (128 per mini-batch)
2024-02-17 11:57:22,213 - Epoch: [97][   79/   79]    Loss 1.555645    Top1 57.460000    Top5 84.990000    
2024-02-17 11:57:22,331 - ==> Top1: 57.460    Top5: 84.990    Loss: 1.556

2024-02-17 11:57:22,342 - ==> Best [Top1: 57.460   Top5: 84.990   Sparsity:0.00   Params: 1341960 on epoch: 97]
2024-02-17 11:57:22,342 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:57:22,458 - 

2024-02-17 11:57:22,459 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:57:32,806 - Epoch: [98][  100/  391]    Overall Loss 1.112636    Objective Loss 1.112636                                        LR 0.100000    Time 0.103390    
2024-02-17 11:57:42,297 - Epoch: [98][  200/  391]    Overall Loss 1.116951    Objective Loss 1.116951                                        LR 0.100000    Time 0.099128    
2024-02-17 11:57:51,726 - Epoch: [98][  300/  391]    Overall Loss 1.128124    Objective Loss 1.128124                                        LR 0.100000    Time 0.097498    
2024-02-17 11:58:00,368 - Epoch: [98][  391/  391]    Overall Loss 1.137896    Objective Loss 1.137896    Top1 58.653846    Top5 89.903846    LR 0.100000    Time 0.096895    
2024-02-17 11:58:00,471 - --- validate (epoch=98)-----------
2024-02-17 11:58:00,472 - 10000 samples (128 per mini-batch)
2024-02-17 11:58:03,249 - Epoch: [98][   79/   79]    Loss 1.759463    Top1 53.370000    Top5 83.050000    
2024-02-17 11:58:03,376 - ==> Top1: 53.370    Top5: 83.050    Loss: 1.759

2024-02-17 11:58:03,396 - ==> Best [Top1: 57.460   Top5: 84.990   Sparsity:0.00   Params: 1341960 on epoch: 97]
2024-02-17 11:58:03,397 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:58:03,478 - 

2024-02-17 11:58:03,479 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:58:12,964 - Epoch: [99][  100/  391]    Overall Loss 1.093764    Objective Loss 1.093764                                        LR 0.100000    Time 0.094769    
2024-02-17 11:58:20,168 - Epoch: [99][  200/  391]    Overall Loss 1.125451    Objective Loss 1.125451                                        LR 0.100000    Time 0.083383    
2024-02-17 11:58:27,323 - Epoch: [99][  300/  391]    Overall Loss 1.129275    Objective Loss 1.129275                                        LR 0.100000    Time 0.079426    
2024-02-17 11:58:34,216 - Epoch: [99][  391/  391]    Overall Loss 1.134985    Objective Loss 1.134985    Top1 65.384615    Top5 91.346154    LR 0.100000    Time 0.078559    
2024-02-17 11:58:34,390 - --- validate (epoch=99)-----------
2024-02-17 11:58:34,390 - 10000 samples (128 per mini-batch)
2024-02-17 11:58:37,078 - Epoch: [99][   79/   79]    Loss 1.652775    Top1 55.010000    Top5 84.220000    
2024-02-17 11:58:37,230 - ==> Top1: 55.010    Top5: 84.220    Loss: 1.653

2024-02-17 11:58:37,241 - ==> Best [Top1: 57.460   Top5: 84.990   Sparsity:0.00   Params: 1341960 on epoch: 97]
2024-02-17 11:58:37,241 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:58:37,313 - 

2024-02-17 11:58:37,313 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:58:47,335 - Epoch: [100][  100/  391]    Overall Loss 0.903404    Objective Loss 0.903404                                        LR 0.023500    Time 0.100145    
2024-02-17 11:58:56,478 - Epoch: [100][  200/  391]    Overall Loss 0.855386    Objective Loss 0.855386                                        LR 0.023500    Time 0.095760    
2024-02-17 11:59:05,882 - Epoch: [100][  300/  391]    Overall Loss 0.837106    Objective Loss 0.837106                                        LR 0.023500    Time 0.095169    
2024-02-17 11:59:14,441 - Epoch: [100][  391/  391]    Overall Loss 0.827087    Objective Loss 0.827087    Top1 75.480769    Top5 95.673077    LR 0.023500    Time 0.094898    
2024-02-17 11:59:14,578 - --- validate (epoch=100)-----------
2024-02-17 11:59:14,579 - 10000 samples (128 per mini-batch)
2024-02-17 11:59:17,322 - Epoch: [100][   79/   79]    Loss 1.230817    Top1 65.550000    Top5 90.210000    
2024-02-17 11:59:17,430 - ==> Top1: 65.550    Top5: 90.210    Loss: 1.231

2024-02-17 11:59:17,440 - ==> Best [Top1: 65.550   Top5: 90.210   Sparsity:0.00   Params: 1341960 on epoch: 100]
2024-02-17 11:59:17,440 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:59:17,531 - 

2024-02-17 11:59:17,531 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:59:27,269 - Epoch: [101][  100/  391]    Overall Loss 0.744482    Objective Loss 0.744482                                        LR 0.023500    Time 0.097302    
2024-02-17 11:59:36,407 - Epoch: [101][  200/  391]    Overall Loss 0.752669    Objective Loss 0.752669                                        LR 0.023500    Time 0.094317    
2024-02-17 11:59:45,915 - Epoch: [101][  300/  391]    Overall Loss 0.749218    Objective Loss 0.749218                                        LR 0.023500    Time 0.094555    
2024-02-17 11:59:54,795 - Epoch: [101][  391/  391]    Overall Loss 0.749171    Objective Loss 0.749171    Top1 78.365385    Top5 96.153846    LR 0.023500    Time 0.095247    
2024-02-17 11:59:54,925 - --- validate (epoch=101)-----------
2024-02-17 11:59:54,927 - 10000 samples (128 per mini-batch)
2024-02-17 11:59:57,639 - Epoch: [101][   79/   79]    Loss 1.232268    Top1 65.080000    Top5 90.050000    
2024-02-17 11:59:57,741 - ==> Top1: 65.080    Top5: 90.050    Loss: 1.232

2024-02-17 11:59:57,750 - ==> Best [Top1: 65.550   Top5: 90.210   Sparsity:0.00   Params: 1341960 on epoch: 100]
2024-02-17 11:59:57,750 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 11:59:57,823 - 

2024-02-17 11:59:57,823 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:00:07,486 - Epoch: [102][  100/  391]    Overall Loss 0.727950    Objective Loss 0.727950                                        LR 0.023500    Time 0.096559    
2024-02-17 12:00:16,623 - Epoch: [102][  200/  391]    Overall Loss 0.724930    Objective Loss 0.724930                                        LR 0.023500    Time 0.093940    
2024-02-17 12:00:25,959 - Epoch: [102][  300/  391]    Overall Loss 0.721203    Objective Loss 0.721203                                        LR 0.023500    Time 0.093731    
2024-02-17 12:00:34,539 - Epoch: [102][  391/  391]    Overall Loss 0.721563    Objective Loss 0.721563    Top1 74.038462    Top5 95.192308    LR 0.023500    Time 0.093848    
2024-02-17 12:00:34,688 - --- validate (epoch=102)-----------
2024-02-17 12:00:34,689 - 10000 samples (128 per mini-batch)
2024-02-17 12:00:37,618 - Epoch: [102][   79/   79]    Loss 1.217137    Top1 65.640000    Top5 90.050000    
2024-02-17 12:00:37,731 - ==> Top1: 65.640    Top5: 90.050    Loss: 1.217

2024-02-17 12:00:37,749 - ==> Best [Top1: 65.640   Top5: 90.050   Sparsity:0.00   Params: 1341960 on epoch: 102]
2024-02-17 12:00:37,749 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:00:37,846 - 

2024-02-17 12:00:37,846 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:00:47,629 - Epoch: [103][  100/  391]    Overall Loss 0.680840    Objective Loss 0.680840                                        LR 0.023500    Time 0.097754    
2024-02-17 12:00:57,006 - Epoch: [103][  200/  391]    Overall Loss 0.682802    Objective Loss 0.682802                                        LR 0.023500    Time 0.095735    
2024-02-17 12:01:06,237 - Epoch: [103][  300/  391]    Overall Loss 0.691215    Objective Loss 0.691215                                        LR 0.023500    Time 0.094576    
2024-02-17 12:01:15,075 - Epoch: [103][  391/  391]    Overall Loss 0.697680    Objective Loss 0.697680    Top1 85.096154    Top5 95.673077    LR 0.023500    Time 0.095157    
2024-02-17 12:01:15,191 - --- validate (epoch=103)-----------
2024-02-17 12:01:15,191 - 10000 samples (128 per mini-batch)
2024-02-17 12:01:17,926 - Epoch: [103][   79/   79]    Loss 1.242191    Top1 65.060000    Top5 89.730000    
2024-02-17 12:01:18,083 - ==> Top1: 65.060    Top5: 89.730    Loss: 1.242

2024-02-17 12:01:18,102 - ==> Best [Top1: 65.640   Top5: 90.050   Sparsity:0.00   Params: 1341960 on epoch: 102]
2024-02-17 12:01:18,102 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:01:18,180 - 

2024-02-17 12:01:18,180 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:01:27,757 - Epoch: [104][  100/  391]    Overall Loss 0.668947    Objective Loss 0.668947                                        LR 0.023500    Time 0.095692    
2024-02-17 12:01:37,300 - Epoch: [104][  200/  391]    Overall Loss 0.680357    Objective Loss 0.680357                                        LR 0.023500    Time 0.095535    
2024-02-17 12:01:46,819 - Epoch: [104][  300/  391]    Overall Loss 0.679509    Objective Loss 0.679509                                        LR 0.023500    Time 0.095404    
2024-02-17 12:01:55,553 - Epoch: [104][  391/  391]    Overall Loss 0.687346    Objective Loss 0.687346    Top1 76.442308    Top5 95.192308    LR 0.023500    Time 0.095524    
2024-02-17 12:01:55,732 - --- validate (epoch=104)-----------
2024-02-17 12:01:55,733 - 10000 samples (128 per mini-batch)
2024-02-17 12:01:58,789 - Epoch: [104][   79/   79]    Loss 1.212613    Top1 66.040000    Top5 90.370000    
2024-02-17 12:01:58,923 - ==> Top1: 66.040    Top5: 90.370    Loss: 1.213

2024-02-17 12:01:58,942 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:01:58,942 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:01:59,034 - 

2024-02-17 12:01:59,035 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:02:08,849 - Epoch: [105][  100/  391]    Overall Loss 0.649788    Objective Loss 0.649788                                        LR 0.023500    Time 0.098061    
2024-02-17 12:02:18,471 - Epoch: [105][  200/  391]    Overall Loss 0.655619    Objective Loss 0.655619                                        LR 0.023500    Time 0.097114    
2024-02-17 12:02:28,382 - Epoch: [105][  300/  391]    Overall Loss 0.663308    Objective Loss 0.663308                                        LR 0.023500    Time 0.097765    
2024-02-17 12:02:37,067 - Epoch: [105][  391/  391]    Overall Loss 0.667088    Objective Loss 0.667088    Top1 77.884615    Top5 98.557692    LR 0.023500    Time 0.097211    
2024-02-17 12:02:37,199 - --- validate (epoch=105)-----------
2024-02-17 12:02:37,200 - 10000 samples (128 per mini-batch)
2024-02-17 12:02:40,012 - Epoch: [105][   79/   79]    Loss 1.222828    Top1 65.370000    Top5 90.160000    
2024-02-17 12:02:40,121 - ==> Top1: 65.370    Top5: 90.160    Loss: 1.223

2024-02-17 12:02:40,143 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:02:40,144 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:02:40,230 - 

2024-02-17 12:02:40,231 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:02:49,888 - Epoch: [106][  100/  391]    Overall Loss 0.647922    Objective Loss 0.647922                                        LR 0.023500    Time 0.096503    
2024-02-17 12:02:59,165 - Epoch: [106][  200/  391]    Overall Loss 0.656212    Objective Loss 0.656212                                        LR 0.023500    Time 0.094612    
2024-02-17 12:03:08,595 - Epoch: [106][  300/  391]    Overall Loss 0.650680    Objective Loss 0.650680                                        LR 0.023500    Time 0.094490    
2024-02-17 12:03:17,084 - Epoch: [106][  391/  391]    Overall Loss 0.652795    Objective Loss 0.652795    Top1 80.288462    Top5 96.634615    LR 0.023500    Time 0.094198    
2024-02-17 12:03:17,245 - --- validate (epoch=106)-----------
2024-02-17 12:03:17,245 - 10000 samples (128 per mini-batch)
2024-02-17 12:03:20,156 - Epoch: [106][   79/   79]    Loss 1.237146    Top1 65.390000    Top5 90.110000    
2024-02-17 12:03:20,311 - ==> Top1: 65.390    Top5: 90.110    Loss: 1.237

2024-02-17 12:03:20,323 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:03:20,323 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:03:20,394 - 

2024-02-17 12:03:20,394 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:03:30,001 - Epoch: [107][  100/  391]    Overall Loss 0.628644    Objective Loss 0.628644                                        LR 0.023500    Time 0.095988    
2024-02-17 12:03:39,748 - Epoch: [107][  200/  391]    Overall Loss 0.638219    Objective Loss 0.638219                                        LR 0.023500    Time 0.096707    
2024-02-17 12:03:49,472 - Epoch: [107][  300/  391]    Overall Loss 0.640107    Objective Loss 0.640107                                        LR 0.023500    Time 0.096866    
2024-02-17 12:03:58,252 - Epoch: [107][  391/  391]    Overall Loss 0.648960    Objective Loss 0.648960    Top1 79.326923    Top5 98.076923    LR 0.023500    Time 0.096765    
2024-02-17 12:03:58,368 - --- validate (epoch=107)-----------
2024-02-17 12:03:58,369 - 10000 samples (128 per mini-batch)
2024-02-17 12:04:01,146 - Epoch: [107][   79/   79]    Loss 1.260121    Top1 65.170000    Top5 89.580000    
2024-02-17 12:04:01,292 - ==> Top1: 65.170    Top5: 89.580    Loss: 1.260

2024-02-17 12:04:01,311 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:04:01,311 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:04:01,386 - 

2024-02-17 12:04:01,387 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:04:11,189 - Epoch: [108][  100/  391]    Overall Loss 0.618427    Objective Loss 0.618427                                        LR 0.023500    Time 0.097944    
2024-02-17 12:04:21,265 - Epoch: [108][  200/  391]    Overall Loss 0.626711    Objective Loss 0.626711                                        LR 0.023500    Time 0.099330    
2024-02-17 12:04:30,464 - Epoch: [108][  300/  391]    Overall Loss 0.635810    Objective Loss 0.635810                                        LR 0.023500    Time 0.096866    
2024-02-17 12:04:39,155 - Epoch: [108][  391/  391]    Overall Loss 0.640143    Objective Loss 0.640143    Top1 82.211538    Top5 99.519231    LR 0.023500    Time 0.096538    
2024-02-17 12:04:39,264 - --- validate (epoch=108)-----------
2024-02-17 12:04:39,265 - 10000 samples (128 per mini-batch)
2024-02-17 12:04:41,945 - Epoch: [108][   79/   79]    Loss 1.233425    Top1 65.550000    Top5 90.400000    
2024-02-17 12:04:42,059 - ==> Top1: 65.550    Top5: 90.400    Loss: 1.233

2024-02-17 12:04:42,077 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:04:42,077 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:04:42,152 - 

2024-02-17 12:04:42,153 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:04:51,913 - Epoch: [109][  100/  391]    Overall Loss 0.608839    Objective Loss 0.608839                                        LR 0.023500    Time 0.097526    
2024-02-17 12:05:01,447 - Epoch: [109][  200/  391]    Overall Loss 0.617831    Objective Loss 0.617831                                        LR 0.023500    Time 0.096406    
2024-02-17 12:05:11,120 - Epoch: [109][  300/  391]    Overall Loss 0.625404    Objective Loss 0.625404                                        LR 0.023500    Time 0.096496    
2024-02-17 12:05:19,883 - Epoch: [109][  391/  391]    Overall Loss 0.631449    Objective Loss 0.631449    Top1 78.365385    Top5 96.153846    LR 0.023500    Time 0.096437    
2024-02-17 12:05:20,024 - --- validate (epoch=109)-----------
2024-02-17 12:05:20,025 - 10000 samples (128 per mini-batch)
2024-02-17 12:05:22,801 - Epoch: [109][   79/   79]    Loss 1.257949    Top1 65.170000    Top5 89.880000    
2024-02-17 12:05:22,943 - ==> Top1: 65.170    Top5: 89.880    Loss: 1.258

2024-02-17 12:05:22,963 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:05:22,964 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:05:23,047 - 

2024-02-17 12:05:23,047 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:05:32,689 - Epoch: [110][  100/  391]    Overall Loss 0.593725    Objective Loss 0.593725                                        LR 0.023500    Time 0.096338    
2024-02-17 12:05:42,061 - Epoch: [110][  200/  391]    Overall Loss 0.605866    Objective Loss 0.605866                                        LR 0.023500    Time 0.095005    
2024-02-17 12:05:51,386 - Epoch: [110][  300/  391]    Overall Loss 0.614432    Objective Loss 0.614432                                        LR 0.023500    Time 0.094405    
2024-02-17 12:05:59,660 - Epoch: [110][  391/  391]    Overall Loss 0.621633    Objective Loss 0.621633    Top1 75.480769    Top5 95.192308    LR 0.023500    Time 0.093582    
2024-02-17 12:05:59,778 - --- validate (epoch=110)-----------
2024-02-17 12:05:59,779 - 10000 samples (128 per mini-batch)
2024-02-17 12:06:02,592 - Epoch: [110][   79/   79]    Loss 1.219577    Top1 66.110000    Top5 90.090000    
2024-02-17 12:06:02,717 - ==> Top1: 66.110    Top5: 90.090    Loss: 1.220

2024-02-17 12:06:02,730 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:06:02,730 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:06:02,820 - 

2024-02-17 12:06:02,820 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:06:12,131 - Epoch: [111][  100/  391]    Overall Loss 0.599360    Objective Loss 0.599360                                        LR 0.023500    Time 0.093026    
2024-02-17 12:06:21,438 - Epoch: [111][  200/  391]    Overall Loss 0.600780    Objective Loss 0.600780                                        LR 0.023500    Time 0.093024    
2024-02-17 12:06:31,142 - Epoch: [111][  300/  391]    Overall Loss 0.612345    Objective Loss 0.612345                                        LR 0.023500    Time 0.094347    
2024-02-17 12:06:40,154 - Epoch: [111][  391/  391]    Overall Loss 0.621217    Objective Loss 0.621217    Top1 82.211538    Top5 97.115385    LR 0.023500    Time 0.095424    
2024-02-17 12:06:40,333 - --- validate (epoch=111)-----------
2024-02-17 12:06:40,334 - 10000 samples (128 per mini-batch)
2024-02-17 12:06:42,915 - Epoch: [111][   79/   79]    Loss 1.307443    Top1 64.350000    Top5 89.100000    
2024-02-17 12:06:43,095 - ==> Top1: 64.350    Top5: 89.100    Loss: 1.307

2024-02-17 12:06:43,114 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:06:43,114 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:06:43,190 - 

2024-02-17 12:06:43,191 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:06:53,304 - Epoch: [112][  100/  391]    Overall Loss 0.589680    Objective Loss 0.589680                                        LR 0.023500    Time 0.101061    
2024-02-17 12:07:02,295 - Epoch: [112][  200/  391]    Overall Loss 0.594518    Objective Loss 0.594518                                        LR 0.023500    Time 0.095459    
2024-02-17 12:07:11,762 - Epoch: [112][  300/  391]    Overall Loss 0.599510    Objective Loss 0.599510                                        LR 0.023500    Time 0.095181    
2024-02-17 12:07:20,334 - Epoch: [112][  391/  391]    Overall Loss 0.609625    Objective Loss 0.609625    Top1 76.442308    Top5 96.153846    LR 0.023500    Time 0.094940    
2024-02-17 12:07:20,508 - --- validate (epoch=112)-----------
2024-02-17 12:07:20,508 - 10000 samples (128 per mini-batch)
2024-02-17 12:07:23,199 - Epoch: [112][   79/   79]    Loss 1.298559    Top1 64.930000    Top5 89.460000    
2024-02-17 12:07:23,312 - ==> Top1: 64.930    Top5: 89.460    Loss: 1.299

2024-02-17 12:07:23,331 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:07:23,332 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:07:23,413 - 

2024-02-17 12:07:23,413 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:07:33,352 - Epoch: [113][  100/  391]    Overall Loss 0.592065    Objective Loss 0.592065                                        LR 0.023500    Time 0.099305    
2024-02-17 12:07:42,295 - Epoch: [113][  200/  391]    Overall Loss 0.593936    Objective Loss 0.593936                                        LR 0.023500    Time 0.094343    
2024-02-17 12:07:51,898 - Epoch: [113][  300/  391]    Overall Loss 0.600268    Objective Loss 0.600268                                        LR 0.023500    Time 0.094890    
2024-02-17 12:08:00,478 - Epoch: [113][  391/  391]    Overall Loss 0.606185    Objective Loss 0.606185    Top1 76.442308    Top5 98.076923    LR 0.023500    Time 0.094737    
2024-02-17 12:08:00,623 - --- validate (epoch=113)-----------
2024-02-17 12:08:00,624 - 10000 samples (128 per mini-batch)
2024-02-17 12:08:03,185 - Epoch: [113][   79/   79]    Loss 1.317711    Top1 64.190000    Top5 89.450000    
2024-02-17 12:08:03,289 - ==> Top1: 64.190    Top5: 89.450    Loss: 1.318

2024-02-17 12:08:03,307 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:08:03,308 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:08:03,384 - 

2024-02-17 12:08:03,384 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:08:13,890 - Epoch: [114][  100/  391]    Overall Loss 0.574343    Objective Loss 0.574343                                        LR 0.023500    Time 0.104983    
2024-02-17 12:08:22,737 - Epoch: [114][  200/  391]    Overall Loss 0.586967    Objective Loss 0.586967                                        LR 0.023500    Time 0.096704    
2024-02-17 12:08:32,080 - Epoch: [114][  300/  391]    Overall Loss 0.596132    Objective Loss 0.596132                                        LR 0.023500    Time 0.095596    
2024-02-17 12:08:40,807 - Epoch: [114][  391/  391]    Overall Loss 0.605783    Objective Loss 0.605783    Top1 80.769231    Top5 96.153846    LR 0.023500    Time 0.095654    
2024-02-17 12:08:40,988 - --- validate (epoch=114)-----------
2024-02-17 12:08:40,989 - 10000 samples (128 per mini-batch)
2024-02-17 12:08:43,767 - Epoch: [114][   79/   79]    Loss 1.309465    Top1 64.210000    Top5 88.980000    
2024-02-17 12:08:43,881 - ==> Top1: 64.210    Top5: 88.980    Loss: 1.309

2024-02-17 12:08:43,892 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:08:43,892 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:08:43,969 - 

2024-02-17 12:08:43,970 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:08:53,589 - Epoch: [115][  100/  391]    Overall Loss 0.574954    Objective Loss 0.574954                                        LR 0.023500    Time 0.096112    
2024-02-17 12:09:02,501 - Epoch: [115][  200/  391]    Overall Loss 0.579106    Objective Loss 0.579106                                        LR 0.023500    Time 0.092593    
2024-02-17 12:09:11,889 - Epoch: [115][  300/  391]    Overall Loss 0.592322    Objective Loss 0.592322                                        LR 0.023500    Time 0.093006    
2024-02-17 12:09:20,585 - Epoch: [115][  391/  391]    Overall Loss 0.601133    Objective Loss 0.601133    Top1 81.730769    Top5 96.153846    LR 0.023500    Time 0.093591    
2024-02-17 12:09:20,751 - --- validate (epoch=115)-----------
2024-02-17 12:09:20,751 - 10000 samples (128 per mini-batch)
2024-02-17 12:09:23,427 - Epoch: [115][   79/   79]    Loss 1.320970    Top1 64.180000    Top5 88.950000    
2024-02-17 12:09:23,527 - ==> Top1: 64.180    Top5: 88.950    Loss: 1.321

2024-02-17 12:09:23,541 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:09:23,541 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:09:23,639 - 

2024-02-17 12:09:23,639 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:09:33,876 - Epoch: [116][  100/  391]    Overall Loss 0.590266    Objective Loss 0.590266                                        LR 0.023500    Time 0.102265    
2024-02-17 12:09:43,128 - Epoch: [116][  200/  391]    Overall Loss 0.591161    Objective Loss 0.591161                                        LR 0.023500    Time 0.097368    
2024-02-17 12:09:52,545 - Epoch: [116][  300/  391]    Overall Loss 0.597120    Objective Loss 0.597120                                        LR 0.023500    Time 0.096286    
2024-02-17 12:10:01,104 - Epoch: [116][  391/  391]    Overall Loss 0.602517    Objective Loss 0.602517    Top1 77.403846    Top5 96.634615    LR 0.023500    Time 0.095754    
2024-02-17 12:10:01,231 - --- validate (epoch=116)-----------
2024-02-17 12:10:01,232 - 10000 samples (128 per mini-batch)
2024-02-17 12:10:03,999 - Epoch: [116][   79/   79]    Loss 1.299381    Top1 65.010000    Top5 89.490000    
2024-02-17 12:10:04,106 - ==> Top1: 65.010    Top5: 89.490    Loss: 1.299

2024-02-17 12:10:04,124 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:10:04,125 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:10:04,200 - 

2024-02-17 12:10:04,201 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:10:14,186 - Epoch: [117][  100/  391]    Overall Loss 0.571592    Objective Loss 0.571592                                        LR 0.023500    Time 0.099775    
2024-02-17 12:10:23,118 - Epoch: [117][  200/  391]    Overall Loss 0.580793    Objective Loss 0.580793                                        LR 0.023500    Time 0.094524    
2024-02-17 12:10:32,335 - Epoch: [117][  300/  391]    Overall Loss 0.587905    Objective Loss 0.587905                                        LR 0.023500    Time 0.093726    
2024-02-17 12:10:40,872 - Epoch: [117][  391/  391]    Overall Loss 0.594131    Objective Loss 0.594131    Top1 77.403846    Top5 95.673077    LR 0.023500    Time 0.093735    
2024-02-17 12:10:41,045 - --- validate (epoch=117)-----------
2024-02-17 12:10:41,046 - 10000 samples (128 per mini-batch)
2024-02-17 12:10:43,810 - Epoch: [117][   79/   79]    Loss 1.343025    Top1 64.040000    Top5 88.790000    
2024-02-17 12:10:43,956 - ==> Top1: 64.040    Top5: 88.790    Loss: 1.343

2024-02-17 12:10:43,974 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:10:43,974 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:10:44,049 - 

2024-02-17 12:10:44,050 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:10:54,066 - Epoch: [118][  100/  391]    Overall Loss 0.565083    Objective Loss 0.565083                                        LR 0.023500    Time 0.100083    
2024-02-17 12:11:01,078 - Epoch: [118][  200/  391]    Overall Loss 0.568931    Objective Loss 0.568931                                        LR 0.023500    Time 0.085084    
2024-02-17 12:11:09,442 - Epoch: [118][  300/  391]    Overall Loss 0.579081    Objective Loss 0.579081                                        LR 0.023500    Time 0.084587    
2024-02-17 12:11:17,758 - Epoch: [118][  391/  391]    Overall Loss 0.589991    Objective Loss 0.589991    Top1 79.326923    Top5 97.596154    LR 0.023500    Time 0.086159    
2024-02-17 12:11:17,909 - --- validate (epoch=118)-----------
2024-02-17 12:11:17,910 - 10000 samples (128 per mini-batch)
2024-02-17 12:11:20,552 - Epoch: [118][   79/   79]    Loss 1.285393    Top1 65.080000    Top5 89.680000    
2024-02-17 12:11:20,662 - ==> Top1: 65.080    Top5: 89.680    Loss: 1.285

2024-02-17 12:11:20,685 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:11:20,685 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:11:20,764 - 

2024-02-17 12:11:20,765 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:11:31,107 - Epoch: [119][  100/  391]    Overall Loss 0.555229    Objective Loss 0.555229                                        LR 0.023500    Time 0.103341    
2024-02-17 12:11:39,994 - Epoch: [119][  200/  391]    Overall Loss 0.563886    Objective Loss 0.563886                                        LR 0.023500    Time 0.096082    
2024-02-17 12:11:48,347 - Epoch: [119][  300/  391]    Overall Loss 0.574306    Objective Loss 0.574306                                        LR 0.023500    Time 0.091884    
2024-02-17 12:11:57,048 - Epoch: [119][  391/  391]    Overall Loss 0.585766    Objective Loss 0.585766    Top1 77.403846    Top5 98.076923    LR 0.023500    Time 0.092741    
2024-02-17 12:11:57,187 - --- validate (epoch=119)-----------
2024-02-17 12:11:57,188 - 10000 samples (128 per mini-batch)
2024-02-17 12:11:59,973 - Epoch: [119][   79/   79]    Loss 1.281214    Top1 65.070000    Top5 89.780000    
2024-02-17 12:12:00,085 - ==> Top1: 65.070    Top5: 89.780    Loss: 1.281

2024-02-17 12:12:00,103 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:12:00,103 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:12:00,186 - 

2024-02-17 12:12:00,186 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:12:10,586 - Epoch: [120][  100/  391]    Overall Loss 0.544596    Objective Loss 0.544596                                        LR 0.023500    Time 0.103925    
2024-02-17 12:12:20,025 - Epoch: [120][  200/  391]    Overall Loss 0.561226    Objective Loss 0.561226                                        LR 0.023500    Time 0.099133    
2024-02-17 12:12:28,881 - Epoch: [120][  300/  391]    Overall Loss 0.570281    Objective Loss 0.570281                                        LR 0.023500    Time 0.095593    
2024-02-17 12:12:37,324 - Epoch: [120][  391/  391]    Overall Loss 0.578666    Objective Loss 0.578666    Top1 80.769231    Top5 98.557692    LR 0.023500    Time 0.094927    
2024-02-17 12:12:37,458 - --- validate (epoch=120)-----------
2024-02-17 12:12:37,459 - 10000 samples (128 per mini-batch)
2024-02-17 12:12:40,306 - Epoch: [120][   79/   79]    Loss 1.283224    Top1 64.780000    Top5 89.990000    
2024-02-17 12:12:40,410 - ==> Top1: 64.780    Top5: 89.990    Loss: 1.283

2024-02-17 12:12:40,427 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:12:40,428 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:12:40,509 - 

2024-02-17 12:12:40,509 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:12:50,586 - Epoch: [121][  100/  391]    Overall Loss 0.539578    Objective Loss 0.539578                                        LR 0.023500    Time 0.100686    
2024-02-17 12:13:00,182 - Epoch: [121][  200/  391]    Overall Loss 0.562012    Objective Loss 0.562012                                        LR 0.023500    Time 0.098299    
2024-02-17 12:13:09,110 - Epoch: [121][  300/  391]    Overall Loss 0.571322    Objective Loss 0.571322                                        LR 0.023500    Time 0.095277    
2024-02-17 12:13:16,907 - Epoch: [121][  391/  391]    Overall Loss 0.579879    Objective Loss 0.579879    Top1 83.173077    Top5 96.634615    LR 0.023500    Time 0.093031    
2024-02-17 12:13:17,044 - --- validate (epoch=121)-----------
2024-02-17 12:13:17,045 - 10000 samples (128 per mini-batch)
2024-02-17 12:13:19,879 - Epoch: [121][   79/   79]    Loss 1.297498    Top1 64.430000    Top5 89.680000    
2024-02-17 12:13:19,987 - ==> Top1: 64.430    Top5: 89.680    Loss: 1.297

2024-02-17 12:13:19,999 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:13:20,000 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:13:20,076 - 

2024-02-17 12:13:20,077 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:13:29,814 - Epoch: [122][  100/  391]    Overall Loss 0.538779    Objective Loss 0.538779                                        LR 0.023500    Time 0.097290    
2024-02-17 12:13:39,216 - Epoch: [122][  200/  391]    Overall Loss 0.547705    Objective Loss 0.547705                                        LR 0.023500    Time 0.095632    
2024-02-17 12:13:48,401 - Epoch: [122][  300/  391]    Overall Loss 0.560586    Objective Loss 0.560586                                        LR 0.023500    Time 0.094354    
2024-02-17 12:13:57,010 - Epoch: [122][  391/  391]    Overall Loss 0.568272    Objective Loss 0.568272    Top1 81.250000    Top5 97.596154    LR 0.023500    Time 0.094399    
2024-02-17 12:13:57,136 - --- validate (epoch=122)-----------
2024-02-17 12:13:57,136 - 10000 samples (128 per mini-batch)
2024-02-17 12:14:00,244 - Epoch: [122][   79/   79]    Loss 1.398728    Top1 63.080000    Top5 88.530000    
2024-02-17 12:14:00,380 - ==> Top1: 63.080    Top5: 88.530    Loss: 1.399

2024-02-17 12:14:00,399 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:14:00,399 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:14:00,479 - 

2024-02-17 12:14:00,480 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:14:10,961 - Epoch: [123][  100/  391]    Overall Loss 0.545120    Objective Loss 0.545120                                        LR 0.023500    Time 0.104727    
2024-02-17 12:14:20,629 - Epoch: [123][  200/  391]    Overall Loss 0.554063    Objective Loss 0.554063                                        LR 0.023500    Time 0.100675    
2024-02-17 12:14:29,813 - Epoch: [123][  300/  391]    Overall Loss 0.566640    Objective Loss 0.566640                                        LR 0.023500    Time 0.097714    
2024-02-17 12:14:37,596 - Epoch: [123][  391/  391]    Overall Loss 0.577591    Objective Loss 0.577591    Top1 75.961538    Top5 97.596154    LR 0.023500    Time 0.094866    
2024-02-17 12:14:37,718 - --- validate (epoch=123)-----------
2024-02-17 12:14:37,719 - 10000 samples (128 per mini-batch)
2024-02-17 12:14:40,544 - Epoch: [123][   79/   79]    Loss 1.291370    Top1 65.150000    Top5 89.640000    
2024-02-17 12:14:40,645 - ==> Top1: 65.150    Top5: 89.640    Loss: 1.291

2024-02-17 12:14:40,664 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:14:40,664 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:14:40,739 - 

2024-02-17 12:14:40,740 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:14:50,746 - Epoch: [124][  100/  391]    Overall Loss 0.551294    Objective Loss 0.551294                                        LR 0.023500    Time 0.099975    
2024-02-17 12:15:00,294 - Epoch: [124][  200/  391]    Overall Loss 0.559176    Objective Loss 0.559176                                        LR 0.023500    Time 0.097702    
2024-02-17 12:15:09,370 - Epoch: [124][  300/  391]    Overall Loss 0.567467    Objective Loss 0.567467                                        LR 0.023500    Time 0.095374    
2024-02-17 12:15:18,130 - Epoch: [124][  391/  391]    Overall Loss 0.575790    Objective Loss 0.575790    Top1 82.692308    Top5 97.596154    LR 0.023500    Time 0.095570    
2024-02-17 12:15:18,282 - --- validate (epoch=124)-----------
2024-02-17 12:15:18,283 - 10000 samples (128 per mini-batch)
2024-02-17 12:15:21,292 - Epoch: [124][   79/   79]    Loss 1.353544    Top1 64.360000    Top5 88.660000    
2024-02-17 12:15:21,404 - ==> Top1: 64.360    Top5: 88.660    Loss: 1.354

2024-02-17 12:15:21,424 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:15:21,425 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:15:21,499 - 

2024-02-17 12:15:21,500 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:15:31,740 - Epoch: [125][  100/  391]    Overall Loss 0.557675    Objective Loss 0.557675                                        LR 0.023500    Time 0.102327    
2024-02-17 12:15:40,928 - Epoch: [125][  200/  391]    Overall Loss 0.559029    Objective Loss 0.559029                                        LR 0.023500    Time 0.097076    
2024-02-17 12:15:48,584 - Epoch: [125][  300/  391]    Overall Loss 0.565728    Objective Loss 0.565728                                        LR 0.023500    Time 0.090224    
2024-02-17 12:15:55,542 - Epoch: [125][  391/  391]    Overall Loss 0.575140    Objective Loss 0.575140    Top1 86.538462    Top5 98.076923    LR 0.023500    Time 0.087010    
2024-02-17 12:15:55,665 - --- validate (epoch=125)-----------
2024-02-17 12:15:55,666 - 10000 samples (128 per mini-batch)
2024-02-17 12:15:58,452 - Epoch: [125][   79/   79]    Loss 1.319069    Top1 63.570000    Top5 89.240000    
2024-02-17 12:15:58,562 - ==> Top1: 63.570    Top5: 89.240    Loss: 1.319

2024-02-17 12:15:58,577 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:15:58,577 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:15:58,668 - 

2024-02-17 12:15:58,669 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:16:09,300 - Epoch: [126][  100/  391]    Overall Loss 0.531751    Objective Loss 0.531751                                        LR 0.023500    Time 0.106199    
2024-02-17 12:16:18,884 - Epoch: [126][  200/  391]    Overall Loss 0.548702    Objective Loss 0.548702                                        LR 0.023500    Time 0.100997    
2024-02-17 12:16:27,979 - Epoch: [126][  300/  391]    Overall Loss 0.562794    Objective Loss 0.562794                                        LR 0.023500    Time 0.097632    
2024-02-17 12:16:36,479 - Epoch: [126][  391/  391]    Overall Loss 0.566014    Objective Loss 0.566014    Top1 80.288462    Top5 98.076923    LR 0.023500    Time 0.096636    
2024-02-17 12:16:36,630 - --- validate (epoch=126)-----------
2024-02-17 12:16:36,630 - 10000 samples (128 per mini-batch)
2024-02-17 12:16:39,495 - Epoch: [126][   79/   79]    Loss 1.300004    Top1 64.620000    Top5 89.500000    
2024-02-17 12:16:39,603 - ==> Top1: 64.620    Top5: 89.500    Loss: 1.300

2024-02-17 12:16:39,621 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:16:39,621 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:16:39,703 - 

2024-02-17 12:16:39,704 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:16:49,736 - Epoch: [127][  100/  391]    Overall Loss 0.541477    Objective Loss 0.541477                                        LR 0.023500    Time 0.100243    
2024-02-17 12:16:59,265 - Epoch: [127][  200/  391]    Overall Loss 0.550580    Objective Loss 0.550580                                        LR 0.023500    Time 0.097745    
2024-02-17 12:17:08,695 - Epoch: [127][  300/  391]    Overall Loss 0.559187    Objective Loss 0.559187                                        LR 0.023500    Time 0.096579    
2024-02-17 12:17:17,093 - Epoch: [127][  391/  391]    Overall Loss 0.570268    Objective Loss 0.570268    Top1 78.846154    Top5 99.038462    LR 0.023500    Time 0.095570    
2024-02-17 12:17:17,214 - --- validate (epoch=127)-----------
2024-02-17 12:17:17,215 - 10000 samples (128 per mini-batch)
2024-02-17 12:17:20,007 - Epoch: [127][   79/   79]    Loss 1.424709    Top1 62.530000    Top5 88.140000    
2024-02-17 12:17:20,121 - ==> Top1: 62.530    Top5: 88.140    Loss: 1.425

2024-02-17 12:17:20,142 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:17:20,142 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:17:20,222 - 

2024-02-17 12:17:20,223 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:17:30,689 - Epoch: [128][  100/  391]    Overall Loss 0.533142    Objective Loss 0.533142                                        LR 0.023500    Time 0.104583    
2024-02-17 12:17:39,972 - Epoch: [128][  200/  391]    Overall Loss 0.540543    Objective Loss 0.540543                                        LR 0.023500    Time 0.098681    
2024-02-17 12:17:49,400 - Epoch: [128][  300/  391]    Overall Loss 0.556833    Objective Loss 0.556833                                        LR 0.023500    Time 0.097195    
2024-02-17 12:17:57,796 - Epoch: [128][  391/  391]    Overall Loss 0.567360    Objective Loss 0.567360    Top1 82.692308    Top5 98.076923    LR 0.023500    Time 0.096037    
2024-02-17 12:17:57,959 - --- validate (epoch=128)-----------
2024-02-17 12:17:57,960 - 10000 samples (128 per mini-batch)
2024-02-17 12:18:00,946 - Epoch: [128][   79/   79]    Loss 1.352653    Top1 63.800000    Top5 88.890000    
2024-02-17 12:18:01,085 - ==> Top1: 63.800    Top5: 88.890    Loss: 1.353

2024-02-17 12:18:01,108 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:18:01,108 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:18:01,187 - 

2024-02-17 12:18:01,188 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:18:11,221 - Epoch: [129][  100/  391]    Overall Loss 0.539323    Objective Loss 0.539323                                        LR 0.023500    Time 0.100259    
2024-02-17 12:18:20,790 - Epoch: [129][  200/  391]    Overall Loss 0.541701    Objective Loss 0.541701                                        LR 0.023500    Time 0.097948    
2024-02-17 12:18:30,309 - Epoch: [129][  300/  391]    Overall Loss 0.555666    Objective Loss 0.555666                                        LR 0.023500    Time 0.097013    
2024-02-17 12:18:38,673 - Epoch: [129][  391/  391]    Overall Loss 0.559215    Objective Loss 0.559215    Top1 81.250000    Top5 98.076923    LR 0.023500    Time 0.095814    
2024-02-17 12:18:38,808 - --- validate (epoch=129)-----------
2024-02-17 12:18:38,809 - 10000 samples (128 per mini-batch)
2024-02-17 12:18:41,627 - Epoch: [129][   79/   79]    Loss 1.362347    Top1 64.070000    Top5 89.020000    
2024-02-17 12:18:41,744 - ==> Top1: 64.070    Top5: 89.020    Loss: 1.362

2024-02-17 12:18:41,762 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:18:41,762 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:18:41,838 - 

2024-02-17 12:18:41,839 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:18:52,513 - Epoch: [130][  100/  391]    Overall Loss 0.514602    Objective Loss 0.514602                                        LR 0.023500    Time 0.106623    
2024-02-17 12:19:02,282 - Epoch: [130][  200/  391]    Overall Loss 0.527521    Objective Loss 0.527521                                        LR 0.023500    Time 0.102134    
2024-02-17 12:19:12,022 - Epoch: [130][  300/  391]    Overall Loss 0.536228    Objective Loss 0.536228                                        LR 0.023500    Time 0.100540    
2024-02-17 12:19:20,200 - Epoch: [130][  391/  391]    Overall Loss 0.550193    Objective Loss 0.550193    Top1 82.692308    Top5 97.596154    LR 0.023500    Time 0.098044    
2024-02-17 12:19:20,368 - --- validate (epoch=130)-----------
2024-02-17 12:19:20,369 - 10000 samples (128 per mini-batch)
2024-02-17 12:19:23,145 - Epoch: [130][   79/   79]    Loss 1.338975    Top1 64.160000    Top5 88.900000    
2024-02-17 12:19:23,250 - ==> Top1: 64.160    Top5: 88.900    Loss: 1.339

2024-02-17 12:19:23,276 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:19:23,276 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:19:23,361 - 

2024-02-17 12:19:23,361 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:19:32,906 - Epoch: [131][  100/  391]    Overall Loss 0.534401    Objective Loss 0.534401                                        LR 0.023500    Time 0.095366    
2024-02-17 12:19:41,713 - Epoch: [131][  200/  391]    Overall Loss 0.541284    Objective Loss 0.541284                                        LR 0.023500    Time 0.091696    
2024-02-17 12:19:49,468 - Epoch: [131][  300/  391]    Overall Loss 0.550552    Objective Loss 0.550552                                        LR 0.023500    Time 0.086967    
2024-02-17 12:19:57,447 - Epoch: [131][  391/  391]    Overall Loss 0.556768    Objective Loss 0.556768    Top1 79.807692    Top5 98.076923    LR 0.023500    Time 0.087124    
2024-02-17 12:19:57,587 - --- validate (epoch=131)-----------
2024-02-17 12:19:57,588 - 10000 samples (128 per mini-batch)
2024-02-17 12:20:00,149 - Epoch: [131][   79/   79]    Loss 1.422520    Top1 62.650000    Top5 88.430000    
2024-02-17 12:20:00,271 - ==> Top1: 62.650    Top5: 88.430    Loss: 1.423

2024-02-17 12:20:00,291 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:20:00,291 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:20:00,368 - 

2024-02-17 12:20:00,368 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:20:10,349 - Epoch: [132][  100/  391]    Overall Loss 0.524812    Objective Loss 0.524812                                        LR 0.023500    Time 0.099730    
2024-02-17 12:20:19,897 - Epoch: [132][  200/  391]    Overall Loss 0.534590    Objective Loss 0.534590                                        LR 0.023500    Time 0.097579    
2024-02-17 12:20:29,309 - Epoch: [132][  300/  391]    Overall Loss 0.547905    Objective Loss 0.547905                                        LR 0.023500    Time 0.096412    
2024-02-17 12:20:37,643 - Epoch: [132][  391/  391]    Overall Loss 0.561220    Objective Loss 0.561220    Top1 83.173077    Top5 98.076923    LR 0.023500    Time 0.095276    
2024-02-17 12:20:37,753 - --- validate (epoch=132)-----------
2024-02-17 12:20:37,754 - 10000 samples (128 per mini-batch)
2024-02-17 12:20:40,831 - Epoch: [132][   79/   79]    Loss 1.339014    Top1 64.200000    Top5 89.530000    
2024-02-17 12:20:40,949 - ==> Top1: 64.200    Top5: 89.530    Loss: 1.339

2024-02-17 12:20:40,968 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:20:40,969 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:20:41,044 - 

2024-02-17 12:20:41,044 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:20:51,033 - Epoch: [133][  100/  391]    Overall Loss 0.513847    Objective Loss 0.513847                                        LR 0.023500    Time 0.099819    
2024-02-17 12:21:00,868 - Epoch: [133][  200/  391]    Overall Loss 0.534833    Objective Loss 0.534833                                        LR 0.023500    Time 0.099059    
2024-02-17 12:21:10,317 - Epoch: [133][  300/  391]    Overall Loss 0.545445    Objective Loss 0.545445                                        LR 0.023500    Time 0.097518    
2024-02-17 12:21:19,611 - Epoch: [133][  391/  391]    Overall Loss 0.557363    Objective Loss 0.557363    Top1 85.096154    Top5 98.557692    LR 0.023500    Time 0.098581    
2024-02-17 12:21:19,732 - --- validate (epoch=133)-----------
2024-02-17 12:21:19,733 - 10000 samples (128 per mini-batch)
2024-02-17 12:21:22,421 - Epoch: [133][   79/   79]    Loss 1.387939    Top1 63.200000    Top5 88.800000    
2024-02-17 12:21:22,522 - ==> Top1: 63.200    Top5: 88.800    Loss: 1.388

2024-02-17 12:21:22,542 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:21:22,542 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:21:22,618 - 

2024-02-17 12:21:22,618 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:21:30,156 - Epoch: [134][  100/  391]    Overall Loss 0.520899    Objective Loss 0.520899                                        LR 0.023500    Time 0.075313    
2024-02-17 12:21:37,732 - Epoch: [134][  200/  391]    Overall Loss 0.534835    Objective Loss 0.534835                                        LR 0.023500    Time 0.075516    
2024-02-17 12:21:44,894 - Epoch: [134][  300/  391]    Overall Loss 0.546189    Objective Loss 0.546189                                        LR 0.023500    Time 0.074205    
2024-02-17 12:21:51,325 - Epoch: [134][  391/  391]    Overall Loss 0.559324    Objective Loss 0.559324    Top1 75.480769    Top5 97.596154    LR 0.023500    Time 0.073373    
2024-02-17 12:21:51,464 - --- validate (epoch=134)-----------
2024-02-17 12:21:51,465 - 10000 samples (128 per mini-batch)
2024-02-17 12:21:54,518 - Epoch: [134][   79/   79]    Loss 1.345968    Top1 64.770000    Top5 88.710000    
2024-02-17 12:21:54,644 - ==> Top1: 64.770    Top5: 88.710    Loss: 1.346

2024-02-17 12:21:54,664 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:21:54,664 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:21:54,741 - 

2024-02-17 12:21:54,741 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:22:04,676 - Epoch: [135][  100/  391]    Overall Loss 0.523758    Objective Loss 0.523758                                        LR 0.023500    Time 0.099278    
2024-02-17 12:22:13,959 - Epoch: [135][  200/  391]    Overall Loss 0.531264    Objective Loss 0.531264                                        LR 0.023500    Time 0.096029    
2024-02-17 12:22:23,457 - Epoch: [135][  300/  391]    Overall Loss 0.546272    Objective Loss 0.546272                                        LR 0.023500    Time 0.095662    
2024-02-17 12:22:32,237 - Epoch: [135][  391/  391]    Overall Loss 0.555146    Objective Loss 0.555146    Top1 81.730769    Top5 98.076923    LR 0.023500    Time 0.095842    
2024-02-17 12:22:32,354 - --- validate (epoch=135)-----------
2024-02-17 12:22:32,355 - 10000 samples (128 per mini-batch)
2024-02-17 12:22:35,097 - Epoch: [135][   79/   79]    Loss 1.408276    Top1 62.730000    Top5 88.480000    
2024-02-17 12:22:35,217 - ==> Top1: 62.730    Top5: 88.480    Loss: 1.408

2024-02-17 12:22:35,236 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:22:35,237 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:22:35,314 - 

2024-02-17 12:22:35,315 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:22:45,471 - Epoch: [136][  100/  391]    Overall Loss 0.515902    Objective Loss 0.515902                                        LR 0.023500    Time 0.101487    
2024-02-17 12:22:54,958 - Epoch: [136][  200/  391]    Overall Loss 0.523318    Objective Loss 0.523318                                        LR 0.023500    Time 0.098153    
2024-02-17 12:23:04,237 - Epoch: [136][  300/  391]    Overall Loss 0.537852    Objective Loss 0.537852                                        LR 0.023500    Time 0.096348    
2024-02-17 12:23:12,844 - Epoch: [136][  391/  391]    Overall Loss 0.549399    Objective Loss 0.549399    Top1 79.326923    Top5 98.076923    LR 0.023500    Time 0.095925    
2024-02-17 12:23:12,970 - --- validate (epoch=136)-----------
2024-02-17 12:23:12,971 - 10000 samples (128 per mini-batch)
2024-02-17 12:23:15,839 - Epoch: [136][   79/   79]    Loss 1.336822    Top1 63.610000    Top5 89.170000    
2024-02-17 12:23:15,985 - ==> Top1: 63.610    Top5: 89.170    Loss: 1.337

2024-02-17 12:23:16,004 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:23:16,004 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:23:16,080 - 

2024-02-17 12:23:16,081 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:23:25,757 - Epoch: [137][  100/  391]    Overall Loss 0.522268    Objective Loss 0.522268                                        LR 0.023500    Time 0.096688    
2024-02-17 12:23:35,193 - Epoch: [137][  200/  391]    Overall Loss 0.522617    Objective Loss 0.522617                                        LR 0.023500    Time 0.095495    
2024-02-17 12:23:44,445 - Epoch: [137][  300/  391]    Overall Loss 0.533800    Objective Loss 0.533800                                        LR 0.023500    Time 0.094489    
2024-02-17 12:23:52,994 - Epoch: [137][  391/  391]    Overall Loss 0.544992    Objective Loss 0.544992    Top1 81.250000    Top5 98.076923    LR 0.023500    Time 0.094349    
2024-02-17 12:23:53,205 - --- validate (epoch=137)-----------
2024-02-17 12:23:53,206 - 10000 samples (128 per mini-batch)
2024-02-17 12:23:55,978 - Epoch: [137][   79/   79]    Loss 1.417838    Top1 62.180000    Top5 88.250000    
2024-02-17 12:23:56,089 - ==> Top1: 62.180    Top5: 88.250    Loss: 1.418

2024-02-17 12:23:56,106 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:23:56,107 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:23:56,182 - 

2024-02-17 12:23:56,183 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:24:06,006 - Epoch: [138][  100/  391]    Overall Loss 0.518913    Objective Loss 0.518913                                        LR 0.023500    Time 0.098164    
2024-02-17 12:24:15,562 - Epoch: [138][  200/  391]    Overall Loss 0.531409    Objective Loss 0.531409                                        LR 0.023500    Time 0.096835    
2024-02-17 12:24:24,803 - Epoch: [138][  300/  391]    Overall Loss 0.538835    Objective Loss 0.538835                                        LR 0.023500    Time 0.095342    
2024-02-17 12:24:33,444 - Epoch: [138][  391/  391]    Overall Loss 0.549000    Objective Loss 0.549000    Top1 77.884615    Top5 96.153846    LR 0.023500    Time 0.095242    
2024-02-17 12:24:33,580 - --- validate (epoch=138)-----------
2024-02-17 12:24:33,581 - 10000 samples (128 per mini-batch)
2024-02-17 12:24:36,327 - Epoch: [138][   79/   79]    Loss 1.504057    Top1 61.290000    Top5 87.450000    
2024-02-17 12:24:36,452 - ==> Top1: 61.290    Top5: 87.450    Loss: 1.504

2024-02-17 12:24:36,463 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:24:36,463 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:24:36,553 - 

2024-02-17 12:24:36,554 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:24:46,201 - Epoch: [139][  100/  391]    Overall Loss 0.522495    Objective Loss 0.522495                                        LR 0.023500    Time 0.096374    
2024-02-17 12:24:55,722 - Epoch: [139][  200/  391]    Overall Loss 0.530554    Objective Loss 0.530554                                        LR 0.023500    Time 0.095764    
2024-02-17 12:25:04,766 - Epoch: [139][  300/  391]    Overall Loss 0.539933    Objective Loss 0.539933                                        LR 0.023500    Time 0.093975    
2024-02-17 12:25:13,197 - Epoch: [139][  391/  391]    Overall Loss 0.545849    Objective Loss 0.545849    Top1 79.326923    Top5 98.076923    LR 0.023500    Time 0.093654    
2024-02-17 12:25:13,395 - --- validate (epoch=139)-----------
2024-02-17 12:25:13,396 - 10000 samples (128 per mini-batch)
2024-02-17 12:25:16,112 - Epoch: [139][   79/   79]    Loss 1.368089    Top1 63.390000    Top5 88.880000    
2024-02-17 12:25:16,253 - ==> Top1: 63.390    Top5: 88.880    Loss: 1.368

2024-02-17 12:25:16,267 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:25:16,267 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:25:16,347 - 

2024-02-17 12:25:16,347 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:25:26,291 - Epoch: [140][  100/  391]    Overall Loss 0.506662    Objective Loss 0.506662                                        LR 0.023500    Time 0.099343    
2024-02-17 12:25:35,866 - Epoch: [140][  200/  391]    Overall Loss 0.516688    Objective Loss 0.516688                                        LR 0.023500    Time 0.097520    
2024-02-17 12:25:45,399 - Epoch: [140][  300/  391]    Overall Loss 0.526371    Objective Loss 0.526371                                        LR 0.023500    Time 0.096772    
2024-02-17 12:25:54,122 - Epoch: [140][  391/  391]    Overall Loss 0.537069    Objective Loss 0.537069    Top1 80.288462    Top5 98.557692    LR 0.023500    Time 0.096547    
2024-02-17 12:25:54,237 - --- validate (epoch=140)-----------
2024-02-17 12:25:54,238 - 10000 samples (128 per mini-batch)
2024-02-17 12:25:56,973 - Epoch: [140][   79/   79]    Loss 1.344355    Top1 63.930000    Top5 89.210000    
2024-02-17 12:25:57,091 - ==> Top1: 63.930    Top5: 89.210    Loss: 1.344

2024-02-17 12:25:57,111 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:25:57,111 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:25:57,188 - 

2024-02-17 12:25:57,188 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:26:06,776 - Epoch: [141][  100/  391]    Overall Loss 0.500247    Objective Loss 0.500247                                        LR 0.023500    Time 0.095809    
2024-02-17 12:26:16,433 - Epoch: [141][  200/  391]    Overall Loss 0.519171    Objective Loss 0.519171                                        LR 0.023500    Time 0.096159    
2024-02-17 12:26:25,856 - Epoch: [141][  300/  391]    Overall Loss 0.528672    Objective Loss 0.528672                                        LR 0.023500    Time 0.095501    
2024-02-17 12:26:34,201 - Epoch: [141][  391/  391]    Overall Loss 0.538246    Objective Loss 0.538246    Top1 81.250000    Top5 98.076923    LR 0.023500    Time 0.094606    
2024-02-17 12:26:34,357 - --- validate (epoch=141)-----------
2024-02-17 12:26:34,358 - 10000 samples (128 per mini-batch)
2024-02-17 12:26:37,003 - Epoch: [141][   79/   79]    Loss 1.428072    Top1 63.040000    Top5 88.220000    
2024-02-17 12:26:37,109 - ==> Top1: 63.040    Top5: 88.220    Loss: 1.428

2024-02-17 12:26:37,123 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:26:37,124 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:26:37,196 - 

2024-02-17 12:26:37,197 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:26:46,944 - Epoch: [142][  100/  391]    Overall Loss 0.508290    Objective Loss 0.508290                                        LR 0.023500    Time 0.097397    
2024-02-17 12:26:56,405 - Epoch: [142][  200/  391]    Overall Loss 0.517290    Objective Loss 0.517290                                        LR 0.023500    Time 0.095983    
2024-02-17 12:27:05,915 - Epoch: [142][  300/  391]    Overall Loss 0.529299    Objective Loss 0.529299                                        LR 0.023500    Time 0.095670    
2024-02-17 12:27:14,678 - Epoch: [142][  391/  391]    Overall Loss 0.539657    Objective Loss 0.539657    Top1 88.942308    Top5 99.038462    LR 0.023500    Time 0.095805    
2024-02-17 12:27:14,782 - --- validate (epoch=142)-----------
2024-02-17 12:27:14,783 - 10000 samples (128 per mini-batch)
2024-02-17 12:27:17,388 - Epoch: [142][   79/   79]    Loss 1.381608    Top1 63.870000    Top5 88.800000    
2024-02-17 12:27:17,558 - ==> Top1: 63.870    Top5: 88.800    Loss: 1.382

2024-02-17 12:27:17,572 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:27:17,572 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:27:17,661 - 

2024-02-17 12:27:17,661 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:27:27,483 - Epoch: [143][  100/  391]    Overall Loss 0.506073    Objective Loss 0.506073                                        LR 0.023500    Time 0.098147    
2024-02-17 12:27:36,715 - Epoch: [143][  200/  391]    Overall Loss 0.516722    Objective Loss 0.516722                                        LR 0.023500    Time 0.095211    
2024-02-17 12:27:46,236 - Epoch: [143][  300/  391]    Overall Loss 0.524782    Objective Loss 0.524782                                        LR 0.023500    Time 0.095193    
2024-02-17 12:27:55,221 - Epoch: [143][  391/  391]    Overall Loss 0.535591    Objective Loss 0.535591    Top1 79.326923    Top5 99.038462    LR 0.023500    Time 0.096005    
2024-02-17 12:27:55,351 - --- validate (epoch=143)-----------
2024-02-17 12:27:55,352 - 10000 samples (128 per mini-batch)
2024-02-17 12:27:57,924 - Epoch: [143][   79/   79]    Loss 1.403229    Top1 62.710000    Top5 88.530000    
2024-02-17 12:27:58,047 - ==> Top1: 62.710    Top5: 88.530    Loss: 1.403

2024-02-17 12:27:58,067 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:27:58,067 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:27:58,141 - 

2024-02-17 12:27:58,142 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:28:07,862 - Epoch: [144][  100/  391]    Overall Loss 0.511261    Objective Loss 0.511261                                        LR 0.023500    Time 0.097124    
2024-02-17 12:28:17,303 - Epoch: [144][  200/  391]    Overall Loss 0.515480    Objective Loss 0.515480                                        LR 0.023500    Time 0.095742    
2024-02-17 12:28:26,865 - Epoch: [144][  300/  391]    Overall Loss 0.523080    Objective Loss 0.523080                                        LR 0.023500    Time 0.095687    
2024-02-17 12:28:35,407 - Epoch: [144][  391/  391]    Overall Loss 0.534124    Objective Loss 0.534124    Top1 83.653846    Top5 99.519231    LR 0.023500    Time 0.095252    
2024-02-17 12:28:35,519 - --- validate (epoch=144)-----------
2024-02-17 12:28:35,520 - 10000 samples (128 per mini-batch)
2024-02-17 12:28:38,444 - Epoch: [144][   79/   79]    Loss 1.392520    Top1 63.310000    Top5 88.690000    
2024-02-17 12:28:38,660 - ==> Top1: 63.310    Top5: 88.690    Loss: 1.393

2024-02-17 12:28:38,679 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:28:38,680 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:28:38,757 - 

2024-02-17 12:28:38,757 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:28:48,908 - Epoch: [145][  100/  391]    Overall Loss 0.497500    Objective Loss 0.497500                                        LR 0.023500    Time 0.101439    
2024-02-17 12:28:57,973 - Epoch: [145][  200/  391]    Overall Loss 0.509858    Objective Loss 0.509858                                        LR 0.023500    Time 0.096015    
2024-02-17 12:29:07,394 - Epoch: [145][  300/  391]    Overall Loss 0.521146    Objective Loss 0.521146                                        LR 0.023500    Time 0.095398    
2024-02-17 12:29:16,355 - Epoch: [145][  391/  391]    Overall Loss 0.532100    Objective Loss 0.532100    Top1 86.057692    Top5 100.000000    LR 0.023500    Time 0.096103    
2024-02-17 12:29:16,475 - --- validate (epoch=145)-----------
2024-02-17 12:29:16,475 - 10000 samples (128 per mini-batch)
2024-02-17 12:29:19,109 - Epoch: [145][   79/   79]    Loss 1.369270    Top1 63.570000    Top5 88.880000    
2024-02-17 12:29:19,275 - ==> Top1: 63.570    Top5: 88.880    Loss: 1.369

2024-02-17 12:29:19,294 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:29:19,294 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:29:19,368 - 

2024-02-17 12:29:19,368 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:29:29,325 - Epoch: [146][  100/  391]    Overall Loss 0.505544    Objective Loss 0.505544                                        LR 0.023500    Time 0.099496    
2024-02-17 12:29:38,542 - Epoch: [146][  200/  391]    Overall Loss 0.508453    Objective Loss 0.508453                                        LR 0.023500    Time 0.095807    
2024-02-17 12:29:47,841 - Epoch: [146][  300/  391]    Overall Loss 0.518757    Objective Loss 0.518757                                        LR 0.023500    Time 0.094852    
2024-02-17 12:29:55,851 - Epoch: [146][  391/  391]    Overall Loss 0.527722    Objective Loss 0.527722    Top1 78.365385    Top5 95.192308    LR 0.023500    Time 0.093251    
2024-02-17 12:29:55,968 - --- validate (epoch=146)-----------
2024-02-17 12:29:55,969 - 10000 samples (128 per mini-batch)
2024-02-17 12:29:58,692 - Epoch: [146][   79/   79]    Loss 1.445637    Top1 62.150000    Top5 88.240000    
2024-02-17 12:29:58,826 - ==> Top1: 62.150    Top5: 88.240    Loss: 1.446

2024-02-17 12:29:58,845 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:29:58,845 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:29:58,921 - 

2024-02-17 12:29:58,921 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:30:08,206 - Epoch: [147][  100/  391]    Overall Loss 0.515762    Objective Loss 0.515762                                        LR 0.023500    Time 0.092775    
2024-02-17 12:30:17,361 - Epoch: [147][  200/  391]    Overall Loss 0.522330    Objective Loss 0.522330                                        LR 0.023500    Time 0.092138    
2024-02-17 12:30:26,076 - Epoch: [147][  300/  391]    Overall Loss 0.527639    Objective Loss 0.527639                                        LR 0.023500    Time 0.090458    
2024-02-17 12:30:34,675 - Epoch: [147][  391/  391]    Overall Loss 0.533106    Objective Loss 0.533106    Top1 87.500000    Top5 99.038462    LR 0.023500    Time 0.091386    
2024-02-17 12:30:34,809 - --- validate (epoch=147)-----------
2024-02-17 12:30:34,810 - 10000 samples (128 per mini-batch)
2024-02-17 12:30:37,419 - Epoch: [147][   79/   79]    Loss 1.429253    Top1 62.560000    Top5 88.260000    
2024-02-17 12:30:37,539 - ==> Top1: 62.560    Top5: 88.260    Loss: 1.429

2024-02-17 12:30:37,557 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:30:37,558 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:30:37,634 - 

2024-02-17 12:30:37,635 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:30:47,882 - Epoch: [148][  100/  391]    Overall Loss 0.483663    Objective Loss 0.483663                                        LR 0.023500    Time 0.102357    
2024-02-17 12:30:57,277 - Epoch: [148][  200/  391]    Overall Loss 0.503727    Objective Loss 0.503727                                        LR 0.023500    Time 0.098128    
2024-02-17 12:31:06,639 - Epoch: [148][  300/  391]    Overall Loss 0.519878    Objective Loss 0.519878                                        LR 0.023500    Time 0.096610    
2024-02-17 12:31:15,213 - Epoch: [148][  391/  391]    Overall Loss 0.528989    Objective Loss 0.528989    Top1 79.326923    Top5 97.115385    LR 0.023500    Time 0.096042    
2024-02-17 12:31:15,387 - --- validate (epoch=148)-----------
2024-02-17 12:31:15,387 - 10000 samples (128 per mini-batch)
2024-02-17 12:31:17,996 - Epoch: [148][   79/   79]    Loss 1.451930    Top1 62.220000    Top5 88.140000    
2024-02-17 12:31:18,114 - ==> Top1: 62.220    Top5: 88.140    Loss: 1.452

2024-02-17 12:31:18,132 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:31:18,132 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:31:18,207 - 

2024-02-17 12:31:18,207 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:31:28,140 - Epoch: [149][  100/  391]    Overall Loss 0.499607    Objective Loss 0.499607                                        LR 0.023500    Time 0.099255    
2024-02-17 12:31:37,328 - Epoch: [149][  200/  391]    Overall Loss 0.514419    Objective Loss 0.514419                                        LR 0.023500    Time 0.095540    
2024-02-17 12:31:46,888 - Epoch: [149][  300/  391]    Overall Loss 0.520507    Objective Loss 0.520507                                        LR 0.023500    Time 0.095543    
2024-02-17 12:31:55,451 - Epoch: [149][  391/  391]    Overall Loss 0.523791    Objective Loss 0.523791    Top1 77.884615    Top5 96.634615    LR 0.023500    Time 0.095196    
2024-02-17 12:31:55,623 - --- validate (epoch=149)-----------
2024-02-17 12:31:55,624 - 10000 samples (128 per mini-batch)
2024-02-17 12:31:58,294 - Epoch: [149][   79/   79]    Loss 1.443723    Top1 62.570000    Top5 88.140000    
2024-02-17 12:31:58,463 - ==> Top1: 62.570    Top5: 88.140    Loss: 1.444

2024-02-17 12:31:58,482 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:31:58,482 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:31:58,556 - 

2024-02-17 12:31:58,557 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:32:08,623 - Epoch: [150][  100/  391]    Overall Loss 0.404760    Objective Loss 0.404760                                        LR 0.005522    Time 0.100589    
2024-02-17 12:32:17,929 - Epoch: [150][  200/  391]    Overall Loss 0.394318    Objective Loss 0.394318                                        LR 0.005522    Time 0.096798    
2024-02-17 12:32:27,159 - Epoch: [150][  300/  391]    Overall Loss 0.381782    Objective Loss 0.381782                                        LR 0.005522    Time 0.095284    
2024-02-17 12:32:35,801 - Epoch: [150][  391/  391]    Overall Loss 0.376352    Objective Loss 0.376352    Top1 91.346154    Top5 99.038462    LR 0.005522    Time 0.095199    
2024-02-17 12:32:35,923 - --- validate (epoch=150)-----------
2024-02-17 12:32:35,924 - 10000 samples (128 per mini-batch)
2024-02-17 12:32:38,573 - Epoch: [150][   79/   79]    Loss 1.232871    Top1 67.010000    Top5 90.670000    
2024-02-17 12:32:38,683 - ==> Top1: 67.010    Top5: 90.670    Loss: 1.233

2024-02-17 12:32:38,702 - ==> Best [Top1: 67.010   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 150]
2024-02-17 12:32:38,702 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:32:38,795 - 

2024-02-17 12:32:38,795 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:32:48,835 - Epoch: [151][  100/  391]    Overall Loss 0.337969    Objective Loss 0.337969                                        LR 0.005522    Time 0.100326    
2024-02-17 12:32:57,995 - Epoch: [151][  200/  391]    Overall Loss 0.335457    Objective Loss 0.335457                                        LR 0.005522    Time 0.095937    
2024-02-17 12:33:07,502 - Epoch: [151][  300/  391]    Overall Loss 0.331588    Objective Loss 0.331588                                        LR 0.005522    Time 0.095633    
2024-02-17 12:33:16,414 - Epoch: [151][  391/  391]    Overall Loss 0.332760    Objective Loss 0.332760    Top1 92.307692    Top5 99.519231    LR 0.005522    Time 0.096156    
2024-02-17 12:33:16,525 - --- validate (epoch=151)-----------
2024-02-17 12:33:16,526 - 10000 samples (128 per mini-batch)
2024-02-17 12:33:19,133 - Epoch: [151][   79/   79]    Loss 1.224059    Top1 67.100000    Top5 90.580000    
2024-02-17 12:33:19,346 - ==> Top1: 67.100    Top5: 90.580    Loss: 1.224

2024-02-17 12:33:19,365 - ==> Best [Top1: 67.100   Top5: 90.580   Sparsity:0.00   Params: 1341960 on epoch: 151]
2024-02-17 12:33:19,365 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:33:19,459 - 

2024-02-17 12:33:19,460 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:33:29,325 - Epoch: [152][  100/  391]    Overall Loss 0.314529    Objective Loss 0.314529                                        LR 0.005522    Time 0.098575    
2024-02-17 12:33:38,366 - Epoch: [152][  200/  391]    Overall Loss 0.308978    Objective Loss 0.308978                                        LR 0.005522    Time 0.094469    
2024-02-17 12:33:48,070 - Epoch: [152][  300/  391]    Overall Loss 0.311261    Objective Loss 0.311261                                        LR 0.005522    Time 0.095312    
2024-02-17 12:33:56,755 - Epoch: [152][  391/  391]    Overall Loss 0.315546    Objective Loss 0.315546    Top1 85.576923    Top5 99.519231    LR 0.005522    Time 0.095330    
2024-02-17 12:33:56,878 - --- validate (epoch=152)-----------
2024-02-17 12:33:56,879 - 10000 samples (128 per mini-batch)
2024-02-17 12:33:59,836 - Epoch: [152][   79/   79]    Loss 1.244324    Top1 67.250000    Top5 90.670000    
2024-02-17 12:33:59,976 - ==> Top1: 67.250    Top5: 90.670    Loss: 1.244

2024-02-17 12:33:59,994 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:33:59,994 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:34:00,088 - 

2024-02-17 12:34:00,088 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:34:10,068 - Epoch: [153][  100/  391]    Overall Loss 0.311488    Objective Loss 0.311488                                        LR 0.005522    Time 0.099718    
2024-02-17 12:34:18,971 - Epoch: [153][  200/  391]    Overall Loss 0.308623    Objective Loss 0.308623                                        LR 0.005522    Time 0.094355    
2024-02-17 12:34:28,398 - Epoch: [153][  300/  391]    Overall Loss 0.307036    Objective Loss 0.307036                                        LR 0.005522    Time 0.094309    
2024-02-17 12:34:37,139 - Epoch: [153][  391/  391]    Overall Loss 0.307062    Objective Loss 0.307062    Top1 88.942308    Top5 100.000000    LR 0.005522    Time 0.094704    
2024-02-17 12:34:37,256 - --- validate (epoch=153)-----------
2024-02-17 12:34:37,256 - 10000 samples (128 per mini-batch)
2024-02-17 12:34:39,833 - Epoch: [153][   79/   79]    Loss 1.239815    Top1 66.950000    Top5 90.360000    
2024-02-17 12:34:40,023 - ==> Top1: 66.950    Top5: 90.360    Loss: 1.240

2024-02-17 12:34:40,041 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:34:40,041 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:34:40,116 - 

2024-02-17 12:34:40,117 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:34:50,244 - Epoch: [154][  100/  391]    Overall Loss 0.287211    Objective Loss 0.287211                                        LR 0.005522    Time 0.101196    
2024-02-17 12:34:59,322 - Epoch: [154][  200/  391]    Overall Loss 0.291827    Objective Loss 0.291827                                        LR 0.005522    Time 0.095966    
2024-02-17 12:35:08,820 - Epoch: [154][  300/  391]    Overall Loss 0.296863    Objective Loss 0.296863                                        LR 0.005522    Time 0.095621    
2024-02-17 12:35:17,595 - Epoch: [154][  391/  391]    Overall Loss 0.298064    Objective Loss 0.298064    Top1 90.865385    Top5 99.038462    LR 0.005522    Time 0.095797    
2024-02-17 12:35:17,715 - --- validate (epoch=154)-----------
2024-02-17 12:35:17,715 - 10000 samples (128 per mini-batch)
2024-02-17 12:35:20,327 - Epoch: [154][   79/   79]    Loss 1.260500    Top1 66.990000    Top5 90.430000    
2024-02-17 12:35:20,422 - ==> Top1: 66.990    Top5: 90.430    Loss: 1.261

2024-02-17 12:35:20,440 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:35:20,440 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:35:20,514 - 

2024-02-17 12:35:20,514 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:35:30,554 - Epoch: [155][  100/  391]    Overall Loss 0.279161    Objective Loss 0.279161                                        LR 0.005522    Time 0.100328    
2024-02-17 12:35:39,586 - Epoch: [155][  200/  391]    Overall Loss 0.287515    Objective Loss 0.287515                                        LR 0.005522    Time 0.095300    
2024-02-17 12:35:48,711 - Epoch: [155][  300/  391]    Overall Loss 0.289487    Objective Loss 0.289487                                        LR 0.005522    Time 0.093932    
2024-02-17 12:35:57,351 - Epoch: [155][  391/  391]    Overall Loss 0.289827    Objective Loss 0.289827    Top1 94.230769    Top5 99.519231    LR 0.005522    Time 0.094157    
2024-02-17 12:35:57,514 - --- validate (epoch=155)-----------
2024-02-17 12:35:57,515 - 10000 samples (128 per mini-batch)
2024-02-17 12:36:00,130 - Epoch: [155][   79/   79]    Loss 1.260405    Top1 66.810000    Top5 90.430000    
2024-02-17 12:36:00,238 - ==> Top1: 66.810    Top5: 90.430    Loss: 1.260

2024-02-17 12:36:00,257 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:36:00,258 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:36:00,334 - 

2024-02-17 12:36:00,335 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:36:10,853 - Epoch: [156][  100/  391]    Overall Loss 0.276131    Objective Loss 0.276131                                        LR 0.005522    Time 0.105111    
2024-02-17 12:36:19,951 - Epoch: [156][  200/  391]    Overall Loss 0.277669    Objective Loss 0.277669                                        LR 0.005522    Time 0.098020    
2024-02-17 12:36:28,914 - Epoch: [156][  300/  391]    Overall Loss 0.281614    Objective Loss 0.281614                                        LR 0.005522    Time 0.095210    
2024-02-17 12:36:37,518 - Epoch: [156][  391/  391]    Overall Loss 0.281630    Objective Loss 0.281630    Top1 91.346154    Top5 99.038462    LR 0.005522    Time 0.095042    
2024-02-17 12:36:37,652 - --- validate (epoch=156)-----------
2024-02-17 12:36:37,652 - 10000 samples (128 per mini-batch)
2024-02-17 12:36:40,245 - Epoch: [156][   79/   79]    Loss 1.252237    Top1 66.960000    Top5 90.480000    
2024-02-17 12:36:40,397 - ==> Top1: 66.960    Top5: 90.480    Loss: 1.252

2024-02-17 12:36:40,417 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:36:40,417 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:36:40,492 - 

2024-02-17 12:36:40,493 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:36:50,626 - Epoch: [157][  100/  391]    Overall Loss 0.266113    Objective Loss 0.266113                                        LR 0.005522    Time 0.101249    
2024-02-17 12:36:59,540 - Epoch: [157][  200/  391]    Overall Loss 0.273138    Objective Loss 0.273138                                        LR 0.005522    Time 0.095174    
2024-02-17 12:37:08,827 - Epoch: [157][  300/  391]    Overall Loss 0.277272    Objective Loss 0.277272                                        LR 0.005522    Time 0.094389    
2024-02-17 12:37:17,713 - Epoch: [157][  391/  391]    Overall Loss 0.279318    Objective Loss 0.279318    Top1 91.826923    Top5 99.038462    LR 0.005522    Time 0.095136    
2024-02-17 12:37:17,825 - --- validate (epoch=157)-----------
2024-02-17 12:37:17,826 - 10000 samples (128 per mini-batch)
2024-02-17 12:37:20,508 - Epoch: [157][   79/   79]    Loss 1.282982    Top1 66.430000    Top5 90.060000    
2024-02-17 12:37:20,628 - ==> Top1: 66.430    Top5: 90.060    Loss: 1.283

2024-02-17 12:37:20,646 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:37:20,647 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:37:20,723 - 

2024-02-17 12:37:20,723 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:37:30,630 - Epoch: [158][  100/  391]    Overall Loss 0.258010    Objective Loss 0.258010                                        LR 0.005522    Time 0.098998    
2024-02-17 12:37:39,565 - Epoch: [158][  200/  391]    Overall Loss 0.270554    Objective Loss 0.270554                                        LR 0.005522    Time 0.094145    
2024-02-17 12:37:48,842 - Epoch: [158][  300/  391]    Overall Loss 0.270927    Objective Loss 0.270927                                        LR 0.005522    Time 0.093671    
2024-02-17 12:37:57,621 - Epoch: [158][  391/  391]    Overall Loss 0.271884    Objective Loss 0.271884    Top1 91.826923    Top5 98.557692    LR 0.005522    Time 0.094310    
2024-02-17 12:37:57,750 - --- validate (epoch=158)-----------
2024-02-17 12:37:57,751 - 10000 samples (128 per mini-batch)
2024-02-17 12:38:00,411 - Epoch: [158][   79/   79]    Loss 1.297263    Top1 66.580000    Top5 90.440000    
2024-02-17 12:38:00,509 - ==> Top1: 66.580    Top5: 90.440    Loss: 1.297

2024-02-17 12:38:00,520 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:38:00,520 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:38:00,595 - 

2024-02-17 12:38:00,596 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:38:10,650 - Epoch: [159][  100/  391]    Overall Loss 0.262810    Objective Loss 0.262810                                        LR 0.005522    Time 0.100470    
2024-02-17 12:38:19,775 - Epoch: [159][  200/  391]    Overall Loss 0.264519    Objective Loss 0.264519                                        LR 0.005522    Time 0.095831    
2024-02-17 12:38:29,274 - Epoch: [159][  300/  391]    Overall Loss 0.265082    Objective Loss 0.265082                                        LR 0.005522    Time 0.095535    
2024-02-17 12:38:38,115 - Epoch: [159][  391/  391]    Overall Loss 0.266618    Objective Loss 0.266618    Top1 89.423077    Top5 98.076923    LR 0.005522    Time 0.095900    
2024-02-17 12:38:38,224 - --- validate (epoch=159)-----------
2024-02-17 12:38:38,225 - 10000 samples (128 per mini-batch)
2024-02-17 12:38:40,827 - Epoch: [159][   79/   79]    Loss 1.277716    Top1 66.960000    Top5 90.310000    
2024-02-17 12:38:40,945 - ==> Top1: 66.960    Top5: 90.310    Loss: 1.278

2024-02-17 12:38:40,964 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:38:40,965 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:38:41,041 - 

2024-02-17 12:38:41,041 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:38:50,941 - Epoch: [160][  100/  391]    Overall Loss 0.251683    Objective Loss 0.251683                                        LR 0.005522    Time 0.098920    
2024-02-17 12:38:59,443 - Epoch: [160][  200/  391]    Overall Loss 0.258876    Objective Loss 0.258876                                        LR 0.005522    Time 0.091946    
2024-02-17 12:39:08,302 - Epoch: [160][  300/  391]    Overall Loss 0.260912    Objective Loss 0.260912                                        LR 0.005522    Time 0.090813    
2024-02-17 12:39:16,903 - Epoch: [160][  391/  391]    Overall Loss 0.262499    Objective Loss 0.262499    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.091663    
2024-02-17 12:39:17,010 - --- validate (epoch=160)-----------
2024-02-17 12:39:17,011 - 10000 samples (128 per mini-batch)
2024-02-17 12:39:19,981 - Epoch: [160][   79/   79]    Loss 1.284113    Top1 66.650000    Top5 90.270000    
2024-02-17 12:39:20,119 - ==> Top1: 66.650    Top5: 90.270    Loss: 1.284

2024-02-17 12:39:20,132 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:39:20,133 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:39:20,206 - 

2024-02-17 12:39:20,206 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:39:27,917 - Epoch: [161][  100/  391]    Overall Loss 0.247426    Objective Loss 0.247426                                        LR 0.005522    Time 0.077041    
2024-02-17 12:39:36,414 - Epoch: [161][  200/  391]    Overall Loss 0.250927    Objective Loss 0.250927                                        LR 0.005522    Time 0.080984    
2024-02-17 12:39:45,838 - Epoch: [161][  300/  391]    Overall Loss 0.255053    Objective Loss 0.255053                                        LR 0.005522    Time 0.085388    
2024-02-17 12:39:54,361 - Epoch: [161][  391/  391]    Overall Loss 0.256470    Objective Loss 0.256470    Top1 94.711538    Top5 99.519231    LR 0.005522    Time 0.087302    
2024-02-17 12:39:54,487 - --- validate (epoch=161)-----------
2024-02-17 12:39:54,488 - 10000 samples (128 per mini-batch)
2024-02-17 12:39:57,126 - Epoch: [161][   79/   79]    Loss 1.288213    Top1 66.610000    Top5 90.280000    
2024-02-17 12:39:57,225 - ==> Top1: 66.610    Top5: 90.280    Loss: 1.288

2024-02-17 12:39:57,235 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:39:57,235 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:39:57,310 - 

2024-02-17 12:39:57,310 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:40:06,543 - Epoch: [162][  100/  391]    Overall Loss 0.245268    Objective Loss 0.245268                                        LR 0.005522    Time 0.092256    
2024-02-17 12:40:15,915 - Epoch: [162][  200/  391]    Overall Loss 0.249498    Objective Loss 0.249498                                        LR 0.005522    Time 0.092964    
2024-02-17 12:40:25,339 - Epoch: [162][  300/  391]    Overall Loss 0.251475    Objective Loss 0.251475                                        LR 0.005522    Time 0.093373    
2024-02-17 12:40:33,673 - Epoch: [162][  391/  391]    Overall Loss 0.253709    Objective Loss 0.253709    Top1 93.269231    Top5 100.000000    LR 0.005522    Time 0.092945    
2024-02-17 12:40:33,779 - --- validate (epoch=162)-----------
2024-02-17 12:40:33,780 - 10000 samples (128 per mini-batch)
2024-02-17 12:40:36,602 - Epoch: [162][   79/   79]    Loss 1.289234    Top1 66.760000    Top5 90.090000    
2024-02-17 12:40:36,762 - ==> Top1: 66.760    Top5: 90.090    Loss: 1.289

2024-02-17 12:40:36,781 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:40:36,781 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:40:36,856 - 

2024-02-17 12:40:36,856 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:40:47,123 - Epoch: [163][  100/  391]    Overall Loss 0.245569    Objective Loss 0.245569                                        LR 0.005522    Time 0.102592    
2024-02-17 12:40:56,727 - Epoch: [163][  200/  391]    Overall Loss 0.245899    Objective Loss 0.245899                                        LR 0.005522    Time 0.099288    
2024-02-17 12:41:05,298 - Epoch: [163][  300/  391]    Overall Loss 0.248700    Objective Loss 0.248700                                        LR 0.005522    Time 0.094747    
2024-02-17 12:41:13,559 - Epoch: [163][  391/  391]    Overall Loss 0.251160    Objective Loss 0.251160    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.093813    
2024-02-17 12:41:13,684 - --- validate (epoch=163)-----------
2024-02-17 12:41:13,685 - 10000 samples (128 per mini-batch)
2024-02-17 12:41:16,233 - Epoch: [163][   79/   79]    Loss 1.309039    Top1 66.980000    Top5 90.130000    
2024-02-17 12:41:16,349 - ==> Top1: 66.980    Top5: 90.130    Loss: 1.309

2024-02-17 12:41:16,369 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:41:16,369 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:41:16,444 - 

2024-02-17 12:41:16,445 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:41:26,564 - Epoch: [164][  100/  391]    Overall Loss 0.241939    Objective Loss 0.241939                                        LR 0.005522    Time 0.101120    
2024-02-17 12:41:36,047 - Epoch: [164][  200/  391]    Overall Loss 0.245077    Objective Loss 0.245077                                        LR 0.005522    Time 0.097951    
2024-02-17 12:41:45,154 - Epoch: [164][  300/  391]    Overall Loss 0.247411    Objective Loss 0.247411                                        LR 0.005522    Time 0.095639    
2024-02-17 12:41:53,750 - Epoch: [164][  391/  391]    Overall Loss 0.249987    Objective Loss 0.249987    Top1 96.153846    Top5 100.000000    LR 0.005522    Time 0.095354    
2024-02-17 12:41:53,859 - --- validate (epoch=164)-----------
2024-02-17 12:41:53,859 - 10000 samples (128 per mini-batch)
2024-02-17 12:41:56,700 - Epoch: [164][   79/   79]    Loss 1.300506    Top1 66.680000    Top5 90.150000    
2024-02-17 12:41:56,802 - ==> Top1: 66.680    Top5: 90.150    Loss: 1.301

2024-02-17 12:41:56,820 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:41:56,820 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:41:56,897 - 

2024-02-17 12:41:56,897 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:42:06,633 - Epoch: [165][  100/  391]    Overall Loss 0.230396    Objective Loss 0.230396                                        LR 0.005522    Time 0.097248    
2024-02-17 12:42:16,184 - Epoch: [165][  200/  391]    Overall Loss 0.239541    Objective Loss 0.239541                                        LR 0.005522    Time 0.096353    
2024-02-17 12:42:25,022 - Epoch: [165][  300/  391]    Overall Loss 0.242799    Objective Loss 0.242799                                        LR 0.005522    Time 0.093678    
2024-02-17 12:42:33,719 - Epoch: [165][  391/  391]    Overall Loss 0.244418    Objective Loss 0.244418    Top1 90.865385    Top5 99.519231    LR 0.005522    Time 0.094107    
2024-02-17 12:42:33,832 - --- validate (epoch=165)-----------
2024-02-17 12:42:33,833 - 10000 samples (128 per mini-batch)
2024-02-17 12:42:36,451 - Epoch: [165][   79/   79]    Loss 1.300255    Top1 66.990000    Top5 90.210000    
2024-02-17 12:42:36,579 - ==> Top1: 66.990    Top5: 90.210    Loss: 1.300

2024-02-17 12:42:36,599 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:42:36,599 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:42:36,680 - 

2024-02-17 12:42:36,681 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:42:46,933 - Epoch: [166][  100/  391]    Overall Loss 0.239461    Objective Loss 0.239461                                        LR 0.005522    Time 0.102435    
2024-02-17 12:42:56,307 - Epoch: [166][  200/  391]    Overall Loss 0.239250    Objective Loss 0.239250                                        LR 0.005522    Time 0.098064    
2024-02-17 12:43:05,404 - Epoch: [166][  300/  391]    Overall Loss 0.242436    Objective Loss 0.242436                                        LR 0.005522    Time 0.095681    
2024-02-17 12:43:14,352 - Epoch: [166][  391/  391]    Overall Loss 0.242766    Objective Loss 0.242766    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.096286    
2024-02-17 12:43:14,474 - --- validate (epoch=166)-----------
2024-02-17 12:43:14,475 - 10000 samples (128 per mini-batch)
2024-02-17 12:43:17,210 - Epoch: [166][   79/   79]    Loss 1.336991    Top1 66.440000    Top5 89.920000    
2024-02-17 12:43:17,383 - ==> Top1: 66.440    Top5: 89.920    Loss: 1.337

2024-02-17 12:43:17,402 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:43:17,402 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:43:17,479 - 

2024-02-17 12:43:17,479 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:43:27,432 - Epoch: [167][  100/  391]    Overall Loss 0.232500    Objective Loss 0.232500                                        LR 0.005522    Time 0.099452    
2024-02-17 12:43:36,820 - Epoch: [167][  200/  391]    Overall Loss 0.234312    Objective Loss 0.234312                                        LR 0.005522    Time 0.096639    
2024-02-17 12:43:45,424 - Epoch: [167][  300/  391]    Overall Loss 0.237491    Objective Loss 0.237491                                        LR 0.005522    Time 0.093091    
2024-02-17 12:43:53,893 - Epoch: [167][  391/  391]    Overall Loss 0.239577    Objective Loss 0.239577    Top1 90.865385    Top5 100.000000    LR 0.005522    Time 0.093074    
2024-02-17 12:43:54,038 - --- validate (epoch=167)-----------
2024-02-17 12:43:54,039 - 10000 samples (128 per mini-batch)
2024-02-17 12:43:56,723 - Epoch: [167][   79/   79]    Loss 1.324298    Top1 66.640000    Top5 90.110000    
2024-02-17 12:43:56,837 - ==> Top1: 66.640    Top5: 90.110    Loss: 1.324

2024-02-17 12:43:56,859 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:43:56,859 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:43:56,935 - 

2024-02-17 12:43:56,935 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:44:04,757 - Epoch: [168][  100/  391]    Overall Loss 0.222409    Objective Loss 0.222409                                        LR 0.005522    Time 0.078156    
2024-02-17 12:44:11,872 - Epoch: [168][  200/  391]    Overall Loss 0.229236    Objective Loss 0.229236                                        LR 0.005522    Time 0.074633    
2024-02-17 12:44:18,939 - Epoch: [168][  300/  391]    Overall Loss 0.233375    Objective Loss 0.233375                                        LR 0.005522    Time 0.073299    
2024-02-17 12:44:26,147 - Epoch: [168][  391/  391]    Overall Loss 0.234968    Objective Loss 0.234968    Top1 92.307692    Top5 100.000000    LR 0.005522    Time 0.074664    
2024-02-17 12:44:26,315 - --- validate (epoch=168)-----------
2024-02-17 12:44:26,316 - 10000 samples (128 per mini-batch)
2024-02-17 12:44:28,881 - Epoch: [168][   79/   79]    Loss 1.331299    Top1 66.520000    Top5 90.190000    
2024-02-17 12:44:29,014 - ==> Top1: 66.520    Top5: 90.190    Loss: 1.331

2024-02-17 12:44:29,036 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:44:29,036 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:44:29,115 - 

2024-02-17 12:44:29,115 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:44:39,349 - Epoch: [169][  100/  391]    Overall Loss 0.231639    Objective Loss 0.231639                                        LR 0.005522    Time 0.102268    
2024-02-17 12:44:48,800 - Epoch: [169][  200/  391]    Overall Loss 0.231629    Objective Loss 0.231629                                        LR 0.005522    Time 0.098363    
2024-02-17 12:44:57,383 - Epoch: [169][  300/  391]    Overall Loss 0.231205    Objective Loss 0.231205                                        LR 0.005522    Time 0.094170    
2024-02-17 12:45:05,529 - Epoch: [169][  391/  391]    Overall Loss 0.233654    Objective Loss 0.233654    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.093076    
2024-02-17 12:45:05,660 - --- validate (epoch=169)-----------
2024-02-17 12:45:05,661 - 10000 samples (128 per mini-batch)
2024-02-17 12:45:08,363 - Epoch: [169][   79/   79]    Loss 1.317482    Top1 66.820000    Top5 90.010000    
2024-02-17 12:45:08,460 - ==> Top1: 66.820    Top5: 90.010    Loss: 1.317

2024-02-17 12:45:08,470 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:45:08,470 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:45:08,545 - 

2024-02-17 12:45:08,546 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:45:18,602 - Epoch: [170][  100/  391]    Overall Loss 0.224247    Objective Loss 0.224247                                        LR 0.005522    Time 0.100489    
2024-02-17 12:45:27,298 - Epoch: [170][  200/  391]    Overall Loss 0.227131    Objective Loss 0.227131                                        LR 0.005522    Time 0.093704    
2024-02-17 12:45:36,558 - Epoch: [170][  300/  391]    Overall Loss 0.231135    Objective Loss 0.231135                                        LR 0.005522    Time 0.093319    
2024-02-17 12:45:44,787 - Epoch: [170][  391/  391]    Overall Loss 0.233916    Objective Loss 0.233916    Top1 92.307692    Top5 99.519231    LR 0.005522    Time 0.092635    
2024-02-17 12:45:44,900 - --- validate (epoch=170)-----------
2024-02-17 12:45:44,900 - 10000 samples (128 per mini-batch)
2024-02-17 12:45:47,684 - Epoch: [170][   79/   79]    Loss 1.330065    Top1 66.560000    Top5 90.030000    
2024-02-17 12:45:47,790 - ==> Top1: 66.560    Top5: 90.030    Loss: 1.330

2024-02-17 12:45:47,800 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:45:47,800 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:45:47,877 - 

2024-02-17 12:45:47,877 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:45:58,057 - Epoch: [171][  100/  391]    Overall Loss 0.220583    Objective Loss 0.220583                                        LR 0.005522    Time 0.101716    
2024-02-17 12:46:07,702 - Epoch: [171][  200/  391]    Overall Loss 0.222879    Objective Loss 0.222879                                        LR 0.005522    Time 0.099057    
2024-02-17 12:46:17,574 - Epoch: [171][  300/  391]    Overall Loss 0.225976    Objective Loss 0.225976                                        LR 0.005522    Time 0.098929    
2024-02-17 12:46:26,038 - Epoch: [171][  391/  391]    Overall Loss 0.226625    Objective Loss 0.226625    Top1 91.346154    Top5 100.000000    LR 0.005522    Time 0.097541    
2024-02-17 12:46:26,161 - --- validate (epoch=171)-----------
2024-02-17 12:46:26,162 - 10000 samples (128 per mini-batch)
2024-02-17 12:46:28,891 - Epoch: [171][   79/   79]    Loss 1.344760    Top1 66.800000    Top5 89.880000    
2024-02-17 12:46:28,998 - ==> Top1: 66.800    Top5: 89.880    Loss: 1.345

2024-02-17 12:46:29,017 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:46:29,017 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:46:29,096 - 

2024-02-17 12:46:29,097 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:46:38,924 - Epoch: [172][  100/  391]    Overall Loss 0.217787    Objective Loss 0.217787                                        LR 0.005522    Time 0.098199    
2024-02-17 12:46:48,058 - Epoch: [172][  200/  391]    Overall Loss 0.222174    Objective Loss 0.222174                                        LR 0.005522    Time 0.094744    
2024-02-17 12:46:57,645 - Epoch: [172][  300/  391]    Overall Loss 0.223789    Objective Loss 0.223789                                        LR 0.005522    Time 0.095102    
2024-02-17 12:47:05,668 - Epoch: [172][  391/  391]    Overall Loss 0.227632    Objective Loss 0.227632    Top1 90.865385    Top5 99.519231    LR 0.005522    Time 0.093477    
2024-02-17 12:47:05,829 - --- validate (epoch=172)-----------
2024-02-17 12:47:05,830 - 10000 samples (128 per mini-batch)
2024-02-17 12:47:08,581 - Epoch: [172][   79/   79]    Loss 1.352497    Top1 66.440000    Top5 89.680000    
2024-02-17 12:47:08,698 - ==> Top1: 66.440    Top5: 89.680    Loss: 1.352

2024-02-17 12:47:08,717 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:47:08,717 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:47:08,791 - 

2024-02-17 12:47:08,792 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:47:18,947 - Epoch: [173][  100/  391]    Overall Loss 0.221297    Objective Loss 0.221297                                        LR 0.005522    Time 0.101473    
2024-02-17 12:47:28,832 - Epoch: [173][  200/  391]    Overall Loss 0.223977    Objective Loss 0.223977                                        LR 0.005522    Time 0.100135    
2024-02-17 12:47:37,986 - Epoch: [173][  300/  391]    Overall Loss 0.227418    Objective Loss 0.227418                                        LR 0.005522    Time 0.097255    
2024-02-17 12:47:46,068 - Epoch: [173][  391/  391]    Overall Loss 0.229388    Objective Loss 0.229388    Top1 92.788462    Top5 100.000000    LR 0.005522    Time 0.095280    
2024-02-17 12:47:46,194 - --- validate (epoch=173)-----------
2024-02-17 12:47:46,195 - 10000 samples (128 per mini-batch)
2024-02-17 12:47:48,916 - Epoch: [173][   79/   79]    Loss 1.343826    Top1 66.600000    Top5 89.750000    
2024-02-17 12:47:49,030 - ==> Top1: 66.600    Top5: 89.750    Loss: 1.344

2024-02-17 12:47:49,049 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:47:49,050 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:47:49,125 - 

2024-02-17 12:47:49,125 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:47:59,213 - Epoch: [174][  100/  391]    Overall Loss 0.217410    Objective Loss 0.217410                                        LR 0.005522    Time 0.100806    
2024-02-17 12:48:08,662 - Epoch: [174][  200/  391]    Overall Loss 0.221184    Objective Loss 0.221184                                        LR 0.005522    Time 0.097620    
2024-02-17 12:48:18,220 - Epoch: [174][  300/  391]    Overall Loss 0.221912    Objective Loss 0.221912                                        LR 0.005522    Time 0.096924    
2024-02-17 12:48:26,838 - Epoch: [174][  391/  391]    Overall Loss 0.225677    Objective Loss 0.225677    Top1 90.865385    Top5 100.000000    LR 0.005522    Time 0.096396    
2024-02-17 12:48:27,022 - --- validate (epoch=174)-----------
2024-02-17 12:48:27,023 - 10000 samples (128 per mini-batch)
2024-02-17 12:48:30,060 - Epoch: [174][   79/   79]    Loss 1.349080    Top1 66.700000    Top5 90.000000    
2024-02-17 12:48:30,165 - ==> Top1: 66.700    Top5: 90.000    Loss: 1.349

2024-02-17 12:48:30,190 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:48:30,191 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:48:30,279 - 

2024-02-17 12:48:30,280 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:48:40,235 - Epoch: [175][  100/  391]    Overall Loss 0.200720    Objective Loss 0.200720                                        LR 0.001298    Time 0.099476    
2024-02-17 12:48:48,567 - Epoch: [175][  200/  391]    Overall Loss 0.199762    Objective Loss 0.199762                                        LR 0.001298    Time 0.091378    
2024-02-17 12:48:57,867 - Epoch: [175][  300/  391]    Overall Loss 0.198328    Objective Loss 0.198328                                        LR 0.001298    Time 0.091903    
2024-02-17 12:49:06,478 - Epoch: [175][  391/  391]    Overall Loss 0.198144    Objective Loss 0.198144    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.092524    
2024-02-17 12:49:06,589 - --- validate (epoch=175)-----------
2024-02-17 12:49:06,590 - 10000 samples (128 per mini-batch)
2024-02-17 12:49:09,220 - Epoch: [175][   79/   79]    Loss 1.320566    Top1 67.480000    Top5 90.300000    
2024-02-17 12:49:09,407 - ==> Top1: 67.480    Top5: 90.300    Loss: 1.321

2024-02-17 12:49:09,428 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:49:09,428 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:49:09,527 - 

2024-02-17 12:49:09,528 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:49:19,120 - Epoch: [176][  100/  391]    Overall Loss 0.194081    Objective Loss 0.194081                                        LR 0.001298    Time 0.095805    
2024-02-17 12:49:28,744 - Epoch: [176][  200/  391]    Overall Loss 0.188834    Objective Loss 0.188834                                        LR 0.001298    Time 0.096003    
2024-02-17 12:49:38,206 - Epoch: [176][  300/  391]    Overall Loss 0.189702    Objective Loss 0.189702                                        LR 0.001298    Time 0.095526    
2024-02-17 12:49:46,884 - Epoch: [176][  391/  391]    Overall Loss 0.191612    Objective Loss 0.191612    Top1 92.307692    Top5 100.000000    LR 0.001298    Time 0.095475    
2024-02-17 12:49:47,064 - --- validate (epoch=176)-----------
2024-02-17 12:49:47,065 - 10000 samples (128 per mini-batch)
2024-02-17 12:49:49,975 - Epoch: [176][   79/   79]    Loss 1.327038    Top1 67.030000    Top5 89.960000    
2024-02-17 12:49:50,089 - ==> Top1: 67.030    Top5: 89.960    Loss: 1.327

2024-02-17 12:49:50,108 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:49:50,108 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:49:50,183 - 

2024-02-17 12:49:50,184 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:49:59,819 - Epoch: [177][  100/  391]    Overall Loss 0.177925    Objective Loss 0.177925                                        LR 0.001298    Time 0.096278    
2024-02-17 12:50:09,435 - Epoch: [177][  200/  391]    Overall Loss 0.182974    Objective Loss 0.182974                                        LR 0.001298    Time 0.096195    
2024-02-17 12:50:18,832 - Epoch: [177][  300/  391]    Overall Loss 0.182750    Objective Loss 0.182750                                        LR 0.001298    Time 0.095436    
2024-02-17 12:50:27,556 - Epoch: [177][  391/  391]    Overall Loss 0.184228    Objective Loss 0.184228    Top1 93.750000    Top5 100.000000    LR 0.001298    Time 0.095524    
2024-02-17 12:50:27,676 - --- validate (epoch=177)-----------
2024-02-17 12:50:27,677 - 10000 samples (128 per mini-batch)
2024-02-17 12:50:30,291 - Epoch: [177][   79/   79]    Loss 1.328166    Top1 67.210000    Top5 90.120000    
2024-02-17 12:50:30,416 - ==> Top1: 67.210    Top5: 90.120    Loss: 1.328

2024-02-17 12:50:30,430 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:50:30,431 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:50:30,509 - 

2024-02-17 12:50:30,509 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:50:40,255 - Epoch: [178][  100/  391]    Overall Loss 0.176956    Objective Loss 0.176956                                        LR 0.001298    Time 0.097380    
2024-02-17 12:50:49,495 - Epoch: [178][  200/  391]    Overall Loss 0.180543    Objective Loss 0.180543                                        LR 0.001298    Time 0.094865    
2024-02-17 12:50:58,701 - Epoch: [178][  300/  391]    Overall Loss 0.182040    Objective Loss 0.182040                                        LR 0.001298    Time 0.093913    
2024-02-17 12:51:07,215 - Epoch: [178][  391/  391]    Overall Loss 0.182313    Objective Loss 0.182313    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.093819    
2024-02-17 12:51:07,438 - --- validate (epoch=178)-----------
2024-02-17 12:51:07,439 - 10000 samples (128 per mini-batch)
2024-02-17 12:51:10,035 - Epoch: [178][   79/   79]    Loss 1.330774    Top1 66.990000    Top5 90.280000    
2024-02-17 12:51:10,149 - ==> Top1: 66.990    Top5: 90.280    Loss: 1.331

2024-02-17 12:51:10,167 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:51:10,167 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:51:10,244 - 

2024-02-17 12:51:10,244 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:51:19,824 - Epoch: [179][  100/  391]    Overall Loss 0.183629    Objective Loss 0.183629                                        LR 0.001298    Time 0.095729    
2024-02-17 12:51:28,938 - Epoch: [179][  200/  391]    Overall Loss 0.180428    Objective Loss 0.180428                                        LR 0.001298    Time 0.093410    
2024-02-17 12:51:38,847 - Epoch: [179][  300/  391]    Overall Loss 0.180396    Objective Loss 0.180396                                        LR 0.001298    Time 0.095287    
2024-02-17 12:51:47,740 - Epoch: [179][  391/  391]    Overall Loss 0.180150    Objective Loss 0.180150    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.095843    
2024-02-17 12:51:47,850 - --- validate (epoch=179)-----------
2024-02-17 12:51:47,851 - 10000 samples (128 per mini-batch)
2024-02-17 12:51:50,389 - Epoch: [179][   79/   79]    Loss 1.327668    Top1 67.260000    Top5 90.160000    
2024-02-17 12:51:50,497 - ==> Top1: 67.260    Top5: 90.160    Loss: 1.328

2024-02-17 12:51:50,516 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:51:50,516 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:51:50,591 - 

2024-02-17 12:51:50,591 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:52:00,571 - Epoch: [180][  100/  391]    Overall Loss 0.178427    Objective Loss 0.178427                                        LR 0.001298    Time 0.099721    
2024-02-17 12:52:09,929 - Epoch: [180][  200/  391]    Overall Loss 0.177399    Objective Loss 0.177399                                        LR 0.001298    Time 0.096629    
2024-02-17 12:52:19,461 - Epoch: [180][  300/  391]    Overall Loss 0.179766    Objective Loss 0.179766                                        LR 0.001298    Time 0.096173    
2024-02-17 12:52:28,172 - Epoch: [180][  391/  391]    Overall Loss 0.180987    Objective Loss 0.180987    Top1 93.750000    Top5 100.000000    LR 0.001298    Time 0.096057    
2024-02-17 12:52:28,288 - --- validate (epoch=180)-----------
2024-02-17 12:52:28,288 - 10000 samples (128 per mini-batch)
2024-02-17 12:52:30,888 - Epoch: [180][   79/   79]    Loss 1.322918    Top1 67.360000    Top5 90.070000    
2024-02-17 12:52:30,999 - ==> Top1: 67.360    Top5: 90.070    Loss: 1.323

2024-02-17 12:52:31,018 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:52:31,018 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:52:31,093 - 

2024-02-17 12:52:31,094 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:52:40,646 - Epoch: [181][  100/  391]    Overall Loss 0.174672    Objective Loss 0.174672                                        LR 0.001298    Time 0.095447    
2024-02-17 12:52:50,016 - Epoch: [181][  200/  391]    Overall Loss 0.177580    Objective Loss 0.177580                                        LR 0.001298    Time 0.094548    
2024-02-17 12:52:59,162 - Epoch: [181][  300/  391]    Overall Loss 0.180572    Objective Loss 0.180572                                        LR 0.001298    Time 0.093502    
2024-02-17 12:53:07,716 - Epoch: [181][  391/  391]    Overall Loss 0.180137    Objective Loss 0.180137    Top1 97.596154    Top5 100.000000    LR 0.001298    Time 0.093607    
2024-02-17 12:53:07,845 - --- validate (epoch=181)-----------
2024-02-17 12:53:07,846 - 10000 samples (128 per mini-batch)
2024-02-17 12:53:10,424 - Epoch: [181][   79/   79]    Loss 1.326092    Top1 67.060000    Top5 89.890000    
2024-02-17 12:53:10,532 - ==> Top1: 67.060    Top5: 89.890    Loss: 1.326

2024-02-17 12:53:10,551 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:53:10,551 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:53:10,627 - 

2024-02-17 12:53:10,627 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:53:20,286 - Epoch: [182][  100/  391]    Overall Loss 0.170630    Objective Loss 0.170630                                        LR 0.001298    Time 0.096514    
2024-02-17 12:53:29,584 - Epoch: [182][  200/  391]    Overall Loss 0.172889    Objective Loss 0.172889                                        LR 0.001298    Time 0.094724    
2024-02-17 12:53:39,085 - Epoch: [182][  300/  391]    Overall Loss 0.174278    Objective Loss 0.174278                                        LR 0.001298    Time 0.094802    
2024-02-17 12:53:48,010 - Epoch: [182][  391/  391]    Overall Loss 0.176877    Objective Loss 0.176877    Top1 93.750000    Top5 99.519231    LR 0.001298    Time 0.095553    
2024-02-17 12:53:48,142 - --- validate (epoch=182)-----------
2024-02-17 12:53:48,143 - 10000 samples (128 per mini-batch)
2024-02-17 12:53:51,078 - Epoch: [182][   79/   79]    Loss 1.325496    Top1 67.070000    Top5 90.160000    
2024-02-17 12:53:51,196 - ==> Top1: 67.070    Top5: 90.160    Loss: 1.325

2024-02-17 12:53:51,210 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:53:51,210 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:53:51,289 - 

2024-02-17 12:53:51,290 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:54:01,015 - Epoch: [183][  100/  391]    Overall Loss 0.172277    Objective Loss 0.172277                                        LR 0.001298    Time 0.097154    
2024-02-17 12:54:10,089 - Epoch: [183][  200/  391]    Overall Loss 0.172480    Objective Loss 0.172480                                        LR 0.001298    Time 0.093925    
2024-02-17 12:54:19,635 - Epoch: [183][  300/  391]    Overall Loss 0.173455    Objective Loss 0.173455                                        LR 0.001298    Time 0.094421    
2024-02-17 12:54:28,369 - Epoch: [183][  391/  391]    Overall Loss 0.173278    Objective Loss 0.173278    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.094769    
2024-02-17 12:54:28,491 - --- validate (epoch=183)-----------
2024-02-17 12:54:28,492 - 10000 samples (128 per mini-batch)
2024-02-17 12:54:31,167 - Epoch: [183][   79/   79]    Loss 1.332109    Top1 67.000000    Top5 90.010000    
2024-02-17 12:54:31,293 - ==> Top1: 67.000    Top5: 90.010    Loss: 1.332

2024-02-17 12:54:31,313 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:54:31,313 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:54:31,391 - 

2024-02-17 12:54:31,392 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:54:39,695 - Epoch: [184][  100/  391]    Overall Loss 0.166013    Objective Loss 0.166013                                        LR 0.001298    Time 0.082969    
2024-02-17 12:54:48,421 - Epoch: [184][  200/  391]    Overall Loss 0.170177    Objective Loss 0.170177                                        LR 0.001298    Time 0.085090    
2024-02-17 12:54:57,682 - Epoch: [184][  300/  391]    Overall Loss 0.173037    Objective Loss 0.173037                                        LR 0.001298    Time 0.087580    
2024-02-17 12:55:05,991 - Epoch: [184][  391/  391]    Overall Loss 0.174257    Objective Loss 0.174257    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.088438    
2024-02-17 12:55:06,126 - --- validate (epoch=184)-----------
2024-02-17 12:55:06,126 - 10000 samples (128 per mini-batch)
2024-02-17 12:55:09,067 - Epoch: [184][   79/   79]    Loss 1.328481    Top1 66.740000    Top5 90.080000    
2024-02-17 12:55:09,166 - ==> Top1: 66.740    Top5: 90.080    Loss: 1.328

2024-02-17 12:55:09,189 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:55:09,190 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:55:09,269 - 

2024-02-17 12:55:09,270 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:55:16,871 - Epoch: [185][  100/  391]    Overall Loss 0.169289    Objective Loss 0.169289                                        LR 0.001298    Time 0.075947    
2024-02-17 12:55:24,796 - Epoch: [185][  200/  391]    Overall Loss 0.170546    Objective Loss 0.170546                                        LR 0.001298    Time 0.077576    
2024-02-17 12:55:33,885 - Epoch: [185][  300/  391]    Overall Loss 0.170117    Objective Loss 0.170117                                        LR 0.001298    Time 0.082000    
2024-02-17 12:55:42,122 - Epoch: [185][  391/  391]    Overall Loss 0.170657    Objective Loss 0.170657    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.083970    
2024-02-17 12:55:42,236 - --- validate (epoch=185)-----------
2024-02-17 12:55:42,237 - 10000 samples (128 per mini-batch)
2024-02-17 12:55:44,772 - Epoch: [185][   79/   79]    Loss 1.333425    Top1 67.110000    Top5 90.130000    
2024-02-17 12:55:44,893 - ==> Top1: 67.110    Top5: 90.130    Loss: 1.333

2024-02-17 12:55:44,911 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:55:44,912 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:55:44,987 - 

2024-02-17 12:55:44,987 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:55:54,685 - Epoch: [186][  100/  391]    Overall Loss 0.168003    Objective Loss 0.168003                                        LR 0.001298    Time 0.096906    
2024-02-17 12:56:03,454 - Epoch: [186][  200/  391]    Overall Loss 0.172662    Objective Loss 0.172662                                        LR 0.001298    Time 0.092273    
2024-02-17 12:56:12,696 - Epoch: [186][  300/  391]    Overall Loss 0.170748    Objective Loss 0.170748                                        LR 0.001298    Time 0.092307    
2024-02-17 12:56:21,389 - Epoch: [186][  391/  391]    Overall Loss 0.171379    Objective Loss 0.171379    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.093044    
2024-02-17 12:56:21,570 - --- validate (epoch=186)-----------
2024-02-17 12:56:21,571 - 10000 samples (128 per mini-batch)
2024-02-17 12:56:24,473 - Epoch: [186][   79/   79]    Loss 1.338057    Top1 66.920000    Top5 90.140000    
2024-02-17 12:56:24,581 - ==> Top1: 66.920    Top5: 90.140    Loss: 1.338

2024-02-17 12:56:24,600 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:56:24,601 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:56:24,676 - 

2024-02-17 12:56:24,676 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:56:34,558 - Epoch: [187][  100/  391]    Overall Loss 0.171068    Objective Loss 0.171068                                        LR 0.001298    Time 0.098743    
2024-02-17 12:56:43,770 - Epoch: [187][  200/  391]    Overall Loss 0.169351    Objective Loss 0.169351                                        LR 0.001298    Time 0.095409    
2024-02-17 12:56:52,516 - Epoch: [187][  300/  391]    Overall Loss 0.170330    Objective Loss 0.170330                                        LR 0.001298    Time 0.092744    
2024-02-17 12:57:00,859 - Epoch: [187][  391/  391]    Overall Loss 0.171265    Objective Loss 0.171265    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.092486    
2024-02-17 12:57:00,996 - --- validate (epoch=187)-----------
2024-02-17 12:57:00,996 - 10000 samples (128 per mini-batch)
2024-02-17 12:57:03,621 - Epoch: [187][   79/   79]    Loss 1.328471    Top1 66.900000    Top5 90.030000    
2024-02-17 12:57:03,737 - ==> Top1: 66.900    Top5: 90.030    Loss: 1.328

2024-02-17 12:57:03,756 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:57:03,756 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:57:03,835 - 

2024-02-17 12:57:03,835 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:57:13,990 - Epoch: [188][  100/  391]    Overall Loss 0.169394    Objective Loss 0.169394                                        LR 0.001298    Time 0.101472    
2024-02-17 12:57:23,365 - Epoch: [188][  200/  391]    Overall Loss 0.168713    Objective Loss 0.168713                                        LR 0.001298    Time 0.097584    
2024-02-17 12:57:32,388 - Epoch: [188][  300/  391]    Overall Loss 0.168231    Objective Loss 0.168231                                        LR 0.001298    Time 0.095118    
2024-02-17 12:57:41,024 - Epoch: [188][  391/  391]    Overall Loss 0.168887    Objective Loss 0.168887    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.095058    
2024-02-17 12:57:41,153 - --- validate (epoch=188)-----------
2024-02-17 12:57:41,154 - 10000 samples (128 per mini-batch)
2024-02-17 12:57:43,726 - Epoch: [188][   79/   79]    Loss 1.331676    Top1 66.910000    Top5 90.020000    
2024-02-17 12:57:43,838 - ==> Top1: 66.910    Top5: 90.020    Loss: 1.332

2024-02-17 12:57:43,857 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:57:43,858 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:57:43,932 - 

2024-02-17 12:57:43,932 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:57:53,908 - Epoch: [189][  100/  391]    Overall Loss 0.165290    Objective Loss 0.165290                                        LR 0.001298    Time 0.099687    
2024-02-17 12:58:03,535 - Epoch: [189][  200/  391]    Overall Loss 0.167899    Objective Loss 0.167899                                        LR 0.001298    Time 0.097953    
2024-02-17 12:58:12,491 - Epoch: [189][  300/  391]    Overall Loss 0.167915    Objective Loss 0.167915                                        LR 0.001298    Time 0.095139    
2024-02-17 12:58:21,161 - Epoch: [189][  391/  391]    Overall Loss 0.170486    Objective Loss 0.170486    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.095160    
2024-02-17 12:58:21,299 - --- validate (epoch=189)-----------
2024-02-17 12:58:21,299 - 10000 samples (128 per mini-batch)
2024-02-17 12:58:23,874 - Epoch: [189][   79/   79]    Loss 1.342635    Top1 66.830000    Top5 90.030000    
2024-02-17 12:58:24,013 - ==> Top1: 66.830    Top5: 90.030    Loss: 1.343

2024-02-17 12:58:24,032 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:58:24,032 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:58:24,108 - 

2024-02-17 12:58:24,108 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:58:34,335 - Epoch: [190][  100/  391]    Overall Loss 0.161642    Objective Loss 0.161642                                        LR 0.001298    Time 0.102185    
2024-02-17 12:58:43,725 - Epoch: [190][  200/  391]    Overall Loss 0.163029    Objective Loss 0.163029                                        LR 0.001298    Time 0.098020    
2024-02-17 12:58:52,854 - Epoch: [190][  300/  391]    Overall Loss 0.164693    Objective Loss 0.164693                                        LR 0.001298    Time 0.095759    
2024-02-17 12:59:01,567 - Epoch: [190][  391/  391]    Overall Loss 0.166364    Objective Loss 0.166364    Top1 91.826923    Top5 100.000000    LR 0.001298    Time 0.095745    
2024-02-17 12:59:01,699 - --- validate (epoch=190)-----------
2024-02-17 12:59:01,700 - 10000 samples (128 per mini-batch)
2024-02-17 12:59:04,314 - Epoch: [190][   79/   79]    Loss 1.333915    Top1 66.940000    Top5 90.200000    
2024-02-17 12:59:04,438 - ==> Top1: 66.940    Top5: 90.200    Loss: 1.334

2024-02-17 12:59:04,457 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:59:04,457 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:59:04,534 - 

2024-02-17 12:59:04,534 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:59:14,591 - Epoch: [191][  100/  391]    Overall Loss 0.167240    Objective Loss 0.167240                                        LR 0.001298    Time 0.100492    
2024-02-17 12:59:23,898 - Epoch: [191][  200/  391]    Overall Loss 0.165507    Objective Loss 0.165507                                        LR 0.001298    Time 0.096758    
2024-02-17 12:59:32,936 - Epoch: [191][  300/  391]    Overall Loss 0.166577    Objective Loss 0.166577                                        LR 0.001298    Time 0.094613    
2024-02-17 12:59:41,510 - Epoch: [191][  391/  391]    Overall Loss 0.167198    Objective Loss 0.167198    Top1 97.115385    Top5 99.519231    LR 0.001298    Time 0.094511    
2024-02-17 12:59:41,674 - --- validate (epoch=191)-----------
2024-02-17 12:59:41,675 - 10000 samples (128 per mini-batch)
2024-02-17 12:59:44,249 - Epoch: [191][   79/   79]    Loss 1.350309    Top1 67.060000    Top5 90.090000    
2024-02-17 12:59:44,375 - ==> Top1: 67.060    Top5: 90.090    Loss: 1.350

2024-02-17 12:59:44,395 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:59:44,395 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 12:59:44,473 - 

2024-02-17 12:59:44,473 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:59:54,300 - Epoch: [192][  100/  391]    Overall Loss 0.163152    Objective Loss 0.163152                                        LR 0.001298    Time 0.098192    
2024-02-17 13:00:03,670 - Epoch: [192][  200/  391]    Overall Loss 0.161818    Objective Loss 0.161818                                        LR 0.001298    Time 0.095925    
2024-02-17 13:00:12,754 - Epoch: [192][  300/  391]    Overall Loss 0.162984    Objective Loss 0.162984                                        LR 0.001298    Time 0.094212    
2024-02-17 13:00:21,818 - Epoch: [192][  391/  391]    Overall Loss 0.164090    Objective Loss 0.164090    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.095455    
2024-02-17 13:00:21,997 - --- validate (epoch=192)-----------
2024-02-17 13:00:21,998 - 10000 samples (128 per mini-batch)
2024-02-17 13:00:24,626 - Epoch: [192][   79/   79]    Loss 1.332412    Top1 66.850000    Top5 90.100000    
2024-02-17 13:00:24,739 - ==> Top1: 66.850    Top5: 90.100    Loss: 1.332

2024-02-17 13:00:24,749 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:00:24,749 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 13:00:24,824 - 

2024-02-17 13:00:24,825 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:00:34,737 - Epoch: [193][  100/  391]    Overall Loss 0.161500    Objective Loss 0.161500                                        LR 0.001298    Time 0.099044    
2024-02-17 13:00:44,137 - Epoch: [193][  200/  391]    Overall Loss 0.163752    Objective Loss 0.163752                                        LR 0.001298    Time 0.096498    
2024-02-17 13:00:53,535 - Epoch: [193][  300/  391]    Overall Loss 0.164758    Objective Loss 0.164758                                        LR 0.001298    Time 0.095642    
2024-02-17 13:01:02,152 - Epoch: [193][  391/  391]    Overall Loss 0.166022    Objective Loss 0.166022    Top1 94.230769    Top5 99.519231    LR 0.001298    Time 0.095411    
2024-02-17 13:01:02,306 - --- validate (epoch=193)-----------
2024-02-17 13:01:02,307 - 10000 samples (128 per mini-batch)
2024-02-17 13:01:05,044 - Epoch: [193][   79/   79]    Loss 1.348482    Top1 66.940000    Top5 90.210000    
2024-02-17 13:01:05,238 - ==> Top1: 66.940    Top5: 90.210    Loss: 1.348

2024-02-17 13:01:05,256 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:01:05,256 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 13:01:05,333 - 

2024-02-17 13:01:05,333 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:01:15,431 - Epoch: [194][  100/  391]    Overall Loss 0.159389    Objective Loss 0.159389                                        LR 0.001298    Time 0.100906    
2024-02-17 13:01:24,972 - Epoch: [194][  200/  391]    Overall Loss 0.162840    Objective Loss 0.162840                                        LR 0.001298    Time 0.098133    
2024-02-17 13:01:32,790 - Epoch: [194][  300/  391]    Overall Loss 0.164014    Objective Loss 0.164014                                        LR 0.001298    Time 0.091466    
2024-02-17 13:01:41,255 - Epoch: [194][  391/  391]    Overall Loss 0.163814    Objective Loss 0.163814    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.091818    
2024-02-17 13:01:41,433 - --- validate (epoch=194)-----------
2024-02-17 13:01:41,433 - 10000 samples (128 per mini-batch)
2024-02-17 13:01:44,030 - Epoch: [194][   79/   79]    Loss 1.345392    Top1 66.930000    Top5 90.060000    
2024-02-17 13:01:44,130 - ==> Top1: 66.930    Top5: 90.060    Loss: 1.345

2024-02-17 13:01:44,149 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:01:44,150 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 13:01:44,223 - 

2024-02-17 13:01:44,224 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:01:54,042 - Epoch: [195][  100/  391]    Overall Loss 0.162932    Objective Loss 0.162932                                        LR 0.001298    Time 0.098106    
2024-02-17 13:02:03,686 - Epoch: [195][  200/  391]    Overall Loss 0.164049    Objective Loss 0.164049                                        LR 0.001298    Time 0.097251    
2024-02-17 13:02:13,447 - Epoch: [195][  300/  391]    Overall Loss 0.164216    Objective Loss 0.164216                                        LR 0.001298    Time 0.097355    
2024-02-17 13:02:21,993 - Epoch: [195][  391/  391]    Overall Loss 0.164087    Objective Loss 0.164087    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.096541    
2024-02-17 13:02:22,122 - --- validate (epoch=195)-----------
2024-02-17 13:02:22,123 - 10000 samples (128 per mini-batch)
2024-02-17 13:02:24,735 - Epoch: [195][   79/   79]    Loss 1.341553    Top1 66.670000    Top5 90.110000    
2024-02-17 13:02:24,845 - ==> Top1: 66.670    Top5: 90.110    Loss: 1.342

2024-02-17 13:02:24,864 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:02:24,864 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 13:02:24,939 - 

2024-02-17 13:02:24,939 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:02:35,176 - Epoch: [196][  100/  391]    Overall Loss 0.164768    Objective Loss 0.164768                                        LR 0.001298    Time 0.102294    
2024-02-17 13:02:44,563 - Epoch: [196][  200/  391]    Overall Loss 0.163550    Objective Loss 0.163550                                        LR 0.001298    Time 0.098057    
2024-02-17 13:02:54,022 - Epoch: [196][  300/  391]    Overall Loss 0.163167    Objective Loss 0.163167                                        LR 0.001298    Time 0.096886    
2024-02-17 13:03:02,474 - Epoch: [196][  391/  391]    Overall Loss 0.163966    Objective Loss 0.163966    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.095942    
2024-02-17 13:03:02,606 - --- validate (epoch=196)-----------
2024-02-17 13:03:02,607 - 10000 samples (128 per mini-batch)
2024-02-17 13:03:05,266 - Epoch: [196][   79/   79]    Loss 1.365839    Top1 66.750000    Top5 89.960000    
2024-02-17 13:03:05,365 - ==> Top1: 66.750    Top5: 89.960    Loss: 1.366

2024-02-17 13:03:05,375 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:03:05,375 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 13:03:05,651 - 

2024-02-17 13:03:05,651 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:03:15,486 - Epoch: [197][  100/  391]    Overall Loss 0.160814    Objective Loss 0.160814                                        LR 0.001298    Time 0.098266    
2024-02-17 13:03:24,629 - Epoch: [197][  200/  391]    Overall Loss 0.160856    Objective Loss 0.160856                                        LR 0.001298    Time 0.094823    
2024-02-17 13:03:34,171 - Epoch: [197][  300/  391]    Overall Loss 0.162088    Objective Loss 0.162088                                        LR 0.001298    Time 0.095007    
2024-02-17 13:03:42,733 - Epoch: [197][  391/  391]    Overall Loss 0.162756    Objective Loss 0.162756    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.094783    
2024-02-17 13:03:42,874 - --- validate (epoch=197)-----------
2024-02-17 13:03:42,875 - 10000 samples (128 per mini-batch)
2024-02-17 13:03:45,407 - Epoch: [197][   79/   79]    Loss 1.361022    Top1 66.480000    Top5 90.110000    
2024-02-17 13:03:45,514 - ==> Top1: 66.480    Top5: 90.110    Loss: 1.361

2024-02-17 13:03:45,533 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:03:45,533 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 13:03:45,608 - 

2024-02-17 13:03:45,608 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:03:55,827 - Epoch: [198][  100/  391]    Overall Loss 0.158934    Objective Loss 0.158934                                        LR 0.001298    Time 0.102114    
2024-02-17 13:04:05,182 - Epoch: [198][  200/  391]    Overall Loss 0.158053    Objective Loss 0.158053                                        LR 0.001298    Time 0.097810    
2024-02-17 13:04:14,686 - Epoch: [198][  300/  391]    Overall Loss 0.161427    Objective Loss 0.161427                                        LR 0.001298    Time 0.096869    
2024-02-17 13:04:23,141 - Epoch: [198][  391/  391]    Overall Loss 0.162402    Objective Loss 0.162402    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.095939    
2024-02-17 13:04:23,254 - --- validate (epoch=198)-----------
2024-02-17 13:04:23,255 - 10000 samples (128 per mini-batch)
2024-02-17 13:04:25,814 - Epoch: [198][   79/   79]    Loss 1.353372    Top1 67.070000    Top5 90.060000    
2024-02-17 13:04:25,926 - ==> Top1: 67.070    Top5: 90.060    Loss: 1.353

2024-02-17 13:04:25,946 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:04:25,946 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 13:04:26,020 - 

2024-02-17 13:04:26,021 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:04:36,039 - Epoch: [199][  100/  391]    Overall Loss 0.161186    Objective Loss 0.161186                                        LR 0.001298    Time 0.100105    
2024-02-17 13:04:45,275 - Epoch: [199][  200/  391]    Overall Loss 0.159144    Objective Loss 0.159144                                        LR 0.001298    Time 0.096209    
2024-02-17 13:04:55,036 - Epoch: [199][  300/  391]    Overall Loss 0.159413    Objective Loss 0.159413                                        LR 0.001298    Time 0.096659    
2024-02-17 13:05:03,707 - Epoch: [199][  391/  391]    Overall Loss 0.160005    Objective Loss 0.160005    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.096330    
2024-02-17 13:05:03,830 - --- validate (epoch=199)-----------
2024-02-17 13:05:03,831 - 10000 samples (128 per mini-batch)
2024-02-17 13:05:06,446 - Epoch: [199][   79/   79]    Loss 1.355071    Top1 66.850000    Top5 90.010000    
2024-02-17 13:05:06,545 - ==> Top1: 66.850    Top5: 90.010    Loss: 1.355

2024-02-17 13:05:06,556 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:05:06,557 - Saving checkpoint to: logs/2024.02.17-105230/checkpoint.pth.tar
2024-02-17 13:05:06,628 - 

2024-02-17 13:05:06,628 - Initiating quantization aware training (QAT)...
2024-02-17 13:05:06,753 - 

2024-02-17 13:05:06,753 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:05:20,352 - Epoch: [200][  100/  391]    Overall Loss 4.692799    Objective Loss 4.692799                                        LR 0.001298    Time 0.135920    
2024-02-17 13:05:33,407 - Epoch: [200][  200/  391]    Overall Loss 3.970608    Objective Loss 3.970608                                        LR 0.001298    Time 0.133205    
2024-02-17 13:05:46,341 - Epoch: [200][  300/  391]    Overall Loss 3.574221    Objective Loss 3.574221                                        LR 0.001298    Time 0.131899    
2024-02-17 13:05:57,758 - Epoch: [200][  391/  391]    Overall Loss 3.330171    Objective Loss 3.330171    Top1 34.134615    Top5 70.673077    LR 0.001298    Time 0.130391    
2024-02-17 13:05:57,937 - --- validate (epoch=200)-----------
2024-02-17 13:05:57,938 - 10000 samples (128 per mini-batch)
2024-02-17 13:06:03,692 - Epoch: [200][   79/   79]    Loss 2.829267    Top1 32.050000    Top5 62.830000    
2024-02-17 13:06:03,802 - ==> Top1: 32.050    Top5: 62.830    Loss: 2.829

2024-02-17 13:06:03,818 - ==> Best [Top1: 32.050   Top5: 62.830   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:06:03,818 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:06:03,894 - 

2024-02-17 13:06:03,894 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:06:18,119 - Epoch: [201][  100/  391]    Overall Loss 2.354713    Objective Loss 2.354713                                        LR 0.001298    Time 0.142155    
2024-02-17 13:06:30,669 - Epoch: [201][  200/  391]    Overall Loss 2.358414    Objective Loss 2.358414                                        LR 0.001298    Time 0.133805    
2024-02-17 13:06:43,399 - Epoch: [201][  300/  391]    Overall Loss 2.315942    Objective Loss 2.315942                                        LR 0.001298    Time 0.131620    
2024-02-17 13:06:55,369 - Epoch: [201][  391/  391]    Overall Loss 2.287698    Objective Loss 2.287698    Top1 35.096154    Top5 67.307692    LR 0.001298    Time 0.131588    
2024-02-17 13:06:55,502 - --- validate (epoch=201)-----------
2024-02-17 13:06:55,503 - 10000 samples (128 per mini-batch)
2024-02-17 13:07:01,245 - Epoch: [201][   79/   79]    Loss 2.123929    Top1 42.920000    Top5 75.100000    
2024-02-17 13:07:01,427 - ==> Top1: 42.920    Top5: 75.100    Loss: 2.124

2024-02-17 13:07:01,436 - ==> Best [Top1: 42.920   Top5: 75.100   Sparsity:0.00   Params: 1341960 on epoch: 201]
2024-02-17 13:07:01,436 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:07:01,514 - 

2024-02-17 13:07:01,514 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:07:14,953 - Epoch: [202][  100/  391]    Overall Loss 2.059048    Objective Loss 2.059048                                        LR 0.001298    Time 0.134313    
2024-02-17 13:07:28,050 - Epoch: [202][  200/  391]    Overall Loss 2.081145    Objective Loss 2.081145                                        LR 0.001298    Time 0.132615    
2024-02-17 13:07:40,996 - Epoch: [202][  300/  391]    Overall Loss 2.071665    Objective Loss 2.071665                                        LR 0.001298    Time 0.131548    
2024-02-17 13:07:52,622 - Epoch: [202][  391/  391]    Overall Loss 2.077229    Objective Loss 2.077229    Top1 46.153846    Top5 77.403846    LR 0.001298    Time 0.130653    
2024-02-17 13:07:52,799 - --- validate (epoch=202)-----------
2024-02-17 13:07:52,800 - 10000 samples (128 per mini-batch)
2024-02-17 13:07:58,910 - Epoch: [202][   79/   79]    Loss 2.253355    Top1 41.540000    Top5 72.460000    
2024-02-17 13:07:59,017 - ==> Top1: 41.540    Top5: 72.460    Loss: 2.253

2024-02-17 13:07:59,034 - ==> Best [Top1: 42.920   Top5: 75.100   Sparsity:0.00   Params: 1341960 on epoch: 201]
2024-02-17 13:07:59,034 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:07:59,098 - 

2024-02-17 13:07:59,099 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:08:12,446 - Epoch: [203][  100/  391]    Overall Loss 2.070768    Objective Loss 2.070768                                        LR 0.001298    Time 0.133392    
2024-02-17 13:08:25,074 - Epoch: [203][  200/  391]    Overall Loss 2.037897    Objective Loss 2.037897                                        LR 0.001298    Time 0.129808    
2024-02-17 13:08:38,009 - Epoch: [203][  300/  391]    Overall Loss 2.034358    Objective Loss 2.034358                                        LR 0.001298    Time 0.129640    
2024-02-17 13:08:49,595 - Epoch: [203][  391/  391]    Overall Loss 2.021870    Objective Loss 2.021870    Top1 46.634615    Top5 73.557692    LR 0.001298    Time 0.129083    
2024-02-17 13:08:49,799 - --- validate (epoch=203)-----------
2024-02-17 13:08:49,800 - 10000 samples (128 per mini-batch)
2024-02-17 13:08:54,501 - Epoch: [203][   79/   79]    Loss 2.171128    Top1 42.370000    Top5 75.110000    
2024-02-17 13:08:54,596 - ==> Top1: 42.370    Top5: 75.110    Loss: 2.171

2024-02-17 13:08:54,612 - ==> Best [Top1: 42.920   Top5: 75.100   Sparsity:0.00   Params: 1341960 on epoch: 201]
2024-02-17 13:08:54,612 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:08:54,662 - 

2024-02-17 13:08:54,663 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:09:07,193 - Epoch: [204][  100/  391]    Overall Loss 2.001784    Objective Loss 2.001784                                        LR 0.001298    Time 0.125238    
2024-02-17 13:09:19,654 - Epoch: [204][  200/  391]    Overall Loss 1.994122    Objective Loss 1.994122                                        LR 0.001298    Time 0.124899    
2024-02-17 13:09:32,560 - Epoch: [204][  300/  391]    Overall Loss 1.983590    Objective Loss 1.983590                                        LR 0.001298    Time 0.126269    
2024-02-17 13:09:44,510 - Epoch: [204][  391/  391]    Overall Loss 1.986432    Objective Loss 1.986432    Top1 43.750000    Top5 79.326923    LR 0.001298    Time 0.127430    
2024-02-17 13:09:44,672 - --- validate (epoch=204)-----------
2024-02-17 13:09:44,673 - 10000 samples (128 per mini-batch)
2024-02-17 13:09:50,281 - Epoch: [204][   79/   79]    Loss 2.222942    Top1 42.550000    Top5 73.770000    
2024-02-17 13:09:50,392 - ==> Top1: 42.550    Top5: 73.770    Loss: 2.223

2024-02-17 13:09:50,411 - ==> Best [Top1: 42.920   Top5: 75.100   Sparsity:0.00   Params: 1341960 on epoch: 201]
2024-02-17 13:09:50,412 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:09:50,477 - 

2024-02-17 13:09:50,477 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:10:03,593 - Epoch: [205][  100/  391]    Overall Loss 1.977991    Objective Loss 1.977991                                        LR 0.001298    Time 0.131075    
2024-02-17 13:10:15,918 - Epoch: [205][  200/  391]    Overall Loss 1.946377    Objective Loss 1.946377                                        LR 0.001298    Time 0.127140    
2024-02-17 13:10:28,543 - Epoch: [205][  300/  391]    Overall Loss 1.969498    Objective Loss 1.969498                                        LR 0.001298    Time 0.126826    
2024-02-17 13:10:40,175 - Epoch: [205][  391/  391]    Overall Loss 1.977168    Objective Loss 1.977168    Top1 41.826923    Top5 76.923077    LR 0.001298    Time 0.127047    
2024-02-17 13:10:40,297 - --- validate (epoch=205)-----------
2024-02-17 13:10:40,298 - 10000 samples (128 per mini-batch)
2024-02-17 13:10:45,693 - Epoch: [205][   79/   79]    Loss 2.011259    Top1 46.220000    Top5 77.330000    
2024-02-17 13:10:45,893 - ==> Top1: 46.220    Top5: 77.330    Loss: 2.011

2024-02-17 13:10:45,911 - ==> Best [Top1: 46.220   Top5: 77.330   Sparsity:0.00   Params: 1341960 on epoch: 205]
2024-02-17 13:10:45,912 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:10:45,992 - 

2024-02-17 13:10:45,993 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:10:59,399 - Epoch: [206][  100/  391]    Overall Loss 2.002719    Objective Loss 2.002719                                        LR 0.001298    Time 0.133981    
2024-02-17 13:11:12,166 - Epoch: [206][  200/  391]    Overall Loss 1.985981    Objective Loss 1.985981                                        LR 0.001298    Time 0.130803    
2024-02-17 13:11:24,930 - Epoch: [206][  300/  391]    Overall Loss 1.966962    Objective Loss 1.966962                                        LR 0.001298    Time 0.129732    
2024-02-17 13:11:36,861 - Epoch: [206][  391/  391]    Overall Loss 1.959746    Objective Loss 1.959746    Top1 45.192308    Top5 80.769231    LR 0.001298    Time 0.130038    
2024-02-17 13:11:36,987 - --- validate (epoch=206)-----------
2024-02-17 13:11:36,988 - 10000 samples (128 per mini-batch)
2024-02-17 13:11:42,475 - Epoch: [206][   79/   79]    Loss 2.294350    Top1 42.080000    Top5 72.150000    
2024-02-17 13:11:42,588 - ==> Top1: 42.080    Top5: 72.150    Loss: 2.294

2024-02-17 13:11:42,605 - ==> Best [Top1: 46.220   Top5: 77.330   Sparsity:0.00   Params: 1341960 on epoch: 205]
2024-02-17 13:11:42,605 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:11:42,668 - 

2024-02-17 13:11:42,668 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:11:56,094 - Epoch: [207][  100/  391]    Overall Loss 1.992121    Objective Loss 1.992121                                        LR 0.001298    Time 0.134179    
2024-02-17 13:12:08,649 - Epoch: [207][  200/  391]    Overall Loss 1.979710    Objective Loss 1.979710                                        LR 0.001298    Time 0.129840    
2024-02-17 13:12:20,733 - Epoch: [207][  300/  391]    Overall Loss 1.960547    Objective Loss 1.960547                                        LR 0.001298    Time 0.126825    
2024-02-17 13:12:32,556 - Epoch: [207][  391/  391]    Overall Loss 1.971891    Objective Loss 1.971891    Top1 40.865385    Top5 75.961538    LR 0.001298    Time 0.127532    
2024-02-17 13:12:32,683 - --- validate (epoch=207)-----------
2024-02-17 13:12:32,684 - 10000 samples (128 per mini-batch)
2024-02-17 13:12:38,934 - Epoch: [207][   79/   79]    Loss 2.368842    Top1 39.220000    Top5 70.900000    
2024-02-17 13:12:39,047 - ==> Top1: 39.220    Top5: 70.900    Loss: 2.369

2024-02-17 13:12:39,066 - ==> Best [Top1: 46.220   Top5: 77.330   Sparsity:0.00   Params: 1341960 on epoch: 205]
2024-02-17 13:12:39,066 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:12:39,132 - 

2024-02-17 13:12:39,132 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:12:52,090 - Epoch: [208][  100/  391]    Overall Loss 1.942308    Objective Loss 1.942308                                        LR 0.001298    Time 0.129500    
2024-02-17 13:13:04,609 - Epoch: [208][  200/  391]    Overall Loss 1.935138    Objective Loss 1.935138                                        LR 0.001298    Time 0.127317    
2024-02-17 13:13:17,274 - Epoch: [208][  300/  391]    Overall Loss 1.949319    Objective Loss 1.949319                                        LR 0.001298    Time 0.127079    
2024-02-17 13:13:28,862 - Epoch: [208][  391/  391]    Overall Loss 1.952726    Objective Loss 1.952726    Top1 47.115385    Top5 74.519231    LR 0.001298    Time 0.127129    
2024-02-17 13:13:28,970 - --- validate (epoch=208)-----------
2024-02-17 13:13:28,971 - 10000 samples (128 per mini-batch)
2024-02-17 13:13:34,583 - Epoch: [208][   79/   79]    Loss 2.023553    Top1 46.750000    Top5 77.050000    
2024-02-17 13:13:34,688 - ==> Top1: 46.750    Top5: 77.050    Loss: 2.024

2024-02-17 13:13:34,919 - ==> Best [Top1: 46.750   Top5: 77.050   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:13:34,919 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:13:34,996 - 

2024-02-17 13:13:34,996 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:13:48,108 - Epoch: [209][  100/  391]    Overall Loss 1.974904    Objective Loss 1.974904                                        LR 0.001298    Time 0.131042    
2024-02-17 13:14:01,161 - Epoch: [209][  200/  391]    Overall Loss 1.965476    Objective Loss 1.965476                                        LR 0.001298    Time 0.130757    
2024-02-17 13:14:14,105 - Epoch: [209][  300/  391]    Overall Loss 1.946208    Objective Loss 1.946208                                        LR 0.001298    Time 0.130302    
2024-02-17 13:14:25,832 - Epoch: [209][  391/  391]    Overall Loss 1.941191    Objective Loss 1.941191    Top1 48.076923    Top5 81.250000    LR 0.001298    Time 0.129955    
2024-02-17 13:14:25,946 - --- validate (epoch=209)-----------
2024-02-17 13:14:25,947 - 10000 samples (128 per mini-batch)
2024-02-17 13:14:32,250 - Epoch: [209][   79/   79]    Loss 2.205558    Top1 43.100000    Top5 74.330000    
2024-02-17 13:14:32,358 - ==> Top1: 43.100    Top5: 74.330    Loss: 2.206

2024-02-17 13:14:32,368 - ==> Best [Top1: 46.750   Top5: 77.050   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:14:32,368 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:14:32,446 - 

2024-02-17 13:14:32,447 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:14:46,184 - Epoch: [210][  100/  391]    Overall Loss 1.871303    Objective Loss 1.871303                                        LR 0.001298    Time 0.137294    
2024-02-17 13:14:58,783 - Epoch: [210][  200/  391]    Overall Loss 1.929294    Objective Loss 1.929294                                        LR 0.001298    Time 0.131618    
2024-02-17 13:15:11,388 - Epoch: [210][  300/  391]    Overall Loss 1.952668    Objective Loss 1.952668                                        LR 0.001298    Time 0.129746    
2024-02-17 13:15:22,826 - Epoch: [210][  391/  391]    Overall Loss 1.942844    Objective Loss 1.942844    Top1 52.403846    Top5 80.769231    LR 0.001298    Time 0.128789    
2024-02-17 13:15:22,941 - --- validate (epoch=210)-----------
2024-02-17 13:15:22,942 - 10000 samples (128 per mini-batch)
2024-02-17 13:15:28,954 - Epoch: [210][   79/   79]    Loss 2.176624    Top1 43.390000    Top5 74.770000    
2024-02-17 13:15:29,055 - ==> Top1: 43.390    Top5: 74.770    Loss: 2.177

2024-02-17 13:15:29,072 - ==> Best [Top1: 46.750   Top5: 77.050   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:15:29,072 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:15:29,138 - 

2024-02-17 13:15:29,138 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:15:42,393 - Epoch: [211][  100/  391]    Overall Loss 1.914333    Objective Loss 1.914333                                        LR 0.001298    Time 0.132469    
2024-02-17 13:15:55,024 - Epoch: [211][  200/  391]    Overall Loss 1.941861    Objective Loss 1.941861                                        LR 0.001298    Time 0.129363    
2024-02-17 13:16:07,857 - Epoch: [211][  300/  391]    Overall Loss 1.903868    Objective Loss 1.903868                                        LR 0.001298    Time 0.129003    
2024-02-17 13:16:18,773 - Epoch: [211][  391/  391]    Overall Loss 1.882874    Objective Loss 1.882874    Top1 52.884615    Top5 84.134615    LR 0.001298    Time 0.126885    
2024-02-17 13:16:18,954 - --- validate (epoch=211)-----------
2024-02-17 13:16:18,955 - 10000 samples (128 per mini-batch)
2024-02-17 13:16:24,475 - Epoch: [211][   79/   79]    Loss 2.123242    Top1 44.740000    Top5 75.730000    
2024-02-17 13:16:24,568 - ==> Top1: 44.740    Top5: 75.730    Loss: 2.123

2024-02-17 13:16:24,588 - ==> Best [Top1: 46.750   Top5: 77.050   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:16:24,588 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:16:24,654 - 

2024-02-17 13:16:24,654 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:16:38,258 - Epoch: [212][  100/  391]    Overall Loss 1.833120    Objective Loss 1.833120                                        LR 0.001298    Time 0.135961    
2024-02-17 13:16:50,991 - Epoch: [212][  200/  391]    Overall Loss 1.824359    Objective Loss 1.824359                                        LR 0.001298    Time 0.131622    
2024-02-17 13:17:02,626 - Epoch: [212][  300/  391]    Overall Loss 1.819063    Objective Loss 1.819063                                        LR 0.001298    Time 0.126517    
2024-02-17 13:17:13,939 - Epoch: [212][  391/  391]    Overall Loss 1.817423    Objective Loss 1.817423    Top1 57.211538    Top5 86.538462    LR 0.001298    Time 0.125995    
2024-02-17 13:17:14,083 - --- validate (epoch=212)-----------
2024-02-17 13:17:14,083 - 10000 samples (128 per mini-batch)
2024-02-17 13:17:19,927 - Epoch: [212][   79/   79]    Loss 2.088930    Top1 46.370000    Top5 76.190000    
2024-02-17 13:17:20,032 - ==> Top1: 46.370    Top5: 76.190    Loss: 2.089

2024-02-17 13:17:20,051 - ==> Best [Top1: 46.750   Top5: 77.050   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:17:20,052 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:17:20,122 - 

2024-02-17 13:17:20,122 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:17:33,588 - Epoch: [213][  100/  391]    Overall Loss 1.839104    Objective Loss 1.839104                                        LR 0.001298    Time 0.134545    
2024-02-17 13:17:46,426 - Epoch: [213][  200/  391]    Overall Loss 1.841456    Objective Loss 1.841456                                        LR 0.001298    Time 0.131433    
2024-02-17 13:17:59,333 - Epoch: [213][  300/  391]    Overall Loss 1.840732    Objective Loss 1.840732                                        LR 0.001298    Time 0.130629    
2024-02-17 13:18:11,334 - Epoch: [213][  391/  391]    Overall Loss 1.856802    Objective Loss 1.856802    Top1 51.923077    Top5 82.211538    LR 0.001298    Time 0.130906    
2024-02-17 13:18:11,463 - --- validate (epoch=213)-----------
2024-02-17 13:18:11,464 - 10000 samples (128 per mini-batch)
2024-02-17 13:18:16,932 - Epoch: [213][   79/   79]    Loss 2.113815    Top1 44.420000    Top5 75.920000    
2024-02-17 13:18:17,033 - ==> Top1: 44.420    Top5: 75.920    Loss: 2.114

2024-02-17 13:18:17,050 - ==> Best [Top1: 46.750   Top5: 77.050   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:18:17,051 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:18:17,103 - 

2024-02-17 13:18:17,104 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:18:29,664 - Epoch: [214][  100/  391]    Overall Loss 1.894560    Objective Loss 1.894560                                        LR 0.001298    Time 0.125539    
2024-02-17 13:18:42,099 - Epoch: [214][  200/  391]    Overall Loss 1.928149    Objective Loss 1.928149                                        LR 0.001298    Time 0.124920    
2024-02-17 13:18:54,671 - Epoch: [214][  300/  391]    Overall Loss 1.891032    Objective Loss 1.891032                                        LR 0.001298    Time 0.125167    
2024-02-17 13:19:06,046 - Epoch: [214][  391/  391]    Overall Loss 1.879853    Objective Loss 1.879853    Top1 47.596154    Top5 81.250000    LR 0.001298    Time 0.125117    
2024-02-17 13:19:06,197 - --- validate (epoch=214)-----------
2024-02-17 13:19:06,198 - 10000 samples (128 per mini-batch)
2024-02-17 13:19:12,750 - Epoch: [214][   79/   79]    Loss 2.380656    Top1 38.760000    Top5 70.170000    
2024-02-17 13:19:12,893 - ==> Top1: 38.760    Top5: 70.170    Loss: 2.381

2024-02-17 13:19:12,911 - ==> Best [Top1: 46.750   Top5: 77.050   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:19:12,912 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:19:12,979 - 

2024-02-17 13:19:12,980 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:19:26,222 - Epoch: [215][  100/  391]    Overall Loss 1.816439    Objective Loss 1.816439                                        LR 0.001298    Time 0.132345    
2024-02-17 13:19:38,783 - Epoch: [215][  200/  391]    Overall Loss 1.831202    Objective Loss 1.831202                                        LR 0.001298    Time 0.128950    
2024-02-17 13:19:50,481 - Epoch: [215][  300/  391]    Overall Loss 1.807045    Objective Loss 1.807045                                        LR 0.001298    Time 0.124947    
2024-02-17 13:20:02,288 - Epoch: [215][  391/  391]    Overall Loss 1.804497    Objective Loss 1.804497    Top1 45.673077    Top5 81.250000    LR 0.001298    Time 0.126051    
2024-02-17 13:20:02,436 - --- validate (epoch=215)-----------
2024-02-17 13:20:02,437 - 10000 samples (128 per mini-batch)
2024-02-17 13:20:08,060 - Epoch: [215][   79/   79]    Loss 1.965996    Top1 47.780000    Top5 78.130000    
2024-02-17 13:20:08,164 - ==> Top1: 47.780    Top5: 78.130    Loss: 1.966

2024-02-17 13:20:08,183 - ==> Best [Top1: 47.780   Top5: 78.130   Sparsity:0.00   Params: 1341960 on epoch: 215]
2024-02-17 13:20:08,183 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:20:08,262 - 

2024-02-17 13:20:08,263 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:20:21,642 - Epoch: [216][  100/  391]    Overall Loss 1.829885    Objective Loss 1.829885                                        LR 0.001298    Time 0.133720    
2024-02-17 13:20:34,146 - Epoch: [216][  200/  391]    Overall Loss 1.819835    Objective Loss 1.819835                                        LR 0.001298    Time 0.129352    
2024-02-17 13:20:46,923 - Epoch: [216][  300/  391]    Overall Loss 1.803039    Objective Loss 1.803039                                        LR 0.001298    Time 0.128806    
2024-02-17 13:20:58,036 - Epoch: [216][  391/  391]    Overall Loss 1.818952    Objective Loss 1.818952    Top1 58.173077    Top5 87.500000    LR 0.001298    Time 0.127240    
2024-02-17 13:20:58,125 - --- validate (epoch=216)-----------
2024-02-17 13:20:58,125 - 10000 samples (128 per mini-batch)
2024-02-17 13:21:03,426 - Epoch: [216][   79/   79]    Loss 2.137646    Top1 44.790000    Top5 75.960000    
2024-02-17 13:21:03,536 - ==> Top1: 44.790    Top5: 75.960    Loss: 2.138

2024-02-17 13:21:03,553 - ==> Best [Top1: 47.780   Top5: 78.130   Sparsity:0.00   Params: 1341960 on epoch: 215]
2024-02-17 13:21:03,553 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:21:03,618 - 

2024-02-17 13:21:03,618 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:21:17,013 - Epoch: [217][  100/  391]    Overall Loss 1.792051    Objective Loss 1.792051                                        LR 0.001298    Time 0.133869    
2024-02-17 13:21:29,973 - Epoch: [217][  200/  391]    Overall Loss 1.778262    Objective Loss 1.778262                                        LR 0.001298    Time 0.131710    
2024-02-17 13:21:42,590 - Epoch: [217][  300/  391]    Overall Loss 1.800022    Objective Loss 1.800022                                        LR 0.001298    Time 0.129849    
2024-02-17 13:21:53,474 - Epoch: [217][  391/  391]    Overall Loss 1.797314    Objective Loss 1.797314    Top1 45.673077    Top5 89.903846    LR 0.001298    Time 0.127452    
2024-02-17 13:21:53,635 - --- validate (epoch=217)-----------
2024-02-17 13:21:53,635 - 10000 samples (128 per mini-batch)
2024-02-17 13:21:59,431 - Epoch: [217][   79/   79]    Loss 1.871873    Top1 49.810000    Top5 79.530000    
2024-02-17 13:21:59,584 - ==> Top1: 49.810    Top5: 79.530    Loss: 1.872

2024-02-17 13:21:59,602 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:21:59,602 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:21:59,685 - 

2024-02-17 13:21:59,686 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:22:13,025 - Epoch: [218][  100/  391]    Overall Loss 1.828871    Objective Loss 1.828871                                        LR 0.001298    Time 0.133318    
2024-02-17 13:22:26,249 - Epoch: [218][  200/  391]    Overall Loss 1.834531    Objective Loss 1.834531                                        LR 0.001298    Time 0.132752    
2024-02-17 13:22:38,291 - Epoch: [218][  300/  391]    Overall Loss 1.834963    Objective Loss 1.834963                                        LR 0.001298    Time 0.128626    
2024-02-17 13:22:49,946 - Epoch: [218][  391/  391]    Overall Loss 1.832941    Objective Loss 1.832941    Top1 50.961538    Top5 79.326923    LR 0.001298    Time 0.128485    
2024-02-17 13:22:50,121 - --- validate (epoch=218)-----------
2024-02-17 13:22:50,122 - 10000 samples (128 per mini-batch)
2024-02-17 13:22:56,595 - Epoch: [218][   79/   79]    Loss 2.310090    Top1 41.710000    Top5 72.130000    
2024-02-17 13:22:56,697 - ==> Top1: 41.710    Top5: 72.130    Loss: 2.310

2024-02-17 13:22:56,707 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:22:56,708 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:22:56,775 - 

2024-02-17 13:22:56,776 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:23:10,574 - Epoch: [219][  100/  391]    Overall Loss 1.791581    Objective Loss 1.791581                                        LR 0.001298    Time 0.137904    
2024-02-17 13:23:23,077 - Epoch: [219][  200/  391]    Overall Loss 1.771847    Objective Loss 1.771847                                        LR 0.001298    Time 0.131442    
2024-02-17 13:23:34,912 - Epoch: [219][  300/  391]    Overall Loss 1.775991    Objective Loss 1.775991                                        LR 0.001298    Time 0.127062    
2024-02-17 13:23:46,907 - Epoch: [219][  391/  391]    Overall Loss 1.795246    Objective Loss 1.795246    Top1 43.269231    Top5 81.730769    LR 0.001298    Time 0.128155    
2024-02-17 13:23:47,021 - --- validate (epoch=219)-----------
2024-02-17 13:23:47,022 - 10000 samples (128 per mini-batch)
2024-02-17 13:23:52,880 - Epoch: [219][   79/   79]    Loss 2.011022    Top1 47.170000    Top5 78.100000    
2024-02-17 13:23:53,001 - ==> Top1: 47.170    Top5: 78.100    Loss: 2.011

2024-02-17 13:23:53,018 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:23:53,018 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:23:53,091 - 

2024-02-17 13:23:53,091 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:24:06,716 - Epoch: [220][  100/  391]    Overall Loss 1.781407    Objective Loss 1.781407                                        LR 0.001298    Time 0.136169    
2024-02-17 13:24:18,953 - Epoch: [220][  200/  391]    Overall Loss 1.823155    Objective Loss 1.823155                                        LR 0.001298    Time 0.129244    
2024-02-17 13:24:29,808 - Epoch: [220][  300/  391]    Overall Loss 1.821658    Objective Loss 1.821658                                        LR 0.001298    Time 0.122334    
2024-02-17 13:24:41,088 - Epoch: [220][  391/  391]    Overall Loss 1.813336    Objective Loss 1.813336    Top1 42.307692    Top5 81.730769    LR 0.001298    Time 0.122700    
2024-02-17 13:24:41,201 - --- validate (epoch=220)-----------
2024-02-17 13:24:41,202 - 10000 samples (128 per mini-batch)
2024-02-17 13:24:47,471 - Epoch: [220][   79/   79]    Loss 2.108349    Top1 44.670000    Top5 75.560000    
2024-02-17 13:24:47,571 - ==> Top1: 44.670    Top5: 75.560    Loss: 2.108

2024-02-17 13:24:47,588 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:24:47,588 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:24:47,652 - 

2024-02-17 13:24:47,652 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:25:00,616 - Epoch: [221][  100/  391]    Overall Loss 1.734709    Objective Loss 1.734709                                        LR 0.001298    Time 0.129555    
2024-02-17 13:25:13,189 - Epoch: [221][  200/  391]    Overall Loss 1.757889    Objective Loss 1.757889                                        LR 0.001298    Time 0.127617    
2024-02-17 13:25:25,863 - Epoch: [221][  300/  391]    Overall Loss 1.770787    Objective Loss 1.770787                                        LR 0.001298    Time 0.127310    
2024-02-17 13:25:36,999 - Epoch: [221][  391/  391]    Overall Loss 1.781477    Objective Loss 1.781477    Top1 48.557692    Top5 80.288462    LR 0.001298    Time 0.126150    
2024-02-17 13:25:37,084 - --- validate (epoch=221)-----------
2024-02-17 13:25:37,084 - 10000 samples (128 per mini-batch)
2024-02-17 13:25:42,214 - Epoch: [221][   79/   79]    Loss 1.950966    Top1 48.870000    Top5 78.670000    
2024-02-17 13:25:42,335 - ==> Top1: 48.870    Top5: 78.670    Loss: 1.951

2024-02-17 13:25:42,355 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:25:42,355 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:25:42,419 - 

2024-02-17 13:25:42,420 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:25:56,088 - Epoch: [222][  100/  391]    Overall Loss 1.741875    Objective Loss 1.741875                                        LR 0.001298    Time 0.136598    
2024-02-17 13:26:08,635 - Epoch: [222][  200/  391]    Overall Loss 1.789806    Objective Loss 1.789806                                        LR 0.001298    Time 0.131008    
2024-02-17 13:26:21,302 - Epoch: [222][  300/  391]    Overall Loss 1.789299    Objective Loss 1.789299                                        LR 0.001298    Time 0.129546    
2024-02-17 13:26:32,852 - Epoch: [222][  391/  391]    Overall Loss 1.784074    Objective Loss 1.784074    Top1 50.480769    Top5 78.365385    LR 0.001298    Time 0.128922    
2024-02-17 13:26:32,986 - --- validate (epoch=222)-----------
2024-02-17 13:26:32,987 - 10000 samples (128 per mini-batch)
2024-02-17 13:26:39,185 - Epoch: [222][   79/   79]    Loss 1.883512    Top1 49.000000    Top5 79.600000    
2024-02-17 13:26:39,313 - ==> Top1: 49.000    Top5: 79.600    Loss: 1.884

2024-02-17 13:26:39,330 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:26:39,330 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:26:39,396 - 

2024-02-17 13:26:39,397 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:26:52,547 - Epoch: [223][  100/  391]    Overall Loss 1.733129    Objective Loss 1.733129                                        LR 0.001298    Time 0.131422    
2024-02-17 13:27:05,167 - Epoch: [223][  200/  391]    Overall Loss 1.752717    Objective Loss 1.752717                                        LR 0.001298    Time 0.128792    
2024-02-17 13:27:18,125 - Epoch: [223][  300/  391]    Overall Loss 1.781458    Objective Loss 1.781458                                        LR 0.001298    Time 0.129036    
2024-02-17 13:27:29,894 - Epoch: [223][  391/  391]    Overall Loss 1.796360    Objective Loss 1.796360    Top1 51.923077    Top5 81.250000    LR 0.001298    Time 0.129091    
2024-02-17 13:27:30,005 - --- validate (epoch=223)-----------
2024-02-17 13:27:30,006 - 10000 samples (128 per mini-batch)
2024-02-17 13:27:35,578 - Epoch: [223][   79/   79]    Loss 1.925794    Top1 49.160000    Top5 79.430000    
2024-02-17 13:27:35,674 - ==> Top1: 49.160    Top5: 79.430    Loss: 1.926

2024-02-17 13:27:35,693 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:27:35,693 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:27:35,759 - 

2024-02-17 13:27:35,759 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:27:49,131 - Epoch: [224][  100/  391]    Overall Loss 1.794239    Objective Loss 1.794239                                        LR 0.001298    Time 0.133638    
2024-02-17 13:28:00,844 - Epoch: [224][  200/  391]    Overall Loss 1.818005    Objective Loss 1.818005                                        LR 0.001298    Time 0.125365    
2024-02-17 13:28:13,252 - Epoch: [224][  300/  391]    Overall Loss 1.823113    Objective Loss 1.823113                                        LR 0.001298    Time 0.124920    
2024-02-17 13:28:24,109 - Epoch: [224][  391/  391]    Overall Loss 1.836178    Objective Loss 1.836178    Top1 42.788462    Top5 76.442308    LR 0.001298    Time 0.123604    
2024-02-17 13:28:24,226 - --- validate (epoch=224)-----------
2024-02-17 13:28:24,227 - 10000 samples (128 per mini-batch)
2024-02-17 13:28:29,200 - Epoch: [224][   79/   79]    Loss 1.933762    Top1 48.200000    Top5 78.770000    
2024-02-17 13:28:29,332 - ==> Top1: 48.200    Top5: 78.770    Loss: 1.934

2024-02-17 13:28:29,348 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:28:29,349 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:28:29,399 - 

2024-02-17 13:28:29,399 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:28:41,088 - Epoch: [225][  100/  391]    Overall Loss 1.807517    Objective Loss 1.807517                                        LR 0.001298    Time 0.116833    
2024-02-17 13:28:53,816 - Epoch: [225][  200/  391]    Overall Loss 1.800229    Objective Loss 1.800229                                        LR 0.001298    Time 0.122032    
2024-02-17 13:29:05,984 - Epoch: [225][  300/  391]    Overall Loss 1.831341    Objective Loss 1.831341                                        LR 0.001298    Time 0.121900    
2024-02-17 13:29:16,407 - Epoch: [225][  391/  391]    Overall Loss 1.834284    Objective Loss 1.834284    Top1 57.211538    Top5 87.019231    LR 0.001298    Time 0.120176    
2024-02-17 13:29:16,557 - --- validate (epoch=225)-----------
2024-02-17 13:29:16,558 - 10000 samples (128 per mini-batch)
2024-02-17 13:29:22,096 - Epoch: [225][   79/   79]    Loss 1.918315    Top1 48.700000    Top5 79.540000    
2024-02-17 13:29:22,209 - ==> Top1: 48.700    Top5: 79.540    Loss: 1.918

2024-02-17 13:29:22,220 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:29:22,220 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:29:22,282 - 

2024-02-17 13:29:22,282 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:29:35,391 - Epoch: [226][  100/  391]    Overall Loss 1.725600    Objective Loss 1.725600                                        LR 0.001298    Time 0.131014    
2024-02-17 13:29:48,394 - Epoch: [226][  200/  391]    Overall Loss 1.784668    Objective Loss 1.784668                                        LR 0.001298    Time 0.130497    
2024-02-17 13:30:00,553 - Epoch: [226][  300/  391]    Overall Loss 1.796319    Objective Loss 1.796319                                        LR 0.001298    Time 0.127511    
2024-02-17 13:30:12,159 - Epoch: [226][  391/  391]    Overall Loss 1.802135    Objective Loss 1.802135    Top1 47.596154    Top5 80.288462    LR 0.001298    Time 0.127506    
2024-02-17 13:30:12,303 - --- validate (epoch=226)-----------
2024-02-17 13:30:12,303 - 10000 samples (128 per mini-batch)
2024-02-17 13:30:17,795 - Epoch: [226][   79/   79]    Loss 2.002975    Top1 46.140000    Top5 77.870000    
2024-02-17 13:30:17,909 - ==> Top1: 46.140    Top5: 77.870    Loss: 2.003

2024-02-17 13:30:17,927 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:30:17,927 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:30:17,991 - 

2024-02-17 13:30:17,992 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:30:31,174 - Epoch: [227][  100/  391]    Overall Loss 1.752606    Objective Loss 1.752606                                        LR 0.001298    Time 0.131750    
2024-02-17 13:30:44,279 - Epoch: [227][  200/  391]    Overall Loss 1.794186    Objective Loss 1.794186                                        LR 0.001298    Time 0.131372    
2024-02-17 13:30:56,793 - Epoch: [227][  300/  391]    Overall Loss 1.838211    Objective Loss 1.838211                                        LR 0.001298    Time 0.129281    
2024-02-17 13:31:08,606 - Epoch: [227][  391/  391]    Overall Loss 1.837271    Objective Loss 1.837271    Top1 50.000000    Top5 77.403846    LR 0.001298    Time 0.129393    
2024-02-17 13:31:08,736 - --- validate (epoch=227)-----------
2024-02-17 13:31:08,737 - 10000 samples (128 per mini-batch)
2024-02-17 13:31:13,511 - Epoch: [227][   79/   79]    Loss 1.932406    Top1 47.890000    Top5 78.590000    
2024-02-17 13:31:13,596 - ==> Top1: 47.890    Top5: 78.590    Loss: 1.932

2024-02-17 13:31:13,610 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:31:13,610 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:31:13,672 - 

2024-02-17 13:31:13,672 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:31:26,817 - Epoch: [228][  100/  391]    Overall Loss 1.904972    Objective Loss 1.904972                                        LR 0.001298    Time 0.131378    
2024-02-17 13:31:39,425 - Epoch: [228][  200/  391]    Overall Loss 1.876701    Objective Loss 1.876701                                        LR 0.001298    Time 0.128700    
2024-02-17 13:31:52,105 - Epoch: [228][  300/  391]    Overall Loss 1.888931    Objective Loss 1.888931                                        LR 0.001298    Time 0.128053    
2024-02-17 13:32:03,939 - Epoch: [228][  391/  391]    Overall Loss 1.867829    Objective Loss 1.867829    Top1 55.288462    Top5 81.250000    LR 0.001298    Time 0.128502    
2024-02-17 13:32:04,051 - --- validate (epoch=228)-----------
2024-02-17 13:32:04,051 - 10000 samples (128 per mini-batch)
2024-02-17 13:32:09,305 - Epoch: [228][   79/   79]    Loss 2.014145    Top1 46.660000    Top5 77.320000    
2024-02-17 13:32:09,413 - ==> Top1: 46.660    Top5: 77.320    Loss: 2.014

2024-02-17 13:32:09,440 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:32:09,441 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:32:09,504 - 

2024-02-17 13:32:09,504 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:32:22,728 - Epoch: [229][  100/  391]    Overall Loss 1.888491    Objective Loss 1.888491                                        LR 0.001298    Time 0.132165    
2024-02-17 13:32:35,453 - Epoch: [229][  200/  391]    Overall Loss 1.858748    Objective Loss 1.858748                                        LR 0.001298    Time 0.129683    
2024-02-17 13:32:48,318 - Epoch: [229][  300/  391]    Overall Loss 1.857402    Objective Loss 1.857402                                        LR 0.001298    Time 0.129322    
2024-02-17 13:33:00,114 - Epoch: [229][  391/  391]    Overall Loss 1.853072    Objective Loss 1.853072    Top1 53.365385    Top5 79.326923    LR 0.001298    Time 0.129380    
2024-02-17 13:33:00,237 - --- validate (epoch=229)-----------
2024-02-17 13:33:00,238 - 10000 samples (128 per mini-batch)
2024-02-17 13:33:06,002 - Epoch: [229][   79/   79]    Loss 1.990974    Top1 47.700000    Top5 77.460000    
2024-02-17 13:33:06,111 - ==> Top1: 47.700    Top5: 77.460    Loss: 1.991

2024-02-17 13:33:06,128 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:33:06,128 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:33:06,195 - 

2024-02-17 13:33:06,195 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:33:19,817 - Epoch: [230][  100/  391]    Overall Loss 1.856948    Objective Loss 1.856948                                        LR 0.001298    Time 0.136138    
2024-02-17 13:33:32,772 - Epoch: [230][  200/  391]    Overall Loss 1.855024    Objective Loss 1.855024                                        LR 0.001298    Time 0.132816    
2024-02-17 13:33:45,922 - Epoch: [230][  300/  391]    Overall Loss 1.864477    Objective Loss 1.864477                                        LR 0.001298    Time 0.132360    
2024-02-17 13:33:58,214 - Epoch: [230][  391/  391]    Overall Loss 1.858394    Objective Loss 1.858394    Top1 52.884615    Top5 81.730769    LR 0.001298    Time 0.132980    
2024-02-17 13:33:58,352 - --- validate (epoch=230)-----------
2024-02-17 13:33:58,353 - 10000 samples (128 per mini-batch)
2024-02-17 13:34:04,328 - Epoch: [230][   79/   79]    Loss 1.956796    Top1 48.150000    Top5 78.710000    
2024-02-17 13:34:04,445 - ==> Top1: 48.150    Top5: 78.710    Loss: 1.957

2024-02-17 13:34:04,463 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:34:04,463 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:34:04,527 - 

2024-02-17 13:34:04,527 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:34:17,302 - Epoch: [231][  100/  391]    Overall Loss 1.846826    Objective Loss 1.846826                                        LR 0.001298    Time 0.127671    
2024-02-17 13:34:29,523 - Epoch: [231][  200/  391]    Overall Loss 1.827427    Objective Loss 1.827427                                        LR 0.001298    Time 0.124918    
2024-02-17 13:34:42,101 - Epoch: [231][  300/  391]    Overall Loss 1.844654    Objective Loss 1.844654                                        LR 0.001298    Time 0.125188    
2024-02-17 13:34:53,762 - Epoch: [231][  391/  391]    Overall Loss 1.844030    Objective Loss 1.844030    Top1 52.884615    Top5 83.173077    LR 0.001298    Time 0.125862    
2024-02-17 13:34:53,909 - --- validate (epoch=231)-----------
2024-02-17 13:34:53,910 - 10000 samples (128 per mini-batch)
2024-02-17 13:34:59,545 - Epoch: [231][   79/   79]    Loss 1.975744    Top1 47.900000    Top5 77.500000    
2024-02-17 13:34:59,654 - ==> Top1: 47.900    Top5: 77.500    Loss: 1.976

2024-02-17 13:34:59,672 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:34:59,672 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:34:59,735 - 

2024-02-17 13:34:59,736 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:35:13,025 - Epoch: [232][  100/  391]    Overall Loss 1.855910    Objective Loss 1.855910                                        LR 0.001298    Time 0.132816    
2024-02-17 13:35:23,650 - Epoch: [232][  200/  391]    Overall Loss 1.827097    Objective Loss 1.827097                                        LR 0.001298    Time 0.119512    
2024-02-17 13:35:36,395 - Epoch: [232][  300/  391]    Overall Loss 1.805087    Objective Loss 1.805087                                        LR 0.001298    Time 0.122143    
2024-02-17 13:35:47,959 - Epoch: [232][  391/  391]    Overall Loss 1.813952    Objective Loss 1.813952    Top1 39.903846    Top5 72.115385    LR 0.001298    Time 0.123280    
2024-02-17 13:35:48,095 - --- validate (epoch=232)-----------
2024-02-17 13:35:48,096 - 10000 samples (128 per mini-batch)
2024-02-17 13:35:53,665 - Epoch: [232][   79/   79]    Loss 2.237501    Top1 41.590000    Top5 73.150000    
2024-02-17 13:35:53,778 - ==> Top1: 41.590    Top5: 73.150    Loss: 2.238

2024-02-17 13:35:54,006 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:35:54,007 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:35:54,065 - 

2024-02-17 13:35:54,065 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:36:07,261 - Epoch: [233][  100/  391]    Overall Loss 1.815532    Objective Loss 1.815532                                        LR 0.001298    Time 0.131886    
2024-02-17 13:36:19,847 - Epoch: [233][  200/  391]    Overall Loss 1.789530    Objective Loss 1.789530                                        LR 0.001298    Time 0.128843    
2024-02-17 13:36:32,939 - Epoch: [233][  300/  391]    Overall Loss 1.789254    Objective Loss 1.789254                                        LR 0.001298    Time 0.129517    
2024-02-17 13:36:44,335 - Epoch: [233][  391/  391]    Overall Loss 1.796357    Objective Loss 1.796357    Top1 42.307692    Top5 73.076923    LR 0.001298    Time 0.128510    
2024-02-17 13:36:44,503 - --- validate (epoch=233)-----------
2024-02-17 13:36:44,503 - 10000 samples (128 per mini-batch)
2024-02-17 13:36:49,695 - Epoch: [233][   79/   79]    Loss 2.305508    Top1 41.820000    Top5 72.660000    
2024-02-17 13:36:49,816 - ==> Top1: 41.820    Top5: 72.660    Loss: 2.306

2024-02-17 13:36:49,834 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:36:49,834 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:36:49,885 - 

2024-02-17 13:36:49,885 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:37:02,371 - Epoch: [234][  100/  391]    Overall Loss 1.874391    Objective Loss 1.874391                                        LR 0.001298    Time 0.124790    
2024-02-17 13:37:15,537 - Epoch: [234][  200/  391]    Overall Loss 1.874832    Objective Loss 1.874832                                        LR 0.001298    Time 0.128200    
2024-02-17 13:37:28,347 - Epoch: [234][  300/  391]    Overall Loss 1.861029    Objective Loss 1.861029                                        LR 0.001298    Time 0.128151    
2024-02-17 13:37:38,868 - Epoch: [234][  391/  391]    Overall Loss 1.860186    Objective Loss 1.860186    Top1 55.288462    Top5 85.096154    LR 0.001298    Time 0.125222    
2024-02-17 13:37:38,998 - --- validate (epoch=234)-----------
2024-02-17 13:37:38,999 - 10000 samples (128 per mini-batch)
2024-02-17 13:37:44,482 - Epoch: [234][   79/   79]    Loss 2.084371    Top1 45.010000    Top5 76.290000    
2024-02-17 13:37:44,644 - ==> Top1: 45.010    Top5: 76.290    Loss: 2.084

2024-02-17 13:37:44,662 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:37:44,662 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:37:44,726 - 

2024-02-17 13:37:44,726 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:37:58,229 - Epoch: [235][  100/  391]    Overall Loss 1.821712    Objective Loss 1.821712                                        LR 0.001298    Time 0.134948    
2024-02-17 13:38:10,922 - Epoch: [235][  200/  391]    Overall Loss 1.817908    Objective Loss 1.817908                                        LR 0.001298    Time 0.130916    
2024-02-17 13:38:23,972 - Epoch: [235][  300/  391]    Overall Loss 1.818022    Objective Loss 1.818022                                        LR 0.001298    Time 0.130759    
2024-02-17 13:38:35,568 - Epoch: [235][  391/  391]    Overall Loss 1.825602    Objective Loss 1.825602    Top1 48.076923    Top5 78.365385    LR 0.001298    Time 0.129971    
2024-02-17 13:38:35,701 - --- validate (epoch=235)-----------
2024-02-17 13:38:35,702 - 10000 samples (128 per mini-batch)
2024-02-17 13:38:41,280 - Epoch: [235][   79/   79]    Loss 2.212054    Top1 43.300000    Top5 74.190000    
2024-02-17 13:38:41,411 - ==> Top1: 43.300    Top5: 74.190    Loss: 2.212

2024-02-17 13:38:41,432 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:38:41,432 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:38:41,497 - 

2024-02-17 13:38:41,497 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:38:54,962 - Epoch: [236][  100/  391]    Overall Loss 1.860498    Objective Loss 1.860498                                        LR 0.001298    Time 0.134574    
2024-02-17 13:39:07,618 - Epoch: [236][  200/  391]    Overall Loss 1.847961    Objective Loss 1.847961                                        LR 0.001298    Time 0.130543    
2024-02-17 13:39:20,633 - Epoch: [236][  300/  391]    Overall Loss 1.821570    Objective Loss 1.821570                                        LR 0.001298    Time 0.130393    
2024-02-17 13:39:32,114 - Epoch: [236][  391/  391]    Overall Loss 1.825297    Objective Loss 1.825297    Top1 53.365385    Top5 86.057692    LR 0.001298    Time 0.129397    
2024-02-17 13:39:32,248 - --- validate (epoch=236)-----------
2024-02-17 13:39:32,249 - 10000 samples (128 per mini-batch)
2024-02-17 13:39:38,214 - Epoch: [236][   79/   79]    Loss 1.900161    Top1 48.810000    Top5 79.330000    
2024-02-17 13:39:38,332 - ==> Top1: 48.810    Top5: 79.330    Loss: 1.900

2024-02-17 13:39:38,351 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:39:38,351 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:39:38,415 - 

2024-02-17 13:39:38,415 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:39:51,578 - Epoch: [237][  100/  391]    Overall Loss 1.774094    Objective Loss 1.774094                                        LR 0.001298    Time 0.131548    
2024-02-17 13:40:04,091 - Epoch: [237][  200/  391]    Overall Loss 1.795875    Objective Loss 1.795875                                        LR 0.001298    Time 0.128316    
2024-02-17 13:40:16,593 - Epoch: [237][  300/  391]    Overall Loss 1.785485    Objective Loss 1.785485                                        LR 0.001298    Time 0.127201    
2024-02-17 13:40:28,295 - Epoch: [237][  391/  391]    Overall Loss 1.783597    Objective Loss 1.783597    Top1 54.326923    Top5 82.211538    LR 0.001298    Time 0.127516    
2024-02-17 13:40:28,475 - --- validate (epoch=237)-----------
2024-02-17 13:40:28,476 - 10000 samples (128 per mini-batch)
2024-02-17 13:40:33,978 - Epoch: [237][   79/   79]    Loss 1.878014    Top1 49.080000    Top5 79.660000    
2024-02-17 13:40:34,091 - ==> Top1: 49.080    Top5: 79.660    Loss: 1.878

2024-02-17 13:40:34,104 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:40:34,104 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:40:34,170 - 

2024-02-17 13:40:34,171 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:40:47,633 - Epoch: [238][  100/  391]    Overall Loss 1.796955    Objective Loss 1.796955                                        LR 0.001298    Time 0.134542    
2024-02-17 13:41:00,504 - Epoch: [238][  200/  391]    Overall Loss 1.802653    Objective Loss 1.802653                                        LR 0.001298    Time 0.131600    
2024-02-17 13:41:12,611 - Epoch: [238][  300/  391]    Overall Loss 1.822808    Objective Loss 1.822808                                        LR 0.001298    Time 0.128075    
2024-02-17 13:41:24,136 - Epoch: [238][  391/  391]    Overall Loss 1.813117    Objective Loss 1.813117    Top1 57.211538    Top5 89.423077    LR 0.001298    Time 0.127734    
2024-02-17 13:41:24,276 - --- validate (epoch=238)-----------
2024-02-17 13:41:24,276 - 10000 samples (128 per mini-batch)
2024-02-17 13:41:30,251 - Epoch: [238][   79/   79]    Loss 1.946670    Top1 47.520000    Top5 78.360000    
2024-02-17 13:41:30,358 - ==> Top1: 47.520    Top5: 78.360    Loss: 1.947

2024-02-17 13:41:30,378 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:41:30,378 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:41:30,444 - 

2024-02-17 13:41:30,445 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:41:42,185 - Epoch: [239][  100/  391]    Overall Loss 1.741293    Objective Loss 1.741293                                        LR 0.001298    Time 0.117339    
2024-02-17 13:41:55,169 - Epoch: [239][  200/  391]    Overall Loss 1.761411    Objective Loss 1.761411                                        LR 0.001298    Time 0.123560    
2024-02-17 13:42:07,729 - Epoch: [239][  300/  391]    Overall Loss 1.789208    Objective Loss 1.789208                                        LR 0.001298    Time 0.124223    
2024-02-17 13:42:18,999 - Epoch: [239][  391/  391]    Overall Loss 1.789742    Objective Loss 1.789742    Top1 49.038462    Top5 84.615385    LR 0.001298    Time 0.124125    
2024-02-17 13:42:19,119 - --- validate (epoch=239)-----------
2024-02-17 13:42:19,119 - 10000 samples (128 per mini-batch)
2024-02-17 13:42:25,191 - Epoch: [239][   79/   79]    Loss 2.123388    Top1 45.330000    Top5 75.920000    
2024-02-17 13:42:25,353 - ==> Top1: 45.330    Top5: 75.920    Loss: 2.123

2024-02-17 13:42:25,373 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:42:25,374 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:42:25,438 - 

2024-02-17 13:42:25,438 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:42:38,632 - Epoch: [240][  100/  391]    Overall Loss 1.750116    Objective Loss 1.750116                                        LR 0.001298    Time 0.131853    
2024-02-17 13:42:51,249 - Epoch: [240][  200/  391]    Overall Loss 1.799295    Objective Loss 1.799295                                        LR 0.001298    Time 0.128989    
2024-02-17 13:43:03,932 - Epoch: [240][  300/  391]    Overall Loss 1.796842    Objective Loss 1.796842                                        LR 0.001298    Time 0.128253    
2024-02-17 13:43:15,773 - Epoch: [240][  391/  391]    Overall Loss 1.794263    Objective Loss 1.794263    Top1 52.884615    Top5 89.423077    LR 0.001298    Time 0.128677    
2024-02-17 13:43:15,947 - --- validate (epoch=240)-----------
2024-02-17 13:43:15,947 - 10000 samples (128 per mini-batch)
2024-02-17 13:43:22,105 - Epoch: [240][   79/   79]    Loss 1.975428    Top1 47.470000    Top5 78.120000    
2024-02-17 13:43:22,216 - ==> Top1: 47.470    Top5: 78.120    Loss: 1.975

2024-02-17 13:43:22,226 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:43:22,226 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:43:22,291 - 

2024-02-17 13:43:22,291 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:43:35,515 - Epoch: [241][  100/  391]    Overall Loss 1.841781    Objective Loss 1.841781                                        LR 0.001298    Time 0.132155    
2024-02-17 13:43:48,133 - Epoch: [241][  200/  391]    Overall Loss 1.844162    Objective Loss 1.844162                                        LR 0.001298    Time 0.129144    
2024-02-17 13:44:00,848 - Epoch: [241][  300/  391]    Overall Loss 1.835355    Objective Loss 1.835355                                        LR 0.001298    Time 0.128463    
2024-02-17 13:44:12,125 - Epoch: [241][  391/  391]    Overall Loss 1.829008    Objective Loss 1.829008    Top1 50.961538    Top5 80.288462    LR 0.001298    Time 0.127395    
2024-02-17 13:44:12,263 - --- validate (epoch=241)-----------
2024-02-17 13:44:12,264 - 10000 samples (128 per mini-batch)
2024-02-17 13:44:17,846 - Epoch: [241][   79/   79]    Loss 1.908854    Top1 49.310000    Top5 78.760000    
2024-02-17 13:44:17,937 - ==> Top1: 49.310    Top5: 78.760    Loss: 1.909

2024-02-17 13:44:17,954 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:44:17,955 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:44:18,013 - 

2024-02-17 13:44:18,013 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:44:31,390 - Epoch: [242][  100/  391]    Overall Loss 1.844584    Objective Loss 1.844584                                        LR 0.001298    Time 0.133699    
2024-02-17 13:44:43,528 - Epoch: [242][  200/  391]    Overall Loss 1.818567    Objective Loss 1.818567                                        LR 0.001298    Time 0.127517    
2024-02-17 13:44:56,088 - Epoch: [242][  300/  391]    Overall Loss 1.823732    Objective Loss 1.823732                                        LR 0.001298    Time 0.126860    
2024-02-17 13:45:07,430 - Epoch: [242][  391/  391]    Overall Loss 1.828488    Objective Loss 1.828488    Top1 53.846154    Top5 81.250000    LR 0.001298    Time 0.126330    
2024-02-17 13:45:07,566 - --- validate (epoch=242)-----------
2024-02-17 13:45:07,567 - 10000 samples (128 per mini-batch)
2024-02-17 13:45:12,908 - Epoch: [242][   79/   79]    Loss 1.984978    Top1 46.740000    Top5 77.660000    
2024-02-17 13:45:12,992 - ==> Top1: 46.740    Top5: 77.660    Loss: 1.985

2024-02-17 13:45:13,008 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:45:13,009 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:45:13,073 - 

2024-02-17 13:45:13,073 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:45:25,921 - Epoch: [243][  100/  391]    Overall Loss 1.762836    Objective Loss 1.762836                                        LR 0.001298    Time 0.128416    
2024-02-17 13:45:37,347 - Epoch: [243][  200/  391]    Overall Loss 1.810159    Objective Loss 1.810159                                        LR 0.001298    Time 0.121315    
2024-02-17 13:45:49,671 - Epoch: [243][  300/  391]    Overall Loss 1.807961    Objective Loss 1.807961                                        LR 0.001298    Time 0.121940    
2024-02-17 13:46:01,239 - Epoch: [243][  391/  391]    Overall Loss 1.814866    Objective Loss 1.814866    Top1 47.115385    Top5 80.288462    LR 0.001298    Time 0.123134    
2024-02-17 13:46:01,406 - --- validate (epoch=243)-----------
2024-02-17 13:46:01,407 - 10000 samples (128 per mini-batch)
2024-02-17 13:46:06,367 - Epoch: [243][   79/   79]    Loss 1.927473    Top1 48.620000    Top5 78.830000    
2024-02-17 13:46:06,467 - ==> Top1: 48.620    Top5: 78.830    Loss: 1.927

2024-02-17 13:46:06,484 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:46:06,484 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:46:06,536 - 

2024-02-17 13:46:06,536 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:46:18,912 - Epoch: [244][  100/  391]    Overall Loss 1.793756    Objective Loss 1.793756                                        LR 0.001298    Time 0.123689    
2024-02-17 13:46:31,678 - Epoch: [244][  200/  391]    Overall Loss 1.777120    Objective Loss 1.777120                                        LR 0.001298    Time 0.125650    
2024-02-17 13:46:44,432 - Epoch: [244][  300/  391]    Overall Loss 1.777750    Objective Loss 1.777750                                        LR 0.001298    Time 0.126264    
2024-02-17 13:46:55,456 - Epoch: [244][  391/  391]    Overall Loss 1.780696    Objective Loss 1.780696    Top1 45.673077    Top5 75.480769    LR 0.001298    Time 0.125063    
2024-02-17 13:46:55,557 - --- validate (epoch=244)-----------
2024-02-17 13:46:55,558 - 10000 samples (128 per mini-batch)
2024-02-17 13:47:00,532 - Epoch: [244][   79/   79]    Loss 2.289523    Top1 40.620000    Top5 72.330000    
2024-02-17 13:47:00,642 - ==> Top1: 40.620    Top5: 72.330    Loss: 2.290

2024-02-17 13:47:00,659 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:47:00,659 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:47:00,724 - 

2024-02-17 13:47:00,724 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:47:14,105 - Epoch: [245][  100/  391]    Overall Loss 1.786288    Objective Loss 1.786288                                        LR 0.001298    Time 0.133726    
2024-02-17 13:47:26,876 - Epoch: [245][  200/  391]    Overall Loss 1.830515    Objective Loss 1.830515                                        LR 0.001298    Time 0.130693    
2024-02-17 13:47:39,339 - Epoch: [245][  300/  391]    Overall Loss 1.813237    Objective Loss 1.813237                                        LR 0.001298    Time 0.128658    
2024-02-17 13:47:50,819 - Epoch: [245][  391/  391]    Overall Loss 1.808284    Objective Loss 1.808284    Top1 56.250000    Top5 87.980769    LR 0.001298    Time 0.128064    
2024-02-17 13:47:50,944 - --- validate (epoch=245)-----------
2024-02-17 13:47:50,945 - 10000 samples (128 per mini-batch)
2024-02-17 13:47:56,707 - Epoch: [245][   79/   79]    Loss 2.008225    Top1 46.820000    Top5 77.690000    
2024-02-17 13:47:56,814 - ==> Top1: 46.820    Top5: 77.690    Loss: 2.008

2024-02-17 13:47:56,830 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:47:56,831 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:47:56,895 - 

2024-02-17 13:47:56,896 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:48:10,215 - Epoch: [246][  100/  391]    Overall Loss 1.777995    Objective Loss 1.777995                                        LR 0.001298    Time 0.133108    
2024-02-17 13:48:21,664 - Epoch: [246][  200/  391]    Overall Loss 1.794224    Objective Loss 1.794224                                        LR 0.001298    Time 0.123777    
2024-02-17 13:48:34,432 - Epoch: [246][  300/  391]    Overall Loss 1.763697    Objective Loss 1.763697                                        LR 0.001298    Time 0.125060    
2024-02-17 13:48:46,079 - Epoch: [246][  391/  391]    Overall Loss 1.763616    Objective Loss 1.763616    Top1 52.884615    Top5 82.211538    LR 0.001298    Time 0.125730    
2024-02-17 13:48:46,254 - --- validate (epoch=246)-----------
2024-02-17 13:48:46,255 - 10000 samples (128 per mini-batch)
2024-02-17 13:48:52,314 - Epoch: [246][   79/   79]    Loss 1.891544    Top1 49.540000    Top5 79.200000    
2024-02-17 13:48:52,482 - ==> Top1: 49.540    Top5: 79.200    Loss: 1.892

2024-02-17 13:48:52,501 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:48:52,501 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:48:52,565 - 

2024-02-17 13:48:52,566 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:49:05,872 - Epoch: [247][  100/  391]    Overall Loss 1.749232    Objective Loss 1.749232                                        LR 0.001298    Time 0.132988    
2024-02-17 13:49:18,680 - Epoch: [247][  200/  391]    Overall Loss 1.772232    Objective Loss 1.772232                                        LR 0.001298    Time 0.130510    
2024-02-17 13:49:31,387 - Epoch: [247][  300/  391]    Overall Loss 1.775502    Objective Loss 1.775502                                        LR 0.001298    Time 0.129348    
2024-02-17 13:49:42,841 - Epoch: [247][  391/  391]    Overall Loss 1.783128    Objective Loss 1.783128    Top1 42.307692    Top5 82.211538    LR 0.001298    Time 0.128528    
2024-02-17 13:49:43,038 - --- validate (epoch=247)-----------
2024-02-17 13:49:43,039 - 10000 samples (128 per mini-batch)
2024-02-17 13:49:48,529 - Epoch: [247][   79/   79]    Loss 2.061595    Top1 46.330000    Top5 76.970000    
2024-02-17 13:49:48,727 - ==> Top1: 46.330    Top5: 76.970    Loss: 2.062

2024-02-17 13:49:48,744 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:49:48,744 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:49:48,809 - 

2024-02-17 13:49:48,809 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:50:01,930 - Epoch: [248][  100/  391]    Overall Loss 1.761380    Objective Loss 1.761380                                        LR 0.001298    Time 0.131139    
2024-02-17 13:50:14,890 - Epoch: [248][  200/  391]    Overall Loss 1.776914    Objective Loss 1.776914                                        LR 0.001298    Time 0.130342    
2024-02-17 13:50:28,150 - Epoch: [248][  300/  391]    Overall Loss 1.800483    Objective Loss 1.800483                                        LR 0.001298    Time 0.131079    
2024-02-17 13:50:39,698 - Epoch: [248][  391/  391]    Overall Loss 1.799473    Objective Loss 1.799473    Top1 48.076923    Top5 75.961538    LR 0.001298    Time 0.130094    
2024-02-17 13:50:39,811 - --- validate (epoch=248)-----------
2024-02-17 13:50:39,812 - 10000 samples (128 per mini-batch)
2024-02-17 13:50:45,827 - Epoch: [248][   79/   79]    Loss 1.993960    Top1 46.510000    Top5 77.500000    
2024-02-17 13:50:45,950 - ==> Top1: 46.510    Top5: 77.500    Loss: 1.994

2024-02-17 13:50:45,968 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:50:45,969 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:50:46,035 - 

2024-02-17 13:50:46,035 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:50:59,221 - Epoch: [249][  100/  391]    Overall Loss 1.788607    Objective Loss 1.788607                                        LR 0.001298    Time 0.131779    
2024-02-17 13:51:11,825 - Epoch: [249][  200/  391]    Overall Loss 1.810186    Objective Loss 1.810186                                        LR 0.001298    Time 0.128884    
2024-02-17 13:51:24,854 - Epoch: [249][  300/  391]    Overall Loss 1.811299    Objective Loss 1.811299                                        LR 0.001298    Time 0.129336    
2024-02-17 13:51:35,980 - Epoch: [249][  391/  391]    Overall Loss 1.818279    Objective Loss 1.818279    Top1 47.115385    Top5 76.923077    LR 0.001298    Time 0.127678    
2024-02-17 13:51:36,115 - --- validate (epoch=249)-----------
2024-02-17 13:51:36,115 - 10000 samples (128 per mini-batch)
2024-02-17 13:51:42,326 - Epoch: [249][   79/   79]    Loss 2.007356    Top1 46.300000    Top5 77.410000    
2024-02-17 13:51:42,456 - ==> Top1: 46.300    Top5: 77.410    Loss: 2.007

2024-02-17 13:51:42,474 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:51:42,474 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:51:42,544 - 

2024-02-17 13:51:42,544 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:51:55,876 - Epoch: [250][  100/  391]    Overall Loss 1.805397    Objective Loss 1.805397                                        LR 0.001298    Time 0.133200    
2024-02-17 13:52:08,029 - Epoch: [250][  200/  391]    Overall Loss 1.817941    Objective Loss 1.817941                                        LR 0.001298    Time 0.127341    
2024-02-17 13:52:20,923 - Epoch: [250][  300/  391]    Overall Loss 1.798789    Objective Loss 1.798789                                        LR 0.001298    Time 0.127856    
2024-02-17 13:52:32,315 - Epoch: [250][  391/  391]    Overall Loss 1.811230    Objective Loss 1.811230    Top1 46.153846    Top5 75.000000    LR 0.001298    Time 0.127223    
2024-02-17 13:52:32,433 - --- validate (epoch=250)-----------
2024-02-17 13:52:32,434 - 10000 samples (128 per mini-batch)
2024-02-17 13:52:38,481 - Epoch: [250][   79/   79]    Loss 2.052825    Top1 46.000000    Top5 75.830000    
2024-02-17 13:52:38,646 - ==> Top1: 46.000    Top5: 75.830    Loss: 2.053

2024-02-17 13:52:38,667 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:52:38,667 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:52:38,735 - 

2024-02-17 13:52:38,735 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:52:51,831 - Epoch: [251][  100/  391]    Overall Loss 1.841579    Objective Loss 1.841579                                        LR 0.001298    Time 0.130883    
2024-02-17 13:53:02,367 - Epoch: [251][  200/  391]    Overall Loss 1.842003    Objective Loss 1.842003                                        LR 0.001298    Time 0.118099    
2024-02-17 13:53:12,341 - Epoch: [251][  300/  391]    Overall Loss 1.825605    Objective Loss 1.825605                                        LR 0.001298    Time 0.111968    
2024-02-17 13:53:22,213 - Epoch: [251][  391/  391]    Overall Loss 1.821849    Objective Loss 1.821849    Top1 56.730769    Top5 84.134615    LR 0.001298    Time 0.111147    
2024-02-17 13:53:22,339 - --- validate (epoch=251)-----------
2024-02-17 13:53:22,340 - 10000 samples (128 per mini-batch)
2024-02-17 13:53:28,376 - Epoch: [251][   79/   79]    Loss 2.349644    Top1 41.200000    Top5 71.580000    
2024-02-17 13:53:28,560 - ==> Top1: 41.200    Top5: 71.580    Loss: 2.350

2024-02-17 13:53:28,582 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:53:28,582 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:53:28,646 - 

2024-02-17 13:53:28,647 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:53:41,900 - Epoch: [252][  100/  391]    Overall Loss 1.804925    Objective Loss 1.804925                                        LR 0.001298    Time 0.132452    
2024-02-17 13:53:54,006 - Epoch: [252][  200/  391]    Overall Loss 1.802977    Objective Loss 1.802977                                        LR 0.001298    Time 0.126736    
2024-02-17 13:54:06,695 - Epoch: [252][  300/  391]    Overall Loss 1.815592    Objective Loss 1.815592                                        LR 0.001298    Time 0.126770    
2024-02-17 13:54:18,132 - Epoch: [252][  391/  391]    Overall Loss 1.807855    Objective Loss 1.807855    Top1 52.403846    Top5 81.250000    LR 0.001298    Time 0.126505    
2024-02-17 13:54:18,260 - --- validate (epoch=252)-----------
2024-02-17 13:54:18,260 - 10000 samples (128 per mini-batch)
2024-02-17 13:54:24,377 - Epoch: [252][   79/   79]    Loss 2.082764    Top1 45.290000    Top5 75.460000    
2024-02-17 13:54:24,478 - ==> Top1: 45.290    Top5: 75.460    Loss: 2.083

2024-02-17 13:54:24,488 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:54:24,488 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:54:24,551 - 

2024-02-17 13:54:24,551 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:54:37,799 - Epoch: [253][  100/  391]    Overall Loss 1.701392    Objective Loss 1.701392                                        LR 0.001298    Time 0.132396    
2024-02-17 13:54:50,083 - Epoch: [253][  200/  391]    Overall Loss 1.734437    Objective Loss 1.734437                                        LR 0.001298    Time 0.127593    
2024-02-17 13:55:03,212 - Epoch: [253][  300/  391]    Overall Loss 1.747561    Objective Loss 1.747561                                        LR 0.001298    Time 0.128811    
2024-02-17 13:55:14,348 - Epoch: [253][  391/  391]    Overall Loss 1.756905    Objective Loss 1.756905    Top1 46.634615    Top5 80.769231    LR 0.001298    Time 0.127301    
2024-02-17 13:55:14,481 - --- validate (epoch=253)-----------
2024-02-17 13:55:14,482 - 10000 samples (128 per mini-batch)
2024-02-17 13:55:20,498 - Epoch: [253][   79/   79]    Loss 2.113886    Top1 44.940000    Top5 76.680000    
2024-02-17 13:55:20,599 - ==> Top1: 44.940    Top5: 76.680    Loss: 2.114

2024-02-17 13:55:20,618 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:55:20,618 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:55:20,686 - 

2024-02-17 13:55:20,686 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:55:33,684 - Epoch: [254][  100/  391]    Overall Loss 1.795101    Objective Loss 1.795101                                        LR 0.001298    Time 0.129901    
2024-02-17 13:55:45,779 - Epoch: [254][  200/  391]    Overall Loss 1.772000    Objective Loss 1.772000                                        LR 0.001298    Time 0.125401    
2024-02-17 13:55:58,541 - Epoch: [254][  300/  391]    Overall Loss 1.785765    Objective Loss 1.785765                                        LR 0.001298    Time 0.126124    
2024-02-17 13:56:10,036 - Epoch: [254][  391/  391]    Overall Loss 1.787635    Objective Loss 1.787635    Top1 59.134615    Top5 85.576923    LR 0.001298    Time 0.126159    
2024-02-17 13:56:10,164 - --- validate (epoch=254)-----------
2024-02-17 13:56:10,164 - 10000 samples (128 per mini-batch)
2024-02-17 13:56:16,408 - Epoch: [254][   79/   79]    Loss 1.884287    Top1 49.270000    Top5 80.160000    
2024-02-17 13:56:16,529 - ==> Top1: 49.270    Top5: 80.160    Loss: 1.884

2024-02-17 13:56:16,541 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:56:16,541 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:56:16,603 - 

2024-02-17 13:56:16,604 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:56:29,822 - Epoch: [255][  100/  391]    Overall Loss 1.762143    Objective Loss 1.762143                                        LR 0.001298    Time 0.132111    
2024-02-17 13:56:42,443 - Epoch: [255][  200/  391]    Overall Loss 1.766679    Objective Loss 1.766679                                        LR 0.001298    Time 0.129133    
2024-02-17 13:56:54,771 - Epoch: [255][  300/  391]    Overall Loss 1.762083    Objective Loss 1.762083                                        LR 0.001298    Time 0.127167    
2024-02-17 13:57:05,191 - Epoch: [255][  391/  391]    Overall Loss 1.763869    Objective Loss 1.763869    Top1 42.307692    Top5 75.480769    LR 0.001298    Time 0.124209    
2024-02-17 13:57:05,301 - --- validate (epoch=255)-----------
2024-02-17 13:57:05,302 - 10000 samples (128 per mini-batch)
2024-02-17 13:57:11,495 - Epoch: [255][   79/   79]    Loss 1.954861    Top1 47.630000    Top5 78.410000    
2024-02-17 13:57:11,610 - ==> Top1: 47.630    Top5: 78.410    Loss: 1.955

2024-02-17 13:57:11,627 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:57:11,627 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:57:11,692 - 

2024-02-17 13:57:11,692 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:57:24,876 - Epoch: [256][  100/  391]    Overall Loss 1.748483    Objective Loss 1.748483                                        LR 0.001298    Time 0.131759    
2024-02-17 13:57:37,442 - Epoch: [256][  200/  391]    Overall Loss 1.785497    Objective Loss 1.785497                                        LR 0.001298    Time 0.128684    
2024-02-17 13:57:50,165 - Epoch: [256][  300/  391]    Overall Loss 1.789988    Objective Loss 1.789988                                        LR 0.001298    Time 0.128184    
2024-02-17 13:58:01,987 - Epoch: [256][  391/  391]    Overall Loss 1.786356    Objective Loss 1.786356    Top1 50.961538    Top5 80.288462    LR 0.001298    Time 0.128575    
2024-02-17 13:58:02,112 - --- validate (epoch=256)-----------
2024-02-17 13:58:02,112 - 10000 samples (128 per mini-batch)
2024-02-17 13:58:08,056 - Epoch: [256][   79/   79]    Loss 1.993588    Top1 46.390000    Top5 78.080000    
2024-02-17 13:58:08,187 - ==> Top1: 46.390    Top5: 78.080    Loss: 1.994

2024-02-17 13:58:08,205 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:58:08,205 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:58:08,271 - 

2024-02-17 13:58:08,271 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:58:21,715 - Epoch: [257][  100/  391]    Overall Loss 1.820970    Objective Loss 1.820970                                        LR 0.001298    Time 0.134360    
2024-02-17 13:58:33,064 - Epoch: [257][  200/  391]    Overall Loss 1.797659    Objective Loss 1.797659                                        LR 0.001298    Time 0.123901    
2024-02-17 13:58:44,674 - Epoch: [257][  300/  391]    Overall Loss 1.783737    Objective Loss 1.783737                                        LR 0.001298    Time 0.121288    
2024-02-17 13:58:56,084 - Epoch: [257][  391/  391]    Overall Loss 1.799859    Objective Loss 1.799859    Top1 57.692308    Top5 88.461538    LR 0.001298    Time 0.122229    
2024-02-17 13:58:56,196 - --- validate (epoch=257)-----------
2024-02-17 13:58:56,197 - 10000 samples (128 per mini-batch)
2024-02-17 13:59:01,543 - Epoch: [257][   79/   79]    Loss 2.125056    Top1 44.070000    Top5 75.510000    
2024-02-17 13:59:01,653 - ==> Top1: 44.070    Top5: 75.510    Loss: 2.125

2024-02-17 13:59:01,661 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:59:01,661 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:59:01,714 - 

2024-02-17 13:59:01,714 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:59:14,919 - Epoch: [258][  100/  391]    Overall Loss 1.793887    Objective Loss 1.793887                                        LR 0.001298    Time 0.131932    
2024-02-17 13:59:27,760 - Epoch: [258][  200/  391]    Overall Loss 1.769779    Objective Loss 1.769779                                        LR 0.001298    Time 0.130148    
2024-02-17 13:59:40,484 - Epoch: [258][  300/  391]    Overall Loss 1.760580    Objective Loss 1.760580                                        LR 0.001298    Time 0.129160    
2024-02-17 13:59:51,925 - Epoch: [258][  391/  391]    Overall Loss 1.761681    Objective Loss 1.761681    Top1 50.000000    Top5 82.211538    LR 0.001298    Time 0.128351    
2024-02-17 13:59:52,049 - --- validate (epoch=258)-----------
2024-02-17 13:59:52,050 - 10000 samples (128 per mini-batch)
2024-02-17 13:59:58,095 - Epoch: [258][   79/   79]    Loss 1.880487    Top1 49.530000    Top5 79.640000    
2024-02-17 13:59:58,227 - ==> Top1: 49.530    Top5: 79.640    Loss: 1.880

2024-02-17 13:59:58,245 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 13:59:58,245 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 13:59:58,313 - 

2024-02-17 13:59:58,313 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:00:11,799 - Epoch: [259][  100/  391]    Overall Loss 1.751630    Objective Loss 1.751630                                        LR 0.001298    Time 0.134780    
2024-02-17 14:00:24,686 - Epoch: [259][  200/  391]    Overall Loss 1.765445    Objective Loss 1.765445                                        LR 0.001298    Time 0.131798    
2024-02-17 14:00:37,281 - Epoch: [259][  300/  391]    Overall Loss 1.766779    Objective Loss 1.766779                                        LR 0.001298    Time 0.129832    
2024-02-17 14:00:49,164 - Epoch: [259][  391/  391]    Overall Loss 1.776035    Objective Loss 1.776035    Top1 55.769231    Top5 87.980769    LR 0.001298    Time 0.129995    
2024-02-17 14:00:49,292 - --- validate (epoch=259)-----------
2024-02-17 14:00:49,292 - 10000 samples (128 per mini-batch)
2024-02-17 14:00:54,444 - Epoch: [259][   79/   79]    Loss 2.253317    Top1 42.650000    Top5 73.320000    
2024-02-17 14:00:54,526 - ==> Top1: 42.650    Top5: 73.320    Loss: 2.253

2024-02-17 14:00:54,545 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:00:54,545 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:00:54,603 - 

2024-02-17 14:00:54,603 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:01:07,929 - Epoch: [260][  100/  391]    Overall Loss 1.769333    Objective Loss 1.769333                                        LR 0.001298    Time 0.133177    
2024-02-17 14:01:20,785 - Epoch: [260][  200/  391]    Overall Loss 1.769414    Objective Loss 1.769414                                        LR 0.001298    Time 0.130844    
2024-02-17 14:01:33,543 - Epoch: [260][  300/  391]    Overall Loss 1.784543    Objective Loss 1.784543                                        LR 0.001298    Time 0.129741    
2024-02-17 14:01:45,152 - Epoch: [260][  391/  391]    Overall Loss 1.798318    Objective Loss 1.798318    Top1 47.115385    Top5 76.923077    LR 0.001298    Time 0.129223    
2024-02-17 14:01:45,278 - --- validate (epoch=260)-----------
2024-02-17 14:01:45,279 - 10000 samples (128 per mini-batch)
2024-02-17 14:01:51,459 - Epoch: [260][   79/   79]    Loss 2.090772    Top1 45.180000    Top5 75.680000    
2024-02-17 14:01:51,568 - ==> Top1: 45.180    Top5: 75.680    Loss: 2.091

2024-02-17 14:01:51,585 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:01:51,585 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:01:51,650 - 

2024-02-17 14:01:51,650 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:02:04,953 - Epoch: [261][  100/  391]    Overall Loss 1.784072    Objective Loss 1.784072                                        LR 0.001298    Time 0.132945    
2024-02-17 14:02:17,456 - Epoch: [261][  200/  391]    Overall Loss 1.800490    Objective Loss 1.800490                                        LR 0.001298    Time 0.128962    
2024-02-17 14:02:29,860 - Epoch: [261][  300/  391]    Overall Loss 1.810461    Objective Loss 1.810461                                        LR 0.001298    Time 0.127307    
2024-02-17 14:02:41,536 - Epoch: [261][  391/  391]    Overall Loss 1.819351    Objective Loss 1.819351    Top1 45.673077    Top5 75.961538    LR 0.001298    Time 0.127527    
2024-02-17 14:02:41,690 - --- validate (epoch=261)-----------
2024-02-17 14:02:41,691 - 10000 samples (128 per mini-batch)
2024-02-17 14:02:47,196 - Epoch: [261][   79/   79]    Loss 2.041264    Top1 45.820000    Top5 77.170000    
2024-02-17 14:02:47,354 - ==> Top1: 45.820    Top5: 77.170    Loss: 2.041

2024-02-17 14:02:47,372 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:02:47,373 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:02:47,439 - 

2024-02-17 14:02:47,439 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:03:00,593 - Epoch: [262][  100/  391]    Overall Loss 1.809507    Objective Loss 1.809507                                        LR 0.001298    Time 0.131431    
2024-02-17 14:03:13,349 - Epoch: [262][  200/  391]    Overall Loss 1.800669    Objective Loss 1.800669                                        LR 0.001298    Time 0.129470    
2024-02-17 14:03:26,036 - Epoch: [262][  300/  391]    Overall Loss 1.792782    Objective Loss 1.792782                                        LR 0.001298    Time 0.128586    
2024-02-17 14:03:37,470 - Epoch: [262][  391/  391]    Overall Loss 1.794564    Objective Loss 1.794564    Top1 49.038462    Top5 83.653846    LR 0.001298    Time 0.127890    
2024-02-17 14:03:37,596 - --- validate (epoch=262)-----------
2024-02-17 14:03:37,597 - 10000 samples (128 per mini-batch)
2024-02-17 14:03:43,051 - Epoch: [262][   79/   79]    Loss 2.130475    Top1 44.690000    Top5 75.450000    
2024-02-17 14:03:43,170 - ==> Top1: 44.690    Top5: 75.450    Loss: 2.130

2024-02-17 14:03:43,189 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:03:43,189 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:03:43,252 - 

2024-02-17 14:03:43,253 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:03:56,361 - Epoch: [263][  100/  391]    Overall Loss 1.817447    Objective Loss 1.817447                                        LR 0.001298    Time 0.131010    
2024-02-17 14:04:09,141 - Epoch: [263][  200/  391]    Overall Loss 1.780798    Objective Loss 1.780798                                        LR 0.001298    Time 0.129380    
2024-02-17 14:04:21,940 - Epoch: [263][  300/  391]    Overall Loss 1.794648    Objective Loss 1.794648                                        LR 0.001298    Time 0.128899    
2024-02-17 14:04:33,451 - Epoch: [263][  391/  391]    Overall Loss 1.786479    Objective Loss 1.786479    Top1 53.846154    Top5 86.538462    LR 0.001298    Time 0.128327    
2024-02-17 14:04:33,577 - --- validate (epoch=263)-----------
2024-02-17 14:04:33,578 - 10000 samples (128 per mini-batch)
2024-02-17 14:04:39,314 - Epoch: [263][   79/   79]    Loss 1.941735    Top1 47.970000    Top5 78.860000    
2024-02-17 14:04:39,450 - ==> Top1: 47.970    Top5: 78.860    Loss: 1.942

2024-02-17 14:04:39,473 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:04:39,473 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:04:39,539 - 

2024-02-17 14:04:39,539 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:04:53,016 - Epoch: [264][  100/  391]    Overall Loss 1.808932    Objective Loss 1.808932                                        LR 0.001298    Time 0.134692    
2024-02-17 14:05:06,079 - Epoch: [264][  200/  391]    Overall Loss 1.796547    Objective Loss 1.796547                                        LR 0.001298    Time 0.132634    
2024-02-17 14:05:19,349 - Epoch: [264][  300/  391]    Overall Loss 1.788860    Objective Loss 1.788860                                        LR 0.001298    Time 0.132636    
2024-02-17 14:05:31,383 - Epoch: [264][  391/  391]    Overall Loss 1.786070    Objective Loss 1.786070    Top1 55.769231    Top5 81.730769    LR 0.001298    Time 0.132533    
2024-02-17 14:05:31,502 - --- validate (epoch=264)-----------
2024-02-17 14:05:31,502 - 10000 samples (128 per mini-batch)
2024-02-17 14:05:36,438 - Epoch: [264][   79/   79]    Loss 1.861456    Top1 49.710000    Top5 80.120000    
2024-02-17 14:05:36,540 - ==> Top1: 49.710    Top5: 80.120    Loss: 1.861

2024-02-17 14:05:36,558 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:05:36,559 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:05:36,627 - 

2024-02-17 14:05:36,627 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:05:49,897 - Epoch: [265][  100/  391]    Overall Loss 1.803262    Objective Loss 1.803262                                        LR 0.001298    Time 0.132621    
2024-02-17 14:06:02,170 - Epoch: [265][  200/  391]    Overall Loss 1.771924    Objective Loss 1.771924                                        LR 0.001298    Time 0.127652    
2024-02-17 14:06:14,685 - Epoch: [265][  300/  391]    Overall Loss 1.768784    Objective Loss 1.768784                                        LR 0.001298    Time 0.126799    
2024-02-17 14:06:25,798 - Epoch: [265][  391/  391]    Overall Loss 1.771608    Objective Loss 1.771608    Top1 48.076923    Top5 80.769231    LR 0.001298    Time 0.125699    
2024-02-17 14:06:25,906 - --- validate (epoch=265)-----------
2024-02-17 14:06:25,907 - 10000 samples (128 per mini-batch)
2024-02-17 14:06:31,688 - Epoch: [265][   79/   79]    Loss 2.013481    Top1 46.550000    Top5 77.470000    
2024-02-17 14:06:31,873 - ==> Top1: 46.550    Top5: 77.470    Loss: 2.013

2024-02-17 14:06:31,882 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:06:31,883 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:06:31,944 - 

2024-02-17 14:06:31,944 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:06:44,978 - Epoch: [266][  100/  391]    Overall Loss 1.769535    Objective Loss 1.769535                                        LR 0.001298    Time 0.130269    
2024-02-17 14:06:56,582 - Epoch: [266][  200/  391]    Overall Loss 1.751425    Objective Loss 1.751425                                        LR 0.001298    Time 0.123131    
2024-02-17 14:07:09,654 - Epoch: [266][  300/  391]    Overall Loss 1.759693    Objective Loss 1.759693                                        LR 0.001298    Time 0.125643    
2024-02-17 14:07:21,693 - Epoch: [266][  391/  391]    Overall Loss 1.773346    Objective Loss 1.773346    Top1 43.750000    Top5 74.519231    LR 0.001298    Time 0.127179    
2024-02-17 14:07:21,825 - --- validate (epoch=266)-----------
2024-02-17 14:07:21,825 - 10000 samples (128 per mini-batch)
2024-02-17 14:07:27,481 - Epoch: [266][   79/   79]    Loss 2.069817    Top1 45.610000    Top5 76.580000    
2024-02-17 14:07:27,590 - ==> Top1: 45.610    Top5: 76.580    Loss: 2.070

2024-02-17 14:07:27,610 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:07:27,610 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:07:27,676 - 

2024-02-17 14:07:27,676 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:07:40,892 - Epoch: [267][  100/  391]    Overall Loss 1.731451    Objective Loss 1.731451                                        LR 0.001298    Time 0.132076    
2024-02-17 14:07:53,635 - Epoch: [267][  200/  391]    Overall Loss 1.745001    Objective Loss 1.745001                                        LR 0.001298    Time 0.129729    
2024-02-17 14:08:06,826 - Epoch: [267][  300/  391]    Overall Loss 1.772524    Objective Loss 1.772524                                        LR 0.001298    Time 0.130438    
2024-02-17 14:08:18,437 - Epoch: [267][  391/  391]    Overall Loss 1.781634    Objective Loss 1.781634    Top1 44.711538    Top5 80.288462    LR 0.001298    Time 0.129763    
2024-02-17 14:08:18,537 - --- validate (epoch=267)-----------
2024-02-17 14:08:18,538 - 10000 samples (128 per mini-batch)
2024-02-17 14:08:23,209 - Epoch: [267][   79/   79]    Loss 1.874607    Top1 49.500000    Top5 79.880000    
2024-02-17 14:08:23,317 - ==> Top1: 49.500    Top5: 79.880    Loss: 1.875

2024-02-17 14:08:23,334 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:08:23,334 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:08:23,398 - 

2024-02-17 14:08:23,398 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:08:36,509 - Epoch: [268][  100/  391]    Overall Loss 1.791989    Objective Loss 1.791989                                        LR 0.001298    Time 0.131034    
2024-02-17 14:08:49,108 - Epoch: [268][  200/  391]    Overall Loss 1.780799    Objective Loss 1.780799                                        LR 0.001298    Time 0.128488    
2024-02-17 14:09:01,584 - Epoch: [268][  300/  391]    Overall Loss 1.790165    Objective Loss 1.790165                                        LR 0.001298    Time 0.127226    
2024-02-17 14:09:12,726 - Epoch: [268][  391/  391]    Overall Loss 1.791410    Objective Loss 1.791410    Top1 54.326923    Top5 84.134615    LR 0.001298    Time 0.126103    
2024-02-17 14:09:12,866 - --- validate (epoch=268)-----------
2024-02-17 14:09:12,867 - 10000 samples (128 per mini-batch)
2024-02-17 14:09:18,494 - Epoch: [268][   79/   79]    Loss 2.173104    Top1 43.790000    Top5 74.580000    
2024-02-17 14:09:18,617 - ==> Top1: 43.790    Top5: 74.580    Loss: 2.173

2024-02-17 14:09:18,639 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:09:18,640 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:09:18,705 - 

2024-02-17 14:09:18,706 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:09:32,061 - Epoch: [269][  100/  391]    Overall Loss 1.812807    Objective Loss 1.812807                                        LR 0.001298    Time 0.133478    
2024-02-17 14:09:44,895 - Epoch: [269][  200/  391]    Overall Loss 1.792274    Objective Loss 1.792274                                        LR 0.001298    Time 0.130882    
2024-02-17 14:09:57,689 - Epoch: [269][  300/  391]    Overall Loss 1.776514    Objective Loss 1.776514                                        LR 0.001298    Time 0.129886    
2024-02-17 14:10:09,390 - Epoch: [269][  391/  391]    Overall Loss 1.781018    Objective Loss 1.781018    Top1 48.557692    Top5 79.807692    LR 0.001298    Time 0.129569    
2024-02-17 14:10:09,587 - --- validate (epoch=269)-----------
2024-02-17 14:10:09,588 - 10000 samples (128 per mini-batch)
2024-02-17 14:10:15,567 - Epoch: [269][   79/   79]    Loss 2.061043    Top1 45.380000    Top5 76.030000    
2024-02-17 14:10:15,695 - ==> Top1: 45.380    Top5: 76.030    Loss: 2.061

2024-02-17 14:10:15,713 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:10:15,713 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:10:15,778 - 

2024-02-17 14:10:15,779 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:10:28,997 - Epoch: [270][  100/  391]    Overall Loss 1.698815    Objective Loss 1.698815                                        LR 0.001298    Time 0.132101    
2024-02-17 14:10:41,856 - Epoch: [270][  200/  391]    Overall Loss 1.721813    Objective Loss 1.721813                                        LR 0.001298    Time 0.130322    
2024-02-17 14:10:54,755 - Epoch: [270][  300/  391]    Overall Loss 1.733049    Objective Loss 1.733049                                        LR 0.001298    Time 0.129861    
2024-02-17 14:11:06,071 - Epoch: [270][  391/  391]    Overall Loss 1.753994    Objective Loss 1.753994    Top1 54.326923    Top5 81.250000    LR 0.001298    Time 0.128567    
2024-02-17 14:11:06,193 - --- validate (epoch=270)-----------
2024-02-17 14:11:06,193 - 10000 samples (128 per mini-batch)
2024-02-17 14:11:11,479 - Epoch: [270][   79/   79]    Loss 1.975287    Top1 46.970000    Top5 78.080000    
2024-02-17 14:11:11,632 - ==> Top1: 46.970    Top5: 78.080    Loss: 1.975

2024-02-17 14:11:11,858 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:11:11,858 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:11:11,917 - 

2024-02-17 14:11:11,918 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:11:25,124 - Epoch: [271][  100/  391]    Overall Loss 1.833563    Objective Loss 1.833563                                        LR 0.001298    Time 0.131984    
2024-02-17 14:11:37,370 - Epoch: [271][  200/  391]    Overall Loss 1.782775    Objective Loss 1.782775                                        LR 0.001298    Time 0.127200    
2024-02-17 14:11:48,780 - Epoch: [271][  300/  391]    Overall Loss 1.783318    Objective Loss 1.783318                                        LR 0.001298    Time 0.122818    
2024-02-17 14:11:59,812 - Epoch: [271][  391/  391]    Overall Loss 1.798652    Objective Loss 1.798652    Top1 51.442308    Top5 83.173077    LR 0.001298    Time 0.122439    
2024-02-17 14:11:59,930 - --- validate (epoch=271)-----------
2024-02-17 14:11:59,931 - 10000 samples (128 per mini-batch)
2024-02-17 14:12:04,888 - Epoch: [271][   79/   79]    Loss 1.901624    Top1 48.790000    Top5 79.000000    
2024-02-17 14:12:05,058 - ==> Top1: 48.790    Top5: 79.000    Loss: 1.902

2024-02-17 14:12:05,075 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:12:05,076 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:12:05,139 - 

2024-02-17 14:12:05,139 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:12:17,401 - Epoch: [272][  100/  391]    Overall Loss 1.796028    Objective Loss 1.796028                                        LR 0.001298    Time 0.122543    
2024-02-17 14:12:29,677 - Epoch: [272][  200/  391]    Overall Loss 1.780047    Objective Loss 1.780047                                        LR 0.001298    Time 0.122627    
2024-02-17 14:12:41,856 - Epoch: [272][  300/  391]    Overall Loss 1.785113    Objective Loss 1.785113                                        LR 0.001298    Time 0.122336    
2024-02-17 14:12:53,016 - Epoch: [272][  391/  391]    Overall Loss 1.786599    Objective Loss 1.786599    Top1 52.884615    Top5 80.769231    LR 0.001298    Time 0.122396    
2024-02-17 14:12:53,152 - --- validate (epoch=272)-----------
2024-02-17 14:12:53,152 - 10000 samples (128 per mini-batch)
2024-02-17 14:12:59,006 - Epoch: [272][   79/   79]    Loss 2.035797    Top1 45.560000    Top5 76.780000    
2024-02-17 14:12:59,101 - ==> Top1: 45.560    Top5: 76.780    Loss: 2.036

2024-02-17 14:12:59,118 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:12:59,118 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:12:59,183 - 

2024-02-17 14:12:59,183 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:13:12,452 - Epoch: [273][  100/  391]    Overall Loss 1.774206    Objective Loss 1.774206                                        LR 0.001298    Time 0.132610    
2024-02-17 14:13:25,366 - Epoch: [273][  200/  391]    Overall Loss 1.775410    Objective Loss 1.775410                                        LR 0.001298    Time 0.130849    
2024-02-17 14:13:38,045 - Epoch: [273][  300/  391]    Overall Loss 1.783291    Objective Loss 1.783291                                        LR 0.001298    Time 0.129479    
2024-02-17 14:13:49,654 - Epoch: [273][  391/  391]    Overall Loss 1.797482    Objective Loss 1.797482    Top1 53.846154    Top5 84.615385    LR 0.001298    Time 0.129026    
2024-02-17 14:13:49,779 - --- validate (epoch=273)-----------
2024-02-17 14:13:49,780 - 10000 samples (128 per mini-batch)
2024-02-17 14:13:55,326 - Epoch: [273][   79/   79]    Loss 1.911830    Top1 48.200000    Top5 79.260000    
2024-02-17 14:13:55,478 - ==> Top1: 48.200    Top5: 79.260    Loss: 1.912

2024-02-17 14:13:55,494 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:13:55,495 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:13:55,572 - 

2024-02-17 14:13:55,572 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:14:09,117 - Epoch: [274][  100/  391]    Overall Loss 1.795630    Objective Loss 1.795630                                        LR 0.001298    Time 0.135364    
2024-02-17 14:14:21,661 - Epoch: [274][  200/  391]    Overall Loss 1.780051    Objective Loss 1.780051                                        LR 0.001298    Time 0.130380    
2024-02-17 14:14:34,444 - Epoch: [274][  300/  391]    Overall Loss 1.787913    Objective Loss 1.787913                                        LR 0.001298    Time 0.129512    
2024-02-17 14:14:44,667 - Epoch: [274][  391/  391]    Overall Loss 1.783056    Objective Loss 1.783056    Top1 53.365385    Top5 83.653846    LR 0.001298    Time 0.125507    
2024-02-17 14:14:44,766 - --- validate (epoch=274)-----------
2024-02-17 14:14:44,767 - 10000 samples (128 per mini-batch)
2024-02-17 14:14:49,572 - Epoch: [274][   79/   79]    Loss 1.894186    Top1 49.140000    Top5 79.880000    
2024-02-17 14:14:49,667 - ==> Top1: 49.140    Top5: 79.880    Loss: 1.894

2024-02-17 14:14:49,683 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:14:49,684 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:14:49,734 - 

2024-02-17 14:14:49,734 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:15:02,459 - Epoch: [275][  100/  391]    Overall Loss 1.765456    Objective Loss 1.765456                                        LR 0.001298    Time 0.127181    
2024-02-17 14:15:15,174 - Epoch: [275][  200/  391]    Overall Loss 1.761915    Objective Loss 1.761915                                        LR 0.001298    Time 0.127144    
2024-02-17 14:15:27,936 - Epoch: [275][  300/  391]    Overall Loss 1.765049    Objective Loss 1.765049                                        LR 0.001298    Time 0.127284    
2024-02-17 14:15:39,529 - Epoch: [275][  391/  391]    Overall Loss 1.768770    Objective Loss 1.768770    Top1 52.884615    Top5 80.769231    LR 0.001298    Time 0.127300    
2024-02-17 14:15:39,670 - --- validate (epoch=275)-----------
2024-02-17 14:15:39,671 - 10000 samples (128 per mini-batch)
2024-02-17 14:15:45,453 - Epoch: [275][   79/   79]    Loss 1.998276    Top1 47.730000    Top5 77.730000    
2024-02-17 14:15:45,577 - ==> Top1: 47.730    Top5: 77.730    Loss: 1.998

2024-02-17 14:15:45,592 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:15:45,592 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:15:45,653 - 

2024-02-17 14:15:45,653 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:15:59,127 - Epoch: [276][  100/  391]    Overall Loss 1.763283    Objective Loss 1.763283                                        LR 0.001298    Time 0.134662    
2024-02-17 14:16:11,774 - Epoch: [276][  200/  391]    Overall Loss 1.747559    Objective Loss 1.747559                                        LR 0.001298    Time 0.130542    
2024-02-17 14:16:24,480 - Epoch: [276][  300/  391]    Overall Loss 1.754682    Objective Loss 1.754682                                        LR 0.001298    Time 0.129363    
2024-02-17 14:16:35,931 - Epoch: [276][  391/  391]    Overall Loss 1.752851    Objective Loss 1.752851    Top1 48.076923    Top5 84.615385    LR 0.001298    Time 0.128531    
2024-02-17 14:16:36,056 - --- validate (epoch=276)-----------
2024-02-17 14:16:36,056 - 10000 samples (128 per mini-batch)
2024-02-17 14:16:41,520 - Epoch: [276][   79/   79]    Loss 2.170502    Top1 42.870000    Top5 74.570000    
2024-02-17 14:16:41,639 - ==> Top1: 42.870    Top5: 74.570    Loss: 2.171

2024-02-17 14:16:41,657 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:16:41,657 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:16:41,724 - 

2024-02-17 14:16:41,724 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:16:54,921 - Epoch: [277][  100/  391]    Overall Loss 1.753479    Objective Loss 1.753479                                        LR 0.001298    Time 0.131885    
2024-02-17 14:17:07,920 - Epoch: [277][  200/  391]    Overall Loss 1.750853    Objective Loss 1.750853                                        LR 0.001298    Time 0.130911    
2024-02-17 14:17:20,511 - Epoch: [277][  300/  391]    Overall Loss 1.750302    Objective Loss 1.750302                                        LR 0.001298    Time 0.129230    
2024-02-17 14:17:31,334 - Epoch: [277][  391/  391]    Overall Loss 1.750021    Objective Loss 1.750021    Top1 58.173077    Top5 85.096154    LR 0.001298    Time 0.126823    
2024-02-17 14:17:31,461 - --- validate (epoch=277)-----------
2024-02-17 14:17:31,462 - 10000 samples (128 per mini-batch)
2024-02-17 14:17:36,377 - Epoch: [277][   79/   79]    Loss 2.124669    Top1 44.400000    Top5 75.510000    
2024-02-17 14:17:36,462 - ==> Top1: 44.400    Top5: 75.510    Loss: 2.125

2024-02-17 14:17:36,480 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:17:36,480 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:17:36,539 - 

2024-02-17 14:17:36,539 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:17:49,737 - Epoch: [278][  100/  391]    Overall Loss 1.787479    Objective Loss 1.787479                                        LR 0.001298    Time 0.131903    
2024-02-17 14:18:02,300 - Epoch: [278][  200/  391]    Overall Loss 1.791103    Objective Loss 1.791103                                        LR 0.001298    Time 0.128741    
2024-02-17 14:18:14,474 - Epoch: [278][  300/  391]    Overall Loss 1.779763    Objective Loss 1.779763                                        LR 0.001298    Time 0.126392    
2024-02-17 14:18:25,590 - Epoch: [278][  391/  391]    Overall Loss 1.786986    Objective Loss 1.786986    Top1 54.326923    Top5 83.653846    LR 0.001298    Time 0.125393    
2024-02-17 14:18:25,760 - --- validate (epoch=278)-----------
2024-02-17 14:18:25,761 - 10000 samples (128 per mini-batch)
2024-02-17 14:18:31,206 - Epoch: [278][   79/   79]    Loss 2.034911    Top1 45.980000    Top5 77.520000    
2024-02-17 14:18:31,320 - ==> Top1: 45.980    Top5: 77.520    Loss: 2.035

2024-02-17 14:18:31,340 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:18:31,341 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:18:31,406 - 

2024-02-17 14:18:31,407 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:18:44,720 - Epoch: [279][  100/  391]    Overall Loss 1.791695    Objective Loss 1.791695                                        LR 0.001298    Time 0.133056    
2024-02-17 14:18:56,822 - Epoch: [279][  200/  391]    Overall Loss 1.780038    Objective Loss 1.780038                                        LR 0.001298    Time 0.127014    
2024-02-17 14:19:09,467 - Epoch: [279][  300/  391]    Overall Loss 1.795272    Objective Loss 1.795272                                        LR 0.001298    Time 0.126809    
2024-02-17 14:19:20,046 - Epoch: [279][  391/  391]    Overall Loss 1.796595    Objective Loss 1.796595    Top1 51.442308    Top5 85.096154    LR 0.001298    Time 0.124342    
2024-02-17 14:19:20,196 - --- validate (epoch=279)-----------
2024-02-17 14:19:20,196 - 10000 samples (128 per mini-batch)
2024-02-17 14:19:26,225 - Epoch: [279][   79/   79]    Loss 2.018737    Top1 46.460000    Top5 77.810000    
2024-02-17 14:19:26,386 - ==> Top1: 46.460    Top5: 77.810    Loss: 2.019

2024-02-17 14:19:26,399 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:19:26,399 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:19:26,477 - 

2024-02-17 14:19:26,477 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:19:40,058 - Epoch: [280][  100/  391]    Overall Loss 1.737022    Objective Loss 1.737022                                        LR 0.001298    Time 0.135702    
2024-02-17 14:19:52,733 - Epoch: [280][  200/  391]    Overall Loss 1.760684    Objective Loss 1.760684                                        LR 0.001298    Time 0.131203    
2024-02-17 14:20:05,442 - Epoch: [280][  300/  391]    Overall Loss 1.776039    Objective Loss 1.776039                                        LR 0.001298    Time 0.129814    
2024-02-17 14:20:16,426 - Epoch: [280][  391/  391]    Overall Loss 1.787303    Objective Loss 1.787303    Top1 51.923077    Top5 81.250000    LR 0.001298    Time 0.127685    
2024-02-17 14:20:16,538 - --- validate (epoch=280)-----------
2024-02-17 14:20:16,539 - 10000 samples (128 per mini-batch)
2024-02-17 14:20:22,551 - Epoch: [280][   79/   79]    Loss 2.043643    Top1 45.380000    Top5 76.790000    
2024-02-17 14:20:22,675 - ==> Top1: 45.380    Top5: 76.790    Loss: 2.044

2024-02-17 14:20:22,692 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:20:22,693 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:20:22,767 - 

2024-02-17 14:20:22,768 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:20:35,841 - Epoch: [281][  100/  391]    Overall Loss 1.803064    Objective Loss 1.803064                                        LR 0.001298    Time 0.130656    
2024-02-17 14:20:48,534 - Epoch: [281][  200/  391]    Overall Loss 1.800840    Objective Loss 1.800840                                        LR 0.001298    Time 0.128769    
2024-02-17 14:21:01,396 - Epoch: [281][  300/  391]    Overall Loss 1.801918    Objective Loss 1.801918                                        LR 0.001298    Time 0.128705    
2024-02-17 14:21:12,766 - Epoch: [281][  391/  391]    Overall Loss 1.807556    Objective Loss 1.807556    Top1 45.192308    Top5 73.557692    LR 0.001298    Time 0.127817    
2024-02-17 14:21:12,889 - --- validate (epoch=281)-----------
2024-02-17 14:21:12,890 - 10000 samples (128 per mini-batch)
2024-02-17 14:21:19,026 - Epoch: [281][   79/   79]    Loss 2.036015    Top1 45.720000    Top5 77.290000    
2024-02-17 14:21:19,162 - ==> Top1: 45.720    Top5: 77.290    Loss: 2.036

2024-02-17 14:21:19,180 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:21:19,180 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:21:19,241 - 

2024-02-17 14:21:19,242 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:21:32,440 - Epoch: [282][  100/  391]    Overall Loss 1.820085    Objective Loss 1.820085                                        LR 0.001298    Time 0.131905    
2024-02-17 14:21:44,730 - Epoch: [282][  200/  391]    Overall Loss 1.813911    Objective Loss 1.813911                                        LR 0.001298    Time 0.127380    
2024-02-17 14:21:57,441 - Epoch: [282][  300/  391]    Overall Loss 1.808799    Objective Loss 1.808799                                        LR 0.001298    Time 0.127272    
2024-02-17 14:22:09,260 - Epoch: [282][  391/  391]    Overall Loss 1.815734    Objective Loss 1.815734    Top1 52.403846    Top5 84.134615    LR 0.001298    Time 0.127867    
2024-02-17 14:22:09,387 - --- validate (epoch=282)-----------
2024-02-17 14:22:09,388 - 10000 samples (128 per mini-batch)
2024-02-17 14:22:15,203 - Epoch: [282][   79/   79]    Loss 2.026560    Top1 46.220000    Top5 77.050000    
2024-02-17 14:22:15,307 - ==> Top1: 46.220    Top5: 77.050    Loss: 2.027

2024-02-17 14:22:15,329 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:22:15,329 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:22:15,389 - 

2024-02-17 14:22:15,389 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:22:28,921 - Epoch: [283][  100/  391]    Overall Loss 1.743103    Objective Loss 1.743103                                        LR 0.001298    Time 0.135205    
2024-02-17 14:22:40,601 - Epoch: [283][  200/  391]    Overall Loss 1.731917    Objective Loss 1.731917                                        LR 0.001298    Time 0.125982    
2024-02-17 14:22:52,975 - Epoch: [283][  300/  391]    Overall Loss 1.749759    Objective Loss 1.749759                                        LR 0.001298    Time 0.125217    
2024-02-17 14:23:04,522 - Epoch: [283][  391/  391]    Overall Loss 1.754050    Objective Loss 1.754050    Top1 51.923077    Top5 82.211538    LR 0.001298    Time 0.125595    
2024-02-17 14:23:04,700 - --- validate (epoch=283)-----------
2024-02-17 14:23:04,700 - 10000 samples (128 per mini-batch)
2024-02-17 14:23:10,534 - Epoch: [283][   79/   79]    Loss 1.960913    Top1 47.450000    Top5 78.250000    
2024-02-17 14:23:10,652 - ==> Top1: 47.450    Top5: 78.250    Loss: 1.961

2024-02-17 14:23:10,670 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:23:10,671 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:23:10,735 - 

2024-02-17 14:23:10,735 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:23:24,358 - Epoch: [284][  100/  391]    Overall Loss 1.786591    Objective Loss 1.786591                                        LR 0.001298    Time 0.136150    
2024-02-17 14:23:37,103 - Epoch: [284][  200/  391]    Overall Loss 1.788487    Objective Loss 1.788487                                        LR 0.001298    Time 0.131772    
2024-02-17 14:23:50,019 - Epoch: [284][  300/  391]    Overall Loss 1.782368    Objective Loss 1.782368                                        LR 0.001298    Time 0.130885    
2024-02-17 14:24:01,348 - Epoch: [284][  391/  391]    Overall Loss 1.779613    Objective Loss 1.779613    Top1 52.884615    Top5 82.692308    LR 0.001298    Time 0.129386    
2024-02-17 14:24:01,473 - --- validate (epoch=284)-----------
2024-02-17 14:24:01,475 - 10000 samples (128 per mini-batch)
2024-02-17 14:24:06,761 - Epoch: [284][   79/   79]    Loss 1.901988    Top1 49.330000    Top5 79.750000    
2024-02-17 14:24:06,858 - ==> Top1: 49.330    Top5: 79.750    Loss: 1.902

2024-02-17 14:24:06,874 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:24:06,874 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:24:06,930 - 

2024-02-17 14:24:06,930 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:24:18,799 - Epoch: [285][  100/  391]    Overall Loss 1.793768    Objective Loss 1.793768                                        LR 0.001298    Time 0.118618    
2024-02-17 14:24:31,278 - Epoch: [285][  200/  391]    Overall Loss 1.779337    Objective Loss 1.779337                                        LR 0.001298    Time 0.121680    
2024-02-17 14:24:42,949 - Epoch: [285][  300/  391]    Overall Loss 1.775946    Objective Loss 1.775946                                        LR 0.001298    Time 0.120012    
2024-02-17 14:24:53,419 - Epoch: [285][  391/  391]    Overall Loss 1.778741    Objective Loss 1.778741    Top1 49.038462    Top5 76.923077    LR 0.001298    Time 0.118849    
2024-02-17 14:24:53,610 - --- validate (epoch=285)-----------
2024-02-17 14:24:53,611 - 10000 samples (128 per mini-batch)
2024-02-17 14:24:59,021 - Epoch: [285][   79/   79]    Loss 2.232683    Top1 41.760000    Top5 73.890000    
2024-02-17 14:24:59,120 - ==> Top1: 41.760    Top5: 73.890    Loss: 2.233

2024-02-17 14:24:59,130 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:24:59,131 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:24:59,193 - 

2024-02-17 14:24:59,193 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:25:12,877 - Epoch: [286][  100/  391]    Overall Loss 1.782560    Objective Loss 1.782560                                        LR 0.001298    Time 0.136767    
2024-02-17 14:25:25,641 - Epoch: [286][  200/  391]    Overall Loss 1.804216    Objective Loss 1.804216                                        LR 0.001298    Time 0.132174    
2024-02-17 14:25:38,215 - Epoch: [286][  300/  391]    Overall Loss 1.808461    Objective Loss 1.808461                                        LR 0.001298    Time 0.130015    
2024-02-17 14:25:49,757 - Epoch: [286][  391/  391]    Overall Loss 1.808025    Objective Loss 1.808025    Top1 50.480769    Top5 80.769231    LR 0.001298    Time 0.129263    
2024-02-17 14:25:49,884 - --- validate (epoch=286)-----------
2024-02-17 14:25:49,885 - 10000 samples (128 per mini-batch)
2024-02-17 14:25:55,458 - Epoch: [286][   79/   79]    Loss 1.897881    Top1 48.480000    Top5 79.100000    
2024-02-17 14:25:55,578 - ==> Top1: 48.480    Top5: 79.100    Loss: 1.898

2024-02-17 14:25:55,596 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:25:55,597 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:25:55,660 - 

2024-02-17 14:25:55,661 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:26:08,768 - Epoch: [287][  100/  391]    Overall Loss 1.735795    Objective Loss 1.735795                                        LR 0.001298    Time 0.130996    
2024-02-17 14:26:21,490 - Epoch: [287][  200/  391]    Overall Loss 1.745443    Objective Loss 1.745443                                        LR 0.001298    Time 0.129082    
2024-02-17 14:26:32,829 - Epoch: [287][  300/  391]    Overall Loss 1.746243    Objective Loss 1.746243                                        LR 0.001298    Time 0.123839    
2024-02-17 14:26:43,408 - Epoch: [287][  391/  391]    Overall Loss 1.744795    Objective Loss 1.744795    Top1 51.442308    Top5 86.538462    LR 0.001298    Time 0.122063    
2024-02-17 14:26:43,599 - --- validate (epoch=287)-----------
2024-02-17 14:26:43,599 - 10000 samples (128 per mini-batch)
2024-02-17 14:26:49,277 - Epoch: [287][   79/   79]    Loss 1.842814    Top1 49.680000    Top5 80.350000    
2024-02-17 14:26:49,390 - ==> Top1: 49.680    Top5: 80.350    Loss: 1.843

2024-02-17 14:26:49,407 - ==> Best [Top1: 49.810   Top5: 79.530   Sparsity:0.00   Params: 1341960 on epoch: 217]
2024-02-17 14:26:49,407 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:26:49,472 - 

2024-02-17 14:26:49,472 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:27:03,028 - Epoch: [288][  100/  391]    Overall Loss 1.739420    Objective Loss 1.739420                                        LR 0.001298    Time 0.135473    
2024-02-17 14:27:15,400 - Epoch: [288][  200/  391]    Overall Loss 1.747757    Objective Loss 1.747757                                        LR 0.001298    Time 0.129573    
2024-02-17 14:27:27,017 - Epoch: [288][  300/  391]    Overall Loss 1.753548    Objective Loss 1.753548                                        LR 0.001298    Time 0.125092    
2024-02-17 14:27:38,868 - Epoch: [288][  391/  391]    Overall Loss 1.757442    Objective Loss 1.757442    Top1 58.653846    Top5 86.057692    LR 0.001298    Time 0.126275    
2024-02-17 14:27:38,987 - --- validate (epoch=288)-----------
2024-02-17 14:27:38,988 - 10000 samples (128 per mini-batch)
2024-02-17 14:27:43,279 - Epoch: [288][   79/   79]    Loss 1.853578    Top1 50.080000    Top5 80.230000    
2024-02-17 14:27:43,376 - ==> Top1: 50.080    Top5: 80.230    Loss: 1.854

2024-02-17 14:27:43,393 - ==> Best [Top1: 50.080   Top5: 80.230   Sparsity:0.00   Params: 1341960 on epoch: 288]
2024-02-17 14:27:43,393 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:27:43,619 - 

2024-02-17 14:27:43,620 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:27:56,340 - Epoch: [289][  100/  391]    Overall Loss 1.735634    Objective Loss 1.735634                                        LR 0.001298    Time 0.127134    
2024-02-17 14:28:09,527 - Epoch: [289][  200/  391]    Overall Loss 1.751627    Objective Loss 1.751627                                        LR 0.001298    Time 0.129473    
2024-02-17 14:28:22,297 - Epoch: [289][  300/  391]    Overall Loss 1.751649    Objective Loss 1.751649                                        LR 0.001298    Time 0.128864    
2024-02-17 14:28:33,906 - Epoch: [289][  391/  391]    Overall Loss 1.761798    Objective Loss 1.761798    Top1 53.846154    Top5 84.615385    LR 0.001298    Time 0.128552    
2024-02-17 14:28:34,030 - --- validate (epoch=289)-----------
2024-02-17 14:28:34,031 - 10000 samples (128 per mini-batch)
2024-02-17 14:28:39,465 - Epoch: [289][   79/   79]    Loss 2.024247    Top1 46.700000    Top5 76.590000    
2024-02-17 14:28:39,609 - ==> Top1: 46.700    Top5: 76.590    Loss: 2.024

2024-02-17 14:28:39,628 - ==> Best [Top1: 50.080   Top5: 80.230   Sparsity:0.00   Params: 1341960 on epoch: 288]
2024-02-17 14:28:39,629 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:28:39,693 - 

2024-02-17 14:28:39,693 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:28:52,911 - Epoch: [290][  100/  391]    Overall Loss 1.717668    Objective Loss 1.717668                                        LR 0.001298    Time 0.132102    
2024-02-17 14:29:04,898 - Epoch: [290][  200/  391]    Overall Loss 1.731879    Objective Loss 1.731879                                        LR 0.001298    Time 0.125963    
2024-02-17 14:29:17,549 - Epoch: [290][  300/  391]    Overall Loss 1.752635    Objective Loss 1.752635                                        LR 0.001298    Time 0.126131    
2024-02-17 14:29:27,504 - Epoch: [290][  391/  391]    Overall Loss 1.758782    Objective Loss 1.758782    Top1 50.480769    Top5 86.057692    LR 0.001298    Time 0.122226    
2024-02-17 14:29:27,638 - --- validate (epoch=290)-----------
2024-02-17 14:29:27,639 - 10000 samples (128 per mini-batch)
2024-02-17 14:29:33,990 - Epoch: [290][   79/   79]    Loss 2.341596    Top1 40.600000    Top5 72.000000    
2024-02-17 14:29:34,119 - ==> Top1: 40.600    Top5: 72.000    Loss: 2.342

2024-02-17 14:29:34,136 - ==> Best [Top1: 50.080   Top5: 80.230   Sparsity:0.00   Params: 1341960 on epoch: 288]
2024-02-17 14:29:34,137 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:29:34,202 - 

2024-02-17 14:29:34,202 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:29:47,347 - Epoch: [291][  100/  391]    Overall Loss 1.791422    Objective Loss 1.791422                                        LR 0.001298    Time 0.131369    
2024-02-17 14:29:59,836 - Epoch: [291][  200/  391]    Overall Loss 1.779714    Objective Loss 1.779714                                        LR 0.001298    Time 0.128103    
2024-02-17 14:30:12,482 - Epoch: [291][  300/  391]    Overall Loss 1.773487    Objective Loss 1.773487                                        LR 0.001298    Time 0.127540    
2024-02-17 14:30:24,004 - Epoch: [291][  391/  391]    Overall Loss 1.774862    Objective Loss 1.774862    Top1 53.365385    Top5 83.653846    LR 0.001298    Time 0.127312    
2024-02-17 14:30:24,130 - --- validate (epoch=291)-----------
2024-02-17 14:30:24,131 - 10000 samples (128 per mini-batch)
2024-02-17 14:30:29,959 - Epoch: [291][   79/   79]    Loss 2.038417    Top1 46.330000    Top5 76.680000    
2024-02-17 14:30:30,129 - ==> Top1: 46.330    Top5: 76.680    Loss: 2.038

2024-02-17 14:30:30,147 - ==> Best [Top1: 50.080   Top5: 80.230   Sparsity:0.00   Params: 1341960 on epoch: 288]
2024-02-17 14:30:30,148 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:30:30,215 - 

2024-02-17 14:30:30,215 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:30:43,595 - Epoch: [292][  100/  391]    Overall Loss 1.727796    Objective Loss 1.727796                                        LR 0.001298    Time 0.133716    
2024-02-17 14:30:56,210 - Epoch: [292][  200/  391]    Overall Loss 1.769957    Objective Loss 1.769957                                        LR 0.001298    Time 0.129909    
2024-02-17 14:31:08,694 - Epoch: [292][  300/  391]    Overall Loss 1.767446    Objective Loss 1.767446                                        LR 0.001298    Time 0.128205    
2024-02-17 14:31:20,600 - Epoch: [292][  391/  391]    Overall Loss 1.768316    Objective Loss 1.768316    Top1 54.807692    Top5 87.019231    LR 0.001298    Time 0.128803    
2024-02-17 14:31:20,790 - --- validate (epoch=292)-----------
2024-02-17 14:31:20,791 - 10000 samples (128 per mini-batch)
2024-02-17 14:31:27,241 - Epoch: [292][   79/   79]    Loss 1.959229    Top1 48.450000    Top5 78.810000    
2024-02-17 14:31:27,340 - ==> Top1: 48.450    Top5: 78.810    Loss: 1.959

2024-02-17 14:31:27,348 - ==> Best [Top1: 50.080   Top5: 80.230   Sparsity:0.00   Params: 1341960 on epoch: 288]
2024-02-17 14:31:27,348 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:31:27,405 - 

2024-02-17 14:31:27,406 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:31:40,880 - Epoch: [293][  100/  391]    Overall Loss 1.671476    Objective Loss 1.671476                                        LR 0.001298    Time 0.134669    
2024-02-17 14:31:53,785 - Epoch: [293][  200/  391]    Overall Loss 1.685227    Objective Loss 1.685227                                        LR 0.001298    Time 0.131831    
2024-02-17 14:32:06,571 - Epoch: [293][  300/  391]    Overall Loss 1.705830    Objective Loss 1.705830                                        LR 0.001298    Time 0.130492    
2024-02-17 14:32:18,128 - Epoch: [293][  391/  391]    Overall Loss 1.718528    Objective Loss 1.718528    Top1 59.134615    Top5 86.538462    LR 0.001298    Time 0.129665    
2024-02-17 14:32:18,224 - --- validate (epoch=293)-----------
2024-02-17 14:32:18,224 - 10000 samples (128 per mini-batch)
2024-02-17 14:32:23,694 - Epoch: [293][   79/   79]    Loss 2.013611    Top1 46.510000    Top5 77.380000    
2024-02-17 14:32:23,809 - ==> Top1: 46.510    Top5: 77.380    Loss: 2.014

2024-02-17 14:32:23,819 - ==> Best [Top1: 50.080   Top5: 80.230   Sparsity:0.00   Params: 1341960 on epoch: 288]
2024-02-17 14:32:23,819 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:32:23,879 - 

2024-02-17 14:32:23,879 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:32:37,471 - Epoch: [294][  100/  391]    Overall Loss 1.773496    Objective Loss 1.773496                                        LR 0.001298    Time 0.135847    
2024-02-17 14:32:49,788 - Epoch: [294][  200/  391]    Overall Loss 1.789452    Objective Loss 1.789452                                        LR 0.001298    Time 0.129483    
2024-02-17 14:33:02,626 - Epoch: [294][  300/  391]    Overall Loss 1.781229    Objective Loss 1.781229                                        LR 0.001298    Time 0.129097    
2024-02-17 14:33:14,147 - Epoch: [294][  391/  391]    Overall Loss 1.766433    Objective Loss 1.766433    Top1 45.192308    Top5 81.250000    LR 0.001298    Time 0.128505    
2024-02-17 14:33:14,328 - --- validate (epoch=294)-----------
2024-02-17 14:33:14,329 - 10000 samples (128 per mini-batch)
2024-02-17 14:33:20,249 - Epoch: [294][   79/   79]    Loss 1.842248    Top1 50.560000    Top5 80.080000    
2024-02-17 14:33:20,358 - ==> Top1: 50.560    Top5: 80.080    Loss: 1.842

2024-02-17 14:33:20,380 - ==> Best [Top1: 50.560   Top5: 80.080   Sparsity:0.00   Params: 1341960 on epoch: 294]
2024-02-17 14:33:20,381 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:33:20,463 - 

2024-02-17 14:33:20,464 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:33:33,720 - Epoch: [295][  100/  391]    Overall Loss 1.832497    Objective Loss 1.832497                                        LR 0.001298    Time 0.132489    
2024-02-17 14:33:45,740 - Epoch: [295][  200/  391]    Overall Loss 1.829861    Objective Loss 1.829861                                        LR 0.001298    Time 0.126319    
2024-02-17 14:33:58,599 - Epoch: [295][  300/  391]    Overall Loss 1.812309    Objective Loss 1.812309                                        LR 0.001298    Time 0.127060    
2024-02-17 14:34:10,219 - Epoch: [295][  391/  391]    Overall Loss 1.802505    Objective Loss 1.802505    Top1 45.192308    Top5 80.769231    LR 0.001298    Time 0.127196    
2024-02-17 14:34:10,363 - --- validate (epoch=295)-----------
2024-02-17 14:34:10,364 - 10000 samples (128 per mini-batch)
2024-02-17 14:34:16,311 - Epoch: [295][   79/   79]    Loss 2.028981    Top1 46.320000    Top5 76.720000    
2024-02-17 14:34:16,475 - ==> Top1: 46.320    Top5: 76.720    Loss: 2.029

2024-02-17 14:34:16,493 - ==> Best [Top1: 50.560   Top5: 80.080   Sparsity:0.00   Params: 1341960 on epoch: 294]
2024-02-17 14:34:16,493 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:34:16,557 - 

2024-02-17 14:34:16,557 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:34:29,975 - Epoch: [296][  100/  391]    Overall Loss 1.796941    Objective Loss 1.796941                                        LR 0.001298    Time 0.134105    
2024-02-17 14:34:42,390 - Epoch: [296][  200/  391]    Overall Loss 1.800626    Objective Loss 1.800626                                        LR 0.001298    Time 0.129103    
2024-02-17 14:34:54,859 - Epoch: [296][  300/  391]    Overall Loss 1.781834    Objective Loss 1.781834                                        LR 0.001298    Time 0.127615    
2024-02-17 14:35:06,052 - Epoch: [296][  391/  391]    Overall Loss 1.765589    Objective Loss 1.765589    Top1 61.538462    Top5 87.980769    LR 0.001298    Time 0.126531    
2024-02-17 14:35:06,145 - --- validate (epoch=296)-----------
2024-02-17 14:35:06,146 - 10000 samples (128 per mini-batch)
2024-02-17 14:35:11,036 - Epoch: [296][   79/   79]    Loss 1.836491    Top1 50.440000    Top5 80.650000    
2024-02-17 14:35:11,126 - ==> Top1: 50.440    Top5: 80.650    Loss: 1.836

2024-02-17 14:35:11,142 - ==> Best [Top1: 50.560   Top5: 80.080   Sparsity:0.00   Params: 1341960 on epoch: 294]
2024-02-17 14:35:11,143 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:35:11,193 - 

2024-02-17 14:35:11,193 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:35:23,257 - Epoch: [297][  100/  391]    Overall Loss 1.756756    Objective Loss 1.756756                                        LR 0.001298    Time 0.120582    
2024-02-17 14:35:35,948 - Epoch: [297][  200/  391]    Overall Loss 1.757809    Objective Loss 1.757809                                        LR 0.001298    Time 0.123721    
2024-02-17 14:35:47,287 - Epoch: [297][  300/  391]    Overall Loss 1.747342    Objective Loss 1.747342                                        LR 0.001298    Time 0.120261    
2024-02-17 14:35:57,514 - Epoch: [297][  391/  391]    Overall Loss 1.739919    Objective Loss 1.739919    Top1 54.326923    Top5 86.538462    LR 0.001298    Time 0.118419    
2024-02-17 14:35:57,644 - --- validate (epoch=297)-----------
2024-02-17 14:35:57,645 - 10000 samples (128 per mini-batch)
2024-02-17 14:36:03,170 - Epoch: [297][   79/   79]    Loss 2.014812    Top1 46.860000    Top5 77.460000    
2024-02-17 14:36:03,275 - ==> Top1: 46.860    Top5: 77.460    Loss: 2.015

2024-02-17 14:36:03,285 - ==> Best [Top1: 50.560   Top5: 80.080   Sparsity:0.00   Params: 1341960 on epoch: 294]
2024-02-17 14:36:03,286 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:36:03,350 - 

2024-02-17 14:36:03,350 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:36:15,712 - Epoch: [298][  100/  391]    Overall Loss 1.736700    Objective Loss 1.736700                                        LR 0.001298    Time 0.123549    
2024-02-17 14:36:27,604 - Epoch: [298][  200/  391]    Overall Loss 1.735813    Objective Loss 1.735813                                        LR 0.001298    Time 0.121210    
2024-02-17 14:36:39,580 - Epoch: [298][  300/  391]    Overall Loss 1.751988    Objective Loss 1.751988                                        LR 0.001298    Time 0.120713    
2024-02-17 14:36:50,856 - Epoch: [298][  391/  391]    Overall Loss 1.764381    Objective Loss 1.764381    Top1 50.480769    Top5 82.211538    LR 0.001298    Time 0.121446    
2024-02-17 14:36:50,978 - --- validate (epoch=298)-----------
2024-02-17 14:36:50,979 - 10000 samples (128 per mini-batch)
2024-02-17 14:36:56,691 - Epoch: [298][   79/   79]    Loss 2.001665    Top1 47.240000    Top5 77.760000    
2024-02-17 14:36:56,852 - ==> Top1: 47.240    Top5: 77.760    Loss: 2.002

2024-02-17 14:36:56,870 - ==> Best [Top1: 50.560   Top5: 80.080   Sparsity:0.00   Params: 1341960 on epoch: 294]
2024-02-17 14:36:56,871 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:36:56,935 - 

2024-02-17 14:36:56,936 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:37:09,875 - Epoch: [299][  100/  391]    Overall Loss 1.731043    Objective Loss 1.731043                                        LR 0.001298    Time 0.129320    
2024-02-17 14:37:22,497 - Epoch: [299][  200/  391]    Overall Loss 1.764776    Objective Loss 1.764776                                        LR 0.001298    Time 0.127744    
2024-02-17 14:37:35,409 - Epoch: [299][  300/  391]    Overall Loss 1.766213    Objective Loss 1.766213                                        LR 0.001298    Time 0.128188    
2024-02-17 14:37:46,951 - Epoch: [299][  391/  391]    Overall Loss 1.763116    Objective Loss 1.763116    Top1 50.000000    Top5 81.250000    LR 0.001298    Time 0.127860    
2024-02-17 14:37:47,070 - --- validate (epoch=299)-----------
2024-02-17 14:37:47,071 - 10000 samples (128 per mini-batch)
2024-02-17 14:37:52,426 - Epoch: [299][   79/   79]    Loss 1.994566    Top1 47.360000    Top5 77.410000    
2024-02-17 14:37:52,530 - ==> Top1: 47.360    Top5: 77.410    Loss: 1.995

2024-02-17 14:37:52,547 - ==> Best [Top1: 50.560   Top5: 80.080   Sparsity:0.00   Params: 1341960 on epoch: 294]
2024-02-17 14:37:52,548 - Saving checkpoint to: logs/2024.02.17-105230/qat_checkpoint.pth.tar
2024-02-17 14:37:52,609 - --- test ---------------------
2024-02-17 14:37:52,610 - 10000 samples (128 per mini-batch)
2024-02-17 14:37:58,010 - Test: [   79/   79]    Loss 1.997545    Top1 47.360000    Top5 77.410000    
2024-02-17 14:37:58,161 - ==> Top1: 47.360    Top5: 77.410    Loss: 1.998

2024-02-17 14:37:58,174 - 
2024-02-17 14:37:58,174 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.02.17-105230/2024.02.17-105230.log
