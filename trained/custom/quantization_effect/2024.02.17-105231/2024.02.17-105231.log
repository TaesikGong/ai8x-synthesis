2024-02-17 10:52:31,454 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.02.17-105231/2024.02.17-105231.log
2024-02-17 10:52:35,003 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2024-02-17 10:52:35,004 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}
2024-02-17 10:52:36,622 - Dataset sizes:
	training=50000
	validation=10000
	test=10000
2024-02-17 10:52:36,622 - Reading compression schedule from: policies/schedule-cifar100-mobilenetv2.yaml
2024-02-17 10:52:36,634 - 

2024-02-17 10:52:36,635 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:52:46,711 - Epoch: [0][  100/  391]    Overall Loss 4.416315    Objective Loss 4.416315                                        LR 0.100000    Time 0.100695    
2024-02-17 10:52:55,664 - Epoch: [0][  200/  391]    Overall Loss 4.286143    Objective Loss 4.286143                                        LR 0.100000    Time 0.095092    
2024-02-17 10:53:04,614 - Epoch: [0][  300/  391]    Overall Loss 4.195390    Objective Loss 4.195390                                        LR 0.100000    Time 0.093211    
2024-02-17 10:53:12,679 - Epoch: [0][  391/  391]    Overall Loss 4.128643    Objective Loss 4.128643    Top1 8.173077    Top5 30.769231    LR 0.100000    Time 0.092134    
2024-02-17 10:53:12,835 - --- validate (epoch=0)-----------
2024-02-17 10:53:12,835 - 10000 samples (128 per mini-batch)
2024-02-17 10:53:15,665 - Epoch: [0][   79/   79]    Loss 4.294045    Top1 5.940000    Top5 21.100000    
2024-02-17 10:53:15,808 - ==> Top1: 5.940    Top5: 21.100    Loss: 4.294

2024-02-17 10:53:16,208 - ==> Best [Top1: 5.940   Top5: 21.100   Sparsity:0.00   Params: 1341960 on epoch: 0]
2024-02-17 10:53:16,208 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 10:53:16,285 - 

2024-02-17 10:53:16,286 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:53:26,202 - Epoch: [1][  100/  391]    Overall Loss 3.804707    Objective Loss 3.804707                                        LR 0.100000    Time 0.099092    
2024-02-17 10:53:35,929 - Epoch: [1][  200/  391]    Overall Loss 3.776933    Objective Loss 3.776933                                        LR 0.100000    Time 0.098156    
2024-02-17 10:53:45,637 - Epoch: [1][  300/  391]    Overall Loss 3.746371    Objective Loss 3.746371                                        LR 0.100000    Time 0.097780    
2024-02-17 10:53:54,453 - Epoch: [1][  391/  391]    Overall Loss 3.722566    Objective Loss 3.722566    Top1 11.538462    Top5 36.538462    LR 0.100000    Time 0.097557    
2024-02-17 10:53:54,630 - --- validate (epoch=1)-----------
2024-02-17 10:53:54,630 - 10000 samples (128 per mini-batch)
2024-02-17 10:53:57,390 - Epoch: [1][   79/   79]    Loss 3.612136    Top1 11.770000    Top5 36.110000    
2024-02-17 10:53:57,546 - ==> Top1: 11.770    Top5: 36.110    Loss: 3.612

2024-02-17 10:53:57,564 - ==> Best [Top1: 11.770   Top5: 36.110   Sparsity:0.00   Params: 1341960 on epoch: 1]
2024-02-17 10:53:57,564 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 10:53:57,655 - 

2024-02-17 10:53:57,655 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:54:07,492 - Epoch: [2][  100/  391]    Overall Loss 3.567376    Objective Loss 3.567376                                        LR 0.100000    Time 0.098298    
2024-02-17 10:54:16,789 - Epoch: [2][  200/  391]    Overall Loss 3.529924    Objective Loss 3.529924                                        LR 0.100000    Time 0.095609    
2024-02-17 10:54:26,077 - Epoch: [2][  300/  391]    Overall Loss 3.513421    Objective Loss 3.513421                                        LR 0.100000    Time 0.094688    
2024-02-17 10:54:34,514 - Epoch: [2][  391/  391]    Overall Loss 3.484502    Objective Loss 3.484502    Top1 13.461538    Top5 45.192308    LR 0.100000    Time 0.094218    
2024-02-17 10:54:34,658 - --- validate (epoch=2)-----------
2024-02-17 10:54:34,659 - 10000 samples (128 per mini-batch)
2024-02-17 10:54:37,807 - Epoch: [2][   79/   79]    Loss 3.470122    Top1 14.090000    Top5 42.420000    
2024-02-17 10:54:37,914 - ==> Top1: 14.090    Top5: 42.420    Loss: 3.470

2024-02-17 10:54:37,933 - ==> Best [Top1: 14.090   Top5: 42.420   Sparsity:0.00   Params: 1341960 on epoch: 2]
2024-02-17 10:54:37,933 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 10:54:38,021 - 

2024-02-17 10:54:38,021 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:54:47,276 - Epoch: [3][  100/  391]    Overall Loss 3.324654    Objective Loss 3.324654                                        LR 0.100000    Time 0.092481    
2024-02-17 10:54:56,519 - Epoch: [3][  200/  391]    Overall Loss 3.292986    Objective Loss 3.292986                                        LR 0.100000    Time 0.092435    
2024-02-17 10:55:05,843 - Epoch: [3][  300/  391]    Overall Loss 3.274662    Objective Loss 3.274662                                        LR 0.100000    Time 0.092688    
2024-02-17 10:55:14,393 - Epoch: [3][  391/  391]    Overall Loss 3.251404    Objective Loss 3.251404    Top1 17.788462    Top5 50.000000    LR 0.100000    Time 0.092975    
2024-02-17 10:55:14,564 - --- validate (epoch=3)-----------
2024-02-17 10:55:14,565 - 10000 samples (128 per mini-batch)
2024-02-17 10:55:17,283 - Epoch: [3][   79/   79]    Loss 3.512974    Top1 16.070000    Top5 42.120000    
2024-02-17 10:55:17,422 - ==> Top1: 16.070    Top5: 42.120    Loss: 3.513

2024-02-17 10:55:17,439 - ==> Best [Top1: 16.070   Top5: 42.120   Sparsity:0.00   Params: 1341960 on epoch: 3]
2024-02-17 10:55:17,440 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 10:55:17,530 - 

2024-02-17 10:55:17,530 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:55:27,545 - Epoch: [4][  100/  391]    Overall Loss 3.115228    Objective Loss 3.115228                                        LR 0.100000    Time 0.100076    
2024-02-17 10:55:37,016 - Epoch: [4][  200/  391]    Overall Loss 3.098125    Objective Loss 3.098125                                        LR 0.100000    Time 0.097373    
2024-02-17 10:55:46,373 - Epoch: [4][  300/  391]    Overall Loss 3.082714    Objective Loss 3.082714                                        LR 0.100000    Time 0.096086    
2024-02-17 10:55:54,830 - Epoch: [4][  391/  391]    Overall Loss 3.057796    Objective Loss 3.057796    Top1 21.153846    Top5 56.730769    LR 0.100000    Time 0.095343    
2024-02-17 10:55:54,957 - --- validate (epoch=4)-----------
2024-02-17 10:55:54,957 - 10000 samples (128 per mini-batch)
2024-02-17 10:55:57,838 - Epoch: [4][   79/   79]    Loss 3.680597    Top1 15.720000    Top5 42.760000    
2024-02-17 10:55:57,957 - ==> Top1: 15.720    Top5: 42.760    Loss: 3.681

2024-02-17 10:55:57,976 - ==> Best [Top1: 16.070   Top5: 42.120   Sparsity:0.00   Params: 1341960 on epoch: 3]
2024-02-17 10:55:57,977 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 10:55:58,051 - 

2024-02-17 10:55:58,051 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:56:07,872 - Epoch: [5][  100/  391]    Overall Loss 2.921600    Objective Loss 2.921600                                        LR 0.100000    Time 0.098140    
2024-02-17 10:56:17,096 - Epoch: [5][  200/  391]    Overall Loss 2.925484    Objective Loss 2.925484                                        LR 0.100000    Time 0.095167    
2024-02-17 10:56:26,335 - Epoch: [5][  300/  391]    Overall Loss 2.903268    Objective Loss 2.903268                                        LR 0.100000    Time 0.094231    
2024-02-17 10:56:34,773 - Epoch: [5][  391/  391]    Overall Loss 2.888552    Objective Loss 2.888552    Top1 24.038462    Top5 55.769231    LR 0.100000    Time 0.093869    
2024-02-17 10:56:34,885 - --- validate (epoch=5)-----------
2024-02-17 10:56:34,885 - 10000 samples (128 per mini-batch)
2024-02-17 10:56:37,519 - Epoch: [5][   79/   79]    Loss 3.252491    Top1 20.500000    Top5 51.040000    
2024-02-17 10:56:37,708 - ==> Top1: 20.500    Top5: 51.040    Loss: 3.252

2024-02-17 10:56:37,727 - ==> Best [Top1: 20.500   Top5: 51.040   Sparsity:0.00   Params: 1341960 on epoch: 5]
2024-02-17 10:56:37,727 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 10:56:37,816 - 

2024-02-17 10:56:37,816 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:56:47,948 - Epoch: [6][  100/  391]    Overall Loss 2.782903    Objective Loss 2.782903                                        LR 0.100000    Time 0.101249    
2024-02-17 10:56:57,325 - Epoch: [6][  200/  391]    Overall Loss 2.783424    Objective Loss 2.783424                                        LR 0.100000    Time 0.097486    
2024-02-17 10:57:06,656 - Epoch: [6][  300/  391]    Overall Loss 2.762626    Objective Loss 2.762626                                        LR 0.100000    Time 0.096080    
2024-02-17 10:57:15,323 - Epoch: [6][  391/  391]    Overall Loss 2.749103    Objective Loss 2.749103    Top1 28.365385    Top5 61.057692    LR 0.100000    Time 0.095874    
2024-02-17 10:57:15,449 - --- validate (epoch=6)-----------
2024-02-17 10:57:15,449 - 10000 samples (128 per mini-batch)
2024-02-17 10:57:18,034 - Epoch: [6][   79/   79]    Loss 2.988540    Top1 23.640000    Top5 56.320000    
2024-02-17 10:57:18,128 - ==> Top1: 23.640    Top5: 56.320    Loss: 2.989

2024-02-17 10:57:18,147 - ==> Best [Top1: 23.640   Top5: 56.320   Sparsity:0.00   Params: 1341960 on epoch: 6]
2024-02-17 10:57:18,147 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 10:57:18,238 - 

2024-02-17 10:57:18,239 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:57:28,204 - Epoch: [7][  100/  391]    Overall Loss 2.656388    Objective Loss 2.656388                                        LR 0.100000    Time 0.099586    
2024-02-17 10:57:37,631 - Epoch: [7][  200/  391]    Overall Loss 2.651310    Objective Loss 2.651310                                        LR 0.100000    Time 0.096904    
2024-02-17 10:57:46,910 - Epoch: [7][  300/  391]    Overall Loss 2.638051    Objective Loss 2.638051                                        LR 0.100000    Time 0.095518    
2024-02-17 10:57:55,460 - Epoch: [7][  391/  391]    Overall Loss 2.628637    Objective Loss 2.628637    Top1 29.807692    Top5 65.865385    LR 0.100000    Time 0.095144    
2024-02-17 10:57:55,594 - --- validate (epoch=7)-----------
2024-02-17 10:57:55,595 - 10000 samples (128 per mini-batch)
2024-02-17 10:57:58,305 - Epoch: [7][   79/   79]    Loss 2.928358    Top1 26.420000    Top5 58.360000    
2024-02-17 10:57:58,434 - ==> Top1: 26.420    Top5: 58.360    Loss: 2.928

2024-02-17 10:57:58,453 - ==> Best [Top1: 26.420   Top5: 58.360   Sparsity:0.00   Params: 1341960 on epoch: 7]
2024-02-17 10:57:58,454 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 10:57:58,545 - 

2024-02-17 10:57:58,545 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:58:08,558 - Epoch: [8][  100/  391]    Overall Loss 2.557369    Objective Loss 2.557369                                        LR 0.100000    Time 0.100053    
2024-02-17 10:58:17,985 - Epoch: [8][  200/  391]    Overall Loss 2.552592    Objective Loss 2.552592                                        LR 0.100000    Time 0.097139    
2024-02-17 10:58:27,423 - Epoch: [8][  300/  391]    Overall Loss 2.547581    Objective Loss 2.547581                                        LR 0.100000    Time 0.096205    
2024-02-17 10:58:35,975 - Epoch: [8][  391/  391]    Overall Loss 2.535819    Objective Loss 2.535819    Top1 30.769231    Top5 67.788462    LR 0.100000    Time 0.095677    
2024-02-17 10:58:36,078 - --- validate (epoch=8)-----------
2024-02-17 10:58:36,079 - 10000 samples (128 per mini-batch)
2024-02-17 10:58:38,701 - Epoch: [8][   79/   79]    Loss 2.706972    Top1 29.560000    Top5 62.900000    
2024-02-17 10:58:38,858 - ==> Top1: 29.560    Top5: 62.900    Loss: 2.707

2024-02-17 10:58:38,876 - ==> Best [Top1: 29.560   Top5: 62.900   Sparsity:0.00   Params: 1341960 on epoch: 8]
2024-02-17 10:58:38,877 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 10:58:38,968 - 

2024-02-17 10:58:38,969 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:58:48,862 - Epoch: [9][  100/  391]    Overall Loss 2.443809    Objective Loss 2.443809                                        LR 0.100000    Time 0.098856    
2024-02-17 10:58:58,460 - Epoch: [9][  200/  391]    Overall Loss 2.461339    Objective Loss 2.461339                                        LR 0.100000    Time 0.097397    
2024-02-17 10:59:07,792 - Epoch: [9][  300/  391]    Overall Loss 2.454683    Objective Loss 2.454683                                        LR 0.100000    Time 0.096023    
2024-02-17 10:59:16,349 - Epoch: [9][  391/  391]    Overall Loss 2.455477    Objective Loss 2.455477    Top1 37.019231    Top5 63.942308    LR 0.100000    Time 0.095548    
2024-02-17 10:59:16,537 - --- validate (epoch=9)-----------
2024-02-17 10:59:16,538 - 10000 samples (128 per mini-batch)
2024-02-17 10:59:19,187 - Epoch: [9][   79/   79]    Loss 3.159537    Top1 24.220000    Top5 55.490000    
2024-02-17 10:59:19,363 - ==> Top1: 24.220    Top5: 55.490    Loss: 3.160

2024-02-17 10:59:19,382 - ==> Best [Top1: 29.560   Top5: 62.900   Sparsity:0.00   Params: 1341960 on epoch: 8]
2024-02-17 10:59:19,383 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 10:59:19,457 - 

2024-02-17 10:59:19,458 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:59:28,938 - Epoch: [10][  100/  391]    Overall Loss 2.413701    Objective Loss 2.413701                                        LR 0.100000    Time 0.094730    
2024-02-17 10:59:38,253 - Epoch: [10][  200/  391]    Overall Loss 2.394479    Objective Loss 2.394479                                        LR 0.100000    Time 0.093922    
2024-02-17 10:59:47,662 - Epoch: [10][  300/  391]    Overall Loss 2.385760    Objective Loss 2.385760                                        LR 0.100000    Time 0.093960    
2024-02-17 10:59:56,263 - Epoch: [10][  391/  391]    Overall Loss 2.385589    Objective Loss 2.385589    Top1 31.250000    Top5 69.711538    LR 0.100000    Time 0.094079    
2024-02-17 10:59:56,375 - --- validate (epoch=10)-----------
2024-02-17 10:59:56,376 - 10000 samples (128 per mini-batch)
2024-02-17 10:59:58,976 - Epoch: [10][   79/   79]    Loss 2.457716    Top1 33.660000    Top5 68.120000    
2024-02-17 10:59:59,143 - ==> Top1: 33.660    Top5: 68.120    Loss: 2.458

2024-02-17 10:59:59,162 - ==> Best [Top1: 33.660   Top5: 68.120   Sparsity:0.00   Params: 1341960 on epoch: 10]
2024-02-17 10:59:59,162 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 10:59:59,252 - 

2024-02-17 10:59:59,252 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:00:08,988 - Epoch: [11][  100/  391]    Overall Loss 2.344371    Objective Loss 2.344371                                        LR 0.100000    Time 0.097295    
2024-02-17 11:00:18,214 - Epoch: [11][  200/  391]    Overall Loss 2.329532    Objective Loss 2.329532                                        LR 0.100000    Time 0.094757    
2024-02-17 11:00:27,573 - Epoch: [11][  300/  391]    Overall Loss 2.320764    Objective Loss 2.320764                                        LR 0.100000    Time 0.094353    
2024-02-17 11:00:36,146 - Epoch: [11][  391/  391]    Overall Loss 2.318187    Objective Loss 2.318187    Top1 40.384615    Top5 67.307692    LR 0.100000    Time 0.094310    
2024-02-17 11:00:36,332 - --- validate (epoch=11)-----------
2024-02-17 11:00:36,333 - 10000 samples (128 per mini-batch)
2024-02-17 11:00:38,883 - Epoch: [11][   79/   79]    Loss 2.570210    Top1 32.130000    Top5 66.130000    
2024-02-17 11:00:38,997 - ==> Top1: 32.130    Top5: 66.130    Loss: 2.570

2024-02-17 11:00:39,015 - ==> Best [Top1: 33.660   Top5: 68.120   Sparsity:0.00   Params: 1341960 on epoch: 10]
2024-02-17 11:00:39,015 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:00:39,090 - 

2024-02-17 11:00:39,090 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:00:49,073 - Epoch: [12][  100/  391]    Overall Loss 2.279788    Objective Loss 2.279788                                        LR 0.100000    Time 0.099758    
2024-02-17 11:00:58,478 - Epoch: [12][  200/  391]    Overall Loss 2.269952    Objective Loss 2.269952                                        LR 0.100000    Time 0.096887    
2024-02-17 11:01:07,889 - Epoch: [12][  300/  391]    Overall Loss 2.270224    Objective Loss 2.270224                                        LR 0.100000    Time 0.095947    
2024-02-17 11:01:16,525 - Epoch: [12][  391/  391]    Overall Loss 2.271273    Objective Loss 2.271273    Top1 36.057692    Top5 73.076923    LR 0.100000    Time 0.095693    
2024-02-17 11:01:16,627 - --- validate (epoch=12)-----------
2024-02-17 11:01:16,628 - 10000 samples (128 per mini-batch)
2024-02-17 11:01:18,886 - Epoch: [12][   79/   79]    Loss 2.388325    Top1 36.210000    Top5 69.180000    
2024-02-17 11:01:19,038 - ==> Top1: 36.210    Top5: 69.180    Loss: 2.388

2024-02-17 11:01:19,054 - ==> Best [Top1: 36.210   Top5: 69.180   Sparsity:0.00   Params: 1341960 on epoch: 12]
2024-02-17 11:01:19,055 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:01:19,134 - 

2024-02-17 11:01:19,134 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:01:27,821 - Epoch: [13][  100/  391]    Overall Loss 2.193745    Objective Loss 2.193745                                        LR 0.100000    Time 0.086811    
2024-02-17 11:01:36,258 - Epoch: [13][  200/  391]    Overall Loss 2.201809    Objective Loss 2.201809                                        LR 0.100000    Time 0.085569    
2024-02-17 11:01:45,453 - Epoch: [13][  300/  391]    Overall Loss 2.204183    Objective Loss 2.204183                                        LR 0.100000    Time 0.087685    
2024-02-17 11:01:54,083 - Epoch: [13][  391/  391]    Overall Loss 2.202856    Objective Loss 2.202856    Top1 39.903846    Top5 68.750000    LR 0.100000    Time 0.089339    
2024-02-17 11:01:54,199 - --- validate (epoch=13)-----------
2024-02-17 11:01:54,200 - 10000 samples (128 per mini-batch)
2024-02-17 11:01:56,349 - Epoch: [13][   79/   79]    Loss 3.709286    Top1 22.120000    Top5 47.120000    
2024-02-17 11:01:56,475 - ==> Top1: 22.120    Top5: 47.120    Loss: 3.709

2024-02-17 11:01:56,491 - ==> Best [Top1: 36.210   Top5: 69.180   Sparsity:0.00   Params: 1341960 on epoch: 12]
2024-02-17 11:01:56,491 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:01:56,568 - 

2024-02-17 11:01:56,568 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:02:06,553 - Epoch: [14][  100/  391]    Overall Loss 2.149988    Objective Loss 2.149988                                        LR 0.100000    Time 0.099777    
2024-02-17 11:02:16,220 - Epoch: [14][  200/  391]    Overall Loss 2.156148    Objective Loss 2.156148                                        LR 0.100000    Time 0.098198    
2024-02-17 11:02:25,731 - Epoch: [14][  300/  391]    Overall Loss 2.161825    Objective Loss 2.161825                                        LR 0.100000    Time 0.097155    
2024-02-17 11:02:33,534 - Epoch: [14][  391/  391]    Overall Loss 2.157114    Objective Loss 2.157114    Top1 41.826923    Top5 75.000000    LR 0.100000    Time 0.094491    
2024-02-17 11:02:33,708 - --- validate (epoch=14)-----------
2024-02-17 11:02:33,709 - 10000 samples (128 per mini-batch)
2024-02-17 11:02:36,480 - Epoch: [14][   79/   79]    Loss 2.429733    Top1 36.470000    Top5 68.480000    
2024-02-17 11:02:36,581 - ==> Top1: 36.470    Top5: 68.480    Loss: 2.430

2024-02-17 11:02:36,602 - ==> Best [Top1: 36.470   Top5: 68.480   Sparsity:0.00   Params: 1341960 on epoch: 14]
2024-02-17 11:02:36,603 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:02:36,705 - 

2024-02-17 11:02:36,706 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:02:45,786 - Epoch: [15][  100/  391]    Overall Loss 2.120016    Objective Loss 2.120016                                        LR 0.100000    Time 0.090694    
2024-02-17 11:02:55,289 - Epoch: [15][  200/  391]    Overall Loss 2.116726    Objective Loss 2.116726                                        LR 0.100000    Time 0.092842    
2024-02-17 11:03:02,745 - Epoch: [15][  300/  391]    Overall Loss 2.114391    Objective Loss 2.114391                                        LR 0.100000    Time 0.086734    
2024-02-17 11:03:09,305 - Epoch: [15][  391/  391]    Overall Loss 2.116584    Objective Loss 2.116584    Top1 47.596154    Top5 78.846154    LR 0.100000    Time 0.083318    
2024-02-17 11:03:09,419 - --- validate (epoch=15)-----------
2024-02-17 11:03:09,420 - 10000 samples (128 per mini-batch)
2024-02-17 11:03:12,137 - Epoch: [15][   79/   79]    Loss 2.365633    Top1 37.020000    Top5 70.350000    
2024-02-17 11:03:12,262 - ==> Top1: 37.020    Top5: 70.350    Loss: 2.366

2024-02-17 11:03:12,284 - ==> Best [Top1: 37.020   Top5: 70.350   Sparsity:0.00   Params: 1341960 on epoch: 15]
2024-02-17 11:03:12,284 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:03:12,383 - 

2024-02-17 11:03:12,383 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:03:22,561 - Epoch: [16][  100/  391]    Overall Loss 2.047925    Objective Loss 2.047925                                        LR 0.100000    Time 0.101695    
2024-02-17 11:03:32,087 - Epoch: [16][  200/  391]    Overall Loss 2.072812    Objective Loss 2.072812                                        LR 0.100000    Time 0.098459    
2024-02-17 11:03:41,736 - Epoch: [16][  300/  391]    Overall Loss 2.064370    Objective Loss 2.064370                                        LR 0.100000    Time 0.097787    
2024-02-17 11:03:50,444 - Epoch: [16][  391/  391]    Overall Loss 2.064254    Objective Loss 2.064254    Top1 44.230769    Top5 77.403846    LR 0.100000    Time 0.097289    
2024-02-17 11:03:50,602 - --- validate (epoch=16)-----------
2024-02-17 11:03:50,604 - 10000 samples (128 per mini-batch)
2024-02-17 11:03:53,646 - Epoch: [16][   79/   79]    Loss 2.320833    Top1 38.740000    Top5 71.180000    
2024-02-17 11:03:53,791 - ==> Top1: 38.740    Top5: 71.180    Loss: 2.321

2024-02-17 11:03:53,808 - ==> Best [Top1: 38.740   Top5: 71.180   Sparsity:0.00   Params: 1341960 on epoch: 16]
2024-02-17 11:03:53,809 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:03:53,903 - 

2024-02-17 11:03:53,904 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:04:01,925 - Epoch: [17][  100/  391]    Overall Loss 2.010597    Objective Loss 2.010597                                        LR 0.100000    Time 0.080148    
2024-02-17 11:04:11,209 - Epoch: [17][  200/  391]    Overall Loss 2.013927    Objective Loss 2.013927                                        LR 0.100000    Time 0.086469    
2024-02-17 11:04:20,927 - Epoch: [17][  300/  391]    Overall Loss 2.022046    Objective Loss 2.022046                                        LR 0.100000    Time 0.090024    
2024-02-17 11:04:29,811 - Epoch: [17][  391/  391]    Overall Loss 2.025805    Objective Loss 2.025805    Top1 44.230769    Top5 77.403846    LR 0.100000    Time 0.091783    
2024-02-17 11:04:29,977 - --- validate (epoch=17)-----------
2024-02-17 11:04:29,978 - 10000 samples (128 per mini-batch)
2024-02-17 11:04:32,762 - Epoch: [17][   79/   79]    Loss 2.210307    Top1 40.720000    Top5 73.640000    
2024-02-17 11:04:32,902 - ==> Top1: 40.720    Top5: 73.640    Loss: 2.210

2024-02-17 11:04:32,918 - ==> Best [Top1: 40.720   Top5: 73.640   Sparsity:0.00   Params: 1341960 on epoch: 17]
2024-02-17 11:04:32,919 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:04:33,055 - 

2024-02-17 11:04:33,055 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:04:43,120 - Epoch: [18][  100/  391]    Overall Loss 2.002099    Objective Loss 2.002099                                        LR 0.100000    Time 0.100535    
2024-02-17 11:04:53,269 - Epoch: [18][  200/  391]    Overall Loss 1.996806    Objective Loss 1.996806                                        LR 0.100000    Time 0.100993    
2024-02-17 11:05:02,609 - Epoch: [18][  300/  391]    Overall Loss 2.003348    Objective Loss 2.003348                                        LR 0.100000    Time 0.098443    
2024-02-17 11:05:11,357 - Epoch: [18][  391/  391]    Overall Loss 2.002153    Objective Loss 2.002153    Top1 42.788462    Top5 75.000000    LR 0.100000    Time 0.097894    
2024-02-17 11:05:11,481 - --- validate (epoch=18)-----------
2024-02-17 11:05:11,482 - 10000 samples (128 per mini-batch)
2024-02-17 11:05:14,256 - Epoch: [18][   79/   79]    Loss 2.192672    Top1 41.320000    Top5 74.030000    
2024-02-17 11:05:14,372 - ==> Top1: 41.320    Top5: 74.030    Loss: 2.193

2024-02-17 11:05:14,394 - ==> Best [Top1: 41.320   Top5: 74.030   Sparsity:0.00   Params: 1341960 on epoch: 18]
2024-02-17 11:05:14,395 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:05:14,489 - 

2024-02-17 11:05:14,490 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:05:22,476 - Epoch: [19][  100/  391]    Overall Loss 1.957174    Objective Loss 1.957174                                        LR 0.100000    Time 0.079791    
2024-02-17 11:05:31,975 - Epoch: [19][  200/  391]    Overall Loss 1.959424    Objective Loss 1.959424                                        LR 0.100000    Time 0.087361    
2024-02-17 11:05:41,626 - Epoch: [19][  300/  391]    Overall Loss 1.953811    Objective Loss 1.953811                                        LR 0.100000    Time 0.090394    
2024-02-17 11:05:50,430 - Epoch: [19][  391/  391]    Overall Loss 1.957145    Objective Loss 1.957145    Top1 46.634615    Top5 79.807692    LR 0.100000    Time 0.091864    
2024-02-17 11:05:50,588 - --- validate (epoch=19)-----------
2024-02-17 11:05:50,589 - 10000 samples (128 per mini-batch)
2024-02-17 11:05:53,331 - Epoch: [19][   79/   79]    Loss 2.076991    Top1 42.880000    Top5 75.840000    
2024-02-17 11:05:53,443 - ==> Top1: 42.880    Top5: 75.840    Loss: 2.077

2024-02-17 11:05:53,464 - ==> Best [Top1: 42.880   Top5: 75.840   Sparsity:0.00   Params: 1341960 on epoch: 19]
2024-02-17 11:05:53,464 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:05:53,567 - 

2024-02-17 11:05:53,568 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:06:03,830 - Epoch: [20][  100/  391]    Overall Loss 1.917232    Objective Loss 1.917232                                        LR 0.100000    Time 0.102543    
2024-02-17 11:06:13,423 - Epoch: [20][  200/  391]    Overall Loss 1.918055    Objective Loss 1.918055                                        LR 0.100000    Time 0.099214    
2024-02-17 11:06:23,233 - Epoch: [20][  300/  391]    Overall Loss 1.924079    Objective Loss 1.924079                                        LR 0.100000    Time 0.098827    
2024-02-17 11:06:31,952 - Epoch: [20][  391/  391]    Overall Loss 1.920631    Objective Loss 1.920631    Top1 50.480769    Top5 82.692308    LR 0.100000    Time 0.098116    
2024-02-17 11:06:32,120 - --- validate (epoch=20)-----------
2024-02-17 11:06:32,121 - 10000 samples (128 per mini-batch)
2024-02-17 11:06:34,963 - Epoch: [20][   79/   79]    Loss 2.159016    Top1 40.930000    Top5 75.230000    
2024-02-17 11:06:35,138 - ==> Top1: 40.930    Top5: 75.230    Loss: 2.159

2024-02-17 11:06:35,154 - ==> Best [Top1: 42.880   Top5: 75.840   Sparsity:0.00   Params: 1341960 on epoch: 19]
2024-02-17 11:06:35,155 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:06:35,238 - 

2024-02-17 11:06:35,239 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:06:45,129 - Epoch: [21][  100/  391]    Overall Loss 1.908032    Objective Loss 1.908032                                        LR 0.100000    Time 0.098829    
2024-02-17 11:06:54,806 - Epoch: [21][  200/  391]    Overall Loss 1.902803    Objective Loss 1.902803                                        LR 0.100000    Time 0.097777    
2024-02-17 11:07:04,427 - Epoch: [21][  300/  391]    Overall Loss 1.902629    Objective Loss 1.902629                                        LR 0.100000    Time 0.097241    
2024-02-17 11:07:13,216 - Epoch: [21][  391/  391]    Overall Loss 1.899149    Objective Loss 1.899149    Top1 44.230769    Top5 75.961538    LR 0.100000    Time 0.097076    
2024-02-17 11:07:13,387 - --- validate (epoch=21)-----------
2024-02-17 11:07:13,388 - 10000 samples (128 per mini-batch)
2024-02-17 11:07:16,236 - Epoch: [21][   79/   79]    Loss 2.036169    Top1 44.570000    Top5 76.830000    
2024-02-17 11:07:16,410 - ==> Top1: 44.570    Top5: 76.830    Loss: 2.036

2024-02-17 11:07:16,429 - ==> Best [Top1: 44.570   Top5: 76.830   Sparsity:0.00   Params: 1341960 on epoch: 21]
2024-02-17 11:07:16,429 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:07:16,523 - 

2024-02-17 11:07:16,523 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:07:26,475 - Epoch: [22][  100/  391]    Overall Loss 1.867678    Objective Loss 1.867678                                        LR 0.100000    Time 0.099451    
2024-02-17 11:07:36,116 - Epoch: [22][  200/  391]    Overall Loss 1.858907    Objective Loss 1.858907                                        LR 0.100000    Time 0.097905    
2024-02-17 11:07:45,728 - Epoch: [22][  300/  391]    Overall Loss 1.868168    Objective Loss 1.868168                                        LR 0.100000    Time 0.097294    
2024-02-17 11:07:54,379 - Epoch: [22][  391/  391]    Overall Loss 1.867814    Objective Loss 1.867814    Top1 46.634615    Top5 80.288462    LR 0.100000    Time 0.096765    
2024-02-17 11:07:54,496 - --- validate (epoch=22)-----------
2024-02-17 11:07:54,497 - 10000 samples (128 per mini-batch)
2024-02-17 11:07:57,387 - Epoch: [22][   79/   79]    Loss 2.015076    Top1 44.960000    Top5 77.410000    
2024-02-17 11:07:57,506 - ==> Top1: 44.960    Top5: 77.410    Loss: 2.015

2024-02-17 11:07:57,525 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:07:57,525 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:07:57,622 - 

2024-02-17 11:07:57,622 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:08:07,409 - Epoch: [23][  100/  391]    Overall Loss 1.836784    Objective Loss 1.836784                                        LR 0.100000    Time 0.097785    
2024-02-17 11:08:16,953 - Epoch: [23][  200/  391]    Overall Loss 1.845296    Objective Loss 1.845296                                        LR 0.100000    Time 0.096589    
2024-02-17 11:08:26,597 - Epoch: [23][  300/  391]    Overall Loss 1.845424    Objective Loss 1.845424                                        LR 0.100000    Time 0.096525    
2024-02-17 11:08:33,413 - Epoch: [23][  391/  391]    Overall Loss 1.846385    Objective Loss 1.846385    Top1 42.788462    Top5 74.519231    LR 0.100000    Time 0.091483    
2024-02-17 11:08:33,564 - --- validate (epoch=23)-----------
2024-02-17 11:08:33,564 - 10000 samples (128 per mini-batch)
2024-02-17 11:08:36,281 - Epoch: [23][   79/   79]    Loss 2.083110    Top1 44.040000    Top5 76.330000    
2024-02-17 11:08:36,395 - ==> Top1: 44.040    Top5: 76.330    Loss: 2.083

2024-02-17 11:08:36,409 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:08:36,410 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:08:36,481 - 

2024-02-17 11:08:36,482 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:08:46,868 - Epoch: [24][  100/  391]    Overall Loss 1.795736    Objective Loss 1.795736                                        LR 0.100000    Time 0.103793    
2024-02-17 11:08:56,424 - Epoch: [24][  200/  391]    Overall Loss 1.800785    Objective Loss 1.800785                                        LR 0.100000    Time 0.099651    
2024-02-17 11:09:06,277 - Epoch: [24][  300/  391]    Overall Loss 1.814031    Objective Loss 1.814031                                        LR 0.100000    Time 0.099261    
2024-02-17 11:09:14,939 - Epoch: [24][  391/  391]    Overall Loss 1.819678    Objective Loss 1.819678    Top1 53.365385    Top5 83.653846    LR 0.100000    Time 0.098303    
2024-02-17 11:09:15,079 - --- validate (epoch=24)-----------
2024-02-17 11:09:15,080 - 10000 samples (128 per mini-batch)
2024-02-17 11:09:18,002 - Epoch: [24][   79/   79]    Loss 2.628696    Top1 35.390000    Top5 68.330000    
2024-02-17 11:09:18,123 - ==> Top1: 35.390    Top5: 68.330    Loss: 2.629

2024-02-17 11:09:18,134 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:09:18,134 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:09:18,219 - 

2024-02-17 11:09:18,220 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:09:28,090 - Epoch: [25][  100/  391]    Overall Loss 1.772238    Objective Loss 1.772238                                        LR 0.100000    Time 0.098531    
2024-02-17 11:09:37,721 - Epoch: [25][  200/  391]    Overall Loss 1.775472    Objective Loss 1.775472                                        LR 0.100000    Time 0.097402    
2024-02-17 11:09:47,433 - Epoch: [25][  300/  391]    Overall Loss 1.788799    Objective Loss 1.788799                                        LR 0.100000    Time 0.097291    
2024-02-17 11:09:56,164 - Epoch: [25][  391/  391]    Overall Loss 1.791222    Objective Loss 1.791222    Top1 48.076923    Top5 81.730769    LR 0.100000    Time 0.096967    
2024-02-17 11:09:56,291 - --- validate (epoch=25)-----------
2024-02-17 11:09:56,292 - 10000 samples (128 per mini-batch)
2024-02-17 11:09:59,213 - Epoch: [25][   79/   79]    Loss 2.358221    Top1 39.670000    Top5 71.360000    
2024-02-17 11:09:59,312 - ==> Top1: 39.670    Top5: 71.360    Loss: 2.358

2024-02-17 11:09:59,331 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:09:59,331 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:09:59,411 - 

2024-02-17 11:09:59,412 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:10:09,364 - Epoch: [26][  100/  391]    Overall Loss 1.733931    Objective Loss 1.733931                                        LR 0.100000    Time 0.099444    
2024-02-17 11:10:19,047 - Epoch: [26][  200/  391]    Overall Loss 1.755922    Objective Loss 1.755922                                        LR 0.100000    Time 0.098113    
2024-02-17 11:10:28,822 - Epoch: [26][  300/  391]    Overall Loss 1.762424    Objective Loss 1.762424                                        LR 0.100000    Time 0.097976    
2024-02-17 11:10:38,171 - Epoch: [26][  391/  391]    Overall Loss 1.766247    Objective Loss 1.766247    Top1 51.442308    Top5 82.692308    LR 0.100000    Time 0.099073    
2024-02-17 11:10:38,292 - --- validate (epoch=26)-----------
2024-02-17 11:10:38,293 - 10000 samples (128 per mini-batch)
2024-02-17 11:10:41,553 - Epoch: [26][   79/   79]    Loss 2.096929    Top1 43.990000    Top5 76.130000    
2024-02-17 11:10:41,661 - ==> Top1: 43.990    Top5: 76.130    Loss: 2.097

2024-02-17 11:10:41,673 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:10:41,674 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:10:41,751 - 

2024-02-17 11:10:41,751 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:10:51,868 - Epoch: [27][  100/  391]    Overall Loss 1.701968    Objective Loss 1.701968                                        LR 0.100000    Time 0.101096    
2024-02-17 11:11:01,256 - Epoch: [27][  200/  391]    Overall Loss 1.725346    Objective Loss 1.725346                                        LR 0.100000    Time 0.097464    
2024-02-17 11:11:10,762 - Epoch: [27][  300/  391]    Overall Loss 1.735624    Objective Loss 1.735624                                        LR 0.100000    Time 0.096645    
2024-02-17 11:11:19,380 - Epoch: [27][  391/  391]    Overall Loss 1.742071    Objective Loss 1.742071    Top1 48.076923    Top5 81.250000    LR 0.100000    Time 0.096184    
2024-02-17 11:11:19,530 - --- validate (epoch=27)-----------
2024-02-17 11:11:19,530 - 10000 samples (128 per mini-batch)
2024-02-17 11:11:22,626 - Epoch: [27][   79/   79]    Loss 2.224197    Top1 41.210000    Top5 73.400000    
2024-02-17 11:11:22,780 - ==> Top1: 41.210    Top5: 73.400    Loss: 2.224

2024-02-17 11:11:22,806 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:11:22,807 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:11:22,893 - 

2024-02-17 11:11:22,893 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:11:33,289 - Epoch: [28][  100/  391]    Overall Loss 1.722163    Objective Loss 1.722163                                        LR 0.100000    Time 0.103878    
2024-02-17 11:11:42,959 - Epoch: [28][  200/  391]    Overall Loss 1.715269    Objective Loss 1.715269                                        LR 0.100000    Time 0.100266    
2024-02-17 11:11:52,520 - Epoch: [28][  300/  391]    Overall Loss 1.721193    Objective Loss 1.721193                                        LR 0.100000    Time 0.098700    
2024-02-17 11:12:01,024 - Epoch: [28][  391/  391]    Overall Loss 1.723180    Objective Loss 1.723180    Top1 50.961538    Top5 80.769231    LR 0.100000    Time 0.097466    
2024-02-17 11:12:01,155 - --- validate (epoch=28)-----------
2024-02-17 11:12:01,155 - 10000 samples (128 per mini-batch)
2024-02-17 11:12:04,020 - Epoch: [28][   79/   79]    Loss 2.306765    Top1 40.140000    Top5 72.850000    
2024-02-17 11:12:04,206 - ==> Top1: 40.140    Top5: 72.850    Loss: 2.307

2024-02-17 11:12:04,224 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:12:04,225 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:12:04,303 - 

2024-02-17 11:12:04,304 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:12:14,489 - Epoch: [29][  100/  391]    Overall Loss 1.684613    Objective Loss 1.684613                                        LR 0.100000    Time 0.101783    
2024-02-17 11:12:24,172 - Epoch: [29][  200/  391]    Overall Loss 1.698510    Objective Loss 1.698510                                        LR 0.100000    Time 0.099284    
2024-02-17 11:12:33,815 - Epoch: [29][  300/  391]    Overall Loss 1.710720    Objective Loss 1.710720                                        LR 0.100000    Time 0.098318    
2024-02-17 11:12:42,375 - Epoch: [29][  391/  391]    Overall Loss 1.714815    Objective Loss 1.714815    Top1 50.000000    Top5 83.173077    LR 0.100000    Time 0.097319    
2024-02-17 11:12:42,514 - --- validate (epoch=29)-----------
2024-02-17 11:12:42,515 - 10000 samples (128 per mini-batch)
2024-02-17 11:12:45,284 - Epoch: [29][   79/   79]    Loss 2.234177    Top1 42.070000    Top5 73.000000    
2024-02-17 11:12:45,394 - ==> Top1: 42.070    Top5: 73.000    Loss: 2.234

2024-02-17 11:12:45,413 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:12:45,414 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:12:45,490 - 

2024-02-17 11:12:45,490 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:12:55,841 - Epoch: [30][  100/  391]    Overall Loss 1.644164    Objective Loss 1.644164                                        LR 0.100000    Time 0.103430    
2024-02-17 11:13:05,455 - Epoch: [30][  200/  391]    Overall Loss 1.668109    Objective Loss 1.668109                                        LR 0.100000    Time 0.099763    
2024-02-17 11:13:15,085 - Epoch: [30][  300/  391]    Overall Loss 1.679802    Objective Loss 1.679802                                        LR 0.100000    Time 0.098595    
2024-02-17 11:13:23,695 - Epoch: [30][  391/  391]    Overall Loss 1.683561    Objective Loss 1.683561    Top1 51.923077    Top5 85.096154    LR 0.100000    Time 0.097659    
2024-02-17 11:13:23,807 - --- validate (epoch=30)-----------
2024-02-17 11:13:23,808 - 10000 samples (128 per mini-batch)
2024-02-17 11:13:26,458 - Epoch: [30][   79/   79]    Loss 2.379153    Top1 39.670000    Top5 71.900000    
2024-02-17 11:13:26,623 - ==> Top1: 39.670    Top5: 71.900    Loss: 2.379

2024-02-17 11:13:26,643 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:13:26,644 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:13:26,719 - 

2024-02-17 11:13:26,719 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:13:36,847 - Epoch: [31][  100/  391]    Overall Loss 1.661652    Objective Loss 1.661652                                        LR 0.100000    Time 0.101209    
2024-02-17 11:13:46,574 - Epoch: [31][  200/  391]    Overall Loss 1.659721    Objective Loss 1.659721                                        LR 0.100000    Time 0.099213    
2024-02-17 11:13:56,108 - Epoch: [31][  300/  391]    Overall Loss 1.663134    Objective Loss 1.663134                                        LR 0.100000    Time 0.097910    
2024-02-17 11:14:04,750 - Epoch: [31][  391/  391]    Overall Loss 1.668387    Objective Loss 1.668387    Top1 56.730769    Top5 82.692308    LR 0.100000    Time 0.097214    
2024-02-17 11:14:04,943 - --- validate (epoch=31)-----------
2024-02-17 11:14:04,944 - 10000 samples (128 per mini-batch)
2024-02-17 11:14:07,501 - Epoch: [31][   79/   79]    Loss 1.868021    Top1 48.600000    Top5 80.400000    
2024-02-17 11:14:07,626 - ==> Top1: 48.600    Top5: 80.400    Loss: 1.868

2024-02-17 11:14:07,645 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:14:07,645 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:14:07,740 - 

2024-02-17 11:14:07,741 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:14:17,865 - Epoch: [32][  100/  391]    Overall Loss 1.627926    Objective Loss 1.627926                                        LR 0.100000    Time 0.101138    
2024-02-17 11:14:27,495 - Epoch: [32][  200/  391]    Overall Loss 1.643109    Objective Loss 1.643109                                        LR 0.100000    Time 0.098695    
2024-02-17 11:14:37,217 - Epoch: [32][  300/  391]    Overall Loss 1.644081    Objective Loss 1.644081                                        LR 0.100000    Time 0.098186    
2024-02-17 11:14:45,968 - Epoch: [32][  391/  391]    Overall Loss 1.646557    Objective Loss 1.646557    Top1 48.076923    Top5 83.653846    LR 0.100000    Time 0.097706    
2024-02-17 11:14:46,107 - --- validate (epoch=32)-----------
2024-02-17 11:14:46,108 - 10000 samples (128 per mini-batch)
2024-02-17 11:14:48,791 - Epoch: [32][   79/   79]    Loss 1.976561    Top1 45.680000    Top5 78.180000    
2024-02-17 11:14:48,910 - ==> Top1: 45.680    Top5: 78.180    Loss: 1.977

2024-02-17 11:14:48,929 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:14:48,929 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:14:49,006 - 

2024-02-17 11:14:49,006 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:14:59,294 - Epoch: [33][  100/  391]    Overall Loss 1.595683    Objective Loss 1.595683                                        LR 0.100000    Time 0.102806    
2024-02-17 11:15:08,772 - Epoch: [33][  200/  391]    Overall Loss 1.606158    Objective Loss 1.606158                                        LR 0.100000    Time 0.098767    
2024-02-17 11:15:18,303 - Epoch: [33][  300/  391]    Overall Loss 1.621585    Objective Loss 1.621585                                        LR 0.100000    Time 0.097601    
2024-02-17 11:15:26,723 - Epoch: [33][  391/  391]    Overall Loss 1.629625    Objective Loss 1.629625    Top1 54.807692    Top5 82.211538    LR 0.100000    Time 0.096409    
2024-02-17 11:15:26,856 - --- validate (epoch=33)-----------
2024-02-17 11:15:26,857 - 10000 samples (128 per mini-batch)
2024-02-17 11:15:29,463 - Epoch: [33][   79/   79]    Loss 1.999365    Top1 46.460000    Top5 78.210000    
2024-02-17 11:15:29,577 - ==> Top1: 46.460    Top5: 78.210    Loss: 1.999

2024-02-17 11:15:29,596 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:15:29,596 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:15:29,670 - 

2024-02-17 11:15:29,671 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:15:40,007 - Epoch: [34][  100/  391]    Overall Loss 1.594835    Objective Loss 1.594835                                        LR 0.100000    Time 0.103290    
2024-02-17 11:15:49,799 - Epoch: [34][  200/  391]    Overall Loss 1.604233    Objective Loss 1.604233                                        LR 0.100000    Time 0.100580    
2024-02-17 11:15:59,492 - Epoch: [34][  300/  391]    Overall Loss 1.605438    Objective Loss 1.605438                                        LR 0.100000    Time 0.099350    
2024-02-17 11:16:07,678 - Epoch: [34][  391/  391]    Overall Loss 1.612352    Objective Loss 1.612352    Top1 54.807692    Top5 87.500000    LR 0.100000    Time 0.097155    
2024-02-17 11:16:07,804 - --- validate (epoch=34)-----------
2024-02-17 11:16:07,805 - 10000 samples (128 per mini-batch)
2024-02-17 11:16:10,530 - Epoch: [34][   79/   79]    Loss 2.018327    Top1 46.060000    Top5 77.830000    
2024-02-17 11:16:10,696 - ==> Top1: 46.060    Top5: 77.830    Loss: 2.018

2024-02-17 11:16:10,716 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:16:10,716 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:16:10,792 - 

2024-02-17 11:16:10,792 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:16:20,863 - Epoch: [35][  100/  391]    Overall Loss 1.575489    Objective Loss 1.575489                                        LR 0.100000    Time 0.100632    
2024-02-17 11:16:30,438 - Epoch: [35][  200/  391]    Overall Loss 1.586312    Objective Loss 1.586312                                        LR 0.100000    Time 0.098171    
2024-02-17 11:16:40,028 - Epoch: [35][  300/  391]    Overall Loss 1.602771    Objective Loss 1.602771                                        LR 0.100000    Time 0.097400    
2024-02-17 11:16:48,599 - Epoch: [35][  391/  391]    Overall Loss 1.606899    Objective Loss 1.606899    Top1 51.442308    Top5 86.057692    LR 0.100000    Time 0.096640    
2024-02-17 11:16:48,720 - --- validate (epoch=35)-----------
2024-02-17 11:16:48,721 - 10000 samples (128 per mini-batch)
2024-02-17 11:16:51,498 - Epoch: [35][   79/   79]    Loss 1.989054    Top1 45.770000    Top5 77.170000    
2024-02-17 11:16:51,605 - ==> Top1: 45.770    Top5: 77.170    Loss: 1.989

2024-02-17 11:16:51,617 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:16:51,617 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:16:51,689 - 

2024-02-17 11:16:51,689 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:17:01,795 - Epoch: [36][  100/  391]    Overall Loss 1.539836    Objective Loss 1.539836                                        LR 0.100000    Time 0.100988    
2024-02-17 11:17:11,281 - Epoch: [36][  200/  391]    Overall Loss 1.562597    Objective Loss 1.562597                                        LR 0.100000    Time 0.097900    
2024-02-17 11:17:20,777 - Epoch: [36][  300/  391]    Overall Loss 1.576128    Objective Loss 1.576128                                        LR 0.100000    Time 0.096906    
2024-02-17 11:17:29,772 - Epoch: [36][  391/  391]    Overall Loss 1.586168    Objective Loss 1.586168    Top1 52.403846    Top5 86.057692    LR 0.100000    Time 0.097349    
2024-02-17 11:17:29,906 - --- validate (epoch=36)-----------
2024-02-17 11:17:29,907 - 10000 samples (128 per mini-batch)
2024-02-17 11:17:32,847 - Epoch: [36][   79/   79]    Loss 1.807605    Top1 50.040000    Top5 81.320000    
2024-02-17 11:17:32,997 - ==> Top1: 50.040    Top5: 81.320    Loss: 1.808

2024-02-17 11:17:33,016 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:17:33,016 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:17:33,109 - 

2024-02-17 11:17:33,109 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:17:43,094 - Epoch: [37][  100/  391]    Overall Loss 1.568262    Objective Loss 1.568262                                        LR 0.100000    Time 0.099776    
2024-02-17 11:17:52,683 - Epoch: [37][  200/  391]    Overall Loss 1.577324    Objective Loss 1.577324                                        LR 0.100000    Time 0.097810    
2024-02-17 11:18:02,094 - Epoch: [37][  300/  391]    Overall Loss 1.573395    Objective Loss 1.573395                                        LR 0.100000    Time 0.096561    
2024-02-17 11:18:10,930 - Epoch: [37][  391/  391]    Overall Loss 1.579745    Objective Loss 1.579745    Top1 52.884615    Top5 84.134615    LR 0.100000    Time 0.096675    
2024-02-17 11:18:11,102 - --- validate (epoch=37)-----------
2024-02-17 11:18:11,103 - 10000 samples (128 per mini-batch)
2024-02-17 11:18:13,831 - Epoch: [37][   79/   79]    Loss 2.064832    Top1 45.690000    Top5 76.750000    
2024-02-17 11:18:13,961 - ==> Top1: 45.690    Top5: 76.750    Loss: 2.065

2024-02-17 11:18:13,981 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:18:13,982 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:18:14,056 - 

2024-02-17 11:18:14,057 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:18:22,704 - Epoch: [38][  100/  391]    Overall Loss 1.527588    Objective Loss 1.527588                                        LR 0.100000    Time 0.086405    
2024-02-17 11:18:32,212 - Epoch: [38][  200/  391]    Overall Loss 1.537939    Objective Loss 1.537939                                        LR 0.100000    Time 0.090716    
2024-02-17 11:18:41,493 - Epoch: [38][  300/  391]    Overall Loss 1.550885    Objective Loss 1.550885                                        LR 0.100000    Time 0.091399    
2024-02-17 11:18:50,304 - Epoch: [38][  391/  391]    Overall Loss 1.551893    Objective Loss 1.551893    Top1 51.923077    Top5 85.096154    LR 0.100000    Time 0.092652    
2024-02-17 11:18:50,430 - --- validate (epoch=38)-----------
2024-02-17 11:18:50,431 - 10000 samples (128 per mini-batch)
2024-02-17 11:18:53,056 - Epoch: [38][   79/   79]    Loss 1.979522    Top1 47.710000    Top5 78.950000    
2024-02-17 11:18:53,211 - ==> Top1: 47.710    Top5: 78.950    Loss: 1.980

2024-02-17 11:18:53,232 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:18:53,232 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:18:53,306 - 

2024-02-17 11:18:53,306 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:19:03,366 - Epoch: [39][  100/  391]    Overall Loss 1.531712    Objective Loss 1.531712                                        LR 0.100000    Time 0.100527    
2024-02-17 11:19:12,873 - Epoch: [39][  200/  391]    Overall Loss 1.542680    Objective Loss 1.542680                                        LR 0.100000    Time 0.097775    
2024-02-17 11:19:22,693 - Epoch: [39][  300/  391]    Overall Loss 1.548814    Objective Loss 1.548814                                        LR 0.100000    Time 0.097903    
2024-02-17 11:19:31,356 - Epoch: [39][  391/  391]    Overall Loss 1.550180    Objective Loss 1.550180    Top1 59.134615    Top5 87.019231    LR 0.100000    Time 0.097262    
2024-02-17 11:19:31,498 - --- validate (epoch=39)-----------
2024-02-17 11:19:31,499 - 10000 samples (128 per mini-batch)
2024-02-17 11:19:34,159 - Epoch: [39][   79/   79]    Loss 1.908074    Top1 47.730000    Top5 79.360000    
2024-02-17 11:19:34,333 - ==> Top1: 47.730    Top5: 79.360    Loss: 1.908

2024-02-17 11:19:34,353 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:19:34,353 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:19:34,429 - 

2024-02-17 11:19:34,429 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:19:44,776 - Epoch: [40][  100/  391]    Overall Loss 1.499845    Objective Loss 1.499845                                        LR 0.100000    Time 0.103390    
2024-02-17 11:19:54,295 - Epoch: [40][  200/  391]    Overall Loss 1.531605    Objective Loss 1.531605                                        LR 0.100000    Time 0.099272    
2024-02-17 11:20:03,731 - Epoch: [40][  300/  391]    Overall Loss 1.545265    Objective Loss 1.545265                                        LR 0.100000    Time 0.097618    
2024-02-17 11:20:12,052 - Epoch: [40][  391/  391]    Overall Loss 1.539392    Objective Loss 1.539392    Top1 58.173077    Top5 89.903846    LR 0.100000    Time 0.096168    
2024-02-17 11:20:12,163 - --- validate (epoch=40)-----------
2024-02-17 11:20:12,164 - 10000 samples (128 per mini-batch)
2024-02-17 11:20:14,789 - Epoch: [40][   79/   79]    Loss 1.845986    Top1 48.800000    Top5 80.410000    
2024-02-17 11:20:14,951 - ==> Top1: 48.800    Top5: 80.410    Loss: 1.846

2024-02-17 11:20:14,971 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:20:14,972 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:20:15,046 - 

2024-02-17 11:20:15,047 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:20:24,959 - Epoch: [41][  100/  391]    Overall Loss 1.485648    Objective Loss 1.485648                                        LR 0.100000    Time 0.099047    
2024-02-17 11:20:34,470 - Epoch: [41][  200/  391]    Overall Loss 1.491010    Objective Loss 1.491010                                        LR 0.100000    Time 0.097059    
2024-02-17 11:20:43,847 - Epoch: [41][  300/  391]    Overall Loss 1.509338    Objective Loss 1.509338                                        LR 0.100000    Time 0.095948    
2024-02-17 11:20:52,024 - Epoch: [41][  391/  391]    Overall Loss 1.520214    Objective Loss 1.520214    Top1 59.134615    Top5 88.942308    LR 0.100000    Time 0.094519    
2024-02-17 11:20:52,144 - --- validate (epoch=41)-----------
2024-02-17 11:20:52,145 - 10000 samples (128 per mini-batch)
2024-02-17 11:20:54,797 - Epoch: [41][   79/   79]    Loss 1.984574    Top1 46.930000    Top5 77.890000    
2024-02-17 11:20:54,896 - ==> Top1: 46.930    Top5: 77.890    Loss: 1.985

2024-02-17 11:20:54,915 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:20:54,915 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:20:54,990 - 

2024-02-17 11:20:54,990 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:21:04,943 - Epoch: [42][  100/  391]    Overall Loss 1.489325    Objective Loss 1.489325                                        LR 0.100000    Time 0.099459    
2024-02-17 11:21:14,427 - Epoch: [42][  200/  391]    Overall Loss 1.483331    Objective Loss 1.483331                                        LR 0.100000    Time 0.097129    
2024-02-17 11:21:23,810 - Epoch: [42][  300/  391]    Overall Loss 1.500461    Objective Loss 1.500461                                        LR 0.100000    Time 0.096015    
2024-02-17 11:21:32,572 - Epoch: [42][  391/  391]    Overall Loss 1.510962    Objective Loss 1.510962    Top1 56.250000    Top5 87.019231    LR 0.100000    Time 0.096067    
2024-02-17 11:21:32,715 - --- validate (epoch=42)-----------
2024-02-17 11:21:32,716 - 10000 samples (128 per mini-batch)
2024-02-17 11:21:35,691 - Epoch: [42][   79/   79]    Loss 1.952823    Top1 47.340000    Top5 78.560000    
2024-02-17 11:21:35,848 - ==> Top1: 47.340    Top5: 78.560    Loss: 1.953

2024-02-17 11:21:35,858 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:21:35,858 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:21:35,933 - 

2024-02-17 11:21:35,934 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:21:46,164 - Epoch: [43][  100/  391]    Overall Loss 1.457503    Objective Loss 1.457503                                        LR 0.100000    Time 0.102231    
2024-02-17 11:21:55,904 - Epoch: [43][  200/  391]    Overall Loss 1.475390    Objective Loss 1.475390                                        LR 0.100000    Time 0.099788    
2024-02-17 11:22:05,676 - Epoch: [43][  300/  391]    Overall Loss 1.492215    Objective Loss 1.492215                                        LR 0.100000    Time 0.099086    
2024-02-17 11:22:14,413 - Epoch: [43][  391/  391]    Overall Loss 1.502405    Objective Loss 1.502405    Top1 54.807692    Top5 82.211538    LR 0.100000    Time 0.098360    
2024-02-17 11:22:14,579 - --- validate (epoch=43)-----------
2024-02-17 11:22:14,579 - 10000 samples (128 per mini-batch)
2024-02-17 11:22:17,165 - Epoch: [43][   79/   79]    Loss 1.839245    Top1 48.860000    Top5 81.150000    
2024-02-17 11:22:17,289 - ==> Top1: 48.860    Top5: 81.150    Loss: 1.839

2024-02-17 11:22:17,309 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:22:17,309 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:22:17,384 - 

2024-02-17 11:22:17,384 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:22:27,698 - Epoch: [44][  100/  391]    Overall Loss 1.465794    Objective Loss 1.465794                                        LR 0.100000    Time 0.103067    
2024-02-17 11:22:37,177 - Epoch: [44][  200/  391]    Overall Loss 1.475357    Objective Loss 1.475357                                        LR 0.100000    Time 0.098905    
2024-02-17 11:22:46,717 - Epoch: [44][  300/  391]    Overall Loss 1.490347    Objective Loss 1.490347                                        LR 0.100000    Time 0.097723    
2024-02-17 11:22:55,335 - Epoch: [44][  391/  391]    Overall Loss 1.492887    Objective Loss 1.492887    Top1 54.807692    Top5 79.326923    LR 0.100000    Time 0.097011    
2024-02-17 11:22:55,479 - --- validate (epoch=44)-----------
2024-02-17 11:22:55,480 - 10000 samples (128 per mini-batch)
2024-02-17 11:22:58,048 - Epoch: [44][   79/   79]    Loss 1.917619    Top1 48.810000    Top5 79.840000    
2024-02-17 11:22:58,217 - ==> Top1: 48.810    Top5: 79.840    Loss: 1.918

2024-02-17 11:22:58,236 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:22:58,236 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:22:58,312 - 

2024-02-17 11:22:58,312 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:23:08,962 - Epoch: [45][  100/  391]    Overall Loss 1.433537    Objective Loss 1.433537                                        LR 0.100000    Time 0.106426    
2024-02-17 11:23:18,985 - Epoch: [45][  200/  391]    Overall Loss 1.449299    Objective Loss 1.449299                                        LR 0.100000    Time 0.103304    
2024-02-17 11:23:28,453 - Epoch: [45][  300/  391]    Overall Loss 1.465140    Objective Loss 1.465140                                        LR 0.100000    Time 0.100414    
2024-02-17 11:23:37,236 - Epoch: [45][  391/  391]    Overall Loss 1.475613    Objective Loss 1.475613    Top1 58.173077    Top5 87.019231    LR 0.100000    Time 0.099496    
2024-02-17 11:23:37,401 - --- validate (epoch=45)-----------
2024-02-17 11:23:37,402 - 10000 samples (128 per mini-batch)
2024-02-17 11:23:40,189 - Epoch: [45][   79/   79]    Loss 1.833996    Top1 49.530000    Top5 80.410000    
2024-02-17 11:23:40,315 - ==> Top1: 49.530    Top5: 80.410    Loss: 1.834

2024-02-17 11:23:40,333 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:23:40,334 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:23:40,407 - 

2024-02-17 11:23:40,408 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:23:50,603 - Epoch: [46][  100/  391]    Overall Loss 1.404987    Objective Loss 1.404987                                        LR 0.100000    Time 0.101882    
2024-02-17 11:24:00,249 - Epoch: [46][  200/  391]    Overall Loss 1.428167    Objective Loss 1.428167                                        LR 0.100000    Time 0.099146    
2024-02-17 11:24:09,610 - Epoch: [46][  300/  391]    Overall Loss 1.445878    Objective Loss 1.445878                                        LR 0.100000    Time 0.097287    
2024-02-17 11:24:18,449 - Epoch: [46][  391/  391]    Overall Loss 1.459151    Objective Loss 1.459151    Top1 62.019231    Top5 93.269231    LR 0.100000    Time 0.097240    
2024-02-17 11:24:18,574 - --- validate (epoch=46)-----------
2024-02-17 11:24:18,576 - 10000 samples (128 per mini-batch)
2024-02-17 11:24:21,577 - Epoch: [46][   79/   79]    Loss 1.885629    Top1 50.330000    Top5 80.910000    
2024-02-17 11:24:21,703 - ==> Top1: 50.330    Top5: 80.910    Loss: 1.886

2024-02-17 11:24:21,722 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:24:21,723 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:24:21,815 - 

2024-02-17 11:24:21,816 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:24:31,807 - Epoch: [47][  100/  391]    Overall Loss 1.427855    Objective Loss 1.427855                                        LR 0.100000    Time 0.099837    
2024-02-17 11:24:41,390 - Epoch: [47][  200/  391]    Overall Loss 1.431328    Objective Loss 1.431328                                        LR 0.100000    Time 0.097812    
2024-02-17 11:24:50,976 - Epoch: [47][  300/  391]    Overall Loss 1.449821    Objective Loss 1.449821                                        LR 0.100000    Time 0.097146    
2024-02-17 11:24:59,176 - Epoch: [47][  391/  391]    Overall Loss 1.456628    Objective Loss 1.456628    Top1 54.807692    Top5 90.384615    LR 0.100000    Time 0.095498    
2024-02-17 11:24:59,294 - --- validate (epoch=47)-----------
2024-02-17 11:24:59,295 - 10000 samples (128 per mini-batch)
2024-02-17 11:25:02,017 - Epoch: [47][   79/   79]    Loss 2.009425    Top1 47.200000    Top5 77.860000    
2024-02-17 11:25:02,143 - ==> Top1: 47.200    Top5: 77.860    Loss: 2.009

2024-02-17 11:25:02,161 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:25:02,162 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:25:02,237 - 

2024-02-17 11:25:02,237 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:25:12,387 - Epoch: [48][  100/  391]    Overall Loss 1.426738    Objective Loss 1.426738                                        LR 0.100000    Time 0.101425    
2024-02-17 11:25:22,016 - Epoch: [48][  200/  391]    Overall Loss 1.439196    Objective Loss 1.439196                                        LR 0.100000    Time 0.098837    
2024-02-17 11:25:31,618 - Epoch: [48][  300/  391]    Overall Loss 1.440461    Objective Loss 1.440461                                        LR 0.100000    Time 0.097885    
2024-02-17 11:25:40,195 - Epoch: [48][  391/  391]    Overall Loss 1.447062    Objective Loss 1.447062    Top1 54.807692    Top5 85.096154    LR 0.100000    Time 0.097029    
2024-02-17 11:25:40,316 - --- validate (epoch=48)-----------
2024-02-17 11:25:40,317 - 10000 samples (128 per mini-batch)
2024-02-17 11:25:42,999 - Epoch: [48][   79/   79]    Loss 1.940782    Top1 48.330000    Top5 79.390000    
2024-02-17 11:25:43,190 - ==> Top1: 48.330    Top5: 79.390    Loss: 1.941

2024-02-17 11:25:43,210 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:25:43,211 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:25:43,285 - 

2024-02-17 11:25:43,285 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:25:53,368 - Epoch: [49][  100/  391]    Overall Loss 1.422912    Objective Loss 1.422912                                        LR 0.100000    Time 0.100755    
2024-02-17 11:26:02,828 - Epoch: [49][  200/  391]    Overall Loss 1.418644    Objective Loss 1.418644                                        LR 0.100000    Time 0.097658    
2024-02-17 11:26:12,352 - Epoch: [49][  300/  391]    Overall Loss 1.426263    Objective Loss 1.426263                                        LR 0.100000    Time 0.096836    
2024-02-17 11:26:20,642 - Epoch: [49][  391/  391]    Overall Loss 1.433422    Objective Loss 1.433422    Top1 62.019231    Top5 87.019231    LR 0.100000    Time 0.095490    
2024-02-17 11:26:20,785 - --- validate (epoch=49)-----------
2024-02-17 11:26:20,786 - 10000 samples (128 per mini-batch)
2024-02-17 11:26:23,430 - Epoch: [49][   79/   79]    Loss 2.283702    Top1 42.440000    Top5 73.860000    
2024-02-17 11:26:23,591 - ==> Top1: 42.440    Top5: 73.860    Loss: 2.284

2024-02-17 11:26:23,611 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:26:23,611 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:26:23,685 - 

2024-02-17 11:26:23,685 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:26:33,826 - Epoch: [50][  100/  391]    Overall Loss 1.403427    Objective Loss 1.403427                                        LR 0.100000    Time 0.101334    
2024-02-17 11:26:42,305 - Epoch: [50][  200/  391]    Overall Loss 1.417837    Objective Loss 1.417837                                        LR 0.100000    Time 0.093042    
2024-02-17 11:26:49,780 - Epoch: [50][  300/  391]    Overall Loss 1.419865    Objective Loss 1.419865                                        LR 0.100000    Time 0.086931    
2024-02-17 11:26:58,244 - Epoch: [50][  391/  391]    Overall Loss 1.429393    Objective Loss 1.429393    Top1 57.211538    Top5 84.615385    LR 0.100000    Time 0.088337    
2024-02-17 11:26:58,352 - --- validate (epoch=50)-----------
2024-02-17 11:26:58,352 - 10000 samples (128 per mini-batch)
2024-02-17 11:27:01,174 - Epoch: [50][   79/   79]    Loss 1.889087    Top1 50.290000    Top5 79.830000    
2024-02-17 11:27:01,270 - ==> Top1: 50.290    Top5: 79.830    Loss: 1.889

2024-02-17 11:27:01,289 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:27:01,289 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:27:01,366 - 

2024-02-17 11:27:01,367 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:27:11,502 - Epoch: [51][  100/  391]    Overall Loss 1.368844    Objective Loss 1.368844                                        LR 0.100000    Time 0.101282    
2024-02-17 11:27:20,329 - Epoch: [51][  200/  391]    Overall Loss 1.400591    Objective Loss 1.400591                                        LR 0.100000    Time 0.094751    
2024-02-17 11:27:30,187 - Epoch: [51][  300/  391]    Overall Loss 1.412388    Objective Loss 1.412388                                        LR 0.100000    Time 0.096010    
2024-02-17 11:27:39,023 - Epoch: [51][  391/  391]    Overall Loss 1.425627    Objective Loss 1.425627    Top1 52.403846    Top5 84.615385    LR 0.100000    Time 0.096254    
2024-02-17 11:27:39,150 - --- validate (epoch=51)-----------
2024-02-17 11:27:39,151 - 10000 samples (128 per mini-batch)
2024-02-17 11:27:41,928 - Epoch: [51][   79/   79]    Loss 1.779124    Top1 51.110000    Top5 81.550000    
2024-02-17 11:27:42,094 - ==> Top1: 51.110    Top5: 81.550    Loss: 1.779

2024-02-17 11:27:42,114 - ==> Best [Top1: 51.110   Top5: 81.550   Sparsity:0.00   Params: 1341960 on epoch: 51]
2024-02-17 11:27:42,114 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:27:42,207 - 

2024-02-17 11:27:42,208 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:27:51,896 - Epoch: [52][  100/  391]    Overall Loss 1.356018    Objective Loss 1.356018                                        LR 0.100000    Time 0.096807    
2024-02-17 11:28:01,531 - Epoch: [52][  200/  391]    Overall Loss 1.368702    Objective Loss 1.368702                                        LR 0.100000    Time 0.096559    
2024-02-17 11:28:11,246 - Epoch: [52][  300/  391]    Overall Loss 1.390229    Objective Loss 1.390229                                        LR 0.100000    Time 0.096741    
2024-02-17 11:28:19,834 - Epoch: [52][  391/  391]    Overall Loss 1.396170    Objective Loss 1.396170    Top1 62.980769    Top5 86.538462    LR 0.100000    Time 0.096179    
2024-02-17 11:28:19,986 - --- validate (epoch=52)-----------
2024-02-17 11:28:19,987 - 10000 samples (128 per mini-batch)
2024-02-17 11:28:22,596 - Epoch: [52][   79/   79]    Loss 1.752732    Top1 51.550000    Top5 81.970000    
2024-02-17 11:28:22,794 - ==> Top1: 51.550    Top5: 81.970    Loss: 1.753

2024-02-17 11:28:22,814 - ==> Best [Top1: 51.550   Top5: 81.970   Sparsity:0.00   Params: 1341960 on epoch: 52]
2024-02-17 11:28:22,814 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:28:22,909 - 

2024-02-17 11:28:22,909 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:28:33,129 - Epoch: [53][  100/  391]    Overall Loss 1.352209    Objective Loss 1.352209                                        LR 0.100000    Time 0.102127    
2024-02-17 11:28:42,797 - Epoch: [53][  200/  391]    Overall Loss 1.385189    Objective Loss 1.385189                                        LR 0.100000    Time 0.099380    
2024-02-17 11:28:52,404 - Epoch: [53][  300/  391]    Overall Loss 1.395419    Objective Loss 1.395419                                        LR 0.100000    Time 0.098262    
2024-02-17 11:29:00,781 - Epoch: [53][  391/  391]    Overall Loss 1.400876    Objective Loss 1.400876    Top1 61.057692    Top5 88.461538    LR 0.100000    Time 0.096807    
2024-02-17 11:29:00,958 - --- validate (epoch=53)-----------
2024-02-17 11:29:00,958 - 10000 samples (128 per mini-batch)
2024-02-17 11:29:03,571 - Epoch: [53][   79/   79]    Loss 1.756776    Top1 52.330000    Top5 82.320000    
2024-02-17 11:29:03,721 - ==> Top1: 52.330    Top5: 82.320    Loss: 1.757

2024-02-17 11:29:03,744 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:29:03,744 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:29:03,835 - 

2024-02-17 11:29:03,836 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:29:13,810 - Epoch: [54][  100/  391]    Overall Loss 1.365076    Objective Loss 1.365076                                        LR 0.100000    Time 0.099664    
2024-02-17 11:29:22,907 - Epoch: [54][  200/  391]    Overall Loss 1.377045    Objective Loss 1.377045                                        LR 0.100000    Time 0.095297    
2024-02-17 11:29:32,578 - Epoch: [54][  300/  391]    Overall Loss 1.386387    Objective Loss 1.386387                                        LR 0.100000    Time 0.095753    
2024-02-17 11:29:41,023 - Epoch: [54][  391/  391]    Overall Loss 1.388969    Objective Loss 1.388969    Top1 61.057692    Top5 86.538462    LR 0.100000    Time 0.095057    
2024-02-17 11:29:41,200 - --- validate (epoch=54)-----------
2024-02-17 11:29:41,202 - 10000 samples (128 per mini-batch)
2024-02-17 11:29:44,145 - Epoch: [54][   79/   79]    Loss 1.958111    Top1 48.140000    Top5 78.360000    
2024-02-17 11:29:44,299 - ==> Top1: 48.140    Top5: 78.360    Loss: 1.958

2024-02-17 11:29:44,318 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:29:44,319 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:29:44,392 - 

2024-02-17 11:29:44,392 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:29:54,493 - Epoch: [55][  100/  391]    Overall Loss 1.354112    Objective Loss 1.354112                                        LR 0.100000    Time 0.100937    
2024-02-17 11:30:04,180 - Epoch: [55][  200/  391]    Overall Loss 1.363370    Objective Loss 1.363370                                        LR 0.100000    Time 0.098881    
2024-02-17 11:30:13,856 - Epoch: [55][  300/  391]    Overall Loss 1.373081    Objective Loss 1.373081                                        LR 0.100000    Time 0.098159    
2024-02-17 11:30:22,094 - Epoch: [55][  391/  391]    Overall Loss 1.380583    Objective Loss 1.380583    Top1 61.538462    Top5 89.903846    LR 0.100000    Time 0.096373    
2024-02-17 11:30:22,268 - --- validate (epoch=55)-----------
2024-02-17 11:30:22,269 - 10000 samples (128 per mini-batch)
2024-02-17 11:30:24,857 - Epoch: [55][   79/   79]    Loss 1.765769    Top1 51.630000    Top5 82.110000    
2024-02-17 11:30:25,010 - ==> Top1: 51.630    Top5: 82.110    Loss: 1.766

2024-02-17 11:30:25,029 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:30:25,029 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:30:25,105 - 

2024-02-17 11:30:25,105 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:30:35,511 - Epoch: [56][  100/  391]    Overall Loss 1.326227    Objective Loss 1.326227                                        LR 0.100000    Time 0.103980    
2024-02-17 11:30:45,108 - Epoch: [56][  200/  391]    Overall Loss 1.348147    Objective Loss 1.348147                                        LR 0.100000    Time 0.099955    
2024-02-17 11:30:54,629 - Epoch: [56][  300/  391]    Overall Loss 1.367657    Objective Loss 1.367657                                        LR 0.100000    Time 0.098360    
2024-02-17 11:31:03,185 - Epoch: [56][  391/  391]    Overall Loss 1.379064    Objective Loss 1.379064    Top1 57.692308    Top5 86.538462    LR 0.100000    Time 0.097339    
2024-02-17 11:31:03,366 - --- validate (epoch=56)-----------
2024-02-17 11:31:03,367 - 10000 samples (128 per mini-batch)
2024-02-17 11:31:06,018 - Epoch: [56][   79/   79]    Loss 1.785426    Top1 51.600000    Top5 81.980000    
2024-02-17 11:31:06,147 - ==> Top1: 51.600    Top5: 81.980    Loss: 1.785

2024-02-17 11:31:06,168 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:31:06,168 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:31:06,246 - 

2024-02-17 11:31:06,246 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:31:16,356 - Epoch: [57][  100/  391]    Overall Loss 1.349260    Objective Loss 1.349260                                        LR 0.100000    Time 0.101027    
2024-02-17 11:31:25,993 - Epoch: [57][  200/  391]    Overall Loss 1.363172    Objective Loss 1.363172                                        LR 0.100000    Time 0.098676    
2024-02-17 11:31:35,455 - Epoch: [57][  300/  391]    Overall Loss 1.365457    Objective Loss 1.365457                                        LR 0.100000    Time 0.097311    
2024-02-17 11:31:44,149 - Epoch: [57][  391/  391]    Overall Loss 1.366706    Objective Loss 1.366706    Top1 62.019231    Top5 88.942308    LR 0.100000    Time 0.096889    
2024-02-17 11:31:44,325 - --- validate (epoch=57)-----------
2024-02-17 11:31:44,326 - 10000 samples (128 per mini-batch)
2024-02-17 11:31:47,182 - Epoch: [57][   79/   79]    Loss 1.894377    Top1 49.460000    Top5 80.050000    
2024-02-17 11:31:47,293 - ==> Top1: 49.460    Top5: 80.050    Loss: 1.894

2024-02-17 11:31:47,312 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:31:47,312 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:31:47,390 - 

2024-02-17 11:31:47,390 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:31:57,728 - Epoch: [58][  100/  391]    Overall Loss 1.325014    Objective Loss 1.325014                                        LR 0.100000    Time 0.103306    
2024-02-17 11:32:07,230 - Epoch: [58][  200/  391]    Overall Loss 1.345946    Objective Loss 1.345946                                        LR 0.100000    Time 0.099140    
2024-02-17 11:32:16,570 - Epoch: [58][  300/  391]    Overall Loss 1.349931    Objective Loss 1.349931                                        LR 0.100000    Time 0.097210    
2024-02-17 11:32:25,438 - Epoch: [58][  391/  391]    Overall Loss 1.360621    Objective Loss 1.360621    Top1 62.980769    Top5 87.019231    LR 0.100000    Time 0.097257    
2024-02-17 11:32:25,560 - --- validate (epoch=58)-----------
2024-02-17 11:32:25,561 - 10000 samples (128 per mini-batch)
2024-02-17 11:32:28,432 - Epoch: [58][   79/   79]    Loss 1.667708    Top1 54.190000    Top5 83.330000    
2024-02-17 11:32:28,535 - ==> Top1: 54.190    Top5: 83.330    Loss: 1.668

2024-02-17 11:32:28,554 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:32:28,555 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:32:28,654 - 

2024-02-17 11:32:28,655 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:32:38,625 - Epoch: [59][  100/  391]    Overall Loss 1.308791    Objective Loss 1.308791                                        LR 0.100000    Time 0.099621    
2024-02-17 11:32:48,396 - Epoch: [59][  200/  391]    Overall Loss 1.326215    Objective Loss 1.326215                                        LR 0.100000    Time 0.098642    
2024-02-17 11:32:57,963 - Epoch: [59][  300/  391]    Overall Loss 1.341077    Objective Loss 1.341077                                        LR 0.100000    Time 0.097638    
2024-02-17 11:33:06,565 - Epoch: [59][  391/  391]    Overall Loss 1.348947    Objective Loss 1.348947    Top1 62.500000    Top5 88.942308    LR 0.100000    Time 0.096904    
2024-02-17 11:33:06,730 - --- validate (epoch=59)-----------
2024-02-17 11:33:06,731 - 10000 samples (128 per mini-batch)
2024-02-17 11:33:09,414 - Epoch: [59][   79/   79]    Loss 1.826966    Top1 50.750000    Top5 81.150000    
2024-02-17 11:33:09,599 - ==> Top1: 50.750    Top5: 81.150    Loss: 1.827

2024-02-17 11:33:09,619 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:33:09,619 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:33:09,694 - 

2024-02-17 11:33:09,694 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:33:20,052 - Epoch: [60][  100/  391]    Overall Loss 1.305665    Objective Loss 1.305665                                        LR 0.100000    Time 0.103498    
2024-02-17 11:33:29,658 - Epoch: [60][  200/  391]    Overall Loss 1.311440    Objective Loss 1.311440                                        LR 0.100000    Time 0.099759    
2024-02-17 11:33:39,414 - Epoch: [60][  300/  391]    Overall Loss 1.328394    Objective Loss 1.328394                                        LR 0.100000    Time 0.099011    
2024-02-17 11:33:48,055 - Epoch: [60][  391/  391]    Overall Loss 1.339107    Objective Loss 1.339107    Top1 63.942308    Top5 88.461538    LR 0.100000    Time 0.098056    
2024-02-17 11:33:48,220 - --- validate (epoch=60)-----------
2024-02-17 11:33:48,221 - 10000 samples (128 per mini-batch)
2024-02-17 11:33:50,919 - Epoch: [60][   79/   79]    Loss 1.691084    Top1 52.590000    Top5 83.400000    
2024-02-17 11:33:51,036 - ==> Top1: 52.590    Top5: 83.400    Loss: 1.691

2024-02-17 11:33:51,055 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:33:51,056 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:33:51,131 - 

2024-02-17 11:33:51,131 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:34:00,865 - Epoch: [61][  100/  391]    Overall Loss 1.296610    Objective Loss 1.296610                                        LR 0.100000    Time 0.097267    
2024-02-17 11:34:10,492 - Epoch: [61][  200/  391]    Overall Loss 1.326394    Objective Loss 1.326394                                        LR 0.100000    Time 0.096746    
2024-02-17 11:34:20,348 - Epoch: [61][  300/  391]    Overall Loss 1.333381    Objective Loss 1.333381                                        LR 0.100000    Time 0.097332    
2024-02-17 11:34:28,665 - Epoch: [61][  391/  391]    Overall Loss 1.339205    Objective Loss 1.339205    Top1 61.538462    Top5 86.538462    LR 0.100000    Time 0.095941    
2024-02-17 11:34:28,833 - --- validate (epoch=61)-----------
2024-02-17 11:34:28,834 - 10000 samples (128 per mini-batch)
2024-02-17 11:34:31,458 - Epoch: [61][   79/   79]    Loss 1.818900    Top1 50.560000    Top5 81.340000    
2024-02-17 11:34:31,630 - ==> Top1: 50.560    Top5: 81.340    Loss: 1.819

2024-02-17 11:34:31,649 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:34:31,649 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:34:31,724 - 

2024-02-17 11:34:31,725 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:34:40,001 - Epoch: [62][  100/  391]    Overall Loss 1.288146    Objective Loss 1.288146                                        LR 0.100000    Time 0.082694    
2024-02-17 11:34:47,497 - Epoch: [62][  200/  391]    Overall Loss 1.302987    Objective Loss 1.302987                                        LR 0.100000    Time 0.078808    
2024-02-17 11:34:54,979 - Epoch: [62][  300/  391]    Overall Loss 1.328770    Objective Loss 1.328770                                        LR 0.100000    Time 0.077469    
2024-02-17 11:35:03,293 - Epoch: [62][  391/  391]    Overall Loss 1.332459    Objective Loss 1.332459    Top1 58.173077    Top5 86.538462    LR 0.100000    Time 0.080692    
2024-02-17 11:35:03,411 - --- validate (epoch=62)-----------
2024-02-17 11:35:03,412 - 10000 samples (128 per mini-batch)
2024-02-17 11:35:06,262 - Epoch: [62][   79/   79]    Loss 1.716351    Top1 52.900000    Top5 82.810000    
2024-02-17 11:35:06,374 - ==> Top1: 52.900    Top5: 82.810    Loss: 1.716

2024-02-17 11:35:06,396 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:35:06,397 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:35:06,478 - 

2024-02-17 11:35:06,478 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:35:16,653 - Epoch: [63][  100/  391]    Overall Loss 1.285306    Objective Loss 1.285306                                        LR 0.100000    Time 0.101678    
2024-02-17 11:35:26,217 - Epoch: [63][  200/  391]    Overall Loss 1.292782    Objective Loss 1.292782                                        LR 0.100000    Time 0.098635    
2024-02-17 11:35:35,652 - Epoch: [63][  300/  391]    Overall Loss 1.312682    Objective Loss 1.312682                                        LR 0.100000    Time 0.097192    
2024-02-17 11:35:44,448 - Epoch: [63][  391/  391]    Overall Loss 1.317608    Objective Loss 1.317608    Top1 61.057692    Top5 87.980769    LR 0.100000    Time 0.097059    
2024-02-17 11:35:44,584 - --- validate (epoch=63)-----------
2024-02-17 11:35:44,585 - 10000 samples (128 per mini-batch)
2024-02-17 11:35:47,494 - Epoch: [63][   79/   79]    Loss 1.956995    Top1 48.160000    Top5 78.860000    
2024-02-17 11:35:47,695 - ==> Top1: 48.160    Top5: 78.860    Loss: 1.957

2024-02-17 11:35:47,715 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:35:47,715 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:35:47,790 - 

2024-02-17 11:35:47,790 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:35:57,871 - Epoch: [64][  100/  391]    Overall Loss 1.297899    Objective Loss 1.297899                                        LR 0.100000    Time 0.100735    
2024-02-17 11:36:07,528 - Epoch: [64][  200/  391]    Overall Loss 1.311936    Objective Loss 1.311936                                        LR 0.100000    Time 0.098628    
2024-02-17 11:36:17,234 - Epoch: [64][  300/  391]    Overall Loss 1.316976    Objective Loss 1.316976                                        LR 0.100000    Time 0.098091    
2024-02-17 11:36:25,204 - Epoch: [64][  391/  391]    Overall Loss 1.320537    Objective Loss 1.320537    Top1 63.461538    Top5 92.307692    LR 0.100000    Time 0.095637    
2024-02-17 11:36:25,336 - --- validate (epoch=64)-----------
2024-02-17 11:36:25,337 - 10000 samples (128 per mini-batch)
2024-02-17 11:36:28,688 - Epoch: [64][   79/   79]    Loss 1.750812    Top1 51.930000    Top5 82.150000    
2024-02-17 11:36:28,815 - ==> Top1: 51.930    Top5: 82.150    Loss: 1.751

2024-02-17 11:36:28,828 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:36:28,828 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:36:28,903 - 

2024-02-17 11:36:28,903 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:36:39,092 - Epoch: [65][  100/  391]    Overall Loss 1.261279    Objective Loss 1.261279                                        LR 0.100000    Time 0.101808    
2024-02-17 11:36:48,717 - Epoch: [65][  200/  391]    Overall Loss 1.285149    Objective Loss 1.285149                                        LR 0.100000    Time 0.099006    
2024-02-17 11:36:58,209 - Epoch: [65][  300/  391]    Overall Loss 1.301239    Objective Loss 1.301239                                        LR 0.100000    Time 0.097631    
2024-02-17 11:37:06,957 - Epoch: [65][  391/  391]    Overall Loss 1.310330    Objective Loss 1.310330    Top1 60.096154    Top5 87.019231    LR 0.100000    Time 0.097271    
2024-02-17 11:37:07,067 - --- validate (epoch=65)-----------
2024-02-17 11:37:07,068 - 10000 samples (128 per mini-batch)
2024-02-17 11:37:09,941 - Epoch: [65][   79/   79]    Loss 1.815695    Top1 51.080000    Top5 80.620000    
2024-02-17 11:37:10,055 - ==> Top1: 51.080    Top5: 80.620    Loss: 1.816

2024-02-17 11:37:10,073 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:37:10,074 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:37:10,154 - 

2024-02-17 11:37:10,155 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:37:20,699 - Epoch: [66][  100/  391]    Overall Loss 1.250810    Objective Loss 1.250810                                        LR 0.100000    Time 0.105371    
2024-02-17 11:37:30,064 - Epoch: [66][  200/  391]    Overall Loss 1.267830    Objective Loss 1.267830                                        LR 0.100000    Time 0.099487    
2024-02-17 11:37:37,274 - Epoch: [66][  300/  391]    Overall Loss 1.283463    Objective Loss 1.283463                                        LR 0.100000    Time 0.090349    
2024-02-17 11:37:44,197 - Epoch: [66][  391/  391]    Overall Loss 1.292746    Objective Loss 1.292746    Top1 49.519231    Top5 80.769231    LR 0.100000    Time 0.087017    
2024-02-17 11:37:44,338 - --- validate (epoch=66)-----------
2024-02-17 11:37:44,339 - 10000 samples (128 per mini-batch)
2024-02-17 11:37:47,090 - Epoch: [66][   79/   79]    Loss 1.934755    Top1 49.240000    Top5 80.590000    
2024-02-17 11:37:47,238 - ==> Top1: 49.240    Top5: 80.590    Loss: 1.935

2024-02-17 11:37:47,251 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:37:47,252 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:37:47,346 - 

2024-02-17 11:37:47,346 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:37:57,435 - Epoch: [67][  100/  391]    Overall Loss 1.269191    Objective Loss 1.269191                                        LR 0.100000    Time 0.100793    
2024-02-17 11:38:06,964 - Epoch: [67][  200/  391]    Overall Loss 1.276007    Objective Loss 1.276007                                        LR 0.100000    Time 0.098023    
2024-02-17 11:38:16,566 - Epoch: [67][  300/  391]    Overall Loss 1.285233    Objective Loss 1.285233                                        LR 0.100000    Time 0.097341    
2024-02-17 11:38:24,098 - Epoch: [67][  391/  391]    Overall Loss 1.298565    Objective Loss 1.298565    Top1 66.346154    Top5 89.423077    LR 0.100000    Time 0.093938    
2024-02-17 11:38:24,275 - --- validate (epoch=67)-----------
2024-02-17 11:38:24,275 - 10000 samples (128 per mini-batch)
2024-02-17 11:38:27,010 - Epoch: [67][   79/   79]    Loss 1.758080    Top1 52.600000    Top5 82.260000    
2024-02-17 11:38:27,159 - ==> Top1: 52.600    Top5: 82.260    Loss: 1.758

2024-02-17 11:38:27,176 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:38:27,176 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:38:27,252 - 

2024-02-17 11:38:27,253 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:38:36,830 - Epoch: [68][  100/  391]    Overall Loss 1.250863    Objective Loss 1.250863                                        LR 0.100000    Time 0.095706    
2024-02-17 11:38:46,349 - Epoch: [68][  200/  391]    Overall Loss 1.268339    Objective Loss 1.268339                                        LR 0.100000    Time 0.095424    
2024-02-17 11:38:55,966 - Epoch: [68][  300/  391]    Overall Loss 1.273696    Objective Loss 1.273696                                        LR 0.100000    Time 0.095659    
2024-02-17 11:39:04,649 - Epoch: [68][  391/  391]    Overall Loss 1.285888    Objective Loss 1.285888    Top1 61.538462    Top5 90.865385    LR 0.100000    Time 0.095590    
2024-02-17 11:39:04,773 - --- validate (epoch=68)-----------
2024-02-17 11:39:04,773 - 10000 samples (128 per mini-batch)
2024-02-17 11:39:07,582 - Epoch: [68][   79/   79]    Loss 1.862298    Top1 50.420000    Top5 80.730000    
2024-02-17 11:39:07,709 - ==> Top1: 50.420    Top5: 80.730    Loss: 1.862

2024-02-17 11:39:07,727 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:39:07,728 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:39:07,802 - 

2024-02-17 11:39:07,803 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:39:17,573 - Epoch: [69][  100/  391]    Overall Loss 1.257213    Objective Loss 1.257213                                        LR 0.100000    Time 0.097625    
2024-02-17 11:39:26,708 - Epoch: [69][  200/  391]    Overall Loss 1.259300    Objective Loss 1.259300                                        LR 0.100000    Time 0.094468    
2024-02-17 11:39:36,322 - Epoch: [69][  300/  391]    Overall Loss 1.275271    Objective Loss 1.275271                                        LR 0.100000    Time 0.095008    
2024-02-17 11:39:44,955 - Epoch: [69][  391/  391]    Overall Loss 1.284235    Objective Loss 1.284235    Top1 63.942308    Top5 87.980769    LR 0.100000    Time 0.094963    
2024-02-17 11:39:45,069 - --- validate (epoch=69)-----------
2024-02-17 11:39:45,070 - 10000 samples (128 per mini-batch)
2024-02-17 11:39:47,863 - Epoch: [69][   79/   79]    Loss 1.776082    Top1 51.820000    Top5 82.050000    
2024-02-17 11:39:48,022 - ==> Top1: 51.820    Top5: 82.050    Loss: 1.776

2024-02-17 11:39:48,041 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:39:48,041 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:39:48,115 - 

2024-02-17 11:39:48,116 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:39:58,571 - Epoch: [70][  100/  391]    Overall Loss 1.246878    Objective Loss 1.246878                                        LR 0.100000    Time 0.104474    
2024-02-17 11:40:08,102 - Epoch: [70][  200/  391]    Overall Loss 1.248874    Objective Loss 1.248874                                        LR 0.100000    Time 0.099872    
2024-02-17 11:40:17,578 - Epoch: [70][  300/  391]    Overall Loss 1.265239    Objective Loss 1.265239                                        LR 0.100000    Time 0.098153    
2024-02-17 11:40:26,417 - Epoch: [70][  391/  391]    Overall Loss 1.268964    Objective Loss 1.268964    Top1 63.461538    Top5 88.461538    LR 0.100000    Time 0.097904    
2024-02-17 11:40:26,553 - --- validate (epoch=70)-----------
2024-02-17 11:40:26,554 - 10000 samples (128 per mini-batch)
2024-02-17 11:40:29,255 - Epoch: [70][   79/   79]    Loss 1.651100    Top1 54.200000    Top5 84.270000    
2024-02-17 11:40:29,360 - ==> Top1: 54.200    Top5: 84.270    Loss: 1.651

2024-02-17 11:40:29,380 - ==> Best [Top1: 54.200   Top5: 84.270   Sparsity:0.00   Params: 1341960 on epoch: 70]
2024-02-17 11:40:29,380 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:40:29,474 - 

2024-02-17 11:40:29,474 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:40:39,601 - Epoch: [71][  100/  391]    Overall Loss 1.214183    Objective Loss 1.214183                                        LR 0.100000    Time 0.101195    
2024-02-17 11:40:49,124 - Epoch: [71][  200/  391]    Overall Loss 1.237423    Objective Loss 1.237423                                        LR 0.100000    Time 0.098191    
2024-02-17 11:40:58,637 - Epoch: [71][  300/  391]    Overall Loss 1.252054    Objective Loss 1.252054                                        LR 0.100000    Time 0.097155    
2024-02-17 11:41:07,490 - Epoch: [71][  391/  391]    Overall Loss 1.269942    Objective Loss 1.269942    Top1 62.500000    Top5 92.307692    LR 0.100000    Time 0.097175    
2024-02-17 11:41:07,660 - --- validate (epoch=71)-----------
2024-02-17 11:41:07,661 - 10000 samples (128 per mini-batch)
2024-02-17 11:41:10,305 - Epoch: [71][   79/   79]    Loss 1.664598    Top1 54.580000    Top5 83.680000    
2024-02-17 11:41:10,401 - ==> Top1: 54.580    Top5: 83.680    Loss: 1.665

2024-02-17 11:41:10,420 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:41:10,420 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:41:10,513 - 

2024-02-17 11:41:10,513 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:41:20,510 - Epoch: [72][  100/  391]    Overall Loss 1.228445    Objective Loss 1.228445                                        LR 0.100000    Time 0.099897    
2024-02-17 11:41:30,202 - Epoch: [72][  200/  391]    Overall Loss 1.231338    Objective Loss 1.231338                                        LR 0.100000    Time 0.098387    
2024-02-17 11:41:40,247 - Epoch: [72][  300/  391]    Overall Loss 1.256302    Objective Loss 1.256302                                        LR 0.100000    Time 0.099059    
2024-02-17 11:41:49,333 - Epoch: [72][  391/  391]    Overall Loss 1.258400    Objective Loss 1.258400    Top1 60.576923    Top5 88.461538    LR 0.100000    Time 0.099229    
2024-02-17 11:41:49,455 - --- validate (epoch=72)-----------
2024-02-17 11:41:49,456 - 10000 samples (128 per mini-batch)
2024-02-17 11:41:52,180 - Epoch: [72][   79/   79]    Loss 1.766127    Top1 52.280000    Top5 81.960000    
2024-02-17 11:41:52,351 - ==> Top1: 52.280    Top5: 81.960    Loss: 1.766

2024-02-17 11:41:52,373 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:41:52,373 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:41:52,456 - 

2024-02-17 11:41:52,457 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:42:01,569 - Epoch: [73][  100/  391]    Overall Loss 1.235244    Objective Loss 1.235244                                        LR 0.100000    Time 0.091049    
2024-02-17 11:42:10,814 - Epoch: [73][  200/  391]    Overall Loss 1.225271    Objective Loss 1.225271                                        LR 0.100000    Time 0.091727    
2024-02-17 11:42:20,250 - Epoch: [73][  300/  391]    Overall Loss 1.248749    Objective Loss 1.248749                                        LR 0.100000    Time 0.092589    
2024-02-17 11:42:28,902 - Epoch: [73][  391/  391]    Overall Loss 1.258203    Objective Loss 1.258203    Top1 62.019231    Top5 92.307692    LR 0.100000    Time 0.093159    
2024-02-17 11:42:29,032 - --- validate (epoch=73)-----------
2024-02-17 11:42:29,033 - 10000 samples (128 per mini-batch)
2024-02-17 11:42:31,646 - Epoch: [73][   79/   79]    Loss 1.792526    Top1 52.330000    Top5 81.370000    
2024-02-17 11:42:31,828 - ==> Top1: 52.330    Top5: 81.370    Loss: 1.793

2024-02-17 11:42:31,846 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:42:31,846 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:42:31,922 - 

2024-02-17 11:42:31,922 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:42:42,337 - Epoch: [74][  100/  391]    Overall Loss 1.213928    Objective Loss 1.213928                                        LR 0.100000    Time 0.104064    
2024-02-17 11:42:51,930 - Epoch: [74][  200/  391]    Overall Loss 1.237431    Objective Loss 1.237431                                        LR 0.100000    Time 0.099976    
2024-02-17 11:43:01,516 - Epoch: [74][  300/  391]    Overall Loss 1.250356    Objective Loss 1.250356                                        LR 0.100000    Time 0.098588    
2024-02-17 11:43:09,979 - Epoch: [74][  391/  391]    Overall Loss 1.257349    Objective Loss 1.257349    Top1 62.980769    Top5 90.865385    LR 0.100000    Time 0.097277    
2024-02-17 11:43:10,157 - --- validate (epoch=74)-----------
2024-02-17 11:43:10,158 - 10000 samples (128 per mini-batch)
2024-02-17 11:43:12,856 - Epoch: [74][   79/   79]    Loss 1.893712    Top1 50.480000    Top5 80.250000    
2024-02-17 11:43:12,967 - ==> Top1: 50.480    Top5: 80.250    Loss: 1.894

2024-02-17 11:43:12,990 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:43:12,990 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:43:13,071 - 

2024-02-17 11:43:13,072 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:43:23,386 - Epoch: [75][  100/  391]    Overall Loss 1.223472    Objective Loss 1.223472                                        LR 0.100000    Time 0.103027    
2024-02-17 11:43:32,976 - Epoch: [75][  200/  391]    Overall Loss 1.242622    Objective Loss 1.242622                                        LR 0.100000    Time 0.099436    
2024-02-17 11:43:42,486 - Epoch: [75][  300/  391]    Overall Loss 1.249421    Objective Loss 1.249421                                        LR 0.100000    Time 0.097976    
2024-02-17 11:43:50,984 - Epoch: [75][  391/  391]    Overall Loss 1.251632    Objective Loss 1.251632    Top1 62.019231    Top5 87.500000    LR 0.100000    Time 0.096897    
2024-02-17 11:43:51,114 - --- validate (epoch=75)-----------
2024-02-17 11:43:51,115 - 10000 samples (128 per mini-batch)
2024-02-17 11:43:53,955 - Epoch: [75][   79/   79]    Loss 1.653270    Top1 54.870000    Top5 83.890000    
2024-02-17 11:43:54,083 - ==> Top1: 54.870    Top5: 83.890    Loss: 1.653

2024-02-17 11:43:54,105 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:43:54,106 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:43:54,215 - 

2024-02-17 11:43:54,215 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:44:04,708 - Epoch: [76][  100/  391]    Overall Loss 1.170750    Objective Loss 1.170750                                        LR 0.100000    Time 0.104836    
2024-02-17 11:44:14,315 - Epoch: [76][  200/  391]    Overall Loss 1.213522    Objective Loss 1.213522                                        LR 0.100000    Time 0.100429    
2024-02-17 11:44:23,857 - Epoch: [76][  300/  391]    Overall Loss 1.231686    Objective Loss 1.231686                                        LR 0.100000    Time 0.098745    
2024-02-17 11:44:32,587 - Epoch: [76][  391/  391]    Overall Loss 1.240826    Objective Loss 1.240826    Top1 67.788462    Top5 90.865385    LR 0.100000    Time 0.098081    
2024-02-17 11:44:32,723 - --- validate (epoch=76)-----------
2024-02-17 11:44:32,724 - 10000 samples (128 per mini-batch)
2024-02-17 11:44:35,621 - Epoch: [76][   79/   79]    Loss 1.814758    Top1 51.530000    Top5 81.440000    
2024-02-17 11:44:35,793 - ==> Top1: 51.530    Top5: 81.440    Loss: 1.815

2024-02-17 11:44:35,823 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:44:35,824 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:44:35,910 - 

2024-02-17 11:44:35,910 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:44:45,758 - Epoch: [77][  100/  391]    Overall Loss 1.180127    Objective Loss 1.180127                                        LR 0.100000    Time 0.098409    
2024-02-17 11:44:55,271 - Epoch: [77][  200/  391]    Overall Loss 1.209763    Objective Loss 1.209763                                        LR 0.100000    Time 0.096748    
2024-02-17 11:45:04,697 - Epoch: [77][  300/  391]    Overall Loss 1.223902    Objective Loss 1.223902                                        LR 0.100000    Time 0.095903    
2024-02-17 11:45:13,499 - Epoch: [77][  391/  391]    Overall Loss 1.233894    Objective Loss 1.233894    Top1 61.538462    Top5 92.307692    LR 0.100000    Time 0.096085    
2024-02-17 11:45:13,705 - --- validate (epoch=77)-----------
2024-02-17 11:45:13,706 - 10000 samples (128 per mini-batch)
2024-02-17 11:45:16,580 - Epoch: [77][   79/   79]    Loss 1.657645    Top1 54.100000    Top5 83.660000    
2024-02-17 11:45:16,708 - ==> Top1: 54.100    Top5: 83.660    Loss: 1.658

2024-02-17 11:45:16,727 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:45:16,728 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:45:16,805 - 

2024-02-17 11:45:16,805 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:45:27,006 - Epoch: [78][  100/  391]    Overall Loss 1.201644    Objective Loss 1.201644                                        LR 0.100000    Time 0.101940    
2024-02-17 11:45:36,505 - Epoch: [78][  200/  391]    Overall Loss 1.203288    Objective Loss 1.203288                                        LR 0.100000    Time 0.098441    
2024-02-17 11:45:46,228 - Epoch: [78][  300/  391]    Overall Loss 1.218297    Objective Loss 1.218297                                        LR 0.100000    Time 0.098022    
2024-02-17 11:45:54,870 - Epoch: [78][  391/  391]    Overall Loss 1.222866    Objective Loss 1.222866    Top1 61.057692    Top5 92.307692    LR 0.100000    Time 0.097301    
2024-02-17 11:45:55,023 - --- validate (epoch=78)-----------
2024-02-17 11:45:55,023 - 10000 samples (128 per mini-batch)
2024-02-17 11:45:57,915 - Epoch: [78][   79/   79]    Loss 1.797887    Top1 52.410000    Top5 81.490000    
2024-02-17 11:45:58,044 - ==> Top1: 52.410    Top5: 81.490    Loss: 1.798

2024-02-17 11:45:58,063 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:45:58,064 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:45:58,139 - 

2024-02-17 11:45:58,139 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:46:08,116 - Epoch: [79][  100/  391]    Overall Loss 1.185781    Objective Loss 1.185781                                        LR 0.100000    Time 0.099695    
2024-02-17 11:46:17,638 - Epoch: [79][  200/  391]    Overall Loss 1.206895    Objective Loss 1.206895                                        LR 0.100000    Time 0.097434    
2024-02-17 11:46:27,176 - Epoch: [79][  300/  391]    Overall Loss 1.223145    Objective Loss 1.223145                                        LR 0.100000    Time 0.096734    
2024-02-17 11:46:35,760 - Epoch: [79][  391/  391]    Overall Loss 1.228430    Objective Loss 1.228430    Top1 60.576923    Top5 89.903846    LR 0.100000    Time 0.096162    
2024-02-17 11:46:35,892 - --- validate (epoch=79)-----------
2024-02-17 11:46:35,893 - 10000 samples (128 per mini-batch)
2024-02-17 11:46:38,684 - Epoch: [79][   79/   79]    Loss 1.761844    Top1 52.140000    Top5 82.710000    
2024-02-17 11:46:38,822 - ==> Top1: 52.140    Top5: 82.710    Loss: 1.762

2024-02-17 11:46:38,842 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:46:38,842 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:46:38,919 - 

2024-02-17 11:46:38,919 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:46:48,987 - Epoch: [80][  100/  391]    Overall Loss 1.183672    Objective Loss 1.183672                                        LR 0.100000    Time 0.100604    
2024-02-17 11:46:58,702 - Epoch: [80][  200/  391]    Overall Loss 1.188290    Objective Loss 1.188290                                        LR 0.100000    Time 0.098854    
2024-02-17 11:47:07,965 - Epoch: [80][  300/  391]    Overall Loss 1.207868    Objective Loss 1.207868                                        LR 0.100000    Time 0.096764    
2024-02-17 11:47:16,691 - Epoch: [80][  391/  391]    Overall Loss 1.216519    Objective Loss 1.216519    Top1 59.615385    Top5 87.019231    LR 0.100000    Time 0.096551    
2024-02-17 11:47:16,832 - --- validate (epoch=80)-----------
2024-02-17 11:47:16,833 - 10000 samples (128 per mini-batch)
2024-02-17 11:47:19,904 - Epoch: [80][   79/   79]    Loss 1.780596    Top1 51.910000    Top5 82.270000    
2024-02-17 11:47:20,049 - ==> Top1: 51.910    Top5: 82.270    Loss: 1.781

2024-02-17 11:47:20,066 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:47:20,067 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:47:20,150 - 

2024-02-17 11:47:20,150 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:47:30,391 - Epoch: [81][  100/  391]    Overall Loss 1.154308    Objective Loss 1.154308                                        LR 0.100000    Time 0.102323    
2024-02-17 11:47:39,880 - Epoch: [81][  200/  391]    Overall Loss 1.192567    Objective Loss 1.192567                                        LR 0.100000    Time 0.098589    
2024-02-17 11:47:49,590 - Epoch: [81][  300/  391]    Overall Loss 1.212697    Objective Loss 1.212697                                        LR 0.100000    Time 0.098076    
2024-02-17 11:47:58,384 - Epoch: [81][  391/  391]    Overall Loss 1.216576    Objective Loss 1.216576    Top1 65.865385    Top5 88.461538    LR 0.100000    Time 0.097730    
2024-02-17 11:47:58,503 - --- validate (epoch=81)-----------
2024-02-17 11:47:58,504 - 10000 samples (128 per mini-batch)
2024-02-17 11:48:01,298 - Epoch: [81][   79/   79]    Loss 1.901255    Top1 50.050000    Top5 80.380000    
2024-02-17 11:48:01,417 - ==> Top1: 50.050    Top5: 80.380    Loss: 1.901

2024-02-17 11:48:01,428 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:48:01,428 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:48:01,504 - 

2024-02-17 11:48:01,504 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:48:11,886 - Epoch: [82][  100/  391]    Overall Loss 1.181685    Objective Loss 1.181685                                        LR 0.100000    Time 0.103734    
2024-02-17 11:48:21,478 - Epoch: [82][  200/  391]    Overall Loss 1.191637    Objective Loss 1.191637                                        LR 0.100000    Time 0.099809    
2024-02-17 11:48:31,066 - Epoch: [82][  300/  391]    Overall Loss 1.195987    Objective Loss 1.195987                                        LR 0.100000    Time 0.098483    
2024-02-17 11:48:39,700 - Epoch: [82][  391/  391]    Overall Loss 1.205439    Objective Loss 1.205439    Top1 65.384615    Top5 88.461538    LR 0.100000    Time 0.097635    
2024-02-17 11:48:39,862 - --- validate (epoch=82)-----------
2024-02-17 11:48:39,863 - 10000 samples (128 per mini-batch)
2024-02-17 11:48:42,829 - Epoch: [82][   79/   79]    Loss 1.780336    Top1 52.640000    Top5 82.950000    
2024-02-17 11:48:42,937 - ==> Top1: 52.640    Top5: 82.950    Loss: 1.780

2024-02-17 11:48:42,958 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:48:42,958 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:48:43,034 - 

2024-02-17 11:48:43,035 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:48:53,159 - Epoch: [83][  100/  391]    Overall Loss 1.189221    Objective Loss 1.189221                                        LR 0.100000    Time 0.101165    
2024-02-17 11:49:02,228 - Epoch: [83][  200/  391]    Overall Loss 1.189462    Objective Loss 1.189462                                        LR 0.100000    Time 0.095908    
2024-02-17 11:49:09,849 - Epoch: [83][  300/  391]    Overall Loss 1.210404    Objective Loss 1.210404                                        LR 0.100000    Time 0.089329    
2024-02-17 11:49:16,852 - Epoch: [83][  391/  391]    Overall Loss 1.212851    Objective Loss 1.212851    Top1 72.596154    Top5 90.865385    LR 0.100000    Time 0.086441    
2024-02-17 11:49:17,040 - --- validate (epoch=83)-----------
2024-02-17 11:49:17,041 - 10000 samples (128 per mini-batch)
2024-02-17 11:49:19,683 - Epoch: [83][   79/   79]    Loss 1.912212    Top1 49.810000    Top5 80.350000    
2024-02-17 11:49:19,782 - ==> Top1: 49.810    Top5: 80.350    Loss: 1.912

2024-02-17 11:49:19,799 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:49:19,800 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:49:19,880 - 

2024-02-17 11:49:19,880 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:49:29,567 - Epoch: [84][  100/  391]    Overall Loss 1.144011    Objective Loss 1.144011                                        LR 0.100000    Time 0.096783    
2024-02-17 11:49:39,473 - Epoch: [84][  200/  391]    Overall Loss 1.156792    Objective Loss 1.156792                                        LR 0.100000    Time 0.097897    
2024-02-17 11:49:49,070 - Epoch: [84][  300/  391]    Overall Loss 1.189619    Objective Loss 1.189619                                        LR 0.100000    Time 0.097239    
2024-02-17 11:49:57,874 - Epoch: [84][  391/  391]    Overall Loss 1.205060    Objective Loss 1.205060    Top1 65.384615    Top5 91.346154    LR 0.100000    Time 0.097114    
2024-02-17 11:49:57,992 - --- validate (epoch=84)-----------
2024-02-17 11:49:57,993 - 10000 samples (128 per mini-batch)
2024-02-17 11:50:00,714 - Epoch: [84][   79/   79]    Loss 1.722928    Top1 53.200000    Top5 83.280000    
2024-02-17 11:50:00,900 - ==> Top1: 53.200    Top5: 83.280    Loss: 1.723

2024-02-17 11:50:00,918 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:50:00,918 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:50:00,999 - 

2024-02-17 11:50:00,999 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:50:11,175 - Epoch: [85][  100/  391]    Overall Loss 1.142040    Objective Loss 1.142040                                        LR 0.100000    Time 0.101674    
2024-02-17 11:50:20,898 - Epoch: [85][  200/  391]    Overall Loss 1.177139    Objective Loss 1.177139                                        LR 0.100000    Time 0.099434    
2024-02-17 11:50:30,537 - Epoch: [85][  300/  391]    Overall Loss 1.182821    Objective Loss 1.182821                                        LR 0.100000    Time 0.098405    
2024-02-17 11:50:39,166 - Epoch: [85][  391/  391]    Overall Loss 1.191620    Objective Loss 1.191620    Top1 67.788462    Top5 89.423077    LR 0.100000    Time 0.097561    
2024-02-17 11:50:39,346 - --- validate (epoch=85)-----------
2024-02-17 11:50:39,347 - 10000 samples (128 per mini-batch)
2024-02-17 11:50:42,169 - Epoch: [85][   79/   79]    Loss 1.687304    Top1 54.170000    Top5 83.350000    
2024-02-17 11:50:42,291 - ==> Top1: 54.170    Top5: 83.350    Loss: 1.687

2024-02-17 11:50:42,312 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:50:42,313 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:50:42,399 - 

2024-02-17 11:50:42,399 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:50:52,932 - Epoch: [86][  100/  391]    Overall Loss 1.130520    Objective Loss 1.130520                                        LR 0.100000    Time 0.105260    
2024-02-17 11:51:02,442 - Epoch: [86][  200/  391]    Overall Loss 1.160144    Objective Loss 1.160144                                        LR 0.100000    Time 0.100155    
2024-02-17 11:51:11,799 - Epoch: [86][  300/  391]    Overall Loss 1.178367    Objective Loss 1.178367                                        LR 0.100000    Time 0.097946    
2024-02-17 11:51:20,958 - Epoch: [86][  391/  391]    Overall Loss 1.190103    Objective Loss 1.190103    Top1 63.942308    Top5 87.980769    LR 0.100000    Time 0.098563    
2024-02-17 11:51:21,075 - --- validate (epoch=86)-----------
2024-02-17 11:51:21,076 - 10000 samples (128 per mini-batch)
2024-02-17 11:51:23,824 - Epoch: [86][   79/   79]    Loss 1.617547    Top1 55.120000    Top5 84.970000    
2024-02-17 11:51:23,926 - ==> Top1: 55.120    Top5: 84.970    Loss: 1.618

2024-02-17 11:51:23,946 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:51:23,946 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:51:24,049 - 

2024-02-17 11:51:24,049 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:51:34,091 - Epoch: [87][  100/  391]    Overall Loss 1.147955    Objective Loss 1.147955                                        LR 0.100000    Time 0.100341    
2024-02-17 11:51:43,583 - Epoch: [87][  200/  391]    Overall Loss 1.160226    Objective Loss 1.160226                                        LR 0.100000    Time 0.097609    
2024-02-17 11:51:53,146 - Epoch: [87][  300/  391]    Overall Loss 1.174703    Objective Loss 1.174703                                        LR 0.100000    Time 0.096934    
2024-02-17 11:52:01,877 - Epoch: [87][  391/  391]    Overall Loss 1.181049    Objective Loss 1.181049    Top1 60.096154    Top5 89.423077    LR 0.100000    Time 0.096693    
2024-02-17 11:52:02,015 - --- validate (epoch=87)-----------
2024-02-17 11:52:02,015 - 10000 samples (128 per mini-batch)
2024-02-17 11:52:04,905 - Epoch: [87][   79/   79]    Loss 1.951335    Top1 49.510000    Top5 80.040000    
2024-02-17 11:52:05,104 - ==> Top1: 49.510    Top5: 80.040    Loss: 1.951

2024-02-17 11:52:05,124 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:52:05,124 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:52:05,202 - 

2024-02-17 11:52:05,202 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:52:15,679 - Epoch: [88][  100/  391]    Overall Loss 1.139977    Objective Loss 1.139977                                        LR 0.100000    Time 0.104696    
2024-02-17 11:52:25,597 - Epoch: [88][  200/  391]    Overall Loss 1.153389    Objective Loss 1.153389                                        LR 0.100000    Time 0.101913    
2024-02-17 11:52:34,991 - Epoch: [88][  300/  391]    Overall Loss 1.167938    Objective Loss 1.167938                                        LR 0.100000    Time 0.099240    
2024-02-17 11:52:43,828 - Epoch: [88][  391/  391]    Overall Loss 1.181036    Objective Loss 1.181036    Top1 69.230769    Top5 93.269231    LR 0.100000    Time 0.098731    
2024-02-17 11:52:43,969 - --- validate (epoch=88)-----------
2024-02-17 11:52:43,970 - 10000 samples (128 per mini-batch)
2024-02-17 11:52:46,713 - Epoch: [88][   79/   79]    Loss 1.785244    Top1 52.210000    Top5 82.760000    
2024-02-17 11:52:46,859 - ==> Top1: 52.210    Top5: 82.760    Loss: 1.785

2024-02-17 11:52:46,871 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:52:46,871 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:52:46,949 - 

2024-02-17 11:52:46,949 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:52:56,964 - Epoch: [89][  100/  391]    Overall Loss 1.127520    Objective Loss 1.127520                                        LR 0.100000    Time 0.100076    
2024-02-17 11:53:06,470 - Epoch: [89][  200/  391]    Overall Loss 1.147720    Objective Loss 1.147720                                        LR 0.100000    Time 0.097546    
2024-02-17 11:53:15,693 - Epoch: [89][  300/  391]    Overall Loss 1.166428    Objective Loss 1.166428                                        LR 0.100000    Time 0.095761    
2024-02-17 11:53:24,542 - Epoch: [89][  391/  391]    Overall Loss 1.177647    Objective Loss 1.177647    Top1 65.384615    Top5 89.423077    LR 0.100000    Time 0.096092    
2024-02-17 11:53:24,671 - --- validate (epoch=89)-----------
2024-02-17 11:53:24,672 - 10000 samples (128 per mini-batch)
2024-02-17 11:53:27,452 - Epoch: [89][   79/   79]    Loss 1.973575    Top1 50.370000    Top5 78.920000    
2024-02-17 11:53:27,578 - ==> Top1: 50.370    Top5: 78.920    Loss: 1.974

2024-02-17 11:53:27,597 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:53:27,597 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:53:27,673 - 

2024-02-17 11:53:27,674 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:53:37,812 - Epoch: [90][  100/  391]    Overall Loss 1.121073    Objective Loss 1.121073                                        LR 0.100000    Time 0.101308    
2024-02-17 11:53:47,422 - Epoch: [90][  200/  391]    Overall Loss 1.145805    Objective Loss 1.145805                                        LR 0.100000    Time 0.098682    
2024-02-17 11:53:56,809 - Epoch: [90][  300/  391]    Overall Loss 1.156484    Objective Loss 1.156484                                        LR 0.100000    Time 0.097065    
2024-02-17 11:54:05,432 - Epoch: [90][  391/  391]    Overall Loss 1.164850    Objective Loss 1.164850    Top1 64.423077    Top5 89.423077    LR 0.100000    Time 0.096516    
2024-02-17 11:54:05,556 - --- validate (epoch=90)-----------
2024-02-17 11:54:05,557 - 10000 samples (128 per mini-batch)
2024-02-17 11:54:08,490 - Epoch: [90][   79/   79]    Loss 1.671971    Top1 54.660000    Top5 83.970000    
2024-02-17 11:54:08,653 - ==> Top1: 54.660    Top5: 83.970    Loss: 1.672

2024-02-17 11:54:08,670 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:54:08,671 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:54:08,744 - 

2024-02-17 11:54:08,745 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:54:18,634 - Epoch: [91][  100/  391]    Overall Loss 1.106952    Objective Loss 1.106952                                        LR 0.100000    Time 0.098823    
2024-02-17 11:54:28,153 - Epoch: [91][  200/  391]    Overall Loss 1.141084    Objective Loss 1.141084                                        LR 0.100000    Time 0.096983    
2024-02-17 11:54:37,263 - Epoch: [91][  300/  391]    Overall Loss 1.156907    Objective Loss 1.156907                                        LR 0.100000    Time 0.095008    
2024-02-17 11:54:46,060 - Epoch: [91][  391/  391]    Overall Loss 1.162098    Objective Loss 1.162098    Top1 61.538462    Top5 86.538462    LR 0.100000    Time 0.095384    
2024-02-17 11:54:46,234 - --- validate (epoch=91)-----------
2024-02-17 11:54:46,235 - 10000 samples (128 per mini-batch)
2024-02-17 11:54:48,925 - Epoch: [91][   79/   79]    Loss 1.832772    Top1 52.330000    Top5 81.380000    
2024-02-17 11:54:49,092 - ==> Top1: 52.330    Top5: 81.380    Loss: 1.833

2024-02-17 11:54:49,106 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:54:49,107 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:54:49,179 - 

2024-02-17 11:54:49,180 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:54:57,989 - Epoch: [92][  100/  391]    Overall Loss 1.128679    Objective Loss 1.128679                                        LR 0.100000    Time 0.088023    
2024-02-17 11:55:05,279 - Epoch: [92][  200/  391]    Overall Loss 1.136649    Objective Loss 1.136649                                        LR 0.100000    Time 0.080442    
2024-02-17 11:55:12,955 - Epoch: [92][  300/  391]    Overall Loss 1.152592    Objective Loss 1.152592                                        LR 0.100000    Time 0.079203    
2024-02-17 11:55:21,116 - Epoch: [92][  391/  391]    Overall Loss 1.165580    Objective Loss 1.165580    Top1 61.057692    Top5 88.461538    LR 0.100000    Time 0.081631    
2024-02-17 11:55:21,267 - --- validate (epoch=92)-----------
2024-02-17 11:55:21,268 - 10000 samples (128 per mini-batch)
2024-02-17 11:55:24,370 - Epoch: [92][   79/   79]    Loss 1.663866    Top1 55.390000    Top5 84.270000    
2024-02-17 11:55:24,503 - ==> Top1: 55.390    Top5: 84.270    Loss: 1.664

2024-02-17 11:55:24,518 - ==> Best [Top1: 55.390   Top5: 84.270   Sparsity:0.00   Params: 1341960 on epoch: 92]
2024-02-17 11:55:24,519 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:55:24,612 - 

2024-02-17 11:55:24,612 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:55:34,584 - Epoch: [93][  100/  391]    Overall Loss 1.132603    Objective Loss 1.132603                                        LR 0.100000    Time 0.099643    
2024-02-17 11:55:44,172 - Epoch: [93][  200/  391]    Overall Loss 1.157660    Objective Loss 1.157660                                        LR 0.100000    Time 0.097734    
2024-02-17 11:55:53,447 - Epoch: [93][  300/  391]    Overall Loss 1.163294    Objective Loss 1.163294                                        LR 0.100000    Time 0.096061    
2024-02-17 11:56:02,044 - Epoch: [93][  391/  391]    Overall Loss 1.168071    Objective Loss 1.168071    Top1 64.903846    Top5 92.307692    LR 0.100000    Time 0.095680    
2024-02-17 11:56:02,165 - --- validate (epoch=93)-----------
2024-02-17 11:56:02,167 - 10000 samples (128 per mini-batch)
2024-02-17 11:56:05,012 - Epoch: [93][   79/   79]    Loss 1.778660    Top1 53.460000    Top5 82.040000    
2024-02-17 11:56:05,168 - ==> Top1: 53.460    Top5: 82.040    Loss: 1.779

2024-02-17 11:56:05,184 - ==> Best [Top1: 55.390   Top5: 84.270   Sparsity:0.00   Params: 1341960 on epoch: 92]
2024-02-17 11:56:05,185 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:56:05,256 - 

2024-02-17 11:56:05,257 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:56:15,309 - Epoch: [94][  100/  391]    Overall Loss 1.124040    Objective Loss 1.124040                                        LR 0.100000    Time 0.100454    
2024-02-17 11:56:22,858 - Epoch: [94][  200/  391]    Overall Loss 1.130566    Objective Loss 1.130566                                        LR 0.100000    Time 0.087948    
2024-02-17 11:56:30,718 - Epoch: [94][  300/  391]    Overall Loss 1.136517    Objective Loss 1.136517                                        LR 0.100000    Time 0.084822    
2024-02-17 11:56:38,280 - Epoch: [94][  391/  391]    Overall Loss 1.153137    Objective Loss 1.153137    Top1 62.019231    Top5 90.865385    LR 0.100000    Time 0.084410    
2024-02-17 11:56:38,404 - --- validate (epoch=94)-----------
2024-02-17 11:56:38,404 - 10000 samples (128 per mini-batch)
2024-02-17 11:56:41,587 - Epoch: [94][   79/   79]    Loss 1.611510    Top1 55.400000    Top5 84.510000    
2024-02-17 11:56:41,711 - ==> Top1: 55.400    Top5: 84.510    Loss: 1.612

2024-02-17 11:56:41,729 - ==> Best [Top1: 55.400   Top5: 84.510   Sparsity:0.00   Params: 1341960 on epoch: 94]
2024-02-17 11:56:41,729 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:56:41,831 - 

2024-02-17 11:56:41,832 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:56:51,955 - Epoch: [95][  100/  391]    Overall Loss 1.102432    Objective Loss 1.102432                                        LR 0.100000    Time 0.101154    
2024-02-17 11:57:01,555 - Epoch: [95][  200/  391]    Overall Loss 1.122185    Objective Loss 1.122185                                        LR 0.100000    Time 0.098558    
2024-02-17 11:57:10,885 - Epoch: [95][  300/  391]    Overall Loss 1.136620    Objective Loss 1.136620                                        LR 0.100000    Time 0.096789    
2024-02-17 11:57:18,945 - Epoch: [95][  391/  391]    Overall Loss 1.147500    Objective Loss 1.147500    Top1 66.826923    Top5 92.307692    LR 0.100000    Time 0.094869    
2024-02-17 11:57:19,113 - --- validate (epoch=95)-----------
2024-02-17 11:57:19,114 - 10000 samples (128 per mini-batch)
2024-02-17 11:57:21,874 - Epoch: [95][   79/   79]    Loss 1.925001    Top1 49.310000    Top5 79.670000    
2024-02-17 11:57:21,994 - ==> Top1: 49.310    Top5: 79.670    Loss: 1.925

2024-02-17 11:57:22,005 - ==> Best [Top1: 55.400   Top5: 84.510   Sparsity:0.00   Params: 1341960 on epoch: 94]
2024-02-17 11:57:22,005 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:57:22,089 - 

2024-02-17 11:57:22,090 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:57:32,733 - Epoch: [96][  100/  391]    Overall Loss 1.119518    Objective Loss 1.119518                                        LR 0.100000    Time 0.106347    
2024-02-17 11:57:42,328 - Epoch: [96][  200/  391]    Overall Loss 1.122402    Objective Loss 1.122402                                        LR 0.100000    Time 0.101131    
2024-02-17 11:57:51,960 - Epoch: [96][  300/  391]    Overall Loss 1.134191    Objective Loss 1.134191                                        LR 0.100000    Time 0.099513    
2024-02-17 11:57:59,936 - Epoch: [96][  391/  391]    Overall Loss 1.143472    Objective Loss 1.143472    Top1 58.653846    Top5 87.980769    LR 0.100000    Time 0.096740    
2024-02-17 11:58:00,045 - --- validate (epoch=96)-----------
2024-02-17 11:58:00,046 - 10000 samples (128 per mini-batch)
2024-02-17 11:58:02,890 - Epoch: [96][   79/   79]    Loss 1.566625    Top1 57.290000    Top5 85.610000    
2024-02-17 11:58:03,079 - ==> Top1: 57.290    Top5: 85.610    Loss: 1.567

2024-02-17 11:58:03,100 - ==> Best [Top1: 57.290   Top5: 85.610   Sparsity:0.00   Params: 1341960 on epoch: 96]
2024-02-17 11:58:03,101 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:58:03,202 - 

2024-02-17 11:58:03,202 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:58:13,267 - Epoch: [97][  100/  391]    Overall Loss 1.097953    Objective Loss 1.097953                                        LR 0.100000    Time 0.100571    
2024-02-17 11:58:22,732 - Epoch: [97][  200/  391]    Overall Loss 1.116361    Objective Loss 1.116361                                        LR 0.100000    Time 0.097590    
2024-02-17 11:58:32,417 - Epoch: [97][  300/  391]    Overall Loss 1.132750    Objective Loss 1.132750                                        LR 0.100000    Time 0.097330    
2024-02-17 11:58:40,883 - Epoch: [97][  391/  391]    Overall Loss 1.143397    Objective Loss 1.143397    Top1 67.788462    Top5 91.826923    LR 0.100000    Time 0.096320    
2024-02-17 11:58:41,002 - --- validate (epoch=97)-----------
2024-02-17 11:58:41,003 - 10000 samples (128 per mini-batch)
2024-02-17 11:58:43,782 - Epoch: [97][   79/   79]    Loss 1.555645    Top1 57.460000    Top5 84.990000    
2024-02-17 11:58:43,899 - ==> Top1: 57.460    Top5: 84.990    Loss: 1.556

2024-02-17 11:58:43,917 - ==> Best [Top1: 57.460   Top5: 84.990   Sparsity:0.00   Params: 1341960 on epoch: 97]
2024-02-17 11:58:43,917 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:58:44,011 - 

2024-02-17 11:58:44,012 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:58:54,494 - Epoch: [98][  100/  391]    Overall Loss 1.112636    Objective Loss 1.112636                                        LR 0.100000    Time 0.104749    
2024-02-17 11:59:04,042 - Epoch: [98][  200/  391]    Overall Loss 1.116951    Objective Loss 1.116951                                        LR 0.100000    Time 0.100093    
2024-02-17 11:59:13,789 - Epoch: [98][  300/  391]    Overall Loss 1.128124    Objective Loss 1.128124                                        LR 0.100000    Time 0.099204    
2024-02-17 11:59:22,390 - Epoch: [98][  391/  391]    Overall Loss 1.137896    Objective Loss 1.137896    Top1 58.653846    Top5 89.903846    LR 0.100000    Time 0.098102    
2024-02-17 11:59:22,507 - --- validate (epoch=98)-----------
2024-02-17 11:59:22,507 - 10000 samples (128 per mini-batch)
2024-02-17 11:59:25,222 - Epoch: [98][   79/   79]    Loss 1.759463    Top1 53.370000    Top5 83.050000    
2024-02-17 11:59:25,389 - ==> Top1: 53.370    Top5: 83.050    Loss: 1.759

2024-02-17 11:59:25,410 - ==> Best [Top1: 57.460   Top5: 84.990   Sparsity:0.00   Params: 1341960 on epoch: 97]
2024-02-17 11:59:25,410 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 11:59:25,486 - 

2024-02-17 11:59:25,486 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:59:35,022 - Epoch: [99][  100/  391]    Overall Loss 1.093764    Objective Loss 1.093764                                        LR 0.100000    Time 0.095284    
2024-02-17 11:59:44,569 - Epoch: [99][  200/  391]    Overall Loss 1.125451    Objective Loss 1.125451                                        LR 0.100000    Time 0.095356    
2024-02-17 11:59:54,250 - Epoch: [99][  300/  391]    Overall Loss 1.129275    Objective Loss 1.129275                                        LR 0.100000    Time 0.095825    
2024-02-17 12:00:02,727 - Epoch: [99][  391/  391]    Overall Loss 1.134985    Objective Loss 1.134985    Top1 65.384615    Top5 91.346154    LR 0.100000    Time 0.095193    
2024-02-17 12:00:02,862 - --- validate (epoch=99)-----------
2024-02-17 12:00:02,863 - 10000 samples (128 per mini-batch)
2024-02-17 12:00:05,651 - Epoch: [99][   79/   79]    Loss 1.652775    Top1 55.010000    Top5 84.220000    
2024-02-17 12:00:05,765 - ==> Top1: 55.010    Top5: 84.220    Loss: 1.653

2024-02-17 12:00:05,783 - ==> Best [Top1: 57.460   Top5: 84.990   Sparsity:0.00   Params: 1341960 on epoch: 97]
2024-02-17 12:00:05,784 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:00:05,859 - 

2024-02-17 12:00:05,860 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:00:15,969 - Epoch: [100][  100/  391]    Overall Loss 0.903404    Objective Loss 0.903404                                        LR 0.023500    Time 0.101023    
2024-02-17 12:00:25,493 - Epoch: [100][  200/  391]    Overall Loss 0.855386    Objective Loss 0.855386                                        LR 0.023500    Time 0.098108    
2024-02-17 12:00:35,229 - Epoch: [100][  300/  391]    Overall Loss 0.837106    Objective Loss 0.837106                                        LR 0.023500    Time 0.097845    
2024-02-17 12:00:43,821 - Epoch: [100][  391/  391]    Overall Loss 0.827087    Objective Loss 0.827087    Top1 75.480769    Top5 95.673077    LR 0.023500    Time 0.097037    
2024-02-17 12:00:43,965 - --- validate (epoch=100)-----------
2024-02-17 12:00:43,965 - 10000 samples (128 per mini-batch)
2024-02-17 12:00:46,722 - Epoch: [100][   79/   79]    Loss 1.230817    Top1 65.550000    Top5 90.210000    
2024-02-17 12:00:46,884 - ==> Top1: 65.550    Top5: 90.210    Loss: 1.231

2024-02-17 12:00:46,899 - ==> Best [Top1: 65.550   Top5: 90.210   Sparsity:0.00   Params: 1341960 on epoch: 100]
2024-02-17 12:00:46,900 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:00:47,023 - 

2024-02-17 12:00:47,023 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:00:57,233 - Epoch: [101][  100/  391]    Overall Loss 0.744482    Objective Loss 0.744482                                        LR 0.023500    Time 0.102013    
2024-02-17 12:01:06,768 - Epoch: [101][  200/  391]    Overall Loss 0.752669    Objective Loss 0.752669                                        LR 0.023500    Time 0.098659    
2024-02-17 12:01:16,360 - Epoch: [101][  300/  391]    Overall Loss 0.749218    Objective Loss 0.749218                                        LR 0.023500    Time 0.097730    
2024-02-17 12:01:24,866 - Epoch: [101][  391/  391]    Overall Loss 0.749171    Objective Loss 0.749171    Top1 78.365385    Top5 96.153846    LR 0.023500    Time 0.096731    
2024-02-17 12:01:24,990 - --- validate (epoch=101)-----------
2024-02-17 12:01:24,991 - 10000 samples (128 per mini-batch)
2024-02-17 12:01:27,863 - Epoch: [101][   79/   79]    Loss 1.232268    Top1 65.080000    Top5 90.050000    
2024-02-17 12:01:27,982 - ==> Top1: 65.080    Top5: 90.050    Loss: 1.232

2024-02-17 12:01:28,001 - ==> Best [Top1: 65.550   Top5: 90.210   Sparsity:0.00   Params: 1341960 on epoch: 100]
2024-02-17 12:01:28,001 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:01:28,077 - 

2024-02-17 12:01:28,078 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:01:38,254 - Epoch: [102][  100/  391]    Overall Loss 0.727950    Objective Loss 0.727950                                        LR 0.023500    Time 0.101686    
2024-02-17 12:01:47,832 - Epoch: [102][  200/  391]    Overall Loss 0.724930    Objective Loss 0.724930                                        LR 0.023500    Time 0.098712    
2024-02-17 12:01:57,269 - Epoch: [102][  300/  391]    Overall Loss 0.721203    Objective Loss 0.721203                                        LR 0.023500    Time 0.097248    
2024-02-17 12:02:05,879 - Epoch: [102][  391/  391]    Overall Loss 0.721563    Objective Loss 0.721563    Top1 74.038462    Top5 95.192308    LR 0.023500    Time 0.096626    
2024-02-17 12:02:06,009 - --- validate (epoch=102)-----------
2024-02-17 12:02:06,009 - 10000 samples (128 per mini-batch)
2024-02-17 12:02:08,890 - Epoch: [102][   79/   79]    Loss 1.217137    Top1 65.640000    Top5 90.050000    
2024-02-17 12:02:09,041 - ==> Top1: 65.640    Top5: 90.050    Loss: 1.217

2024-02-17 12:02:09,056 - ==> Best [Top1: 65.640   Top5: 90.050   Sparsity:0.00   Params: 1341960 on epoch: 102]
2024-02-17 12:02:09,056 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:02:09,150 - 

2024-02-17 12:02:09,150 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:02:19,379 - Epoch: [103][  100/  391]    Overall Loss 0.680840    Objective Loss 0.680840                                        LR 0.023500    Time 0.102217    
2024-02-17 12:02:29,206 - Epoch: [103][  200/  391]    Overall Loss 0.682802    Objective Loss 0.682802                                        LR 0.023500    Time 0.100222    
2024-02-17 12:02:38,639 - Epoch: [103][  300/  391]    Overall Loss 0.691215    Objective Loss 0.691215                                        LR 0.023500    Time 0.098242    
2024-02-17 12:02:47,392 - Epoch: [103][  391/  391]    Overall Loss 0.697680    Objective Loss 0.697680    Top1 85.096154    Top5 95.673077    LR 0.023500    Time 0.097754    
2024-02-17 12:02:47,545 - --- validate (epoch=103)-----------
2024-02-17 12:02:47,545 - 10000 samples (128 per mini-batch)
2024-02-17 12:02:50,538 - Epoch: [103][   79/   79]    Loss 1.242191    Top1 65.060000    Top5 89.730000    
2024-02-17 12:02:50,663 - ==> Top1: 65.060    Top5: 89.730    Loss: 1.242

2024-02-17 12:02:50,678 - ==> Best [Top1: 65.640   Top5: 90.050   Sparsity:0.00   Params: 1341960 on epoch: 102]
2024-02-17 12:02:50,678 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:02:50,757 - 

2024-02-17 12:02:50,757 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:03:00,818 - Epoch: [104][  100/  391]    Overall Loss 0.668947    Objective Loss 0.668947                                        LR 0.023500    Time 0.100536    
2024-02-17 12:03:10,792 - Epoch: [104][  200/  391]    Overall Loss 0.680357    Objective Loss 0.680357                                        LR 0.023500    Time 0.100112    
2024-02-17 12:03:20,408 - Epoch: [104][  300/  391]    Overall Loss 0.679509    Objective Loss 0.679509                                        LR 0.023500    Time 0.098779    
2024-02-17 12:03:29,021 - Epoch: [104][  391/  391]    Overall Loss 0.687346    Objective Loss 0.687346    Top1 76.442308    Top5 95.192308    LR 0.023500    Time 0.097806    
2024-02-17 12:03:29,158 - --- validate (epoch=104)-----------
2024-02-17 12:03:29,158 - 10000 samples (128 per mini-batch)
2024-02-17 12:03:32,186 - Epoch: [104][   79/   79]    Loss 1.212613    Top1 66.040000    Top5 90.370000    
2024-02-17 12:03:32,349 - ==> Top1: 66.040    Top5: 90.370    Loss: 1.213

2024-02-17 12:03:32,370 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:03:32,370 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:03:32,459 - 

2024-02-17 12:03:32,460 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:03:42,665 - Epoch: [105][  100/  391]    Overall Loss 0.649788    Objective Loss 0.649788                                        LR 0.023500    Time 0.101978    
2024-02-17 12:03:51,430 - Epoch: [105][  200/  391]    Overall Loss 0.655619    Objective Loss 0.655619                                        LR 0.023500    Time 0.094793    
2024-02-17 12:04:00,938 - Epoch: [105][  300/  391]    Overall Loss 0.663308    Objective Loss 0.663308                                        LR 0.023500    Time 0.094875    
2024-02-17 12:04:09,586 - Epoch: [105][  391/  391]    Overall Loss 0.667088    Objective Loss 0.667088    Top1 77.884615    Top5 98.557692    LR 0.023500    Time 0.094901    
2024-02-17 12:04:09,725 - --- validate (epoch=105)-----------
2024-02-17 12:04:09,726 - 10000 samples (128 per mini-batch)
2024-02-17 12:04:12,596 - Epoch: [105][   79/   79]    Loss 1.222828    Top1 65.370000    Top5 90.160000    
2024-02-17 12:04:12,768 - ==> Top1: 65.370    Top5: 90.160    Loss: 1.223

2024-02-17 12:04:12,787 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:04:12,787 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:04:12,864 - 

2024-02-17 12:04:12,864 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:04:22,322 - Epoch: [106][  100/  391]    Overall Loss 0.647922    Objective Loss 0.647922                                        LR 0.023500    Time 0.094509    
2024-02-17 12:04:31,449 - Epoch: [106][  200/  391]    Overall Loss 0.656212    Objective Loss 0.656212                                        LR 0.023500    Time 0.092868    
2024-02-17 12:04:40,680 - Epoch: [106][  300/  391]    Overall Loss 0.650680    Objective Loss 0.650680                                        LR 0.023500    Time 0.092664    
2024-02-17 12:04:49,051 - Epoch: [106][  391/  391]    Overall Loss 0.652795    Objective Loss 0.652795    Top1 80.288462    Top5 96.634615    LR 0.023500    Time 0.092499    
2024-02-17 12:04:49,191 - --- validate (epoch=106)-----------
2024-02-17 12:04:49,192 - 10000 samples (128 per mini-batch)
2024-02-17 12:04:52,015 - Epoch: [106][   79/   79]    Loss 1.237146    Top1 65.390000    Top5 90.110000    
2024-02-17 12:04:52,135 - ==> Top1: 65.390    Top5: 90.110    Loss: 1.237

2024-02-17 12:04:52,146 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:04:52,146 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:04:52,239 - 

2024-02-17 12:04:52,239 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:05:02,292 - Epoch: [107][  100/  391]    Overall Loss 0.628644    Objective Loss 0.628644                                        LR 0.023500    Time 0.100445    
2024-02-17 12:05:12,028 - Epoch: [107][  200/  391]    Overall Loss 0.638219    Objective Loss 0.638219                                        LR 0.023500    Time 0.098879    
2024-02-17 12:05:21,856 - Epoch: [107][  300/  391]    Overall Loss 0.640107    Objective Loss 0.640107                                        LR 0.023500    Time 0.098665    
2024-02-17 12:05:31,102 - Epoch: [107][  391/  391]    Overall Loss 0.648960    Objective Loss 0.648960    Top1 79.326923    Top5 98.076923    LR 0.023500    Time 0.099337    
2024-02-17 12:05:31,223 - --- validate (epoch=107)-----------
2024-02-17 12:05:31,224 - 10000 samples (128 per mini-batch)
2024-02-17 12:05:34,156 - Epoch: [107][   79/   79]    Loss 1.260121    Top1 65.170000    Top5 89.580000    
2024-02-17 12:05:34,327 - ==> Top1: 65.170    Top5: 89.580    Loss: 1.260

2024-02-17 12:05:34,346 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:05:34,347 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:05:34,420 - 

2024-02-17 12:05:34,420 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:05:44,862 - Epoch: [108][  100/  391]    Overall Loss 0.618427    Objective Loss 0.618427                                        LR 0.023500    Time 0.104341    
2024-02-17 12:05:54,434 - Epoch: [108][  200/  391]    Overall Loss 0.626711    Objective Loss 0.626711                                        LR 0.023500    Time 0.100013    
2024-02-17 12:06:04,022 - Epoch: [108][  300/  391]    Overall Loss 0.635810    Objective Loss 0.635810                                        LR 0.023500    Time 0.098617    
2024-02-17 12:06:12,838 - Epoch: [108][  391/  391]    Overall Loss 0.640143    Objective Loss 0.640143    Top1 82.211538    Top5 99.519231    LR 0.023500    Time 0.098204    
2024-02-17 12:06:12,987 - --- validate (epoch=108)-----------
2024-02-17 12:06:12,988 - 10000 samples (128 per mini-batch)
2024-02-17 12:06:15,716 - Epoch: [108][   79/   79]    Loss 1.233425    Top1 65.550000    Top5 90.400000    
2024-02-17 12:06:15,831 - ==> Top1: 65.550    Top5: 90.400    Loss: 1.233

2024-02-17 12:06:15,850 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:06:15,850 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:06:15,925 - 

2024-02-17 12:06:15,925 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:06:25,829 - Epoch: [109][  100/  391]    Overall Loss 0.608839    Objective Loss 0.608839                                        LR 0.023500    Time 0.098965    
2024-02-17 12:06:35,461 - Epoch: [109][  200/  391]    Overall Loss 0.617831    Objective Loss 0.617831                                        LR 0.023500    Time 0.097621    
2024-02-17 12:06:44,935 - Epoch: [109][  300/  391]    Overall Loss 0.625404    Objective Loss 0.625404                                        LR 0.023500    Time 0.096647    
2024-02-17 12:06:53,637 - Epoch: [109][  391/  391]    Overall Loss 0.631449    Objective Loss 0.631449    Top1 78.365385    Top5 96.153846    LR 0.023500    Time 0.096398    
2024-02-17 12:06:53,771 - --- validate (epoch=109)-----------
2024-02-17 12:06:53,772 - 10000 samples (128 per mini-batch)
2024-02-17 12:06:56,625 - Epoch: [109][   79/   79]    Loss 1.257949    Top1 65.170000    Top5 89.880000    
2024-02-17 12:06:56,748 - ==> Top1: 65.170    Top5: 89.880    Loss: 1.258

2024-02-17 12:06:56,766 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:06:56,766 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:06:56,840 - 

2024-02-17 12:06:56,840 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:07:07,138 - Epoch: [110][  100/  391]    Overall Loss 0.593725    Objective Loss 0.593725                                        LR 0.023500    Time 0.102902    
2024-02-17 12:07:16,837 - Epoch: [110][  200/  391]    Overall Loss 0.605866    Objective Loss 0.605866                                        LR 0.023500    Time 0.099925    
2024-02-17 12:07:26,351 - Epoch: [110][  300/  391]    Overall Loss 0.614432    Objective Loss 0.614432                                        LR 0.023500    Time 0.098313    
2024-02-17 12:07:35,009 - Epoch: [110][  391/  391]    Overall Loss 0.621633    Objective Loss 0.621633    Top1 75.480769    Top5 95.192308    LR 0.023500    Time 0.097567    
2024-02-17 12:07:35,127 - --- validate (epoch=110)-----------
2024-02-17 12:07:35,129 - 10000 samples (128 per mini-batch)
2024-02-17 12:07:38,030 - Epoch: [110][   79/   79]    Loss 1.219577    Top1 66.110000    Top5 90.090000    
2024-02-17 12:07:38,222 - ==> Top1: 66.110    Top5: 90.090    Loss: 1.220

2024-02-17 12:07:38,242 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:07:38,243 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:07:38,337 - 

2024-02-17 12:07:38,337 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:07:48,283 - Epoch: [111][  100/  391]    Overall Loss 0.599360    Objective Loss 0.599360                                        LR 0.023500    Time 0.099387    
2024-02-17 12:07:57,954 - Epoch: [111][  200/  391]    Overall Loss 0.600780    Objective Loss 0.600780                                        LR 0.023500    Time 0.098024    
2024-02-17 12:08:07,551 - Epoch: [111][  300/  391]    Overall Loss 0.612345    Objective Loss 0.612345                                        LR 0.023500    Time 0.097325    
2024-02-17 12:08:16,544 - Epoch: [111][  391/  391]    Overall Loss 0.621217    Objective Loss 0.621217    Top1 82.211538    Top5 97.115385    LR 0.023500    Time 0.097664    
2024-02-17 12:08:16,667 - --- validate (epoch=111)-----------
2024-02-17 12:08:16,668 - 10000 samples (128 per mini-batch)
2024-02-17 12:08:19,513 - Epoch: [111][   79/   79]    Loss 1.307443    Top1 64.350000    Top5 89.100000    
2024-02-17 12:08:19,707 - ==> Top1: 64.350    Top5: 89.100    Loss: 1.307

2024-02-17 12:08:19,727 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:08:19,728 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:08:19,803 - 

2024-02-17 12:08:19,803 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:08:29,904 - Epoch: [112][  100/  391]    Overall Loss 0.589680    Objective Loss 0.589680                                        LR 0.023500    Time 0.100933    
2024-02-17 12:08:39,501 - Epoch: [112][  200/  391]    Overall Loss 0.594518    Objective Loss 0.594518                                        LR 0.023500    Time 0.098434    
2024-02-17 12:08:48,894 - Epoch: [112][  300/  391]    Overall Loss 0.599510    Objective Loss 0.599510                                        LR 0.023500    Time 0.096917    
2024-02-17 12:08:57,842 - Epoch: [112][  391/  391]    Overall Loss 0.609625    Objective Loss 0.609625    Top1 76.442308    Top5 96.153846    LR 0.023500    Time 0.097236    
2024-02-17 12:08:57,983 - --- validate (epoch=112)-----------
2024-02-17 12:08:57,984 - 10000 samples (128 per mini-batch)
2024-02-17 12:09:00,987 - Epoch: [112][   79/   79]    Loss 1.298559    Top1 64.930000    Top5 89.460000    
2024-02-17 12:09:01,105 - ==> Top1: 64.930    Top5: 89.460    Loss: 1.299

2024-02-17 12:09:01,125 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:09:01,125 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:09:01,207 - 

2024-02-17 12:09:01,207 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:09:11,445 - Epoch: [113][  100/  391]    Overall Loss 0.592065    Objective Loss 0.592065                                        LR 0.023500    Time 0.102297    
2024-02-17 12:09:21,016 - Epoch: [113][  200/  391]    Overall Loss 0.593936    Objective Loss 0.593936                                        LR 0.023500    Time 0.098982    
2024-02-17 12:09:30,306 - Epoch: [113][  300/  391]    Overall Loss 0.600268    Objective Loss 0.600268                                        LR 0.023500    Time 0.096939    
2024-02-17 12:09:39,112 - Epoch: [113][  391/  391]    Overall Loss 0.606185    Objective Loss 0.606185    Top1 76.442308    Top5 98.076923    LR 0.023500    Time 0.096889    
2024-02-17 12:09:39,276 - --- validate (epoch=113)-----------
2024-02-17 12:09:39,276 - 10000 samples (128 per mini-batch)
2024-02-17 12:09:42,050 - Epoch: [113][   79/   79]    Loss 1.317711    Top1 64.190000    Top5 89.450000    
2024-02-17 12:09:42,187 - ==> Top1: 64.190    Top5: 89.450    Loss: 1.318

2024-02-17 12:09:42,199 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:09:42,199 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:09:42,280 - 

2024-02-17 12:09:42,281 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:09:52,438 - Epoch: [114][  100/  391]    Overall Loss 0.574343    Objective Loss 0.574343                                        LR 0.023500    Time 0.101466    
2024-02-17 12:10:02,041 - Epoch: [114][  200/  391]    Overall Loss 0.586967    Objective Loss 0.586967                                        LR 0.023500    Time 0.098725    
2024-02-17 12:10:11,407 - Epoch: [114][  300/  391]    Overall Loss 0.596132    Objective Loss 0.596132                                        LR 0.023500    Time 0.097022    
2024-02-17 12:10:20,217 - Epoch: [114][  391/  391]    Overall Loss 0.605783    Objective Loss 0.605783    Top1 80.769231    Top5 96.153846    LR 0.023500    Time 0.096964    
2024-02-17 12:10:20,387 - --- validate (epoch=114)-----------
2024-02-17 12:10:20,388 - 10000 samples (128 per mini-batch)
2024-02-17 12:10:23,560 - Epoch: [114][   79/   79]    Loss 1.309465    Top1 64.210000    Top5 88.980000    
2024-02-17 12:10:23,697 - ==> Top1: 64.210    Top5: 88.980    Loss: 1.309

2024-02-17 12:10:23,715 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:10:23,715 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:10:23,793 - 

2024-02-17 12:10:23,794 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:10:33,980 - Epoch: [115][  100/  391]    Overall Loss 0.574954    Objective Loss 0.574954                                        LR 0.023500    Time 0.101786    
2024-02-17 12:10:43,384 - Epoch: [115][  200/  391]    Overall Loss 0.579106    Objective Loss 0.579106                                        LR 0.023500    Time 0.097890    
2024-02-17 12:10:52,933 - Epoch: [115][  300/  391]    Overall Loss 0.592322    Objective Loss 0.592322                                        LR 0.023500    Time 0.097076    
2024-02-17 12:11:01,717 - Epoch: [115][  391/  391]    Overall Loss 0.601133    Objective Loss 0.601133    Top1 81.730769    Top5 96.153846    LR 0.023500    Time 0.096936    
2024-02-17 12:11:01,895 - --- validate (epoch=115)-----------
2024-02-17 12:11:01,895 - 10000 samples (128 per mini-batch)
2024-02-17 12:11:04,907 - Epoch: [115][   79/   79]    Loss 1.320970    Top1 64.180000    Top5 88.950000    
2024-02-17 12:11:05,078 - ==> Top1: 64.180    Top5: 88.950    Loss: 1.321

2024-02-17 12:11:05,089 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:11:05,089 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:11:05,191 - 

2024-02-17 12:11:05,191 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:11:15,455 - Epoch: [116][  100/  391]    Overall Loss 0.590266    Objective Loss 0.590266                                        LR 0.023500    Time 0.102551    
2024-02-17 12:11:24,796 - Epoch: [116][  200/  391]    Overall Loss 0.591161    Objective Loss 0.591161                                        LR 0.023500    Time 0.097957    
2024-02-17 12:11:34,397 - Epoch: [116][  300/  391]    Overall Loss 0.597120    Objective Loss 0.597120                                        LR 0.023500    Time 0.097295    
2024-02-17 12:11:43,506 - Epoch: [116][  391/  391]    Overall Loss 0.602517    Objective Loss 0.602517    Top1 77.403846    Top5 96.634615    LR 0.023500    Time 0.097936    
2024-02-17 12:11:43,647 - --- validate (epoch=116)-----------
2024-02-17 12:11:43,647 - 10000 samples (128 per mini-batch)
2024-02-17 12:11:46,542 - Epoch: [116][   79/   79]    Loss 1.299381    Top1 65.010000    Top5 89.490000    
2024-02-17 12:11:46,656 - ==> Top1: 65.010    Top5: 89.490    Loss: 1.299

2024-02-17 12:11:46,675 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:11:46,675 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:11:46,757 - 

2024-02-17 12:11:46,757 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:11:56,924 - Epoch: [117][  100/  391]    Overall Loss 0.571592    Objective Loss 0.571592                                        LR 0.023500    Time 0.101591    
2024-02-17 12:12:06,261 - Epoch: [117][  200/  391]    Overall Loss 0.580793    Objective Loss 0.580793                                        LR 0.023500    Time 0.097455    
2024-02-17 12:12:16,014 - Epoch: [117][  300/  391]    Overall Loss 0.587905    Objective Loss 0.587905                                        LR 0.023500    Time 0.097466    
2024-02-17 12:12:24,581 - Epoch: [117][  391/  391]    Overall Loss 0.594131    Objective Loss 0.594131    Top1 77.403846    Top5 95.673077    LR 0.023500    Time 0.096683    
2024-02-17 12:12:24,712 - --- validate (epoch=117)-----------
2024-02-17 12:12:24,713 - 10000 samples (128 per mini-batch)
2024-02-17 12:12:27,571 - Epoch: [117][   79/   79]    Loss 1.343025    Top1 64.040000    Top5 88.790000    
2024-02-17 12:12:27,732 - ==> Top1: 64.040    Top5: 88.790    Loss: 1.343

2024-02-17 12:12:27,751 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:12:27,751 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:12:27,825 - 

2024-02-17 12:12:27,825 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:12:38,048 - Epoch: [118][  100/  391]    Overall Loss 0.565083    Objective Loss 0.565083                                        LR 0.023500    Time 0.102153    
2024-02-17 12:12:46,404 - Epoch: [118][  200/  391]    Overall Loss 0.568931    Objective Loss 0.568931                                        LR 0.023500    Time 0.092834    
2024-02-17 12:12:54,205 - Epoch: [118][  300/  391]    Overall Loss 0.579081    Objective Loss 0.579081                                        LR 0.023500    Time 0.087881    
2024-02-17 12:13:02,773 - Epoch: [118][  391/  391]    Overall Loss 0.589991    Objective Loss 0.589991    Top1 79.326923    Top5 97.596154    LR 0.023500    Time 0.089330    
2024-02-17 12:13:02,914 - --- validate (epoch=118)-----------
2024-02-17 12:13:02,914 - 10000 samples (128 per mini-batch)
2024-02-17 12:13:05,855 - Epoch: [118][   79/   79]    Loss 1.285393    Top1 65.080000    Top5 89.680000    
2024-02-17 12:13:05,963 - ==> Top1: 65.080    Top5: 89.680    Loss: 1.285

2024-02-17 12:13:05,980 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:13:05,980 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:13:06,079 - 

2024-02-17 12:13:06,079 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:13:16,666 - Epoch: [119][  100/  391]    Overall Loss 0.555229    Objective Loss 0.555229                                        LR 0.023500    Time 0.105763    
2024-02-17 12:13:25,926 - Epoch: [119][  200/  391]    Overall Loss 0.563886    Objective Loss 0.563886                                        LR 0.023500    Time 0.099158    
2024-02-17 12:13:35,631 - Epoch: [119][  300/  391]    Overall Loss 0.574306    Objective Loss 0.574306                                        LR 0.023500    Time 0.098443    
2024-02-17 12:13:44,476 - Epoch: [119][  391/  391]    Overall Loss 0.585766    Objective Loss 0.585766    Top1 77.403846    Top5 98.076923    LR 0.023500    Time 0.098143    
2024-02-17 12:13:44,586 - --- validate (epoch=119)-----------
2024-02-17 12:13:44,587 - 10000 samples (128 per mini-batch)
2024-02-17 12:13:47,337 - Epoch: [119][   79/   79]    Loss 1.281214    Top1 65.070000    Top5 89.780000    
2024-02-17 12:13:47,470 - ==> Top1: 65.070    Top5: 89.780    Loss: 1.281

2024-02-17 12:13:47,489 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:13:47,490 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:13:47,566 - 

2024-02-17 12:13:47,567 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:13:58,154 - Epoch: [120][  100/  391]    Overall Loss 0.544596    Objective Loss 0.544596                                        LR 0.023500    Time 0.105757    
2024-02-17 12:14:07,464 - Epoch: [120][  200/  391]    Overall Loss 0.561226    Objective Loss 0.561226                                        LR 0.023500    Time 0.099408    
2024-02-17 12:14:17,130 - Epoch: [120][  300/  391]    Overall Loss 0.570281    Objective Loss 0.570281                                        LR 0.023500    Time 0.098476    
2024-02-17 12:14:25,728 - Epoch: [120][  391/  391]    Overall Loss 0.578666    Objective Loss 0.578666    Top1 80.769231    Top5 98.557692    LR 0.023500    Time 0.097539    
2024-02-17 12:14:25,886 - --- validate (epoch=120)-----------
2024-02-17 12:14:25,888 - 10000 samples (128 per mini-batch)
2024-02-17 12:14:28,638 - Epoch: [120][   79/   79]    Loss 1.283224    Top1 64.780000    Top5 89.990000    
2024-02-17 12:14:28,731 - ==> Top1: 64.780    Top5: 89.990    Loss: 1.283

2024-02-17 12:14:28,752 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:14:28,752 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:14:28,830 - 

2024-02-17 12:14:28,830 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:14:38,619 - Epoch: [121][  100/  391]    Overall Loss 0.539578    Objective Loss 0.539578                                        LR 0.023500    Time 0.097813    
2024-02-17 12:14:48,086 - Epoch: [121][  200/  391]    Overall Loss 0.562012    Objective Loss 0.562012                                        LR 0.023500    Time 0.096222    
2024-02-17 12:14:57,572 - Epoch: [121][  300/  391]    Overall Loss 0.571322    Objective Loss 0.571322                                        LR 0.023500    Time 0.095755    
2024-02-17 12:15:06,127 - Epoch: [121][  391/  391]    Overall Loss 0.579879    Objective Loss 0.579879    Top1 83.173077    Top5 96.634615    LR 0.023500    Time 0.095339    
2024-02-17 12:15:06,268 - --- validate (epoch=121)-----------
2024-02-17 12:15:06,269 - 10000 samples (128 per mini-batch)
2024-02-17 12:15:09,007 - Epoch: [121][   79/   79]    Loss 1.297498    Top1 64.430000    Top5 89.680000    
2024-02-17 12:15:09,174 - ==> Top1: 64.430    Top5: 89.680    Loss: 1.297

2024-02-17 12:15:09,190 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:15:09,191 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:15:09,272 - 

2024-02-17 12:15:09,273 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:15:19,125 - Epoch: [122][  100/  391]    Overall Loss 0.538779    Objective Loss 0.538779                                        LR 0.023500    Time 0.098382    
2024-02-17 12:15:28,618 - Epoch: [122][  200/  391]    Overall Loss 0.547705    Objective Loss 0.547705                                        LR 0.023500    Time 0.096635    
2024-02-17 12:15:38,356 - Epoch: [122][  300/  391]    Overall Loss 0.560586    Objective Loss 0.560586                                        LR 0.023500    Time 0.096871    
2024-02-17 12:15:46,815 - Epoch: [122][  391/  391]    Overall Loss 0.568272    Objective Loss 0.568272    Top1 81.250000    Top5 97.596154    LR 0.023500    Time 0.095947    
2024-02-17 12:15:46,934 - --- validate (epoch=122)-----------
2024-02-17 12:15:46,934 - 10000 samples (128 per mini-batch)
2024-02-17 12:15:50,010 - Epoch: [122][   79/   79]    Loss 1.398728    Top1 63.080000    Top5 88.530000    
2024-02-17 12:15:50,150 - ==> Top1: 63.080    Top5: 88.530    Loss: 1.399

2024-02-17 12:15:50,167 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:15:50,168 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:15:50,249 - 

2024-02-17 12:15:50,250 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:15:59,953 - Epoch: [123][  100/  391]    Overall Loss 0.545120    Objective Loss 0.545120                                        LR 0.023500    Time 0.096915    
2024-02-17 12:16:09,700 - Epoch: [123][  200/  391]    Overall Loss 0.554063    Objective Loss 0.554063                                        LR 0.023500    Time 0.097170    
2024-02-17 12:16:19,391 - Epoch: [123][  300/  391]    Overall Loss 0.566640    Objective Loss 0.566640                                        LR 0.023500    Time 0.097068    
2024-02-17 12:16:27,948 - Epoch: [123][  391/  391]    Overall Loss 0.577591    Objective Loss 0.577591    Top1 75.961538    Top5 97.596154    LR 0.023500    Time 0.096350    
2024-02-17 12:16:28,126 - --- validate (epoch=123)-----------
2024-02-17 12:16:28,126 - 10000 samples (128 per mini-batch)
2024-02-17 12:16:30,957 - Epoch: [123][   79/   79]    Loss 1.291370    Top1 65.150000    Top5 89.640000    
2024-02-17 12:16:31,122 - ==> Top1: 65.150    Top5: 89.640    Loss: 1.291

2024-02-17 12:16:31,142 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:16:31,142 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:16:31,234 - 

2024-02-17 12:16:31,234 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:16:41,306 - Epoch: [124][  100/  391]    Overall Loss 0.551294    Objective Loss 0.551294                                        LR 0.023500    Time 0.100605    
2024-02-17 12:16:50,850 - Epoch: [124][  200/  391]    Overall Loss 0.559176    Objective Loss 0.559176                                        LR 0.023500    Time 0.097998    
2024-02-17 12:17:00,086 - Epoch: [124][  300/  391]    Overall Loss 0.567467    Objective Loss 0.567467                                        LR 0.023500    Time 0.096103    
2024-02-17 12:17:09,001 - Epoch: [124][  391/  391]    Overall Loss 0.575790    Objective Loss 0.575790    Top1 82.692308    Top5 97.596154    LR 0.023500    Time 0.096527    
2024-02-17 12:17:09,152 - --- validate (epoch=124)-----------
2024-02-17 12:17:09,152 - 10000 samples (128 per mini-batch)
2024-02-17 12:17:12,024 - Epoch: [124][   79/   79]    Loss 1.353544    Top1 64.360000    Top5 88.660000    
2024-02-17 12:17:12,154 - ==> Top1: 64.360    Top5: 88.660    Loss: 1.354

2024-02-17 12:17:12,173 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:17:12,173 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:17:12,248 - 

2024-02-17 12:17:12,248 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:17:22,163 - Epoch: [125][  100/  391]    Overall Loss 0.557675    Objective Loss 0.557675                                        LR 0.023500    Time 0.099073    
2024-02-17 12:17:31,694 - Epoch: [125][  200/  391]    Overall Loss 0.559029    Objective Loss 0.559029                                        LR 0.023500    Time 0.097171    
2024-02-17 12:17:41,232 - Epoch: [125][  300/  391]    Overall Loss 0.565728    Objective Loss 0.565728                                        LR 0.023500    Time 0.096558    
2024-02-17 12:17:50,203 - Epoch: [125][  391/  391]    Overall Loss 0.575140    Objective Loss 0.575140    Top1 86.538462    Top5 98.076923    LR 0.023500    Time 0.097019    
2024-02-17 12:17:50,325 - --- validate (epoch=125)-----------
2024-02-17 12:17:50,326 - 10000 samples (128 per mini-batch)
2024-02-17 12:17:53,151 - Epoch: [125][   79/   79]    Loss 1.319069    Top1 63.570000    Top5 89.240000    
2024-02-17 12:17:53,279 - ==> Top1: 63.570    Top5: 89.240    Loss: 1.319

2024-02-17 12:17:53,298 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:17:53,298 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:17:53,373 - 

2024-02-17 12:17:53,374 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:18:03,526 - Epoch: [126][  100/  391]    Overall Loss 0.531751    Objective Loss 0.531751                                        LR 0.023500    Time 0.101443    
2024-02-17 12:18:13,117 - Epoch: [126][  200/  391]    Overall Loss 0.548702    Objective Loss 0.548702                                        LR 0.023500    Time 0.098658    
2024-02-17 12:18:22,810 - Epoch: [126][  300/  391]    Overall Loss 0.562794    Objective Loss 0.562794                                        LR 0.023500    Time 0.098067    
2024-02-17 12:18:31,606 - Epoch: [126][  391/  391]    Overall Loss 0.566014    Objective Loss 0.566014    Top1 80.288462    Top5 98.076923    LR 0.023500    Time 0.097727    
2024-02-17 12:18:31,744 - --- validate (epoch=126)-----------
2024-02-17 12:18:31,745 - 10000 samples (128 per mini-batch)
2024-02-17 12:18:34,612 - Epoch: [126][   79/   79]    Loss 1.300004    Top1 64.620000    Top5 89.500000    
2024-02-17 12:18:34,780 - ==> Top1: 64.620    Top5: 89.500    Loss: 1.300

2024-02-17 12:18:34,799 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:18:34,800 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:18:34,877 - 

2024-02-17 12:18:34,877 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:18:44,914 - Epoch: [127][  100/  391]    Overall Loss 0.541477    Objective Loss 0.541477                                        LR 0.023500    Time 0.100293    
2024-02-17 12:18:54,597 - Epoch: [127][  200/  391]    Overall Loss 0.550580    Objective Loss 0.550580                                        LR 0.023500    Time 0.098541    
2024-02-17 12:19:04,432 - Epoch: [127][  300/  391]    Overall Loss 0.559187    Objective Loss 0.559187                                        LR 0.023500    Time 0.098460    
2024-02-17 12:19:13,307 - Epoch: [127][  391/  391]    Overall Loss 0.570268    Objective Loss 0.570268    Top1 78.846154    Top5 99.038462    LR 0.023500    Time 0.098232    
2024-02-17 12:19:13,437 - --- validate (epoch=127)-----------
2024-02-17 12:19:13,438 - 10000 samples (128 per mini-batch)
2024-02-17 12:19:16,343 - Epoch: [127][   79/   79]    Loss 1.424709    Top1 62.530000    Top5 88.140000    
2024-02-17 12:19:16,500 - ==> Top1: 62.530    Top5: 88.140    Loss: 1.425

2024-02-17 12:19:16,513 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:19:16,513 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:19:16,597 - 

2024-02-17 12:19:16,597 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:19:26,143 - Epoch: [128][  100/  391]    Overall Loss 0.533142    Objective Loss 0.533142                                        LR 0.023500    Time 0.095390    
2024-02-17 12:19:34,996 - Epoch: [128][  200/  391]    Overall Loss 0.540543    Objective Loss 0.540543                                        LR 0.023500    Time 0.091940    
2024-02-17 12:19:44,604 - Epoch: [128][  300/  391]    Overall Loss 0.556833    Objective Loss 0.556833                                        LR 0.023500    Time 0.093306    
2024-02-17 12:19:53,437 - Epoch: [128][  391/  391]    Overall Loss 0.567360    Objective Loss 0.567360    Top1 82.692308    Top5 98.076923    LR 0.023500    Time 0.094170    
2024-02-17 12:19:53,577 - --- validate (epoch=128)-----------
2024-02-17 12:19:53,578 - 10000 samples (128 per mini-batch)
2024-02-17 12:19:56,461 - Epoch: [128][   79/   79]    Loss 1.352653    Top1 63.800000    Top5 88.890000    
2024-02-17 12:19:56,599 - ==> Top1: 63.800    Top5: 88.890    Loss: 1.353

2024-02-17 12:19:56,622 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:19:56,622 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:19:56,703 - 

2024-02-17 12:19:56,703 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:20:06,839 - Epoch: [129][  100/  391]    Overall Loss 0.539323    Objective Loss 0.539323                                        LR 0.023500    Time 0.101283    
2024-02-17 12:20:16,560 - Epoch: [129][  200/  391]    Overall Loss 0.541701    Objective Loss 0.541701                                        LR 0.023500    Time 0.099222    
2024-02-17 12:20:26,257 - Epoch: [129][  300/  391]    Overall Loss 0.555666    Objective Loss 0.555666                                        LR 0.023500    Time 0.098454    
2024-02-17 12:20:35,280 - Epoch: [129][  391/  391]    Overall Loss 0.559215    Objective Loss 0.559215    Top1 81.250000    Top5 98.076923    LR 0.023500    Time 0.098606    
2024-02-17 12:20:35,416 - --- validate (epoch=129)-----------
2024-02-17 12:20:35,417 - 10000 samples (128 per mini-batch)
2024-02-17 12:20:38,371 - Epoch: [129][   79/   79]    Loss 1.362347    Top1 64.070000    Top5 89.020000    
2024-02-17 12:20:38,556 - ==> Top1: 64.070    Top5: 89.020    Loss: 1.362

2024-02-17 12:20:38,573 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:20:38,574 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:20:38,655 - 

2024-02-17 12:20:38,655 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:20:48,817 - Epoch: [130][  100/  391]    Overall Loss 0.514602    Objective Loss 0.514602                                        LR 0.023500    Time 0.101544    
2024-02-17 12:20:58,155 - Epoch: [130][  200/  391]    Overall Loss 0.527521    Objective Loss 0.527521                                        LR 0.023500    Time 0.097439    
2024-02-17 12:21:07,716 - Epoch: [130][  300/  391]    Overall Loss 0.536228    Objective Loss 0.536228                                        LR 0.023500    Time 0.096814    
2024-02-17 12:21:16,566 - Epoch: [130][  391/  391]    Overall Loss 0.550193    Objective Loss 0.550193    Top1 82.692308    Top5 97.596154    LR 0.023500    Time 0.096905    
2024-02-17 12:21:16,688 - --- validate (epoch=130)-----------
2024-02-17 12:21:16,688 - 10000 samples (128 per mini-batch)
2024-02-17 12:21:19,399 - Epoch: [130][   79/   79]    Loss 1.338975    Top1 64.160000    Top5 88.900000    
2024-02-17 12:21:19,522 - ==> Top1: 64.160    Top5: 88.900    Loss: 1.339

2024-02-17 12:21:19,543 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:21:19,544 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:21:19,626 - 

2024-02-17 12:21:19,626 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:21:29,779 - Epoch: [131][  100/  391]    Overall Loss 0.534401    Objective Loss 0.534401                                        LR 0.023500    Time 0.101454    
2024-02-17 12:21:39,376 - Epoch: [131][  200/  391]    Overall Loss 0.541284    Objective Loss 0.541284                                        LR 0.023500    Time 0.098689    
2024-02-17 12:21:48,895 - Epoch: [131][  300/  391]    Overall Loss 0.550552    Objective Loss 0.550552                                        LR 0.023500    Time 0.097508    
2024-02-17 12:21:57,427 - Epoch: [131][  391/  391]    Overall Loss 0.556768    Objective Loss 0.556768    Top1 79.807692    Top5 98.076923    LR 0.023500    Time 0.096625    
2024-02-17 12:21:57,583 - --- validate (epoch=131)-----------
2024-02-17 12:21:57,584 - 10000 samples (128 per mini-batch)
2024-02-17 12:22:00,467 - Epoch: [131][   79/   79]    Loss 1.422520    Top1 62.650000    Top5 88.430000    
2024-02-17 12:22:00,618 - ==> Top1: 62.650    Top5: 88.430    Loss: 1.423

2024-02-17 12:22:00,638 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:22:00,639 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:22:00,715 - 

2024-02-17 12:22:00,716 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:22:10,909 - Epoch: [132][  100/  391]    Overall Loss 0.524812    Objective Loss 0.524812                                        LR 0.023500    Time 0.101853    
2024-02-17 12:22:20,451 - Epoch: [132][  200/  391]    Overall Loss 0.534590    Objective Loss 0.534590                                        LR 0.023500    Time 0.098614    
2024-02-17 12:22:29,950 - Epoch: [132][  300/  391]    Overall Loss 0.547905    Objective Loss 0.547905                                        LR 0.023500    Time 0.097395    
2024-02-17 12:22:38,307 - Epoch: [132][  391/  391]    Overall Loss 0.561220    Objective Loss 0.561220    Top1 83.173077    Top5 98.076923    LR 0.023500    Time 0.096089    
2024-02-17 12:22:38,465 - --- validate (epoch=132)-----------
2024-02-17 12:22:38,466 - 10000 samples (128 per mini-batch)
2024-02-17 12:22:41,219 - Epoch: [132][   79/   79]    Loss 1.339014    Top1 64.200000    Top5 89.530000    
2024-02-17 12:22:41,311 - ==> Top1: 64.200    Top5: 89.530    Loss: 1.339

2024-02-17 12:22:41,329 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:22:41,329 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:22:41,411 - 

2024-02-17 12:22:41,411 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:22:51,998 - Epoch: [133][  100/  391]    Overall Loss 0.513847    Objective Loss 0.513847                                        LR 0.023500    Time 0.105745    
2024-02-17 12:23:01,710 - Epoch: [133][  200/  391]    Overall Loss 0.534833    Objective Loss 0.534833                                        LR 0.023500    Time 0.101410    
2024-02-17 12:23:11,305 - Epoch: [133][  300/  391]    Overall Loss 0.545445    Objective Loss 0.545445                                        LR 0.023500    Time 0.099575    
2024-02-17 12:23:19,661 - Epoch: [133][  391/  391]    Overall Loss 0.557363    Objective Loss 0.557363    Top1 85.096154    Top5 98.557692    LR 0.023500    Time 0.097763    
2024-02-17 12:23:19,782 - --- validate (epoch=133)-----------
2024-02-17 12:23:19,783 - 10000 samples (128 per mini-batch)
2024-02-17 12:23:22,513 - Epoch: [133][   79/   79]    Loss 1.387939    Top1 63.200000    Top5 88.800000    
2024-02-17 12:23:22,636 - ==> Top1: 63.200    Top5: 88.800    Loss: 1.388

2024-02-17 12:23:22,657 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:23:22,657 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:23:22,736 - 

2024-02-17 12:23:22,736 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:23:33,218 - Epoch: [134][  100/  391]    Overall Loss 0.520899    Objective Loss 0.520899                                        LR 0.023500    Time 0.104741    
2024-02-17 12:23:42,456 - Epoch: [134][  200/  391]    Overall Loss 0.534835    Objective Loss 0.534835                                        LR 0.023500    Time 0.098539    
2024-02-17 12:23:51,961 - Epoch: [134][  300/  391]    Overall Loss 0.546189    Objective Loss 0.546189                                        LR 0.023500    Time 0.097360    
2024-02-17 12:24:00,347 - Epoch: [134][  391/  391]    Overall Loss 0.559324    Objective Loss 0.559324    Top1 75.480769    Top5 97.596154    LR 0.023500    Time 0.096140    
2024-02-17 12:24:00,498 - --- validate (epoch=134)-----------
2024-02-17 12:24:00,499 - 10000 samples (128 per mini-batch)
2024-02-17 12:24:03,157 - Epoch: [134][   79/   79]    Loss 1.345968    Top1 64.770000    Top5 88.710000    
2024-02-17 12:24:03,272 - ==> Top1: 64.770    Top5: 88.710    Loss: 1.346

2024-02-17 12:24:03,291 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:24:03,291 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:24:03,371 - 

2024-02-17 12:24:03,372 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:24:13,584 - Epoch: [135][  100/  391]    Overall Loss 0.523758    Objective Loss 0.523758                                        LR 0.023500    Time 0.102044    
2024-02-17 12:24:22,899 - Epoch: [135][  200/  391]    Overall Loss 0.531264    Objective Loss 0.531264                                        LR 0.023500    Time 0.097579    
2024-02-17 12:24:32,504 - Epoch: [135][  300/  391]    Overall Loss 0.546272    Objective Loss 0.546272                                        LR 0.023500    Time 0.097054    
2024-02-17 12:24:40,971 - Epoch: [135][  391/  391]    Overall Loss 0.555146    Objective Loss 0.555146    Top1 81.730769    Top5 98.076923    LR 0.023500    Time 0.096111    
2024-02-17 12:24:41,134 - --- validate (epoch=135)-----------
2024-02-17 12:24:41,135 - 10000 samples (128 per mini-batch)
2024-02-17 12:24:43,837 - Epoch: [135][   79/   79]    Loss 1.408276    Top1 62.730000    Top5 88.480000    
2024-02-17 12:24:44,007 - ==> Top1: 62.730    Top5: 88.480    Loss: 1.408

2024-02-17 12:24:44,022 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:24:44,022 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:24:44,110 - 

2024-02-17 12:24:44,110 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:24:54,442 - Epoch: [136][  100/  391]    Overall Loss 0.515902    Objective Loss 0.515902                                        LR 0.023500    Time 0.103238    
2024-02-17 12:25:03,923 - Epoch: [136][  200/  391]    Overall Loss 0.523318    Objective Loss 0.523318                                        LR 0.023500    Time 0.099005    
2024-02-17 12:25:13,484 - Epoch: [136][  300/  391]    Overall Loss 0.537852    Objective Loss 0.537852                                        LR 0.023500    Time 0.097859    
2024-02-17 12:25:22,038 - Epoch: [136][  391/  391]    Overall Loss 0.549399    Objective Loss 0.549399    Top1 79.326923    Top5 98.076923    LR 0.023500    Time 0.096949    
2024-02-17 12:25:22,163 - --- validate (epoch=136)-----------
2024-02-17 12:25:22,164 - 10000 samples (128 per mini-batch)
2024-02-17 12:25:25,034 - Epoch: [136][   79/   79]    Loss 1.336822    Top1 63.610000    Top5 89.170000    
2024-02-17 12:25:25,152 - ==> Top1: 63.610    Top5: 89.170    Loss: 1.337

2024-02-17 12:25:25,171 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:25:25,171 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:25:25,254 - 

2024-02-17 12:25:25,255 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:25:35,397 - Epoch: [137][  100/  391]    Overall Loss 0.522268    Objective Loss 0.522268                                        LR 0.023500    Time 0.101322    
2024-02-17 12:25:45,192 - Epoch: [137][  200/  391]    Overall Loss 0.522617    Objective Loss 0.522617                                        LR 0.023500    Time 0.099609    
2024-02-17 12:25:54,864 - Epoch: [137][  300/  391]    Overall Loss 0.533800    Objective Loss 0.533800                                        LR 0.023500    Time 0.098633    
2024-02-17 12:26:03,749 - Epoch: [137][  391/  391]    Overall Loss 0.544992    Objective Loss 0.544992    Top1 81.250000    Top5 98.076923    LR 0.023500    Time 0.098390    
2024-02-17 12:26:03,935 - --- validate (epoch=137)-----------
2024-02-17 12:26:03,936 - 10000 samples (128 per mini-batch)
2024-02-17 12:26:06,945 - Epoch: [137][   79/   79]    Loss 1.417838    Top1 62.180000    Top5 88.250000    
2024-02-17 12:26:07,064 - ==> Top1: 62.180    Top5: 88.250    Loss: 1.418

2024-02-17 12:26:07,083 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:26:07,084 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:26:07,159 - 

2024-02-17 12:26:07,159 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:26:17,684 - Epoch: [138][  100/  391]    Overall Loss 0.518913    Objective Loss 0.518913                                        LR 0.023500    Time 0.105169    
2024-02-17 12:26:27,256 - Epoch: [138][  200/  391]    Overall Loss 0.531409    Objective Loss 0.531409                                        LR 0.023500    Time 0.100421    
2024-02-17 12:26:36,431 - Epoch: [138][  300/  391]    Overall Loss 0.538835    Objective Loss 0.538835                                        LR 0.023500    Time 0.097516    
2024-02-17 12:26:45,084 - Epoch: [138][  391/  391]    Overall Loss 0.549000    Objective Loss 0.549000    Top1 77.884615    Top5 96.153846    LR 0.023500    Time 0.096941    
2024-02-17 12:26:45,234 - --- validate (epoch=138)-----------
2024-02-17 12:26:45,235 - 10000 samples (128 per mini-batch)
2024-02-17 12:26:48,163 - Epoch: [138][   79/   79]    Loss 1.504057    Top1 61.290000    Top5 87.450000    
2024-02-17 12:26:48,272 - ==> Top1: 61.290    Top5: 87.450    Loss: 1.504

2024-02-17 12:26:48,293 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:26:48,293 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:26:48,370 - 

2024-02-17 12:26:48,371 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:26:58,623 - Epoch: [139][  100/  391]    Overall Loss 0.522495    Objective Loss 0.522495                                        LR 0.023500    Time 0.102443    
2024-02-17 12:27:08,155 - Epoch: [139][  200/  391]    Overall Loss 0.530554    Objective Loss 0.530554                                        LR 0.023500    Time 0.098856    
2024-02-17 12:27:17,588 - Epoch: [139][  300/  391]    Overall Loss 0.539933    Objective Loss 0.539933                                        LR 0.023500    Time 0.097334    
2024-02-17 12:27:26,421 - Epoch: [139][  391/  391]    Overall Loss 0.545849    Objective Loss 0.545849    Top1 79.326923    Top5 98.076923    LR 0.023500    Time 0.097260    
2024-02-17 12:27:26,549 - --- validate (epoch=139)-----------
2024-02-17 12:27:26,549 - 10000 samples (128 per mini-batch)
2024-02-17 12:27:29,390 - Epoch: [139][   79/   79]    Loss 1.368089    Top1 63.390000    Top5 88.880000    
2024-02-17 12:27:29,516 - ==> Top1: 63.390    Top5: 88.880    Loss: 1.368

2024-02-17 12:27:29,537 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:27:29,537 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:27:29,612 - 

2024-02-17 12:27:29,612 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:27:39,586 - Epoch: [140][  100/  391]    Overall Loss 0.506662    Objective Loss 0.506662                                        LR 0.023500    Time 0.099668    
2024-02-17 12:27:49,141 - Epoch: [140][  200/  391]    Overall Loss 0.516688    Objective Loss 0.516688                                        LR 0.023500    Time 0.097583    
2024-02-17 12:27:58,573 - Epoch: [140][  300/  391]    Overall Loss 0.526371    Objective Loss 0.526371                                        LR 0.023500    Time 0.096481    
2024-02-17 12:28:07,561 - Epoch: [140][  391/  391]    Overall Loss 0.537069    Objective Loss 0.537069    Top1 80.288462    Top5 98.557692    LR 0.023500    Time 0.097002    
2024-02-17 12:28:07,724 - --- validate (epoch=140)-----------
2024-02-17 12:28:07,725 - 10000 samples (128 per mini-batch)
2024-02-17 12:28:10,759 - Epoch: [140][   79/   79]    Loss 1.344355    Top1 63.930000    Top5 89.210000    
2024-02-17 12:28:10,886 - ==> Top1: 63.930    Top5: 89.210    Loss: 1.344

2024-02-17 12:28:10,905 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:28:10,905 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:28:10,980 - 

2024-02-17 12:28:10,981 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:28:21,042 - Epoch: [141][  100/  391]    Overall Loss 0.500247    Objective Loss 0.500247                                        LR 0.023500    Time 0.100541    
2024-02-17 12:28:30,694 - Epoch: [141][  200/  391]    Overall Loss 0.519171    Objective Loss 0.519171                                        LR 0.023500    Time 0.098504    
2024-02-17 12:28:40,317 - Epoch: [141][  300/  391]    Overall Loss 0.528672    Objective Loss 0.528672                                        LR 0.023500    Time 0.097734    
2024-02-17 12:28:49,049 - Epoch: [141][  391/  391]    Overall Loss 0.538246    Objective Loss 0.538246    Top1 81.250000    Top5 98.076923    LR 0.023500    Time 0.097308    
2024-02-17 12:28:49,244 - --- validate (epoch=141)-----------
2024-02-17 12:28:49,245 - 10000 samples (128 per mini-batch)
2024-02-17 12:28:51,863 - Epoch: [141][   79/   79]    Loss 1.428072    Top1 63.040000    Top5 88.220000    
2024-02-17 12:28:52,036 - ==> Top1: 63.040    Top5: 88.220    Loss: 1.428

2024-02-17 12:28:52,056 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:28:52,056 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:28:52,138 - 

2024-02-17 12:28:52,139 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:29:01,941 - Epoch: [142][  100/  391]    Overall Loss 0.508290    Objective Loss 0.508290                                        LR 0.023500    Time 0.097949    
2024-02-17 12:29:09,418 - Epoch: [142][  200/  391]    Overall Loss 0.517290    Objective Loss 0.517290                                        LR 0.023500    Time 0.086344    
2024-02-17 12:29:16,635 - Epoch: [142][  300/  391]    Overall Loss 0.529299    Objective Loss 0.529299                                        LR 0.023500    Time 0.081606    
2024-02-17 12:29:23,377 - Epoch: [142][  391/  391]    Overall Loss 0.539657    Objective Loss 0.539657    Top1 88.942308    Top5 99.038462    LR 0.023500    Time 0.079847    
2024-02-17 12:29:23,549 - --- validate (epoch=142)-----------
2024-02-17 12:29:23,549 - 10000 samples (128 per mini-batch)
2024-02-17 12:29:26,512 - Epoch: [142][   79/   79]    Loss 1.381608    Top1 63.870000    Top5 88.800000    
2024-02-17 12:29:26,627 - ==> Top1: 63.870    Top5: 88.800    Loss: 1.382

2024-02-17 12:29:26,648 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:29:26,648 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:29:26,727 - 

2024-02-17 12:29:26,728 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:29:36,823 - Epoch: [143][  100/  391]    Overall Loss 0.506073    Objective Loss 0.506073                                        LR 0.023500    Time 0.100871    
2024-02-17 12:29:45,987 - Epoch: [143][  200/  391]    Overall Loss 0.516722    Objective Loss 0.516722                                        LR 0.023500    Time 0.096234    
2024-02-17 12:29:55,640 - Epoch: [143][  300/  391]    Overall Loss 0.524782    Objective Loss 0.524782                                        LR 0.023500    Time 0.096317    
2024-02-17 12:30:04,555 - Epoch: [143][  391/  391]    Overall Loss 0.535591    Objective Loss 0.535591    Top1 79.326923    Top5 99.038462    LR 0.023500    Time 0.096691    
2024-02-17 12:30:04,729 - --- validate (epoch=143)-----------
2024-02-17 12:30:04,729 - 10000 samples (128 per mini-batch)
2024-02-17 12:30:07,482 - Epoch: [143][   79/   79]    Loss 1.403229    Top1 62.710000    Top5 88.530000    
2024-02-17 12:30:07,613 - ==> Top1: 62.710    Top5: 88.530    Loss: 1.403

2024-02-17 12:30:07,631 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:30:07,631 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:30:07,736 - 

2024-02-17 12:30:07,736 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:30:17,363 - Epoch: [144][  100/  391]    Overall Loss 0.511261    Objective Loss 0.511261                                        LR 0.023500    Time 0.096196    
2024-02-17 12:30:26,803 - Epoch: [144][  200/  391]    Overall Loss 0.515480    Objective Loss 0.515480                                        LR 0.023500    Time 0.095272    
2024-02-17 12:30:36,382 - Epoch: [144][  300/  391]    Overall Loss 0.523080    Objective Loss 0.523080                                        LR 0.023500    Time 0.095431    
2024-02-17 12:30:45,132 - Epoch: [144][  391/  391]    Overall Loss 0.534124    Objective Loss 0.534124    Top1 83.653846    Top5 99.519231    LR 0.023500    Time 0.095588    
2024-02-17 12:30:45,299 - --- validate (epoch=144)-----------
2024-02-17 12:30:45,300 - 10000 samples (128 per mini-batch)
2024-02-17 12:30:48,207 - Epoch: [144][   79/   79]    Loss 1.392520    Top1 63.310000    Top5 88.690000    
2024-02-17 12:30:48,379 - ==> Top1: 63.310    Top5: 88.690    Loss: 1.393

2024-02-17 12:30:48,397 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:30:48,397 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:30:48,477 - 

2024-02-17 12:30:48,478 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:30:58,486 - Epoch: [145][  100/  391]    Overall Loss 0.497500    Objective Loss 0.497500                                        LR 0.023500    Time 0.100000    
2024-02-17 12:31:07,772 - Epoch: [145][  200/  391]    Overall Loss 0.509858    Objective Loss 0.509858                                        LR 0.023500    Time 0.096410    
2024-02-17 12:31:17,350 - Epoch: [145][  300/  391]    Overall Loss 0.521146    Objective Loss 0.521146                                        LR 0.023500    Time 0.096184    
2024-02-17 12:31:26,177 - Epoch: [145][  391/  391]    Overall Loss 0.532100    Objective Loss 0.532100    Top1 86.057692    Top5 100.000000    LR 0.023500    Time 0.096363    
2024-02-17 12:31:26,306 - --- validate (epoch=145)-----------
2024-02-17 12:31:26,307 - 10000 samples (128 per mini-batch)
2024-02-17 12:31:29,218 - Epoch: [145][   79/   79]    Loss 1.369270    Top1 63.570000    Top5 88.880000    
2024-02-17 12:31:29,337 - ==> Top1: 63.570    Top5: 88.880    Loss: 1.369

2024-02-17 12:31:29,348 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:31:29,348 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:31:29,423 - 

2024-02-17 12:31:29,423 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:31:39,766 - Epoch: [146][  100/  391]    Overall Loss 0.505544    Objective Loss 0.505544                                        LR 0.023500    Time 0.103349    
2024-02-17 12:31:49,089 - Epoch: [146][  200/  391]    Overall Loss 0.508453    Objective Loss 0.508453                                        LR 0.023500    Time 0.098267    
2024-02-17 12:31:58,658 - Epoch: [146][  300/  391]    Overall Loss 0.518757    Objective Loss 0.518757                                        LR 0.023500    Time 0.097394    
2024-02-17 12:32:07,448 - Epoch: [146][  391/  391]    Overall Loss 0.527722    Objective Loss 0.527722    Top1 78.365385    Top5 95.192308    LR 0.023500    Time 0.097197    
2024-02-17 12:32:07,620 - --- validate (epoch=146)-----------
2024-02-17 12:32:07,621 - 10000 samples (128 per mini-batch)
2024-02-17 12:32:10,453 - Epoch: [146][   79/   79]    Loss 1.445637    Top1 62.150000    Top5 88.240000    
2024-02-17 12:32:10,559 - ==> Top1: 62.150    Top5: 88.240    Loss: 1.446

2024-02-17 12:32:10,576 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:32:10,576 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:32:10,653 - 

2024-02-17 12:32:10,654 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:32:20,705 - Epoch: [147][  100/  391]    Overall Loss 0.515762    Objective Loss 0.515762                                        LR 0.023500    Time 0.100434    
2024-02-17 12:32:30,297 - Epoch: [147][  200/  391]    Overall Loss 0.522330    Objective Loss 0.522330                                        LR 0.023500    Time 0.098149    
2024-02-17 12:32:39,473 - Epoch: [147][  300/  391]    Overall Loss 0.527639    Objective Loss 0.527639                                        LR 0.023500    Time 0.096005    
2024-02-17 12:32:47,497 - Epoch: [147][  391/  391]    Overall Loss 0.533106    Objective Loss 0.533106    Top1 87.500000    Top5 99.038462    LR 0.023500    Time 0.094173    
2024-02-17 12:32:47,629 - --- validate (epoch=147)-----------
2024-02-17 12:32:47,629 - 10000 samples (128 per mini-batch)
2024-02-17 12:32:50,459 - Epoch: [147][   79/   79]    Loss 1.429253    Top1 62.560000    Top5 88.260000    
2024-02-17 12:32:50,664 - ==> Top1: 62.560    Top5: 88.260    Loss: 1.429

2024-02-17 12:32:50,680 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:32:50,680 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:32:50,762 - 

2024-02-17 12:32:50,762 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:33:01,394 - Epoch: [148][  100/  391]    Overall Loss 0.483663    Objective Loss 0.483663                                        LR 0.023500    Time 0.106238    
2024-02-17 12:33:10,772 - Epoch: [148][  200/  391]    Overall Loss 0.503727    Objective Loss 0.503727                                        LR 0.023500    Time 0.099986    
2024-02-17 12:33:20,358 - Epoch: [148][  300/  391]    Overall Loss 0.519878    Objective Loss 0.519878                                        LR 0.023500    Time 0.098598    
2024-02-17 12:33:28,982 - Epoch: [148][  391/  391]    Overall Loss 0.528989    Objective Loss 0.528989    Top1 79.326923    Top5 97.115385    LR 0.023500    Time 0.097696    
2024-02-17 12:33:29,104 - --- validate (epoch=148)-----------
2024-02-17 12:33:29,105 - 10000 samples (128 per mini-batch)
2024-02-17 12:33:32,044 - Epoch: [148][   79/   79]    Loss 1.451930    Top1 62.220000    Top5 88.140000    
2024-02-17 12:33:32,178 - ==> Top1: 62.220    Top5: 88.140    Loss: 1.452

2024-02-17 12:33:32,189 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:33:32,189 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:33:32,270 - 

2024-02-17 12:33:32,271 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:33:42,429 - Epoch: [149][  100/  391]    Overall Loss 0.499607    Objective Loss 0.499607                                        LR 0.023500    Time 0.101509    
2024-02-17 12:33:52,236 - Epoch: [149][  200/  391]    Overall Loss 0.514419    Objective Loss 0.514419                                        LR 0.023500    Time 0.099765    
2024-02-17 12:34:01,822 - Epoch: [149][  300/  391]    Overall Loss 0.520507    Objective Loss 0.520507                                        LR 0.023500    Time 0.098446    
2024-02-17 12:34:10,611 - Epoch: [149][  391/  391]    Overall Loss 0.523791    Objective Loss 0.523791    Top1 77.884615    Top5 96.634615    LR 0.023500    Time 0.098002    
2024-02-17 12:34:10,771 - --- validate (epoch=149)-----------
2024-02-17 12:34:10,772 - 10000 samples (128 per mini-batch)
2024-02-17 12:34:13,901 - Epoch: [149][   79/   79]    Loss 1.443723    Top1 62.570000    Top5 88.140000    
2024-02-17 12:34:14,025 - ==> Top1: 62.570    Top5: 88.140    Loss: 1.444

2024-02-17 12:34:14,040 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:34:14,040 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:34:14,114 - 

2024-02-17 12:34:14,114 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:34:24,296 - Epoch: [150][  100/  391]    Overall Loss 0.404760    Objective Loss 0.404760                                        LR 0.005522    Time 0.101748    
2024-02-17 12:34:33,960 - Epoch: [150][  200/  391]    Overall Loss 0.394318    Objective Loss 0.394318                                        LR 0.005522    Time 0.099167    
2024-02-17 12:34:43,481 - Epoch: [150][  300/  391]    Overall Loss 0.381782    Objective Loss 0.381782                                        LR 0.005522    Time 0.097834    
2024-02-17 12:34:52,227 - Epoch: [150][  391/  391]    Overall Loss 0.376352    Objective Loss 0.376352    Top1 91.346154    Top5 99.038462    LR 0.005522    Time 0.097419    
2024-02-17 12:34:52,351 - --- validate (epoch=150)-----------
2024-02-17 12:34:52,352 - 10000 samples (128 per mini-batch)
2024-02-17 12:34:55,491 - Epoch: [150][   79/   79]    Loss 1.232871    Top1 67.010000    Top5 90.670000    
2024-02-17 12:34:55,623 - ==> Top1: 67.010    Top5: 90.670    Loss: 1.233

2024-02-17 12:34:55,639 - ==> Best [Top1: 67.010   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 150]
2024-02-17 12:34:55,639 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:34:55,737 - 

2024-02-17 12:34:55,737 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:35:05,867 - Epoch: [151][  100/  391]    Overall Loss 0.337969    Objective Loss 0.337969                                        LR 0.005522    Time 0.101211    
2024-02-17 12:35:15,638 - Epoch: [151][  200/  391]    Overall Loss 0.335457    Objective Loss 0.335457                                        LR 0.005522    Time 0.099439    
2024-02-17 12:35:25,230 - Epoch: [151][  300/  391]    Overall Loss 0.331588    Objective Loss 0.331588                                        LR 0.005522    Time 0.098251    
2024-02-17 12:35:34,048 - Epoch: [151][  391/  391]    Overall Loss 0.332760    Objective Loss 0.332760    Top1 92.307692    Top5 99.519231    LR 0.005522    Time 0.097926    
2024-02-17 12:35:34,196 - --- validate (epoch=151)-----------
2024-02-17 12:35:34,197 - 10000 samples (128 per mini-batch)
2024-02-17 12:35:37,079 - Epoch: [151][   79/   79]    Loss 1.224059    Top1 67.100000    Top5 90.580000    
2024-02-17 12:35:37,191 - ==> Top1: 67.100    Top5: 90.580    Loss: 1.224

2024-02-17 12:35:37,211 - ==> Best [Top1: 67.100   Top5: 90.580   Sparsity:0.00   Params: 1341960 on epoch: 151]
2024-02-17 12:35:37,212 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:35:37,318 - 

2024-02-17 12:35:37,319 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:35:47,385 - Epoch: [152][  100/  391]    Overall Loss 0.314529    Objective Loss 0.314529                                        LR 0.005522    Time 0.100578    
2024-02-17 12:35:57,006 - Epoch: [152][  200/  391]    Overall Loss 0.308978    Objective Loss 0.308978                                        LR 0.005522    Time 0.098371    
2024-02-17 12:36:06,566 - Epoch: [152][  300/  391]    Overall Loss 0.311261    Objective Loss 0.311261                                        LR 0.005522    Time 0.097434    
2024-02-17 12:36:15,236 - Epoch: [152][  391/  391]    Overall Loss 0.315546    Objective Loss 0.315546    Top1 85.576923    Top5 99.519231    LR 0.005522    Time 0.096922    
2024-02-17 12:36:15,388 - --- validate (epoch=152)-----------
2024-02-17 12:36:15,388 - 10000 samples (128 per mini-batch)
2024-02-17 12:36:18,246 - Epoch: [152][   79/   79]    Loss 1.244324    Top1 67.250000    Top5 90.670000    
2024-02-17 12:36:18,367 - ==> Top1: 67.250    Top5: 90.670    Loss: 1.244

2024-02-17 12:36:18,388 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:36:18,389 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:36:18,495 - 

2024-02-17 12:36:18,496 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:36:28,148 - Epoch: [153][  100/  391]    Overall Loss 0.311488    Objective Loss 0.311488                                        LR 0.005522    Time 0.096440    
2024-02-17 12:36:37,610 - Epoch: [153][  200/  391]    Overall Loss 0.308623    Objective Loss 0.308623                                        LR 0.005522    Time 0.095509    
2024-02-17 12:36:47,148 - Epoch: [153][  300/  391]    Overall Loss 0.307036    Objective Loss 0.307036                                        LR 0.005522    Time 0.095449    
2024-02-17 12:36:55,589 - Epoch: [153][  391/  391]    Overall Loss 0.307062    Objective Loss 0.307062    Top1 88.942308    Top5 100.000000    LR 0.005522    Time 0.094813    
2024-02-17 12:36:55,705 - --- validate (epoch=153)-----------
2024-02-17 12:36:55,706 - 10000 samples (128 per mini-batch)
2024-02-17 12:36:58,562 - Epoch: [153][   79/   79]    Loss 1.239815    Top1 66.950000    Top5 90.360000    
2024-02-17 12:36:58,698 - ==> Top1: 66.950    Top5: 90.360    Loss: 1.240

2024-02-17 12:36:58,719 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:36:58,719 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:36:58,796 - 

2024-02-17 12:36:58,797 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:37:08,731 - Epoch: [154][  100/  391]    Overall Loss 0.287211    Objective Loss 0.287211                                        LR 0.005522    Time 0.099276    
2024-02-17 12:37:18,464 - Epoch: [154][  200/  391]    Overall Loss 0.291827    Objective Loss 0.291827                                        LR 0.005522    Time 0.098279    
2024-02-17 12:37:28,783 - Epoch: [154][  300/  391]    Overall Loss 0.296863    Objective Loss 0.296863                                        LR 0.005522    Time 0.099898    
2024-02-17 12:37:37,517 - Epoch: [154][  391/  391]    Overall Loss 0.298064    Objective Loss 0.298064    Top1 90.865385    Top5 99.038462    LR 0.005522    Time 0.098976    
2024-02-17 12:37:37,638 - --- validate (epoch=154)-----------
2024-02-17 12:37:37,639 - 10000 samples (128 per mini-batch)
2024-02-17 12:37:40,585 - Epoch: [154][   79/   79]    Loss 1.260500    Top1 66.990000    Top5 90.430000    
2024-02-17 12:37:40,700 - ==> Top1: 66.990    Top5: 90.430    Loss: 1.261

2024-02-17 12:37:40,712 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:37:40,712 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:37:40,787 - 

2024-02-17 12:37:40,787 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:37:50,740 - Epoch: [155][  100/  391]    Overall Loss 0.279161    Objective Loss 0.279161                                        LR 0.005522    Time 0.099453    
2024-02-17 12:38:00,402 - Epoch: [155][  200/  391]    Overall Loss 0.287515    Objective Loss 0.287515                                        LR 0.005522    Time 0.098013    
2024-02-17 12:38:09,879 - Epoch: [155][  300/  391]    Overall Loss 0.289487    Objective Loss 0.289487                                        LR 0.005522    Time 0.096919    
2024-02-17 12:38:18,246 - Epoch: [155][  391/  391]    Overall Loss 0.289827    Objective Loss 0.289827    Top1 94.230769    Top5 99.519231    LR 0.005522    Time 0.095752    
2024-02-17 12:38:18,405 - --- validate (epoch=155)-----------
2024-02-17 12:38:18,406 - 10000 samples (128 per mini-batch)
2024-02-17 12:38:21,227 - Epoch: [155][   79/   79]    Loss 1.260405    Top1 66.810000    Top5 90.430000    
2024-02-17 12:38:21,345 - ==> Top1: 66.810    Top5: 90.430    Loss: 1.260

2024-02-17 12:38:21,365 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:38:21,365 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:38:21,444 - 

2024-02-17 12:38:21,445 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:38:31,853 - Epoch: [156][  100/  391]    Overall Loss 0.276131    Objective Loss 0.276131                                        LR 0.005522    Time 0.104009    
2024-02-17 12:38:41,349 - Epoch: [156][  200/  391]    Overall Loss 0.277669    Objective Loss 0.277669                                        LR 0.005522    Time 0.099465    
2024-02-17 12:38:50,848 - Epoch: [156][  300/  391]    Overall Loss 0.281614    Objective Loss 0.281614                                        LR 0.005522    Time 0.097958    
2024-02-17 12:38:59,245 - Epoch: [156][  391/  391]    Overall Loss 0.281630    Objective Loss 0.281630    Top1 91.346154    Top5 99.038462    LR 0.005522    Time 0.096624    
2024-02-17 12:38:59,390 - --- validate (epoch=156)-----------
2024-02-17 12:38:59,391 - 10000 samples (128 per mini-batch)
2024-02-17 12:39:02,284 - Epoch: [156][   79/   79]    Loss 1.252237    Top1 66.960000    Top5 90.480000    
2024-02-17 12:39:02,393 - ==> Top1: 66.960    Top5: 90.480    Loss: 1.252

2024-02-17 12:39:02,411 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:39:02,411 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:39:02,492 - 

2024-02-17 12:39:02,492 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:39:12,451 - Epoch: [157][  100/  391]    Overall Loss 0.266113    Objective Loss 0.266113                                        LR 0.005522    Time 0.099519    
2024-02-17 12:39:22,126 - Epoch: [157][  200/  391]    Overall Loss 0.273138    Objective Loss 0.273138                                        LR 0.005522    Time 0.098111    
2024-02-17 12:39:31,933 - Epoch: [157][  300/  391]    Overall Loss 0.277272    Objective Loss 0.277272                                        LR 0.005522    Time 0.098081    
2024-02-17 12:39:41,020 - Epoch: [157][  391/  391]    Overall Loss 0.279318    Objective Loss 0.279318    Top1 91.826923    Top5 99.038462    LR 0.005522    Time 0.098483    
2024-02-17 12:39:41,148 - --- validate (epoch=157)-----------
2024-02-17 12:39:41,148 - 10000 samples (128 per mini-batch)
2024-02-17 12:39:44,034 - Epoch: [157][   79/   79]    Loss 1.282982    Top1 66.430000    Top5 90.060000    
2024-02-17 12:39:44,140 - ==> Top1: 66.430    Top5: 90.060    Loss: 1.283

2024-02-17 12:39:44,160 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:39:44,161 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:39:44,236 - 

2024-02-17 12:39:44,237 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:39:54,450 - Epoch: [158][  100/  391]    Overall Loss 0.258010    Objective Loss 0.258010                                        LR 0.005522    Time 0.102065    
2024-02-17 12:40:03,676 - Epoch: [158][  200/  391]    Overall Loss 0.270554    Objective Loss 0.270554                                        LR 0.005522    Time 0.097140    
2024-02-17 12:40:13,228 - Epoch: [158][  300/  391]    Overall Loss 0.270927    Objective Loss 0.270927                                        LR 0.005522    Time 0.096585    
2024-02-17 12:40:21,849 - Epoch: [158][  391/  391]    Overall Loss 0.271884    Objective Loss 0.271884    Top1 91.826923    Top5 98.557692    LR 0.005522    Time 0.096143    
2024-02-17 12:40:21,966 - --- validate (epoch=158)-----------
2024-02-17 12:40:21,967 - 10000 samples (128 per mini-batch)
2024-02-17 12:40:24,663 - Epoch: [158][   79/   79]    Loss 1.297263    Top1 66.580000    Top5 90.440000    
2024-02-17 12:40:24,771 - ==> Top1: 66.580    Top5: 90.440    Loss: 1.297

2024-02-17 12:40:24,792 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:40:24,792 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:40:24,868 - 

2024-02-17 12:40:24,868 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:40:35,095 - Epoch: [159][  100/  391]    Overall Loss 0.262810    Objective Loss 0.262810                                        LR 0.005522    Time 0.102197    
2024-02-17 12:40:44,662 - Epoch: [159][  200/  391]    Overall Loss 0.264519    Objective Loss 0.264519                                        LR 0.005522    Time 0.098912    
2024-02-17 12:40:54,212 - Epoch: [159][  300/  391]    Overall Loss 0.265082    Objective Loss 0.265082                                        LR 0.005522    Time 0.097760    
2024-02-17 12:41:02,496 - Epoch: [159][  391/  391]    Overall Loss 0.266618    Objective Loss 0.266618    Top1 89.423077    Top5 98.076923    LR 0.005522    Time 0.096184    
2024-02-17 12:41:02,670 - --- validate (epoch=159)-----------
2024-02-17 12:41:02,670 - 10000 samples (128 per mini-batch)
2024-02-17 12:41:05,301 - Epoch: [159][   79/   79]    Loss 1.277716    Top1 66.960000    Top5 90.310000    
2024-02-17 12:41:05,482 - ==> Top1: 66.960    Top5: 90.310    Loss: 1.278

2024-02-17 12:41:05,501 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:41:05,501 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:41:05,578 - 

2024-02-17 12:41:05,578 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:41:16,246 - Epoch: [160][  100/  391]    Overall Loss 0.251683    Objective Loss 0.251683                                        LR 0.005522    Time 0.106608    
2024-02-17 12:41:25,770 - Epoch: [160][  200/  391]    Overall Loss 0.258876    Objective Loss 0.258876                                        LR 0.005522    Time 0.100893    
2024-02-17 12:41:35,413 - Epoch: [160][  300/  391]    Overall Loss 0.260912    Objective Loss 0.260912                                        LR 0.005522    Time 0.099390    
2024-02-17 12:41:43,734 - Epoch: [160][  391/  391]    Overall Loss 0.262499    Objective Loss 0.262499    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.097530    
2024-02-17 12:41:43,852 - --- validate (epoch=160)-----------
2024-02-17 12:41:43,852 - 10000 samples (128 per mini-batch)
2024-02-17 12:41:46,551 - Epoch: [160][   79/   79]    Loss 1.284113    Top1 66.650000    Top5 90.270000    
2024-02-17 12:41:46,665 - ==> Top1: 66.650    Top5: 90.270    Loss: 1.284

2024-02-17 12:41:46,676 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:41:46,676 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:41:46,754 - 

2024-02-17 12:41:46,754 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:41:56,428 - Epoch: [161][  100/  391]    Overall Loss 0.247426    Objective Loss 0.247426                                        LR 0.005522    Time 0.096662    
2024-02-17 12:42:05,564 - Epoch: [161][  200/  391]    Overall Loss 0.250927    Objective Loss 0.250927                                        LR 0.005522    Time 0.093989    
2024-02-17 12:42:15,231 - Epoch: [161][  300/  391]    Overall Loss 0.255053    Objective Loss 0.255053                                        LR 0.005522    Time 0.094869    
2024-02-17 12:42:23,656 - Epoch: [161][  391/  391]    Overall Loss 0.256470    Objective Loss 0.256470    Top1 94.711538    Top5 99.519231    LR 0.005522    Time 0.094327    
2024-02-17 12:42:23,782 - --- validate (epoch=161)-----------
2024-02-17 12:42:23,783 - 10000 samples (128 per mini-batch)
2024-02-17 12:42:26,418 - Epoch: [161][   79/   79]    Loss 1.288213    Top1 66.610000    Top5 90.280000    
2024-02-17 12:42:26,596 - ==> Top1: 66.610    Top5: 90.280    Loss: 1.288

2024-02-17 12:42:26,616 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:42:26,616 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:42:26,689 - 

2024-02-17 12:42:26,690 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:42:36,691 - Epoch: [162][  100/  391]    Overall Loss 0.245268    Objective Loss 0.245268                                        LR 0.005522    Time 0.099942    
2024-02-17 12:42:46,131 - Epoch: [162][  200/  391]    Overall Loss 0.249498    Objective Loss 0.249498                                        LR 0.005522    Time 0.097147    
2024-02-17 12:42:55,620 - Epoch: [162][  300/  391]    Overall Loss 0.251475    Objective Loss 0.251475                                        LR 0.005522    Time 0.096380    
2024-02-17 12:43:04,266 - Epoch: [162][  391/  391]    Overall Loss 0.253709    Objective Loss 0.253709    Top1 93.269231    Top5 100.000000    LR 0.005522    Time 0.096050    
2024-02-17 12:43:04,423 - --- validate (epoch=162)-----------
2024-02-17 12:43:04,424 - 10000 samples (128 per mini-batch)
2024-02-17 12:43:07,277 - Epoch: [162][   79/   79]    Loss 1.289234    Top1 66.760000    Top5 90.090000    
2024-02-17 12:43:07,382 - ==> Top1: 66.760    Top5: 90.090    Loss: 1.289

2024-02-17 12:43:07,409 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:43:07,409 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:43:07,492 - 

2024-02-17 12:43:07,493 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:43:17,751 - Epoch: [163][  100/  391]    Overall Loss 0.245569    Objective Loss 0.245569                                        LR 0.005522    Time 0.102509    
2024-02-17 12:43:27,260 - Epoch: [163][  200/  391]    Overall Loss 0.245899    Objective Loss 0.245899                                        LR 0.005522    Time 0.098778    
2024-02-17 12:43:34,864 - Epoch: [163][  300/  391]    Overall Loss 0.248700    Objective Loss 0.248700                                        LR 0.005522    Time 0.091188    
2024-02-17 12:43:41,565 - Epoch: [163][  391/  391]    Overall Loss 0.251160    Objective Loss 0.251160    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.087093    
2024-02-17 12:43:41,703 - --- validate (epoch=163)-----------
2024-02-17 12:43:41,704 - 10000 samples (128 per mini-batch)
2024-02-17 12:43:44,636 - Epoch: [163][   79/   79]    Loss 1.309039    Top1 66.980000    Top5 90.130000    
2024-02-17 12:43:44,750 - ==> Top1: 66.980    Top5: 90.130    Loss: 1.309

2024-02-17 12:43:44,769 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:43:44,769 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:43:44,846 - 

2024-02-17 12:43:44,846 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:43:53,816 - Epoch: [164][  100/  391]    Overall Loss 0.241939    Objective Loss 0.241939                                        LR 0.005522    Time 0.089632    
2024-02-17 12:44:02,998 - Epoch: [164][  200/  391]    Overall Loss 0.245077    Objective Loss 0.245077                                        LR 0.005522    Time 0.090701    
2024-02-17 12:44:12,467 - Epoch: [164][  300/  391]    Overall Loss 0.247411    Objective Loss 0.247411                                        LR 0.005522    Time 0.092007    
2024-02-17 12:44:21,165 - Epoch: [164][  391/  391]    Overall Loss 0.249987    Objective Loss 0.249987    Top1 96.153846    Top5 100.000000    LR 0.005522    Time 0.092831    
2024-02-17 12:44:21,320 - --- validate (epoch=164)-----------
2024-02-17 12:44:21,320 - 10000 samples (128 per mini-batch)
2024-02-17 12:44:24,282 - Epoch: [164][   79/   79]    Loss 1.300506    Top1 66.680000    Top5 90.150000    
2024-02-17 12:44:24,392 - ==> Top1: 66.680    Top5: 90.150    Loss: 1.301

2024-02-17 12:44:24,412 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:44:24,412 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:44:24,504 - 

2024-02-17 12:44:24,505 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:44:34,641 - Epoch: [165][  100/  391]    Overall Loss 0.230396    Objective Loss 0.230396                                        LR 0.005522    Time 0.101285    
2024-02-17 12:44:44,305 - Epoch: [165][  200/  391]    Overall Loss 0.239541    Objective Loss 0.239541                                        LR 0.005522    Time 0.098934    
2024-02-17 12:44:53,960 - Epoch: [165][  300/  391]    Overall Loss 0.242799    Objective Loss 0.242799                                        LR 0.005522    Time 0.098124    
2024-02-17 12:45:02,645 - Epoch: [165][  391/  391]    Overall Loss 0.244418    Objective Loss 0.244418    Top1 90.865385    Top5 99.519231    LR 0.005522    Time 0.097489    
2024-02-17 12:45:02,798 - --- validate (epoch=165)-----------
2024-02-17 12:45:02,799 - 10000 samples (128 per mini-batch)
2024-02-17 12:45:05,884 - Epoch: [165][   79/   79]    Loss 1.300255    Top1 66.990000    Top5 90.210000    
2024-02-17 12:45:06,026 - ==> Top1: 66.990    Top5: 90.210    Loss: 1.300

2024-02-17 12:45:06,046 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:45:06,046 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:45:06,141 - 

2024-02-17 12:45:06,142 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:45:16,650 - Epoch: [166][  100/  391]    Overall Loss 0.239461    Objective Loss 0.239461                                        LR 0.005522    Time 0.105009    
2024-02-17 12:45:26,256 - Epoch: [166][  200/  391]    Overall Loss 0.239250    Objective Loss 0.239250                                        LR 0.005522    Time 0.100510    
2024-02-17 12:45:33,992 - Epoch: [166][  300/  391]    Overall Loss 0.242436    Objective Loss 0.242436                                        LR 0.005522    Time 0.092782    
2024-02-17 12:45:40,859 - Epoch: [166][  391/  391]    Overall Loss 0.242766    Objective Loss 0.242766    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.088741    
2024-02-17 12:45:41,057 - --- validate (epoch=166)-----------
2024-02-17 12:45:41,058 - 10000 samples (128 per mini-batch)
2024-02-17 12:45:43,882 - Epoch: [166][   79/   79]    Loss 1.336991    Top1 66.440000    Top5 89.920000    
2024-02-17 12:45:44,006 - ==> Top1: 66.440    Top5: 89.920    Loss: 1.337

2024-02-17 12:45:44,025 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:45:44,025 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:45:44,102 - 

2024-02-17 12:45:44,102 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:45:54,171 - Epoch: [167][  100/  391]    Overall Loss 0.232500    Objective Loss 0.232500                                        LR 0.005522    Time 0.100616    
2024-02-17 12:46:03,605 - Epoch: [167][  200/  391]    Overall Loss 0.234312    Objective Loss 0.234312                                        LR 0.005522    Time 0.097456    
2024-02-17 12:46:13,098 - Epoch: [167][  300/  391]    Overall Loss 0.237491    Objective Loss 0.237491                                        LR 0.005522    Time 0.096601    
2024-02-17 12:46:21,822 - Epoch: [167][  391/  391]    Overall Loss 0.239577    Objective Loss 0.239577    Top1 90.865385    Top5 100.000000    LR 0.005522    Time 0.096421    
2024-02-17 12:46:21,938 - --- validate (epoch=167)-----------
2024-02-17 12:46:21,939 - 10000 samples (128 per mini-batch)
2024-02-17 12:46:24,815 - Epoch: [167][   79/   79]    Loss 1.324298    Top1 66.640000    Top5 90.110000    
2024-02-17 12:46:24,993 - ==> Top1: 66.640    Top5: 90.110    Loss: 1.324

2024-02-17 12:46:25,012 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:46:25,012 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:46:25,088 - 

2024-02-17 12:46:25,088 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:46:35,343 - Epoch: [168][  100/  391]    Overall Loss 0.222409    Objective Loss 0.222409                                        LR 0.005522    Time 0.102467    
2024-02-17 12:46:45,143 - Epoch: [168][  200/  391]    Overall Loss 0.229236    Objective Loss 0.229236                                        LR 0.005522    Time 0.100210    
2024-02-17 12:46:54,725 - Epoch: [168][  300/  391]    Overall Loss 0.233375    Objective Loss 0.233375                                        LR 0.005522    Time 0.098731    
2024-02-17 12:47:03,194 - Epoch: [168][  391/  391]    Overall Loss 0.234968    Objective Loss 0.234968    Top1 92.307692    Top5 100.000000    LR 0.005522    Time 0.097401    
2024-02-17 12:47:03,352 - --- validate (epoch=168)-----------
2024-02-17 12:47:03,353 - 10000 samples (128 per mini-batch)
2024-02-17 12:47:06,233 - Epoch: [168][   79/   79]    Loss 1.331299    Top1 66.520000    Top5 90.190000    
2024-02-17 12:47:06,399 - ==> Top1: 66.520    Top5: 90.190    Loss: 1.331

2024-02-17 12:47:06,419 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:47:06,419 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:47:06,495 - 

2024-02-17 12:47:06,496 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:47:16,702 - Epoch: [169][  100/  391]    Overall Loss 0.231639    Objective Loss 0.231639                                        LR 0.005522    Time 0.101987    
2024-02-17 12:47:26,774 - Epoch: [169][  200/  391]    Overall Loss 0.231629    Objective Loss 0.231629                                        LR 0.005522    Time 0.101331    
2024-02-17 12:47:36,416 - Epoch: [169][  300/  391]    Overall Loss 0.231205    Objective Loss 0.231205                                        LR 0.005522    Time 0.099677    
2024-02-17 12:47:45,211 - Epoch: [169][  391/  391]    Overall Loss 0.233654    Objective Loss 0.233654    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.098962    
2024-02-17 12:47:45,385 - --- validate (epoch=169)-----------
2024-02-17 12:47:45,385 - 10000 samples (128 per mini-batch)
2024-02-17 12:47:48,216 - Epoch: [169][   79/   79]    Loss 1.317482    Top1 66.820000    Top5 90.010000    
2024-02-17 12:47:48,349 - ==> Top1: 66.820    Top5: 90.010    Loss: 1.317

2024-02-17 12:47:48,368 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:47:48,368 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:47:48,471 - 

2024-02-17 12:47:48,471 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:47:58,729 - Epoch: [170][  100/  391]    Overall Loss 0.224247    Objective Loss 0.224247                                        LR 0.005522    Time 0.102496    
2024-02-17 12:48:08,619 - Epoch: [170][  200/  391]    Overall Loss 0.227131    Objective Loss 0.227131                                        LR 0.005522    Time 0.100674    
2024-02-17 12:48:18,191 - Epoch: [170][  300/  391]    Overall Loss 0.231135    Objective Loss 0.231135                                        LR 0.005522    Time 0.099009    
2024-02-17 12:48:27,035 - Epoch: [170][  391/  391]    Overall Loss 0.233916    Objective Loss 0.233916    Top1 92.307692    Top5 99.519231    LR 0.005522    Time 0.098575    
2024-02-17 12:48:27,153 - --- validate (epoch=170)-----------
2024-02-17 12:48:27,154 - 10000 samples (128 per mini-batch)
2024-02-17 12:48:30,021 - Epoch: [170][   79/   79]    Loss 1.330065    Top1 66.560000    Top5 90.030000    
2024-02-17 12:48:30,127 - ==> Top1: 66.560    Top5: 90.030    Loss: 1.330

2024-02-17 12:48:30,147 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:48:30,147 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:48:30,226 - 

2024-02-17 12:48:30,226 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:48:40,506 - Epoch: [171][  100/  391]    Overall Loss 0.220583    Objective Loss 0.220583                                        LR 0.005522    Time 0.102724    
2024-02-17 12:48:50,032 - Epoch: [171][  200/  391]    Overall Loss 0.222879    Objective Loss 0.222879                                        LR 0.005522    Time 0.098970    
2024-02-17 12:48:59,665 - Epoch: [171][  300/  391]    Overall Loss 0.225976    Objective Loss 0.225976                                        LR 0.005522    Time 0.098075    
2024-02-17 12:49:08,429 - Epoch: [171][  391/  391]    Overall Loss 0.226625    Objective Loss 0.226625    Top1 91.346154    Top5 100.000000    LR 0.005522    Time 0.097652    
2024-02-17 12:49:08,576 - --- validate (epoch=171)-----------
2024-02-17 12:49:08,577 - 10000 samples (128 per mini-batch)
2024-02-17 12:49:11,276 - Epoch: [171][   79/   79]    Loss 1.344760    Top1 66.800000    Top5 89.880000    
2024-02-17 12:49:11,445 - ==> Top1: 66.800    Top5: 89.880    Loss: 1.345

2024-02-17 12:49:11,461 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:49:11,461 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:49:11,539 - 

2024-02-17 12:49:11,539 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:49:21,832 - Epoch: [172][  100/  391]    Overall Loss 0.217787    Objective Loss 0.217787                                        LR 0.005522    Time 0.102849    
2024-02-17 12:49:31,456 - Epoch: [172][  200/  391]    Overall Loss 0.222174    Objective Loss 0.222174                                        LR 0.005522    Time 0.099523    
2024-02-17 12:49:41,496 - Epoch: [172][  300/  391]    Overall Loss 0.223789    Objective Loss 0.223789                                        LR 0.005522    Time 0.099799    
2024-02-17 12:49:50,435 - Epoch: [172][  391/  391]    Overall Loss 0.227632    Objective Loss 0.227632    Top1 90.865385    Top5 99.519231    LR 0.005522    Time 0.099425    
2024-02-17 12:49:50,571 - --- validate (epoch=172)-----------
2024-02-17 12:49:50,571 - 10000 samples (128 per mini-batch)
2024-02-17 12:49:53,585 - Epoch: [172][   79/   79]    Loss 1.352497    Top1 66.440000    Top5 89.680000    
2024-02-17 12:49:53,754 - ==> Top1: 66.440    Top5: 89.680    Loss: 1.352

2024-02-17 12:49:53,767 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:49:53,768 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:49:53,840 - 

2024-02-17 12:49:53,840 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:50:04,325 - Epoch: [173][  100/  391]    Overall Loss 0.221297    Objective Loss 0.221297                                        LR 0.005522    Time 0.104757    
2024-02-17 12:50:14,186 - Epoch: [173][  200/  391]    Overall Loss 0.223977    Objective Loss 0.223977                                        LR 0.005522    Time 0.101659    
2024-02-17 12:50:23,704 - Epoch: [173][  300/  391]    Overall Loss 0.227418    Objective Loss 0.227418                                        LR 0.005522    Time 0.099484    
2024-02-17 12:50:32,499 - Epoch: [173][  391/  391]    Overall Loss 0.229388    Objective Loss 0.229388    Top1 92.788462    Top5 100.000000    LR 0.005522    Time 0.098814    
2024-02-17 12:50:32,629 - --- validate (epoch=173)-----------
2024-02-17 12:50:32,630 - 10000 samples (128 per mini-batch)
2024-02-17 12:50:35,564 - Epoch: [173][   79/   79]    Loss 1.343826    Top1 66.600000    Top5 89.750000    
2024-02-17 12:50:35,700 - ==> Top1: 66.600    Top5: 89.750    Loss: 1.344

2024-02-17 12:50:35,724 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:50:35,724 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:50:35,806 - 

2024-02-17 12:50:35,806 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:50:45,875 - Epoch: [174][  100/  391]    Overall Loss 0.217410    Objective Loss 0.217410                                        LR 0.005522    Time 0.100615    
2024-02-17 12:50:55,440 - Epoch: [174][  200/  391]    Overall Loss 0.221184    Objective Loss 0.221184                                        LR 0.005522    Time 0.098109    
2024-02-17 12:51:05,003 - Epoch: [174][  300/  391]    Overall Loss 0.221912    Objective Loss 0.221912                                        LR 0.005522    Time 0.097268    
2024-02-17 12:51:13,868 - Epoch: [174][  391/  391]    Overall Loss 0.225677    Objective Loss 0.225677    Top1 90.865385    Top5 100.000000    LR 0.005522    Time 0.097291    
2024-02-17 12:51:14,000 - --- validate (epoch=174)-----------
2024-02-17 12:51:14,000 - 10000 samples (128 per mini-batch)
2024-02-17 12:51:16,967 - Epoch: [174][   79/   79]    Loss 1.349080    Top1 66.700000    Top5 90.000000    
2024-02-17 12:51:17,081 - ==> Top1: 66.700    Top5: 90.000    Loss: 1.349

2024-02-17 12:51:17,094 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:51:17,095 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:51:17,183 - 

2024-02-17 12:51:17,184 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:51:27,147 - Epoch: [175][  100/  391]    Overall Loss 0.200720    Objective Loss 0.200720                                        LR 0.001298    Time 0.099550    
2024-02-17 12:51:36,750 - Epoch: [175][  200/  391]    Overall Loss 0.199762    Objective Loss 0.199762                                        LR 0.001298    Time 0.097770    
2024-02-17 12:51:46,271 - Epoch: [175][  300/  391]    Overall Loss 0.198328    Objective Loss 0.198328                                        LR 0.001298    Time 0.096901    
2024-02-17 12:51:55,108 - Epoch: [175][  391/  391]    Overall Loss 0.198144    Objective Loss 0.198144    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.096939    
2024-02-17 12:51:55,253 - --- validate (epoch=175)-----------
2024-02-17 12:51:55,253 - 10000 samples (128 per mini-batch)
2024-02-17 12:51:58,179 - Epoch: [175][   79/   79]    Loss 1.320566    Top1 67.480000    Top5 90.300000    
2024-02-17 12:51:58,293 - ==> Top1: 67.480    Top5: 90.300    Loss: 1.321

2024-02-17 12:51:58,314 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:51:58,314 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:51:58,412 - 

2024-02-17 12:51:58,412 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:52:08,779 - Epoch: [176][  100/  391]    Overall Loss 0.194081    Objective Loss 0.194081                                        LR 0.001298    Time 0.103599    
2024-02-17 12:52:18,382 - Epoch: [176][  200/  391]    Overall Loss 0.188834    Objective Loss 0.188834                                        LR 0.001298    Time 0.099789    
2024-02-17 12:52:28,023 - Epoch: [176][  300/  391]    Overall Loss 0.189702    Objective Loss 0.189702                                        LR 0.001298    Time 0.098648    
2024-02-17 12:52:36,796 - Epoch: [176][  391/  391]    Overall Loss 0.191612    Objective Loss 0.191612    Top1 92.307692    Top5 100.000000    LR 0.001298    Time 0.098116    
2024-02-17 12:52:36,908 - --- validate (epoch=176)-----------
2024-02-17 12:52:36,910 - 10000 samples (128 per mini-batch)
2024-02-17 12:52:39,847 - Epoch: [176][   79/   79]    Loss 1.327038    Top1 67.030000    Top5 89.960000    
2024-02-17 12:52:40,015 - ==> Top1: 67.030    Top5: 89.960    Loss: 1.327

2024-02-17 12:52:40,034 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:52:40,034 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:52:40,113 - 

2024-02-17 12:52:40,113 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:52:50,017 - Epoch: [177][  100/  391]    Overall Loss 0.177925    Objective Loss 0.177925                                        LR 0.001298    Time 0.098961    
2024-02-17 12:52:59,422 - Epoch: [177][  200/  391]    Overall Loss 0.182974    Objective Loss 0.182974                                        LR 0.001298    Time 0.096484    
2024-02-17 12:53:08,911 - Epoch: [177][  300/  391]    Overall Loss 0.182750    Objective Loss 0.182750                                        LR 0.001298    Time 0.095937    
2024-02-17 12:53:17,600 - Epoch: [177][  391/  391]    Overall Loss 0.184228    Objective Loss 0.184228    Top1 93.750000    Top5 100.000000    LR 0.001298    Time 0.095820    
2024-02-17 12:53:17,727 - --- validate (epoch=177)-----------
2024-02-17 12:53:17,727 - 10000 samples (128 per mini-batch)
2024-02-17 12:53:20,701 - Epoch: [177][   79/   79]    Loss 1.328166    Top1 67.210000    Top5 90.120000    
2024-02-17 12:53:20,909 - ==> Top1: 67.210    Top5: 90.120    Loss: 1.328

2024-02-17 12:53:20,924 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:53:20,924 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:53:21,009 - 

2024-02-17 12:53:21,010 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:53:31,267 - Epoch: [178][  100/  391]    Overall Loss 0.176956    Objective Loss 0.176956                                        LR 0.001298    Time 0.102495    
2024-02-17 12:53:40,760 - Epoch: [178][  200/  391]    Overall Loss 0.180543    Objective Loss 0.180543                                        LR 0.001298    Time 0.098688    
2024-02-17 12:53:50,294 - Epoch: [178][  300/  391]    Overall Loss 0.182040    Objective Loss 0.182040                                        LR 0.001298    Time 0.097559    
2024-02-17 12:53:58,955 - Epoch: [178][  391/  391]    Overall Loss 0.182313    Objective Loss 0.182313    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.096994    
2024-02-17 12:53:59,110 - --- validate (epoch=178)-----------
2024-02-17 12:53:59,111 - 10000 samples (128 per mini-batch)
2024-02-17 12:54:02,173 - Epoch: [178][   79/   79]    Loss 1.330774    Top1 66.990000    Top5 90.280000    
2024-02-17 12:54:02,304 - ==> Top1: 66.990    Top5: 90.280    Loss: 1.331

2024-02-17 12:54:02,316 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:54:02,316 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:54:02,391 - 

2024-02-17 12:54:02,391 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:54:12,404 - Epoch: [179][  100/  391]    Overall Loss 0.183629    Objective Loss 0.183629                                        LR 0.001298    Time 0.100045    
2024-02-17 12:54:21,840 - Epoch: [179][  200/  391]    Overall Loss 0.180428    Objective Loss 0.180428                                        LR 0.001298    Time 0.097181    
2024-02-17 12:54:31,366 - Epoch: [179][  300/  391]    Overall Loss 0.180396    Objective Loss 0.180396                                        LR 0.001298    Time 0.096525    
2024-02-17 12:54:40,007 - Epoch: [179][  391/  391]    Overall Loss 0.180150    Objective Loss 0.180150    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.096149    
2024-02-17 12:54:40,137 - --- validate (epoch=179)-----------
2024-02-17 12:54:40,138 - 10000 samples (128 per mini-batch)
2024-02-17 12:54:43,314 - Epoch: [179][   79/   79]    Loss 1.327668    Top1 67.260000    Top5 90.160000    
2024-02-17 12:54:43,441 - ==> Top1: 67.260    Top5: 90.160    Loss: 1.328

2024-02-17 12:54:43,458 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:54:43,458 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:54:43,531 - 

2024-02-17 12:54:43,531 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:54:53,251 - Epoch: [180][  100/  391]    Overall Loss 0.178427    Objective Loss 0.178427                                        LR 0.001298    Time 0.097122    
2024-02-17 12:55:02,941 - Epoch: [180][  200/  391]    Overall Loss 0.177399    Objective Loss 0.177399                                        LR 0.001298    Time 0.096990    
2024-02-17 12:55:12,319 - Epoch: [180][  300/  391]    Overall Loss 0.179766    Objective Loss 0.179766                                        LR 0.001298    Time 0.095902    
2024-02-17 12:55:21,063 - Epoch: [180][  391/  391]    Overall Loss 0.180987    Objective Loss 0.180987    Top1 93.750000    Top5 100.000000    LR 0.001298    Time 0.095935    
2024-02-17 12:55:21,183 - --- validate (epoch=180)-----------
2024-02-17 12:55:21,184 - 10000 samples (128 per mini-batch)
2024-02-17 12:55:24,199 - Epoch: [180][   79/   79]    Loss 1.322918    Top1 67.360000    Top5 90.070000    
2024-02-17 12:55:24,312 - ==> Top1: 67.360    Top5: 90.070    Loss: 1.323

2024-02-17 12:55:24,327 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:55:24,327 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:55:24,397 - 

2024-02-17 12:55:24,397 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:55:34,481 - Epoch: [181][  100/  391]    Overall Loss 0.174672    Objective Loss 0.174672                                        LR 0.001298    Time 0.100766    
2024-02-17 12:55:43,990 - Epoch: [181][  200/  391]    Overall Loss 0.177580    Objective Loss 0.177580                                        LR 0.001298    Time 0.097902    
2024-02-17 12:55:53,650 - Epoch: [181][  300/  391]    Overall Loss 0.180572    Objective Loss 0.180572                                        LR 0.001298    Time 0.097452    
2024-02-17 12:56:02,306 - Epoch: [181][  391/  391]    Overall Loss 0.180137    Objective Loss 0.180137    Top1 97.596154    Top5 100.000000    LR 0.001298    Time 0.096900    
2024-02-17 12:56:02,473 - --- validate (epoch=181)-----------
2024-02-17 12:56:02,474 - 10000 samples (128 per mini-batch)
2024-02-17 12:56:05,356 - Epoch: [181][   79/   79]    Loss 1.326092    Top1 67.060000    Top5 89.890000    
2024-02-17 12:56:05,495 - ==> Top1: 67.060    Top5: 89.890    Loss: 1.326

2024-02-17 12:56:05,514 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:56:05,514 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:56:05,589 - 

2024-02-17 12:56:05,590 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:56:15,341 - Epoch: [182][  100/  391]    Overall Loss 0.170630    Objective Loss 0.170630                                        LR 0.001298    Time 0.097437    
2024-02-17 12:56:24,926 - Epoch: [182][  200/  391]    Overall Loss 0.172889    Objective Loss 0.172889                                        LR 0.001298    Time 0.096620    
2024-02-17 12:56:34,581 - Epoch: [182][  300/  391]    Overall Loss 0.174278    Objective Loss 0.174278                                        LR 0.001298    Time 0.096583    
2024-02-17 12:56:43,204 - Epoch: [182][  391/  391]    Overall Loss 0.176877    Objective Loss 0.176877    Top1 93.750000    Top5 99.519231    LR 0.001298    Time 0.096147    
2024-02-17 12:56:43,323 - --- validate (epoch=182)-----------
2024-02-17 12:56:43,324 - 10000 samples (128 per mini-batch)
2024-02-17 12:56:46,515 - Epoch: [182][   79/   79]    Loss 1.325496    Top1 67.070000    Top5 90.160000    
2024-02-17 12:56:46,629 - ==> Top1: 67.070    Top5: 90.160    Loss: 1.325

2024-02-17 12:56:46,649 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:56:46,649 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:56:46,729 - 

2024-02-17 12:56:46,730 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:56:56,869 - Epoch: [183][  100/  391]    Overall Loss 0.172277    Objective Loss 0.172277                                        LR 0.001298    Time 0.101314    
2024-02-17 12:57:06,314 - Epoch: [183][  200/  391]    Overall Loss 0.172480    Objective Loss 0.172480                                        LR 0.001298    Time 0.097858    
2024-02-17 12:57:16,114 - Epoch: [183][  300/  391]    Overall Loss 0.173455    Objective Loss 0.173455                                        LR 0.001298    Time 0.097893    
2024-02-17 12:57:24,987 - Epoch: [183][  391/  391]    Overall Loss 0.173278    Objective Loss 0.173278    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.097790    
2024-02-17 12:57:25,131 - --- validate (epoch=183)-----------
2024-02-17 12:57:25,132 - 10000 samples (128 per mini-batch)
2024-02-17 12:57:27,974 - Epoch: [183][   79/   79]    Loss 1.332109    Top1 67.000000    Top5 90.010000    
2024-02-17 12:57:28,091 - ==> Top1: 67.000    Top5: 90.010    Loss: 1.332

2024-02-17 12:57:28,111 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:57:28,111 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:57:28,187 - 

2024-02-17 12:57:28,188 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:57:38,132 - Epoch: [184][  100/  391]    Overall Loss 0.166013    Objective Loss 0.166013                                        LR 0.001298    Time 0.099365    
2024-02-17 12:57:47,709 - Epoch: [184][  200/  391]    Overall Loss 0.170177    Objective Loss 0.170177                                        LR 0.001298    Time 0.097544    
2024-02-17 12:57:57,426 - Epoch: [184][  300/  391]    Overall Loss 0.173037    Objective Loss 0.173037                                        LR 0.001298    Time 0.097404    
2024-02-17 12:58:06,500 - Epoch: [184][  391/  391]    Overall Loss 0.174257    Objective Loss 0.174257    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.097931    
2024-02-17 12:58:06,621 - --- validate (epoch=184)-----------
2024-02-17 12:58:06,622 - 10000 samples (128 per mini-batch)
2024-02-17 12:58:09,507 - Epoch: [184][   79/   79]    Loss 1.328481    Top1 66.740000    Top5 90.080000    
2024-02-17 12:58:09,648 - ==> Top1: 66.740    Top5: 90.080    Loss: 1.328

2024-02-17 12:58:09,669 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:58:09,669 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:58:09,744 - 

2024-02-17 12:58:09,744 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:58:19,743 - Epoch: [185][  100/  391]    Overall Loss 0.169289    Objective Loss 0.169289                                        LR 0.001298    Time 0.099911    
2024-02-17 12:58:29,251 - Epoch: [185][  200/  391]    Overall Loss 0.170546    Objective Loss 0.170546                                        LR 0.001298    Time 0.097472    
2024-02-17 12:58:38,700 - Epoch: [185][  300/  391]    Overall Loss 0.170117    Objective Loss 0.170117                                        LR 0.001298    Time 0.096462    
2024-02-17 12:58:47,261 - Epoch: [185][  391/  391]    Overall Loss 0.170657    Objective Loss 0.170657    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.095897    
2024-02-17 12:58:47,368 - --- validate (epoch=185)-----------
2024-02-17 12:58:47,369 - 10000 samples (128 per mini-batch)
2024-02-17 12:58:50,319 - Epoch: [185][   79/   79]    Loss 1.333425    Top1 67.110000    Top5 90.130000    
2024-02-17 12:58:50,471 - ==> Top1: 67.110    Top5: 90.130    Loss: 1.333

2024-02-17 12:58:50,493 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:58:50,493 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:58:50,574 - 

2024-02-17 12:58:50,574 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:59:00,612 - Epoch: [186][  100/  391]    Overall Loss 0.168003    Objective Loss 0.168003                                        LR 0.001298    Time 0.100291    
2024-02-17 12:59:10,281 - Epoch: [186][  200/  391]    Overall Loss 0.172662    Objective Loss 0.172662                                        LR 0.001298    Time 0.098469    
2024-02-17 12:59:19,874 - Epoch: [186][  300/  391]    Overall Loss 0.170748    Objective Loss 0.170748                                        LR 0.001298    Time 0.097606    
2024-02-17 12:59:28,518 - Epoch: [186][  391/  391]    Overall Loss 0.171379    Objective Loss 0.171379    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.096986    
2024-02-17 12:59:28,690 - --- validate (epoch=186)-----------
2024-02-17 12:59:28,692 - 10000 samples (128 per mini-batch)
2024-02-17 12:59:31,968 - Epoch: [186][   79/   79]    Loss 1.338057    Top1 66.920000    Top5 90.140000    
2024-02-17 12:59:32,137 - ==> Top1: 66.920    Top5: 90.140    Loss: 1.338

2024-02-17 12:59:32,156 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:59:32,156 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 12:59:32,240 - 

2024-02-17 12:59:32,241 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:59:42,378 - Epoch: [187][  100/  391]    Overall Loss 0.171068    Objective Loss 0.171068                                        LR 0.001298    Time 0.101294    
2024-02-17 12:59:52,125 - Epoch: [187][  200/  391]    Overall Loss 0.169351    Objective Loss 0.169351                                        LR 0.001298    Time 0.099357    
2024-02-17 13:00:01,592 - Epoch: [187][  300/  391]    Overall Loss 0.170330    Objective Loss 0.170330                                        LR 0.001298    Time 0.097778    
2024-02-17 13:00:10,631 - Epoch: [187][  391/  391]    Overall Loss 0.171265    Objective Loss 0.171265    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.098130    
2024-02-17 13:00:10,821 - --- validate (epoch=187)-----------
2024-02-17 13:00:10,822 - 10000 samples (128 per mini-batch)
2024-02-17 13:00:13,742 - Epoch: [187][   79/   79]    Loss 1.328471    Top1 66.900000    Top5 90.030000    
2024-02-17 13:00:13,856 - ==> Top1: 66.900    Top5: 90.030    Loss: 1.328

2024-02-17 13:00:13,875 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:00:13,875 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 13:00:13,956 - 

2024-02-17 13:00:13,957 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:00:24,192 - Epoch: [188][  100/  391]    Overall Loss 0.169394    Objective Loss 0.169394                                        LR 0.001298    Time 0.102275    
2024-02-17 13:00:33,799 - Epoch: [188][  200/  391]    Overall Loss 0.168713    Objective Loss 0.168713                                        LR 0.001298    Time 0.099149    
2024-02-17 13:00:43,404 - Epoch: [188][  300/  391]    Overall Loss 0.168231    Objective Loss 0.168231                                        LR 0.001298    Time 0.098102    
2024-02-17 13:00:52,359 - Epoch: [188][  391/  391]    Overall Loss 0.168887    Objective Loss 0.168887    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.098163    
2024-02-17 13:00:52,484 - --- validate (epoch=188)-----------
2024-02-17 13:00:52,485 - 10000 samples (128 per mini-batch)
2024-02-17 13:00:55,468 - Epoch: [188][   79/   79]    Loss 1.331676    Top1 66.910000    Top5 90.020000    
2024-02-17 13:00:55,579 - ==> Top1: 66.910    Top5: 90.020    Loss: 1.332

2024-02-17 13:00:55,598 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:00:55,599 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 13:00:55,694 - 

2024-02-17 13:00:55,694 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:01:05,987 - Epoch: [189][  100/  391]    Overall Loss 0.165290    Objective Loss 0.165290                                        LR 0.001298    Time 0.102853    
2024-02-17 13:01:15,592 - Epoch: [189][  200/  391]    Overall Loss 0.167899    Objective Loss 0.167899                                        LR 0.001298    Time 0.099429    
2024-02-17 13:01:25,274 - Epoch: [189][  300/  391]    Overall Loss 0.167915    Objective Loss 0.167915                                        LR 0.001298    Time 0.098545    
2024-02-17 13:01:33,617 - Epoch: [189][  391/  391]    Overall Loss 0.170486    Objective Loss 0.170486    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.096936    
2024-02-17 13:01:33,787 - --- validate (epoch=189)-----------
2024-02-17 13:01:33,788 - 10000 samples (128 per mini-batch)
2024-02-17 13:01:36,722 - Epoch: [189][   79/   79]    Loss 1.342635    Top1 66.830000    Top5 90.030000    
2024-02-17 13:01:36,853 - ==> Top1: 66.830    Top5: 90.030    Loss: 1.343

2024-02-17 13:01:36,872 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:01:36,873 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 13:01:36,957 - 

2024-02-17 13:01:36,957 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:01:47,218 - Epoch: [190][  100/  391]    Overall Loss 0.161642    Objective Loss 0.161642                                        LR 0.001298    Time 0.102527    
2024-02-17 13:01:56,903 - Epoch: [190][  200/  391]    Overall Loss 0.163029    Objective Loss 0.163029                                        LR 0.001298    Time 0.099667    
2024-02-17 13:02:06,379 - Epoch: [190][  300/  391]    Overall Loss 0.164693    Objective Loss 0.164693                                        LR 0.001298    Time 0.098016    
2024-02-17 13:02:14,664 - Epoch: [190][  391/  391]    Overall Loss 0.166364    Objective Loss 0.166364    Top1 91.826923    Top5 100.000000    LR 0.001298    Time 0.096381    
2024-02-17 13:02:14,812 - --- validate (epoch=190)-----------
2024-02-17 13:02:14,813 - 10000 samples (128 per mini-batch)
2024-02-17 13:02:17,983 - Epoch: [190][   79/   79]    Loss 1.333915    Top1 66.940000    Top5 90.200000    
2024-02-17 13:02:18,097 - ==> Top1: 66.940    Top5: 90.200    Loss: 1.334

2024-02-17 13:02:18,113 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:02:18,114 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 13:02:18,194 - 

2024-02-17 13:02:18,195 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:02:28,144 - Epoch: [191][  100/  391]    Overall Loss 0.167240    Objective Loss 0.167240                                        LR 0.001298    Time 0.099415    
2024-02-17 13:02:37,794 - Epoch: [191][  200/  391]    Overall Loss 0.165507    Objective Loss 0.165507                                        LR 0.001298    Time 0.097936    
2024-02-17 13:02:47,099 - Epoch: [191][  300/  391]    Overall Loss 0.166577    Objective Loss 0.166577                                        LR 0.001298    Time 0.096290    
2024-02-17 13:02:55,748 - Epoch: [191][  391/  391]    Overall Loss 0.167198    Objective Loss 0.167198    Top1 97.115385    Top5 99.519231    LR 0.001298    Time 0.095990    
2024-02-17 13:02:55,898 - --- validate (epoch=191)-----------
2024-02-17 13:02:55,899 - 10000 samples (128 per mini-batch)
2024-02-17 13:02:58,634 - Epoch: [191][   79/   79]    Loss 1.350309    Top1 67.060000    Top5 90.090000    
2024-02-17 13:02:58,734 - ==> Top1: 67.060    Top5: 90.090    Loss: 1.350

2024-02-17 13:02:58,756 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:02:58,756 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 13:02:58,836 - 

2024-02-17 13:02:58,837 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:03:08,932 - Epoch: [192][  100/  391]    Overall Loss 0.163152    Objective Loss 0.163152                                        LR 0.001298    Time 0.100839    
2024-02-17 13:03:18,508 - Epoch: [192][  200/  391]    Overall Loss 0.161818    Objective Loss 0.161818                                        LR 0.001298    Time 0.098274    
2024-02-17 13:03:27,874 - Epoch: [192][  300/  391]    Overall Loss 0.162984    Objective Loss 0.162984                                        LR 0.001298    Time 0.096723    
2024-02-17 13:03:36,542 - Epoch: [192][  391/  391]    Overall Loss 0.164090    Objective Loss 0.164090    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.096371    
2024-02-17 13:03:36,666 - --- validate (epoch=192)-----------
2024-02-17 13:03:36,667 - 10000 samples (128 per mini-batch)
2024-02-17 13:03:39,699 - Epoch: [192][   79/   79]    Loss 1.332412    Top1 66.850000    Top5 90.100000    
2024-02-17 13:03:39,838 - ==> Top1: 66.850    Top5: 90.100    Loss: 1.332

2024-02-17 13:03:39,858 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:03:39,859 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 13:03:39,934 - 

2024-02-17 13:03:39,935 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:03:50,008 - Epoch: [193][  100/  391]    Overall Loss 0.161500    Objective Loss 0.161500                                        LR 0.001298    Time 0.100661    
2024-02-17 13:03:59,714 - Epoch: [193][  200/  391]    Overall Loss 0.163752    Objective Loss 0.163752                                        LR 0.001298    Time 0.098837    
2024-02-17 13:04:09,172 - Epoch: [193][  300/  391]    Overall Loss 0.164758    Objective Loss 0.164758                                        LR 0.001298    Time 0.097402    
2024-02-17 13:04:17,943 - Epoch: [193][  391/  391]    Overall Loss 0.166022    Objective Loss 0.166022    Top1 94.230769    Top5 99.519231    LR 0.001298    Time 0.097154    
2024-02-17 13:04:18,094 - --- validate (epoch=193)-----------
2024-02-17 13:04:18,095 - 10000 samples (128 per mini-batch)
2024-02-17 13:04:20,905 - Epoch: [193][   79/   79]    Loss 1.348482    Top1 66.940000    Top5 90.210000    
2024-02-17 13:04:21,012 - ==> Top1: 66.940    Top5: 90.210    Loss: 1.348

2024-02-17 13:04:21,032 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:04:21,032 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 13:04:21,109 - 

2024-02-17 13:04:21,109 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:04:31,382 - Epoch: [194][  100/  391]    Overall Loss 0.159389    Objective Loss 0.159389                                        LR 0.001298    Time 0.102647    
2024-02-17 13:04:40,627 - Epoch: [194][  200/  391]    Overall Loss 0.162840    Objective Loss 0.162840                                        LR 0.001298    Time 0.097529    
2024-02-17 13:04:50,249 - Epoch: [194][  300/  391]    Overall Loss 0.164014    Objective Loss 0.164014                                        LR 0.001298    Time 0.097077    
2024-02-17 13:04:58,956 - Epoch: [194][  391/  391]    Overall Loss 0.163814    Objective Loss 0.163814    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.096742    
2024-02-17 13:04:59,124 - --- validate (epoch=194)-----------
2024-02-17 13:04:59,124 - 10000 samples (128 per mini-batch)
2024-02-17 13:05:01,920 - Epoch: [194][   79/   79]    Loss 1.345392    Top1 66.930000    Top5 90.060000    
2024-02-17 13:05:02,025 - ==> Top1: 66.930    Top5: 90.060    Loss: 1.345

2024-02-17 13:05:02,045 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:05:02,045 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 13:05:02,123 - 

2024-02-17 13:05:02,123 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:05:12,139 - Epoch: [195][  100/  391]    Overall Loss 0.162932    Objective Loss 0.162932                                        LR 0.001298    Time 0.100082    
2024-02-17 13:05:21,548 - Epoch: [195][  200/  391]    Overall Loss 0.164049    Objective Loss 0.164049                                        LR 0.001298    Time 0.097065    
2024-02-17 13:05:31,225 - Epoch: [195][  300/  391]    Overall Loss 0.164216    Objective Loss 0.164216                                        LR 0.001298    Time 0.096949    
2024-02-17 13:05:39,977 - Epoch: [195][  391/  391]    Overall Loss 0.164087    Objective Loss 0.164087    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.096759    
2024-02-17 13:05:40,101 - --- validate (epoch=195)-----------
2024-02-17 13:05:40,102 - 10000 samples (128 per mini-batch)
2024-02-17 13:05:42,939 - Epoch: [195][   79/   79]    Loss 1.341553    Top1 66.670000    Top5 90.110000    
2024-02-17 13:05:43,043 - ==> Top1: 66.670    Top5: 90.110    Loss: 1.342

2024-02-17 13:05:43,063 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:05:43,063 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 13:05:43,140 - 

2024-02-17 13:05:43,141 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:05:53,708 - Epoch: [196][  100/  391]    Overall Loss 0.164768    Objective Loss 0.164768                                        LR 0.001298    Time 0.105600    
2024-02-17 13:06:03,048 - Epoch: [196][  200/  391]    Overall Loss 0.163550    Objective Loss 0.163550                                        LR 0.001298    Time 0.099474    
2024-02-17 13:06:12,660 - Epoch: [196][  300/  391]    Overall Loss 0.163167    Objective Loss 0.163167                                        LR 0.001298    Time 0.098341    
2024-02-17 13:06:21,324 - Epoch: [196][  391/  391]    Overall Loss 0.163966    Objective Loss 0.163966    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.097603    
2024-02-17 13:06:21,478 - --- validate (epoch=196)-----------
2024-02-17 13:06:21,479 - 10000 samples (128 per mini-batch)
2024-02-17 13:06:24,159 - Epoch: [196][   79/   79]    Loss 1.365839    Top1 66.750000    Top5 89.960000    
2024-02-17 13:06:24,335 - ==> Top1: 66.750    Top5: 89.960    Loss: 1.366

2024-02-17 13:06:24,354 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:06:24,354 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 13:06:24,428 - 

2024-02-17 13:06:24,429 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:06:34,372 - Epoch: [197][  100/  391]    Overall Loss 0.160814    Objective Loss 0.160814                                        LR 0.001298    Time 0.099364    
2024-02-17 13:06:43,648 - Epoch: [197][  200/  391]    Overall Loss 0.160856    Objective Loss 0.160856                                        LR 0.001298    Time 0.096036    
2024-02-17 13:06:53,236 - Epoch: [197][  300/  391]    Overall Loss 0.162088    Objective Loss 0.162088                                        LR 0.001298    Time 0.095969    
2024-02-17 13:07:01,902 - Epoch: [197][  391/  391]    Overall Loss 0.162756    Objective Loss 0.162756    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.095787    
2024-02-17 13:07:02,078 - --- validate (epoch=197)-----------
2024-02-17 13:07:02,079 - 10000 samples (128 per mini-batch)
2024-02-17 13:07:04,811 - Epoch: [197][   79/   79]    Loss 1.361022    Top1 66.480000    Top5 90.110000    
2024-02-17 13:07:04,917 - ==> Top1: 66.480    Top5: 90.110    Loss: 1.361

2024-02-17 13:07:04,936 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:07:04,936 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 13:07:05,013 - 

2024-02-17 13:07:05,013 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:07:14,409 - Epoch: [198][  100/  391]    Overall Loss 0.158934    Objective Loss 0.158934                                        LR 0.001298    Time 0.093890    
2024-02-17 13:07:23,137 - Epoch: [198][  200/  391]    Overall Loss 0.158053    Objective Loss 0.158053                                        LR 0.001298    Time 0.090564    
2024-02-17 13:07:32,665 - Epoch: [198][  300/  391]    Overall Loss 0.161427    Objective Loss 0.161427                                        LR 0.001298    Time 0.092122    
2024-02-17 13:07:41,218 - Epoch: [198][  391/  391]    Overall Loss 0.162402    Objective Loss 0.162402    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.092545    
2024-02-17 13:07:41,350 - --- validate (epoch=198)-----------
2024-02-17 13:07:41,352 - 10000 samples (128 per mini-batch)
2024-02-17 13:07:44,245 - Epoch: [198][   79/   79]    Loss 1.353372    Top1 67.070000    Top5 90.060000    
2024-02-17 13:07:44,401 - ==> Top1: 67.070    Top5: 90.060    Loss: 1.353

2024-02-17 13:07:44,421 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:07:44,421 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 13:07:44,525 - 

2024-02-17 13:07:44,525 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:07:54,481 - Epoch: [199][  100/  391]    Overall Loss 0.161186    Objective Loss 0.161186                                        LR 0.001298    Time 0.099483    
2024-02-17 13:08:03,584 - Epoch: [199][  200/  391]    Overall Loss 0.159144    Objective Loss 0.159144                                        LR 0.001298    Time 0.095235    
2024-02-17 13:08:13,115 - Epoch: [199][  300/  391]    Overall Loss 0.159413    Objective Loss 0.159413                                        LR 0.001298    Time 0.095244    
2024-02-17 13:08:21,805 - Epoch: [199][  391/  391]    Overall Loss 0.160005    Objective Loss 0.160005    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.095291    
2024-02-17 13:08:21,968 - --- validate (epoch=199)-----------
2024-02-17 13:08:21,969 - 10000 samples (128 per mini-batch)
2024-02-17 13:08:24,935 - Epoch: [199][   79/   79]    Loss 1.355071    Top1 66.850000    Top5 90.010000    
2024-02-17 13:08:25,043 - ==> Top1: 66.850    Top5: 90.010    Loss: 1.355

2024-02-17 13:08:25,063 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:08:25,063 - Saving checkpoint to: logs/2024.02.17-105231/checkpoint.pth.tar
2024-02-17 13:08:25,139 - 

2024-02-17 13:08:25,139 - Initiating quantization aware training (QAT)...
2024-02-17 13:08:25,268 - 

2024-02-17 13:08:25,269 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:08:38,973 - Epoch: [200][  100/  391]    Overall Loss 4.612708    Objective Loss 4.612708                                        LR 0.001298    Time 0.136979    
2024-02-17 13:08:51,734 - Epoch: [200][  200/  391]    Overall Loss 4.609731    Objective Loss 4.609731                                        LR 0.001298    Time 0.132266    
2024-02-17 13:09:03,841 - Epoch: [200][  300/  391]    Overall Loss 4.608685    Objective Loss 4.608685                                        LR 0.001298    Time 0.128519    
2024-02-17 13:09:15,666 - Epoch: [200][  391/  391]    Overall Loss 4.607965    Objective Loss 4.607965    Top1 0.961538    Top5 4.807692    LR 0.001298    Time 0.128841    
2024-02-17 13:09:15,811 - --- validate (epoch=200)-----------
2024-02-17 13:09:15,812 - 10000 samples (128 per mini-batch)
2024-02-17 13:09:22,006 - Epoch: [200][   79/   79]    Loss 4.605793    Top1 1.090000    Top5 5.280000    
2024-02-17 13:09:22,195 - ==> Top1: 1.090    Top5: 5.280    Loss: 4.606

2024-02-17 13:09:22,212 - ==> Best [Top1: 1.090   Top5: 5.280   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:09:22,213 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:09:22,283 - 

2024-02-17 13:09:22,283 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:09:35,911 - Epoch: [201][  100/  391]    Overall Loss 4.606686    Objective Loss 4.606686                                        LR 0.001298    Time 0.136186    
2024-02-17 13:09:49,009 - Epoch: [201][  200/  391]    Overall Loss 4.606871    Objective Loss 4.606871                                        LR 0.001298    Time 0.133554    
2024-02-17 13:10:02,025 - Epoch: [201][  300/  391]    Overall Loss 4.606544    Objective Loss 4.606544                                        LR 0.001298    Time 0.132409    
2024-02-17 13:10:13,903 - Epoch: [201][  391/  391]    Overall Loss 4.606586    Objective Loss 4.606586    Top1 0.480769    Top5 3.846154    LR 0.001298    Time 0.131958    
2024-02-17 13:10:14,048 - --- validate (epoch=201)-----------
2024-02-17 13:10:14,049 - 10000 samples (128 per mini-batch)
2024-02-17 13:10:18,662 - Epoch: [201][   79/   79]    Loss 4.605538    Top1 1.030000    Top5 5.130000    
2024-02-17 13:10:18,771 - ==> Top1: 1.030    Top5: 5.130    Loss: 4.606

2024-02-17 13:10:18,788 - ==> Best [Top1: 1.090   Top5: 5.280   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:10:18,788 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:10:18,839 - 

2024-02-17 13:10:18,839 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:10:32,588 - Epoch: [202][  100/  391]    Overall Loss 4.605803    Objective Loss 4.605803                                        LR 0.001298    Time 0.137422    
2024-02-17 13:10:45,713 - Epoch: [202][  200/  391]    Overall Loss 4.605748    Objective Loss 4.605748                                        LR 0.001298    Time 0.134314    
2024-02-17 13:10:59,139 - Epoch: [202][  300/  391]    Overall Loss 4.605654    Objective Loss 4.605654                                        LR 0.001298    Time 0.134278    
2024-02-17 13:11:11,554 - Epoch: [202][  391/  391]    Overall Loss 4.605698    Objective Loss 4.605698    Top1 0.480769    Top5 3.846154    LR 0.001298    Time 0.134764    
2024-02-17 13:11:11,680 - --- validate (epoch=202)-----------
2024-02-17 13:11:11,681 - 10000 samples (128 per mini-batch)
2024-02-17 13:11:18,179 - Epoch: [202][   79/   79]    Loss 4.605434    Top1 1.060000    Top5 5.190000    
2024-02-17 13:11:18,301 - ==> Top1: 1.060    Top5: 5.190    Loss: 4.605

2024-02-17 13:11:18,318 - ==> Best [Top1: 1.090   Top5: 5.280   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:11:18,319 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:11:18,382 - 

2024-02-17 13:11:18,383 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:11:31,752 - Epoch: [203][  100/  391]    Overall Loss 4.605796    Objective Loss 4.605796                                        LR 0.001298    Time 0.133614    
2024-02-17 13:11:44,336 - Epoch: [203][  200/  391]    Overall Loss 4.605725    Objective Loss 4.605725                                        LR 0.001298    Time 0.129702    
2024-02-17 13:11:57,493 - Epoch: [203][  300/  391]    Overall Loss 4.605673    Objective Loss 4.605673                                        LR 0.001298    Time 0.130308    
2024-02-17 13:12:09,290 - Epoch: [203][  391/  391]    Overall Loss 4.605719    Objective Loss 4.605719    Top1 0.480769    Top5 4.326923    LR 0.001298    Time 0.130139    
2024-02-17 13:12:09,399 - --- validate (epoch=203)-----------
2024-02-17 13:12:09,400 - 10000 samples (128 per mini-batch)
2024-02-17 13:12:14,671 - Epoch: [203][   79/   79]    Loss 4.605536    Top1 0.950000    Top5 5.180000    
2024-02-17 13:12:14,769 - ==> Top1: 0.950    Top5: 5.180    Loss: 4.606

2024-02-17 13:12:14,788 - ==> Best [Top1: 1.090   Top5: 5.280   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:12:14,788 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:12:14,852 - 

2024-02-17 13:12:14,852 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:12:28,708 - Epoch: [204][  100/  391]    Overall Loss 4.605782    Objective Loss 4.605782                                        LR 0.001298    Time 0.138476    
2024-02-17 13:12:41,931 - Epoch: [204][  200/  391]    Overall Loss 4.605522    Objective Loss 4.605522                                        LR 0.001298    Time 0.135328    
2024-02-17 13:12:55,013 - Epoch: [204][  300/  391]    Overall Loss 4.605438    Objective Loss 4.605438                                        LR 0.001298    Time 0.133811    
2024-02-17 13:13:06,983 - Epoch: [204][  391/  391]    Overall Loss 4.605426    Objective Loss 4.605426    Top1 1.442308    Top5 5.288462    LR 0.001298    Time 0.133271    
2024-02-17 13:13:07,107 - --- validate (epoch=204)-----------
2024-02-17 13:13:07,108 - 10000 samples (128 per mini-batch)
2024-02-17 13:13:13,392 - Epoch: [204][   79/   79]    Loss 4.606154    Top1 0.890000    Top5 4.740000    
2024-02-17 13:13:13,514 - ==> Top1: 0.890    Top5: 4.740    Loss: 4.606

2024-02-17 13:13:13,532 - ==> Best [Top1: 1.090   Top5: 5.280   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:13:13,532 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:13:13,603 - 

2024-02-17 13:13:13,603 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:13:27,475 - Epoch: [205][  100/  391]    Overall Loss 4.606056    Objective Loss 4.606056                                        LR 0.001298    Time 0.138633    
2024-02-17 13:13:40,759 - Epoch: [205][  200/  391]    Overall Loss 4.605681    Objective Loss 4.605681                                        LR 0.001298    Time 0.135711    
2024-02-17 13:13:54,196 - Epoch: [205][  300/  391]    Overall Loss 4.605404    Objective Loss 4.605404                                        LR 0.001298    Time 0.135249    
2024-02-17 13:14:06,291 - Epoch: [205][  391/  391]    Overall Loss 4.605325    Objective Loss 4.605325    Top1 0.961538    Top5 7.211538    LR 0.001298    Time 0.134693    
2024-02-17 13:14:06,432 - --- validate (epoch=205)-----------
2024-02-17 13:14:06,433 - 10000 samples (128 per mini-batch)
2024-02-17 13:14:11,479 - Epoch: [205][   79/   79]    Loss 4.605477    Top1 1.040000    Top5 4.840000    
2024-02-17 13:14:11,576 - ==> Top1: 1.040    Top5: 4.840    Loss: 4.605

2024-02-17 13:14:11,593 - ==> Best [Top1: 1.090   Top5: 5.280   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:14:11,594 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:14:11,655 - 

2024-02-17 13:14:11,655 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:14:25,455 - Epoch: [206][  100/  391]    Overall Loss 4.609242    Objective Loss 4.609242                                        LR 0.001298    Time 0.137923    
2024-02-17 13:14:38,553 - Epoch: [206][  200/  391]    Overall Loss 4.609148    Objective Loss 4.609148                                        LR 0.001298    Time 0.134426    
2024-02-17 13:14:51,498 - Epoch: [206][  300/  391]    Overall Loss 4.608582    Objective Loss 4.608582                                        LR 0.001298    Time 0.132754    
2024-02-17 13:15:02,714 - Epoch: [206][  391/  391]    Overall Loss 4.610524    Objective Loss 4.610524    Top1 0.000000    Top5 4.326923    LR 0.001298    Time 0.130531    
2024-02-17 13:15:02,842 - --- validate (epoch=206)-----------
2024-02-17 13:15:02,843 - 10000 samples (128 per mini-batch)
2024-02-17 13:15:08,513 - Epoch: [206][   79/   79]    Loss 4.629510    Top1 1.000000    Top5 4.830000    
2024-02-17 13:15:08,655 - ==> Top1: 1.000    Top5: 4.830    Loss: 4.630

2024-02-17 13:15:08,674 - ==> Best [Top1: 1.090   Top5: 5.280   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:15:08,675 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:15:08,738 - 

2024-02-17 13:15:08,738 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:15:22,605 - Epoch: [207][  100/  391]    Overall Loss 4.617521    Objective Loss 4.617521                                        LR 0.001298    Time 0.138590    
2024-02-17 13:15:35,807 - Epoch: [207][  200/  391]    Overall Loss 4.616413    Objective Loss 4.616413                                        LR 0.001298    Time 0.135280    
2024-02-17 13:15:48,957 - Epoch: [207][  300/  391]    Overall Loss 4.615138    Objective Loss 4.615138                                        LR 0.001298    Time 0.134003    
2024-02-17 13:16:01,189 - Epoch: [207][  391/  391]    Overall Loss 4.613940    Objective Loss 4.613940    Top1 0.961538    Top5 6.730769    LR 0.001298    Time 0.134088    
2024-02-17 13:16:01,339 - --- validate (epoch=207)-----------
2024-02-17 13:16:01,340 - 10000 samples (128 per mini-batch)
2024-02-17 13:16:06,903 - Epoch: [207][   79/   79]    Loss 4.609435    Top1 0.950000    Top5 5.050000    
2024-02-17 13:16:07,004 - ==> Top1: 0.950    Top5: 5.050    Loss: 4.609

2024-02-17 13:16:07,022 - ==> Best [Top1: 1.090   Top5: 5.280   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:16:07,022 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:16:07,072 - 

2024-02-17 13:16:07,072 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:16:19,578 - Epoch: [208][  100/  391]    Overall Loss 4.606544    Objective Loss 4.606544                                        LR 0.001298    Time 0.124997    
2024-02-17 13:16:32,780 - Epoch: [208][  200/  391]    Overall Loss 4.607224    Objective Loss 4.607224                                        LR 0.001298    Time 0.128484    
2024-02-17 13:16:46,166 - Epoch: [208][  300/  391]    Overall Loss 4.607201    Objective Loss 4.607201                                        LR 0.001298    Time 0.130260    
2024-02-17 13:16:57,482 - Epoch: [208][  391/  391]    Overall Loss 4.607147    Objective Loss 4.607147    Top1 1.923077    Top5 4.326923    LR 0.001298    Time 0.128874    
2024-02-17 13:16:57,620 - --- validate (epoch=208)-----------
2024-02-17 13:16:57,621 - 10000 samples (128 per mini-batch)
2024-02-17 13:17:02,593 - Epoch: [208][   79/   79]    Loss 4.606129    Top1 0.990000    Top5 4.990000    
2024-02-17 13:17:02,679 - ==> Top1: 0.990    Top5: 4.990    Loss: 4.606

2024-02-17 13:17:02,698 - ==> Best [Top1: 1.090   Top5: 5.280   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:17:02,698 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:17:02,748 - 

2024-02-17 13:17:02,748 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:17:15,050 - Epoch: [209][  100/  391]    Overall Loss 4.606001    Objective Loss 4.606001                                        LR 0.001298    Time 0.122962    
2024-02-17 13:17:28,347 - Epoch: [209][  200/  391]    Overall Loss 4.605925    Objective Loss 4.605925                                        LR 0.001298    Time 0.127935    
2024-02-17 13:17:41,720 - Epoch: [209][  300/  391]    Overall Loss 4.605892    Objective Loss 4.605892                                        LR 0.001298    Time 0.129851    
2024-02-17 13:17:53,974 - Epoch: [209][  391/  391]    Overall Loss 4.605761    Objective Loss 4.605761    Top1 0.961538    Top5 7.211538    LR 0.001298    Time 0.130958    
2024-02-17 13:17:54,098 - --- validate (epoch=209)-----------
2024-02-17 13:17:54,099 - 10000 samples (128 per mini-batch)
2024-02-17 13:18:00,114 - Epoch: [209][   79/   79]    Loss 4.606205    Top1 1.140000    Top5 5.150000    
2024-02-17 13:18:00,230 - ==> Top1: 1.140    Top5: 5.150    Loss: 4.606

2024-02-17 13:18:00,245 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:18:00,245 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:18:00,329 - 

2024-02-17 13:18:00,329 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:18:14,433 - Epoch: [210][  100/  391]    Overall Loss 4.605484    Objective Loss 4.605484                                        LR 0.001298    Time 0.140956    
2024-02-17 13:18:27,374 - Epoch: [210][  200/  391]    Overall Loss 4.605830    Objective Loss 4.605830                                        LR 0.001298    Time 0.135159    
2024-02-17 13:18:40,426 - Epoch: [210][  300/  391]    Overall Loss 4.605995    Objective Loss 4.605995                                        LR 0.001298    Time 0.133594    
2024-02-17 13:18:52,699 - Epoch: [210][  391/  391]    Overall Loss 4.606104    Objective Loss 4.606104    Top1 0.480769    Top5 3.365385    LR 0.001298    Time 0.133879    
2024-02-17 13:18:52,874 - --- validate (epoch=210)-----------
2024-02-17 13:18:52,876 - 10000 samples (128 per mini-batch)
2024-02-17 13:18:58,707 - Epoch: [210][   79/   79]    Loss 4.605979    Top1 1.070000    Top5 5.170000    
2024-02-17 13:18:58,866 - ==> Top1: 1.070    Top5: 5.170    Loss: 4.606

2024-02-17 13:18:58,884 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:18:58,884 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:18:58,938 - 

2024-02-17 13:18:58,938 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:19:11,723 - Epoch: [211][  100/  391]    Overall Loss 4.606213    Objective Loss 4.606213                                        LR 0.001298    Time 0.127791    
2024-02-17 13:19:25,260 - Epoch: [211][  200/  391]    Overall Loss 4.606265    Objective Loss 4.606265                                        LR 0.001298    Time 0.131553    
2024-02-17 13:19:39,588 - Epoch: [211][  300/  391]    Overall Loss 4.605949    Objective Loss 4.605949                                        LR 0.001298    Time 0.135438    
2024-02-17 13:19:50,817 - Epoch: [211][  391/  391]    Overall Loss 4.605764    Objective Loss 4.605764    Top1 0.000000    Top5 3.846154    LR 0.001298    Time 0.132625    
2024-02-17 13:19:50,934 - --- validate (epoch=211)-----------
2024-02-17 13:19:50,935 - 10000 samples (128 per mini-batch)
2024-02-17 13:19:57,130 - Epoch: [211][   79/   79]    Loss 4.605476    Top1 1.120000    Top5 5.080000    
2024-02-17 13:19:57,229 - ==> Top1: 1.120    Top5: 5.080    Loss: 4.605

2024-02-17 13:19:57,242 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:19:57,242 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:19:57,301 - 

2024-02-17 13:19:57,302 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:20:11,600 - Epoch: [212][  100/  391]    Overall Loss 4.605516    Objective Loss 4.605516                                        LR 0.001298    Time 0.142894    
2024-02-17 13:20:25,186 - Epoch: [212][  200/  391]    Overall Loss 4.605520    Objective Loss 4.605520                                        LR 0.001298    Time 0.139349    
2024-02-17 13:20:38,321 - Epoch: [212][  300/  391]    Overall Loss 4.605561    Objective Loss 4.605561                                        LR 0.001298    Time 0.136669    
2024-02-17 13:20:50,587 - Epoch: [212][  391/  391]    Overall Loss 4.605492    Objective Loss 4.605492    Top1 0.961538    Top5 5.769231    LR 0.001298    Time 0.136220    
2024-02-17 13:20:50,748 - --- validate (epoch=212)-----------
2024-02-17 13:20:50,749 - 10000 samples (128 per mini-batch)
2024-02-17 13:20:55,637 - Epoch: [212][   79/   79]    Loss 4.605661    Top1 1.120000    Top5 5.070000    
2024-02-17 13:20:55,738 - ==> Top1: 1.120    Top5: 5.070    Loss: 4.606

2024-02-17 13:20:55,749 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:20:55,749 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:20:55,801 - 

2024-02-17 13:20:55,801 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:21:08,726 - Epoch: [213][  100/  391]    Overall Loss 4.605206    Objective Loss 4.605206                                        LR 0.001298    Time 0.129191    
2024-02-17 13:21:21,904 - Epoch: [213][  200/  391]    Overall Loss 4.605904    Objective Loss 4.605904                                        LR 0.001298    Time 0.130462    
2024-02-17 13:21:34,860 - Epoch: [213][  300/  391]    Overall Loss 4.605980    Objective Loss 4.605980                                        LR 0.001298    Time 0.130146    
2024-02-17 13:21:46,889 - Epoch: [213][  391/  391]    Overall Loss 4.605798    Objective Loss 4.605798    Top1 0.961538    Top5 5.288462    LR 0.001298    Time 0.130610    
2024-02-17 13:21:47,027 - --- validate (epoch=213)-----------
2024-02-17 13:21:47,029 - 10000 samples (128 per mini-batch)
2024-02-17 13:21:52,717 - Epoch: [213][   79/   79]    Loss 4.605141    Top1 1.040000    Top5 5.290000    
2024-02-17 13:21:52,862 - ==> Top1: 1.040    Top5: 5.290    Loss: 4.605

2024-02-17 13:21:52,885 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:21:52,885 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:21:52,947 - 

2024-02-17 13:21:52,948 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:22:06,855 - Epoch: [214][  100/  391]    Overall Loss 4.605527    Objective Loss 4.605527                                        LR 0.001298    Time 0.138996    
2024-02-17 13:22:19,905 - Epoch: [214][  200/  391]    Overall Loss 4.605415    Objective Loss 4.605415                                        LR 0.001298    Time 0.134725    
2024-02-17 13:22:32,732 - Epoch: [214][  300/  391]    Overall Loss 4.617319    Objective Loss 4.617319                                        LR 0.001298    Time 0.132556    
2024-02-17 13:22:44,789 - Epoch: [214][  391/  391]    Overall Loss 4.624024    Objective Loss 4.624024    Top1 2.884615    Top5 3.846154    LR 0.001298    Time 0.132530    
2024-02-17 13:22:44,921 - --- validate (epoch=214)-----------
2024-02-17 13:22:44,922 - 10000 samples (128 per mini-batch)
2024-02-17 13:22:50,598 - Epoch: [214][   79/   79]    Loss 4.637179    Top1 0.960000    Top5 5.100000    
2024-02-17 13:22:50,722 - ==> Top1: 0.960    Top5: 5.100    Loss: 4.637

2024-02-17 13:22:50,740 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:22:50,740 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:22:50,804 - 

2024-02-17 13:22:50,805 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:23:04,327 - Epoch: [215][  100/  391]    Overall Loss 4.631900    Objective Loss 4.631900                                        LR 0.001298    Time 0.135152    
2024-02-17 13:23:17,410 - Epoch: [215][  200/  391]    Overall Loss 4.629640    Objective Loss 4.629640                                        LR 0.001298    Time 0.132963    
2024-02-17 13:23:30,658 - Epoch: [215][  300/  391]    Overall Loss 4.626809    Objective Loss 4.626809                                        LR 0.001298    Time 0.132786    
2024-02-17 13:23:42,839 - Epoch: [215][  391/  391]    Overall Loss 4.625654    Objective Loss 4.625654    Top1 0.480769    Top5 7.211538    LR 0.001298    Time 0.133023    
2024-02-17 13:23:42,992 - --- validate (epoch=215)-----------
2024-02-17 13:23:42,993 - 10000 samples (128 per mini-batch)
2024-02-17 13:23:49,241 - Epoch: [215][   79/   79]    Loss 4.617891    Top1 1.000000    Top5 4.920000    
2024-02-17 13:23:49,361 - ==> Top1: 1.000    Top5: 4.920    Loss: 4.618

2024-02-17 13:23:49,378 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:23:49,378 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:23:49,449 - 

2024-02-17 13:23:49,449 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:24:03,472 - Epoch: [216][  100/  391]    Overall Loss 4.618446    Objective Loss 4.618446                                        LR 0.001298    Time 0.140149    
2024-02-17 13:24:16,408 - Epoch: [216][  200/  391]    Overall Loss 4.616776    Objective Loss 4.616776                                        LR 0.001298    Time 0.134729    
2024-02-17 13:24:28,556 - Epoch: [216][  300/  391]    Overall Loss 4.616139    Objective Loss 4.616139                                        LR 0.001298    Time 0.130296    
2024-02-17 13:24:40,224 - Epoch: [216][  391/  391]    Overall Loss 4.615903    Objective Loss 4.615903    Top1 1.923077    Top5 7.211538    LR 0.001298    Time 0.129803    
2024-02-17 13:24:40,351 - --- validate (epoch=216)-----------
2024-02-17 13:24:40,351 - 10000 samples (128 per mini-batch)
2024-02-17 13:24:46,487 - Epoch: [216][   79/   79]    Loss 4.614639    Top1 1.070000    Top5 5.230000    
2024-02-17 13:24:46,593 - ==> Top1: 1.070    Top5: 5.230    Loss: 4.615

2024-02-17 13:24:46,614 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:24:46,614 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:24:46,679 - 

2024-02-17 13:24:46,679 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:25:00,546 - Epoch: [217][  100/  391]    Overall Loss 4.614947    Objective Loss 4.614947                                        LR 0.001298    Time 0.138584    
2024-02-17 13:25:13,912 - Epoch: [217][  200/  391]    Overall Loss 4.614649    Objective Loss 4.614649                                        LR 0.001298    Time 0.136100    
2024-02-17 13:25:27,294 - Epoch: [217][  300/  391]    Overall Loss 4.618690    Objective Loss 4.618690                                        LR 0.001298    Time 0.135323    
2024-02-17 13:25:38,188 - Epoch: [217][  391/  391]    Overall Loss 4.619223    Objective Loss 4.619223    Top1 0.961538    Top5 2.403846    LR 0.001298    Time 0.131679    
2024-02-17 13:25:38,282 - --- validate (epoch=217)-----------
2024-02-17 13:25:38,283 - 10000 samples (128 per mini-batch)
2024-02-17 13:25:43,859 - Epoch: [217][   79/   79]    Loss 4.616101    Top1 1.100000    Top5 5.060000    
2024-02-17 13:25:44,021 - ==> Top1: 1.100    Top5: 5.060    Loss: 4.616

2024-02-17 13:25:44,038 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:25:44,039 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:25:44,107 - 

2024-02-17 13:25:44,107 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:25:56,974 - Epoch: [218][  100/  391]    Overall Loss 4.617606    Objective Loss 4.617606                                        LR 0.001298    Time 0.128584    
2024-02-17 13:26:09,332 - Epoch: [218][  200/  391]    Overall Loss 4.616542    Objective Loss 4.616542                                        LR 0.001298    Time 0.126062    
2024-02-17 13:26:22,846 - Epoch: [218][  300/  391]    Overall Loss 4.615783    Objective Loss 4.615783                                        LR 0.001298    Time 0.129071    
2024-02-17 13:26:35,012 - Epoch: [218][  391/  391]    Overall Loss 4.614966    Objective Loss 4.614966    Top1 1.923077    Top5 4.807692    LR 0.001298    Time 0.130134    
2024-02-17 13:26:35,139 - --- validate (epoch=218)-----------
2024-02-17 13:26:35,139 - 10000 samples (128 per mini-batch)
2024-02-17 13:26:41,156 - Epoch: [218][   79/   79]    Loss 4.611142    Top1 1.030000    Top5 4.970000    
2024-02-17 13:26:41,259 - ==> Top1: 1.030    Top5: 4.970    Loss: 4.611

2024-02-17 13:26:41,277 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:26:41,277 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:26:41,343 - 

2024-02-17 13:26:41,343 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:26:55,155 - Epoch: [219][  100/  391]    Overall Loss 4.613366    Objective Loss 4.613366                                        LR 0.001298    Time 0.138040    
2024-02-17 13:27:08,174 - Epoch: [219][  200/  391]    Overall Loss 4.612702    Objective Loss 4.612702                                        LR 0.001298    Time 0.134093    
2024-02-17 13:27:21,314 - Epoch: [219][  300/  391]    Overall Loss 4.612809    Objective Loss 4.612809                                        LR 0.001298    Time 0.133178    
2024-02-17 13:27:33,237 - Epoch: [219][  391/  391]    Overall Loss 4.612561    Objective Loss 4.612561    Top1 0.961538    Top5 4.326923    LR 0.001298    Time 0.132665    
2024-02-17 13:27:33,351 - --- validate (epoch=219)-----------
2024-02-17 13:27:33,351 - 10000 samples (128 per mini-batch)
2024-02-17 13:27:39,115 - Epoch: [219][   79/   79]    Loss 4.609964    Top1 0.920000    Top5 5.010000    
2024-02-17 13:27:39,220 - ==> Top1: 0.920    Top5: 5.010    Loss: 4.610

2024-02-17 13:27:39,240 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:27:39,241 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:27:39,304 - 

2024-02-17 13:27:39,304 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:27:53,111 - Epoch: [220][  100/  391]    Overall Loss 4.611286    Objective Loss 4.611286                                        LR 0.001298    Time 0.137992    
2024-02-17 13:28:04,717 - Epoch: [220][  200/  391]    Overall Loss 4.610418    Objective Loss 4.610418                                        LR 0.001298    Time 0.127007    
2024-02-17 13:28:17,715 - Epoch: [220][  300/  391]    Overall Loss 4.626902    Objective Loss 4.626902                                        LR 0.001298    Time 0.127980    
2024-02-17 13:28:28,918 - Epoch: [220][  391/  391]    Overall Loss 4.645600    Objective Loss 4.645600    Top1 1.923077    Top5 7.211538    LR 0.001298    Time 0.126836    
2024-02-17 13:28:29,038 - --- validate (epoch=220)-----------
2024-02-17 13:28:29,039 - 10000 samples (128 per mini-batch)
2024-02-17 13:28:34,070 - Epoch: [220][   79/   79]    Loss 4.702512    Top1 0.910000    Top5 5.000000    
2024-02-17 13:28:34,168 - ==> Top1: 0.910    Top5: 5.000    Loss: 4.703

2024-02-17 13:28:34,184 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:28:34,185 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:28:34,235 - 

2024-02-17 13:28:34,235 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:28:45,892 - Epoch: [221][  100/  391]    Overall Loss 4.684184    Objective Loss 4.684184                                        LR 0.001298    Time 0.116521    
2024-02-17 13:28:58,954 - Epoch: [221][  200/  391]    Overall Loss 4.673997    Objective Loss 4.673997                                        LR 0.001298    Time 0.123543    
2024-02-17 13:29:11,844 - Epoch: [221][  300/  391]    Overall Loss 4.667915    Objective Loss 4.667915                                        LR 0.001298    Time 0.125314    
2024-02-17 13:29:22,974 - Epoch: [221][  391/  391]    Overall Loss 4.659538    Objective Loss 4.659538    Top1 0.961538    Top5 4.326923    LR 0.001298    Time 0.124603    
2024-02-17 13:29:23,111 - --- validate (epoch=221)-----------
2024-02-17 13:29:23,112 - 10000 samples (128 per mini-batch)
2024-02-17 13:29:28,410 - Epoch: [221][   79/   79]    Loss 4.627793    Top1 1.070000    Top5 5.110000    
2024-02-17 13:29:28,507 - ==> Top1: 1.070    Top5: 5.110    Loss: 4.628

2024-02-17 13:29:28,523 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:29:28,523 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:29:28,572 - 

2024-02-17 13:29:28,572 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:29:41,853 - Epoch: [222][  100/  391]    Overall Loss 4.652668    Objective Loss 4.652668                                        LR 0.001298    Time 0.132752    
2024-02-17 13:29:54,950 - Epoch: [222][  200/  391]    Overall Loss 4.648370    Objective Loss 4.648370                                        LR 0.001298    Time 0.131836    
2024-02-17 13:30:07,705 - Epoch: [222][  300/  391]    Overall Loss 4.644712    Objective Loss 4.644712                                        LR 0.001298    Time 0.130391    
2024-02-17 13:30:19,989 - Epoch: [222][  391/  391]    Overall Loss 4.640688    Objective Loss 4.640688    Top1 1.923077    Top5 6.730769    LR 0.001298    Time 0.131449    
2024-02-17 13:30:20,128 - --- validate (epoch=222)-----------
2024-02-17 13:30:20,128 - 10000 samples (128 per mini-batch)
2024-02-17 13:30:26,426 - Epoch: [222][   79/   79]    Loss 4.619908    Top1 1.000000    Top5 5.020000    
2024-02-17 13:30:26,545 - ==> Top1: 1.000    Top5: 5.020    Loss: 4.620

2024-02-17 13:30:26,566 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:30:26,566 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:30:26,633 - 

2024-02-17 13:30:26,634 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:30:40,418 - Epoch: [223][  100/  391]    Overall Loss 4.618431    Objective Loss 4.618431                                        LR 0.001298    Time 0.137721    
2024-02-17 13:30:52,744 - Epoch: [223][  200/  391]    Overall Loss 4.617455    Objective Loss 4.617455                                        LR 0.001298    Time 0.130469    
2024-02-17 13:31:03,444 - Epoch: [223][  300/  391]    Overall Loss 4.616061    Objective Loss 4.616061                                        LR 0.001298    Time 0.122632    
2024-02-17 13:31:13,834 - Epoch: [223][  391/  391]    Overall Loss 4.615539    Objective Loss 4.615539    Top1 2.403846    Top5 7.211538    LR 0.001298    Time 0.120656    
2024-02-17 13:31:13,947 - --- validate (epoch=223)-----------
2024-02-17 13:31:13,948 - 10000 samples (128 per mini-batch)
2024-02-17 13:31:19,907 - Epoch: [223][   79/   79]    Loss 4.611982    Top1 0.900000    Top5 4.990000    
2024-02-17 13:31:20,015 - ==> Top1: 0.900    Top5: 4.990    Loss: 4.612

2024-02-17 13:31:20,035 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:31:20,035 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:31:20,110 - 

2024-02-17 13:31:20,110 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:31:34,182 - Epoch: [224][  100/  391]    Overall Loss 4.612478    Objective Loss 4.612478                                        LR 0.001298    Time 0.140648    
2024-02-17 13:31:47,595 - Epoch: [224][  200/  391]    Overall Loss 4.612422    Objective Loss 4.612422                                        LR 0.001298    Time 0.137364    
2024-02-17 13:32:00,665 - Epoch: [224][  300/  391]    Overall Loss 4.612211    Objective Loss 4.612211                                        LR 0.001298    Time 0.135126    
2024-02-17 13:32:12,183 - Epoch: [224][  391/  391]    Overall Loss 4.631604    Objective Loss 4.631604    Top1 0.480769    Top5 2.403846    LR 0.001298    Time 0.133124    
2024-02-17 13:32:12,316 - --- validate (epoch=224)-----------
2024-02-17 13:32:12,318 - 10000 samples (128 per mini-batch)
2024-02-17 13:32:18,510 - Epoch: [224][   79/   79]    Loss 4.680679    Top1 1.000000    Top5 5.090000    
2024-02-17 13:32:18,634 - ==> Top1: 1.000    Top5: 5.090    Loss: 4.681

2024-02-17 13:32:18,652 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:32:18,652 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:32:18,716 - 

2024-02-17 13:32:18,716 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:32:32,337 - Epoch: [225][  100/  391]    Overall Loss 4.650398    Objective Loss 4.650398                                        LR 0.001298    Time 0.136134    
2024-02-17 13:32:45,759 - Epoch: [225][  200/  391]    Overall Loss 4.640403    Objective Loss 4.640403                                        LR 0.001298    Time 0.135150    
2024-02-17 13:32:59,357 - Epoch: [225][  300/  391]    Overall Loss 4.635053    Objective Loss 4.635053                                        LR 0.001298    Time 0.135407    
2024-02-17 13:33:11,187 - Epoch: [225][  391/  391]    Overall Loss 4.631121    Objective Loss 4.631121    Top1 2.884615    Top5 5.288462    LR 0.001298    Time 0.134137    
2024-02-17 13:33:11,348 - --- validate (epoch=225)-----------
2024-02-17 13:33:11,349 - 10000 samples (128 per mini-batch)
2024-02-17 13:33:16,966 - Epoch: [225][   79/   79]    Loss 4.612896    Top1 1.030000    Top5 4.850000    
2024-02-17 13:33:17,107 - ==> Top1: 1.030    Top5: 4.850    Loss: 4.613

2024-02-17 13:33:17,124 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:33:17,124 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:33:17,187 - 

2024-02-17 13:33:17,187 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:33:31,297 - Epoch: [226][  100/  391]    Overall Loss 4.616601    Objective Loss 4.616601                                        LR 0.001298    Time 0.141025    
2024-02-17 13:33:44,611 - Epoch: [226][  200/  391]    Overall Loss 4.616483    Objective Loss 4.616483                                        LR 0.001298    Time 0.137060    
2024-02-17 13:33:57,856 - Epoch: [226][  300/  391]    Overall Loss 4.616573    Objective Loss 4.616573                                        LR 0.001298    Time 0.135505    
2024-02-17 13:34:09,858 - Epoch: [226][  391/  391]    Overall Loss 4.616570    Objective Loss 4.616570    Top1 1.442308    Top5 8.173077    LR 0.001298    Time 0.134654    
2024-02-17 13:34:10,008 - --- validate (epoch=226)-----------
2024-02-17 13:34:10,008 - 10000 samples (128 per mini-batch)
2024-02-17 13:34:15,217 - Epoch: [226][   79/   79]    Loss 4.613647    Top1 1.000000    Top5 4.810000    
2024-02-17 13:34:15,325 - ==> Top1: 1.000    Top5: 4.810    Loss: 4.614

2024-02-17 13:34:15,341 - ==> Best [Top1: 1.140   Top5: 5.150   Sparsity:0.00   Params: 1341960 on epoch: 209]
2024-02-17 13:34:15,342 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:34:15,390 - 

2024-02-17 13:34:15,390 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:34:27,900 - Epoch: [227][  100/  391]    Overall Loss 4.614016    Objective Loss 4.614016                                        LR 0.001298    Time 0.125034    
2024-02-17 13:34:41,237 - Epoch: [227][  200/  391]    Overall Loss 4.614556    Objective Loss 4.614556                                        LR 0.001298    Time 0.129174    
2024-02-17 13:34:54,498 - Epoch: [227][  300/  391]    Overall Loss 4.614071    Objective Loss 4.614071                                        LR 0.001298    Time 0.130306    
2024-02-17 13:35:06,535 - Epoch: [227][  391/  391]    Overall Loss 4.614001    Objective Loss 4.614001    Top1 0.480769    Top5 4.807692    LR 0.001298    Time 0.130752    
2024-02-17 13:35:06,695 - --- validate (epoch=227)-----------
2024-02-17 13:35:06,696 - 10000 samples (128 per mini-batch)
2024-02-17 13:35:12,441 - Epoch: [227][   79/   79]    Loss 4.610573    Top1 1.150000    Top5 4.880000    
2024-02-17 13:35:12,601 - ==> Top1: 1.150    Top5: 4.880    Loss: 4.611

2024-02-17 13:35:12,621 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:35:12,622 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:35:12,707 - 

2024-02-17 13:35:12,707 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:35:26,645 - Epoch: [228][  100/  391]    Overall Loss 4.613496    Objective Loss 4.613496                                        LR 0.001298    Time 0.139302    
2024-02-17 13:35:40,005 - Epoch: [228][  200/  391]    Overall Loss 4.614485    Objective Loss 4.614485                                        LR 0.001298    Time 0.136425    
2024-02-17 13:35:53,431 - Epoch: [228][  300/  391]    Overall Loss 4.613695    Objective Loss 4.613695                                        LR 0.001298    Time 0.135687    
2024-02-17 13:36:05,290 - Epoch: [228][  391/  391]    Overall Loss 4.613784    Objective Loss 4.613784    Top1 2.403846    Top5 9.134615    LR 0.001298    Time 0.134426    
2024-02-17 13:36:05,417 - --- validate (epoch=228)-----------
2024-02-17 13:36:05,418 - 10000 samples (128 per mini-batch)
2024-02-17 13:36:11,677 - Epoch: [228][   79/   79]    Loss 4.611032    Top1 0.950000    Top5 5.010000    
2024-02-17 13:36:11,793 - ==> Top1: 0.950    Top5: 5.010    Loss: 4.611

2024-02-17 13:36:11,811 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:36:11,812 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:36:11,876 - 

2024-02-17 13:36:11,877 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:36:25,729 - Epoch: [229][  100/  391]    Overall Loss 4.613176    Objective Loss 4.613176                                        LR 0.001298    Time 0.138445    
2024-02-17 13:36:39,179 - Epoch: [229][  200/  391]    Overall Loss 4.612721    Objective Loss 4.612721                                        LR 0.001298    Time 0.136450    
2024-02-17 13:36:52,176 - Epoch: [229][  300/  391]    Overall Loss 4.612258    Objective Loss 4.612258                                        LR 0.001298    Time 0.134273    
2024-02-17 13:37:04,215 - Epoch: [229][  391/  391]    Overall Loss 4.612041    Objective Loss 4.612041    Top1 0.480769    Top5 5.769231    LR 0.001298    Time 0.133802    
2024-02-17 13:37:04,396 - --- validate (epoch=229)-----------
2024-02-17 13:37:04,396 - 10000 samples (128 per mini-batch)
2024-02-17 13:37:10,400 - Epoch: [229][   79/   79]    Loss 4.612379    Top1 1.010000    Top5 5.140000    
2024-02-17 13:37:10,536 - ==> Top1: 1.010    Top5: 5.140    Loss: 4.612

2024-02-17 13:37:10,554 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:37:10,555 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:37:10,618 - 

2024-02-17 13:37:10,619 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:37:24,594 - Epoch: [230][  100/  391]    Overall Loss 4.611804    Objective Loss 4.611804                                        LR 0.001298    Time 0.139677    
2024-02-17 13:37:37,281 - Epoch: [230][  200/  391]    Overall Loss 4.612004    Objective Loss 4.612004                                        LR 0.001298    Time 0.133249    
2024-02-17 13:37:50,500 - Epoch: [230][  300/  391]    Overall Loss 4.612051    Objective Loss 4.612051                                        LR 0.001298    Time 0.132879    
2024-02-17 13:38:02,533 - Epoch: [230][  391/  391]    Overall Loss 4.612119    Objective Loss 4.612119    Top1 0.480769    Top5 5.288462    LR 0.001298    Time 0.132717    
2024-02-17 13:38:02,653 - --- validate (epoch=230)-----------
2024-02-17 13:38:02,654 - 10000 samples (128 per mini-batch)
2024-02-17 13:38:08,951 - Epoch: [230][   79/   79]    Loss 4.609723    Top1 0.890000    Top5 4.780000    
2024-02-17 13:38:09,068 - ==> Top1: 0.890    Top5: 4.780    Loss: 4.610

2024-02-17 13:38:09,088 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:38:09,088 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:38:09,153 - 

2024-02-17 13:38:09,153 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:38:23,042 - Epoch: [231][  100/  391]    Overall Loss 4.610642    Objective Loss 4.610642                                        LR 0.001298    Time 0.138817    
2024-02-17 13:38:36,291 - Epoch: [231][  200/  391]    Overall Loss 4.610776    Objective Loss 4.610776                                        LR 0.001298    Time 0.135625    
2024-02-17 13:38:49,301 - Epoch: [231][  300/  391]    Overall Loss 4.611795    Objective Loss 4.611795                                        LR 0.001298    Time 0.133768    
2024-02-17 13:39:01,321 - Epoch: [231][  391/  391]    Overall Loss 4.611512    Objective Loss 4.611512    Top1 0.480769    Top5 3.365385    LR 0.001298    Time 0.133363    
2024-02-17 13:39:01,535 - --- validate (epoch=231)-----------
2024-02-17 13:39:01,536 - 10000 samples (128 per mini-batch)
2024-02-17 13:39:06,322 - Epoch: [231][   79/   79]    Loss 4.609648    Top1 1.030000    Top5 4.850000    
2024-02-17 13:39:06,436 - ==> Top1: 1.030    Top5: 4.850    Loss: 4.610

2024-02-17 13:39:06,453 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:39:06,453 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:39:06,506 - 

2024-02-17 13:39:06,506 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:39:20,479 - Epoch: [232][  100/  391]    Overall Loss 4.611392    Objective Loss 4.611392                                        LR 0.001298    Time 0.139615    
2024-02-17 13:39:33,814 - Epoch: [232][  200/  391]    Overall Loss 4.611839    Objective Loss 4.611839                                        LR 0.001298    Time 0.136459    
2024-02-17 13:39:47,107 - Epoch: [232][  300/  391]    Overall Loss 4.611699    Objective Loss 4.611699                                        LR 0.001298    Time 0.135268    
2024-02-17 13:39:58,813 - Epoch: [232][  391/  391]    Overall Loss 4.611607    Objective Loss 4.611607    Top1 0.000000    Top5 3.846154    LR 0.001298    Time 0.133713    
2024-02-17 13:39:58,940 - --- validate (epoch=232)-----------
2024-02-17 13:39:58,941 - 10000 samples (128 per mini-batch)
2024-02-17 13:40:04,487 - Epoch: [232][   79/   79]    Loss 4.609647    Top1 1.000000    Top5 5.020000    
2024-02-17 13:40:04,579 - ==> Top1: 1.000    Top5: 5.020    Loss: 4.610

2024-02-17 13:40:04,596 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:40:04,597 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:40:04,649 - 

2024-02-17 13:40:04,649 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:40:18,246 - Epoch: [233][  100/  391]    Overall Loss 4.611927    Objective Loss 4.611927                                        LR 0.001298    Time 0.135901    
2024-02-17 13:40:31,258 - Epoch: [233][  200/  391]    Overall Loss 4.611257    Objective Loss 4.611257                                        LR 0.001298    Time 0.132986    
2024-02-17 13:40:43,826 - Epoch: [233][  300/  391]    Overall Loss 4.611141    Objective Loss 4.611141                                        LR 0.001298    Time 0.130537    
2024-02-17 13:40:54,130 - Epoch: [233][  391/  391]    Overall Loss 4.611108    Objective Loss 4.611108    Top1 1.923077    Top5 6.730769    LR 0.001298    Time 0.126499    
2024-02-17 13:40:54,241 - --- validate (epoch=233)-----------
2024-02-17 13:40:54,242 - 10000 samples (128 per mini-batch)
2024-02-17 13:40:59,915 - Epoch: [233][   79/   79]    Loss 4.610319    Top1 1.090000    Top5 5.100000    
2024-02-17 13:41:00,031 - ==> Top1: 1.090    Top5: 5.100    Loss: 4.610

2024-02-17 13:41:00,049 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:41:00,049 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:41:00,116 - 

2024-02-17 13:41:00,116 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:41:13,369 - Epoch: [234][  100/  391]    Overall Loss 4.610082    Objective Loss 4.610082                                        LR 0.001298    Time 0.132456    
2024-02-17 13:41:26,694 - Epoch: [234][  200/  391]    Overall Loss 4.610526    Objective Loss 4.610526                                        LR 0.001298    Time 0.132825    
2024-02-17 13:41:39,929 - Epoch: [234][  300/  391]    Overall Loss 4.610575    Objective Loss 4.610575                                        LR 0.001298    Time 0.132649    
2024-02-17 13:41:51,914 - Epoch: [234][  391/  391]    Overall Loss 4.610854    Objective Loss 4.610854    Top1 0.480769    Top5 3.846154    LR 0.001298    Time 0.132416    
2024-02-17 13:41:52,078 - --- validate (epoch=234)-----------
2024-02-17 13:41:52,079 - 10000 samples (128 per mini-batch)
2024-02-17 13:41:58,253 - Epoch: [234][   79/   79]    Loss 4.608527    Top1 1.000000    Top5 5.280000    
2024-02-17 13:41:58,374 - ==> Top1: 1.000    Top5: 5.280    Loss: 4.609

2024-02-17 13:41:58,391 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:41:58,391 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:41:58,455 - 

2024-02-17 13:41:58,455 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:42:11,234 - Epoch: [235][  100/  391]    Overall Loss 4.609194    Objective Loss 4.609194                                        LR 0.001298    Time 0.127718    
2024-02-17 13:42:24,219 - Epoch: [235][  200/  391]    Overall Loss 4.609626    Objective Loss 4.609626                                        LR 0.001298    Time 0.128759    
2024-02-17 13:42:37,459 - Epoch: [235][  300/  391]    Overall Loss 4.610787    Objective Loss 4.610787                                        LR 0.001298    Time 0.129959    
2024-02-17 13:42:49,204 - Epoch: [235][  391/  391]    Overall Loss 4.610713    Objective Loss 4.610713    Top1 0.961538    Top5 2.884615    LR 0.001298    Time 0.129739    
2024-02-17 13:42:49,364 - --- validate (epoch=235)-----------
2024-02-17 13:42:49,366 - 10000 samples (128 per mini-batch)
2024-02-17 13:42:56,011 - Epoch: [235][   79/   79]    Loss 4.611978    Top1 0.990000    Top5 5.040000    
2024-02-17 13:42:56,116 - ==> Top1: 0.990    Top5: 5.040    Loss: 4.612

2024-02-17 13:42:56,125 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:42:56,125 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:42:56,190 - 

2024-02-17 13:42:56,190 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:43:10,233 - Epoch: [236][  100/  391]    Overall Loss 4.612254    Objective Loss 4.612254                                        LR 0.001298    Time 0.140346    
2024-02-17 13:43:23,735 - Epoch: [236][  200/  391]    Overall Loss 4.611699    Objective Loss 4.611699                                        LR 0.001298    Time 0.137657    
2024-02-17 13:43:37,116 - Epoch: [236][  300/  391]    Overall Loss 4.612041    Objective Loss 4.612041                                        LR 0.001298    Time 0.136359    
2024-02-17 13:43:48,855 - Epoch: [236][  391/  391]    Overall Loss 4.611951    Objective Loss 4.611951    Top1 1.923077    Top5 6.730769    LR 0.001298    Time 0.134633    
2024-02-17 13:43:48,956 - --- validate (epoch=236)-----------
2024-02-17 13:43:48,956 - 10000 samples (128 per mini-batch)
2024-02-17 13:43:54,537 - Epoch: [236][   79/   79]    Loss 4.609537    Top1 1.050000    Top5 5.030000    
2024-02-17 13:43:54,654 - ==> Top1: 1.050    Top5: 5.030    Loss: 4.610

2024-02-17 13:43:54,672 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:43:54,672 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:43:54,737 - 

2024-02-17 13:43:54,737 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:44:05,959 - Epoch: [237][  100/  391]    Overall Loss 4.611257    Objective Loss 4.611257                                        LR 0.001298    Time 0.112151    
2024-02-17 13:44:19,208 - Epoch: [237][  200/  391]    Overall Loss 4.611176    Objective Loss 4.611176                                        LR 0.001298    Time 0.122300    
2024-02-17 13:44:32,658 - Epoch: [237][  300/  391]    Overall Loss 4.611007    Objective Loss 4.611007                                        LR 0.001298    Time 0.126350    
2024-02-17 13:44:44,651 - Epoch: [237][  391/  391]    Overall Loss 4.611453    Objective Loss 4.611453    Top1 0.961538    Top5 3.846154    LR 0.001298    Time 0.127605    
2024-02-17 13:44:44,768 - --- validate (epoch=237)-----------
2024-02-17 13:44:44,769 - 10000 samples (128 per mini-batch)
2024-02-17 13:44:50,729 - Epoch: [237][   79/   79]    Loss 4.609862    Top1 0.910000    Top5 4.980000    
2024-02-17 13:44:50,857 - ==> Top1: 0.910    Top5: 4.980    Loss: 4.610

2024-02-17 13:44:50,874 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:44:50,875 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:44:50,943 - 

2024-02-17 13:44:50,943 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:45:04,948 - Epoch: [238][  100/  391]    Overall Loss 4.611310    Objective Loss 4.611310                                        LR 0.001298    Time 0.139972    
2024-02-17 13:45:17,022 - Epoch: [238][  200/  391]    Overall Loss 4.610743    Objective Loss 4.610743                                        LR 0.001298    Time 0.130334    
2024-02-17 13:45:27,634 - Epoch: [238][  300/  391]    Overall Loss 4.610959    Objective Loss 4.610959                                        LR 0.001298    Time 0.122254    
2024-02-17 13:45:35,787 - Epoch: [238][  391/  391]    Overall Loss 4.610844    Objective Loss 4.610844    Top1 0.000000    Top5 3.846154    LR 0.001298    Time 0.114646    
2024-02-17 13:45:35,932 - --- validate (epoch=238)-----------
2024-02-17 13:45:35,933 - 10000 samples (128 per mini-batch)
2024-02-17 13:45:41,510 - Epoch: [238][   79/   79]    Loss 4.608268    Top1 1.010000    Top5 5.210000    
2024-02-17 13:45:41,639 - ==> Top1: 1.010    Top5: 5.210    Loss: 4.608

2024-02-17 13:45:41,657 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:45:41,657 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:45:41,721 - 

2024-02-17 13:45:41,721 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:45:54,230 - Epoch: [239][  100/  391]    Overall Loss 4.610119    Objective Loss 4.610119                                        LR 0.001298    Time 0.125017    
2024-02-17 13:46:06,148 - Epoch: [239][  200/  391]    Overall Loss 4.610439    Objective Loss 4.610439                                        LR 0.001298    Time 0.122080    
2024-02-17 13:46:18,787 - Epoch: [239][  300/  391]    Overall Loss 4.610475    Objective Loss 4.610475                                        LR 0.001298    Time 0.123502    
2024-02-17 13:46:30,722 - Epoch: [239][  391/  391]    Overall Loss 4.610536    Objective Loss 4.610536    Top1 2.403846    Top5 4.326923    LR 0.001298    Time 0.125271    
2024-02-17 13:46:30,838 - --- validate (epoch=239)-----------
2024-02-17 13:46:30,839 - 10000 samples (128 per mini-batch)
2024-02-17 13:46:37,403 - Epoch: [239][   79/   79]    Loss 4.609605    Top1 1.000000    Top5 5.080000    
2024-02-17 13:46:37,573 - ==> Top1: 1.000    Top5: 5.080    Loss: 4.610

2024-02-17 13:46:37,595 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:46:37,595 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:46:37,660 - 

2024-02-17 13:46:37,660 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:46:51,202 - Epoch: [240][  100/  391]    Overall Loss 4.611087    Objective Loss 4.611087                                        LR 0.001298    Time 0.135346    
2024-02-17 13:47:03,629 - Epoch: [240][  200/  391]    Overall Loss 4.610618    Objective Loss 4.610618                                        LR 0.001298    Time 0.129785    
2024-02-17 13:47:16,420 - Epoch: [240][  300/  391]    Overall Loss 4.610216    Objective Loss 4.610216                                        LR 0.001298    Time 0.129145    
2024-02-17 13:47:26,318 - Epoch: [240][  391/  391]    Overall Loss 4.610185    Objective Loss 4.610185    Top1 0.000000    Top5 2.884615    LR 0.001298    Time 0.124395    
2024-02-17 13:47:26,442 - --- validate (epoch=240)-----------
2024-02-17 13:47:26,443 - 10000 samples (128 per mini-batch)
2024-02-17 13:47:32,194 - Epoch: [240][   79/   79]    Loss 4.609267    Top1 0.900000    Top5 5.080000    
2024-02-17 13:47:32,297 - ==> Top1: 0.900    Top5: 5.080    Loss: 4.609

2024-02-17 13:47:32,314 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:47:32,315 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:47:32,380 - 

2024-02-17 13:47:32,380 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:47:46,070 - Epoch: [241][  100/  391]    Overall Loss 4.609827    Objective Loss 4.609827                                        LR 0.001298    Time 0.136816    
2024-02-17 13:47:59,471 - Epoch: [241][  200/  391]    Overall Loss 4.610220    Objective Loss 4.610220                                        LR 0.001298    Time 0.135387    
2024-02-17 13:48:12,874 - Epoch: [241][  300/  391]    Overall Loss 4.609978    Objective Loss 4.609978                                        LR 0.001298    Time 0.134918    
2024-02-17 13:48:24,422 - Epoch: [241][  391/  391]    Overall Loss 4.609802    Objective Loss 4.609802    Top1 0.480769    Top5 4.326923    LR 0.001298    Time 0.133043    
2024-02-17 13:48:24,578 - --- validate (epoch=241)-----------
2024-02-17 13:48:24,579 - 10000 samples (128 per mini-batch)
2024-02-17 13:48:30,384 - Epoch: [241][   79/   79]    Loss 4.607894    Top1 1.140000    Top5 4.940000    
2024-02-17 13:48:30,544 - ==> Top1: 1.140    Top5: 4.940    Loss: 4.608

2024-02-17 13:48:30,561 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:48:30,562 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:48:30,626 - 

2024-02-17 13:48:30,626 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:48:44,657 - Epoch: [242][  100/  391]    Overall Loss 4.609693    Objective Loss 4.609693                                        LR 0.001298    Time 0.140226    
2024-02-17 13:48:57,818 - Epoch: [242][  200/  391]    Overall Loss 4.609398    Objective Loss 4.609398                                        LR 0.001298    Time 0.135892    
2024-02-17 13:49:11,147 - Epoch: [242][  300/  391]    Overall Loss 4.609198    Objective Loss 4.609198                                        LR 0.001298    Time 0.135009    
2024-02-17 13:49:22,917 - Epoch: [242][  391/  391]    Overall Loss 4.609145    Objective Loss 4.609145    Top1 0.480769    Top5 4.807692    LR 0.001298    Time 0.133677    
2024-02-17 13:49:23,088 - --- validate (epoch=242)-----------
2024-02-17 13:49:23,089 - 10000 samples (128 per mini-batch)
2024-02-17 13:49:28,692 - Epoch: [242][   79/   79]    Loss 4.609204    Top1 1.040000    Top5 5.190000    
2024-02-17 13:49:28,804 - ==> Top1: 1.040    Top5: 5.190    Loss: 4.609

2024-02-17 13:49:28,821 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:49:28,822 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:49:28,885 - 

2024-02-17 13:49:28,886 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:49:41,957 - Epoch: [243][  100/  391]    Overall Loss 4.609751    Objective Loss 4.609751                                        LR 0.001298    Time 0.130645    
2024-02-17 13:49:55,120 - Epoch: [243][  200/  391]    Overall Loss 4.609218    Objective Loss 4.609218                                        LR 0.001298    Time 0.131112    
2024-02-17 13:50:08,326 - Epoch: [243][  300/  391]    Overall Loss 4.609002    Objective Loss 4.609002                                        LR 0.001298    Time 0.131410    
2024-02-17 13:50:20,470 - Epoch: [243][  391/  391]    Overall Loss 4.608732    Objective Loss 4.608732    Top1 2.403846    Top5 5.288462    LR 0.001298    Time 0.131872    
2024-02-17 13:50:20,641 - --- validate (epoch=243)-----------
2024-02-17 13:50:20,641 - 10000 samples (128 per mini-batch)
2024-02-17 13:50:26,394 - Epoch: [243][   79/   79]    Loss 4.606912    Top1 1.130000    Top5 4.980000    
2024-02-17 13:50:26,509 - ==> Top1: 1.130    Top5: 4.980    Loss: 4.607

2024-02-17 13:50:26,530 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:50:26,530 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:50:26,596 - 

2024-02-17 13:50:26,596 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:50:40,253 - Epoch: [244][  100/  391]    Overall Loss 4.609023    Objective Loss 4.609023                                        LR 0.001298    Time 0.136499    
2024-02-17 13:50:51,939 - Epoch: [244][  200/  391]    Overall Loss 4.609155    Objective Loss 4.609155                                        LR 0.001298    Time 0.126657    
2024-02-17 13:51:05,190 - Epoch: [244][  300/  391]    Overall Loss 4.609630    Objective Loss 4.609630                                        LR 0.001298    Time 0.128593    
2024-02-17 13:51:17,508 - Epoch: [244][  391/  391]    Overall Loss 4.609823    Objective Loss 4.609823    Top1 0.961538    Top5 3.846154    LR 0.001298    Time 0.130157    
2024-02-17 13:51:17,632 - --- validate (epoch=244)-----------
2024-02-17 13:51:17,633 - 10000 samples (128 per mini-batch)
2024-02-17 13:51:23,706 - Epoch: [244][   79/   79]    Loss 4.608086    Top1 1.000000    Top5 5.190000    
2024-02-17 13:51:23,809 - ==> Top1: 1.000    Top5: 5.190    Loss: 4.608

2024-02-17 13:51:23,828 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:51:23,829 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:51:23,893 - 

2024-02-17 13:51:23,893 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:51:37,360 - Epoch: [245][  100/  391]    Overall Loss 4.608670    Objective Loss 4.608670                                        LR 0.001298    Time 0.134596    
2024-02-17 13:51:50,845 - Epoch: [245][  200/  391]    Overall Loss 4.608714    Objective Loss 4.608714                                        LR 0.001298    Time 0.134699    
2024-02-17 13:52:03,478 - Epoch: [245][  300/  391]    Overall Loss 4.608995    Objective Loss 4.608995                                        LR 0.001298    Time 0.131893    
2024-02-17 13:52:15,549 - Epoch: [245][  391/  391]    Overall Loss 4.609109    Objective Loss 4.609109    Top1 0.480769    Top5 6.250000    LR 0.001298    Time 0.132058    
2024-02-17 13:52:15,667 - --- validate (epoch=245)-----------
2024-02-17 13:52:15,668 - 10000 samples (128 per mini-batch)
2024-02-17 13:52:21,380 - Epoch: [245][   79/   79]    Loss 4.609221    Top1 0.970000    Top5 5.190000    
2024-02-17 13:52:21,510 - ==> Top1: 0.970    Top5: 5.190    Loss: 4.609

2024-02-17 13:52:21,527 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:52:21,528 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:52:21,598 - 

2024-02-17 13:52:21,599 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:52:35,374 - Epoch: [246][  100/  391]    Overall Loss 4.608956    Objective Loss 4.608956                                        LR 0.001298    Time 0.137678    
2024-02-17 13:52:48,608 - Epoch: [246][  200/  391]    Overall Loss 4.609572    Objective Loss 4.609572                                        LR 0.001298    Time 0.134981    
2024-02-17 13:53:00,927 - Epoch: [246][  300/  391]    Overall Loss 4.609179    Objective Loss 4.609179                                        LR 0.001298    Time 0.131037    
2024-02-17 13:53:12,247 - Epoch: [246][  391/  391]    Overall Loss 4.608913    Objective Loss 4.608913    Top1 0.480769    Top5 6.250000    LR 0.001298    Time 0.129481    
2024-02-17 13:53:12,345 - --- validate (epoch=246)-----------
2024-02-17 13:53:12,345 - 10000 samples (128 per mini-batch)
2024-02-17 13:53:17,454 - Epoch: [246][   79/   79]    Loss 4.607086    Top1 0.930000    Top5 5.260000    
2024-02-17 13:53:17,570 - ==> Top1: 0.930    Top5: 5.260    Loss: 4.607

2024-02-17 13:53:17,587 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:53:17,588 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:53:17,638 - 

2024-02-17 13:53:17,638 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:53:30,694 - Epoch: [247][  100/  391]    Overall Loss 4.609413    Objective Loss 4.609413                                        LR 0.001298    Time 0.130504    
2024-02-17 13:53:44,078 - Epoch: [247][  200/  391]    Overall Loss 4.608903    Objective Loss 4.608903                                        LR 0.001298    Time 0.132142    
2024-02-17 13:53:56,922 - Epoch: [247][  300/  391]    Overall Loss 4.608461    Objective Loss 4.608461                                        LR 0.001298    Time 0.130892    
2024-02-17 13:54:09,067 - Epoch: [247][  391/  391]    Overall Loss 4.608417    Objective Loss 4.608417    Top1 2.403846    Top5 7.692308    LR 0.001298    Time 0.131478    
2024-02-17 13:54:09,195 - --- validate (epoch=247)-----------
2024-02-17 13:54:09,195 - 10000 samples (128 per mini-batch)
2024-02-17 13:54:15,672 - Epoch: [247][   79/   79]    Loss 4.606073    Top1 0.930000    Top5 4.950000    
2024-02-17 13:54:15,855 - ==> Top1: 0.930    Top5: 4.950    Loss: 4.606

2024-02-17 13:54:15,876 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:54:15,876 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:54:15,940 - 

2024-02-17 13:54:15,940 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:54:29,771 - Epoch: [248][  100/  391]    Overall Loss 4.607934    Objective Loss 4.607934                                        LR 0.001298    Time 0.138236    
2024-02-17 13:54:43,239 - Epoch: [248][  200/  391]    Overall Loss 4.607970    Objective Loss 4.607970                                        LR 0.001298    Time 0.136432    
2024-02-17 13:54:55,792 - Epoch: [248][  300/  391]    Overall Loss 4.608092    Objective Loss 4.608092                                        LR 0.001298    Time 0.132782    
2024-02-17 13:55:07,902 - Epoch: [248][  391/  391]    Overall Loss 4.608408    Objective Loss 4.608408    Top1 0.000000    Top5 3.846154    LR 0.001298    Time 0.132838    
2024-02-17 13:55:08,103 - --- validate (epoch=248)-----------
2024-02-17 13:55:08,104 - 10000 samples (128 per mini-batch)
2024-02-17 13:55:14,486 - Epoch: [248][   79/   79]    Loss 4.609333    Top1 1.000000    Top5 5.270000    
2024-02-17 13:55:14,626 - ==> Top1: 1.000    Top5: 5.270    Loss: 4.609

2024-02-17 13:55:14,644 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:55:14,645 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:55:14,710 - 

2024-02-17 13:55:14,711 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:55:28,444 - Epoch: [249][  100/  391]    Overall Loss 4.608243    Objective Loss 4.608243                                        LR 0.001298    Time 0.137257    
2024-02-17 13:55:40,796 - Epoch: [249][  200/  391]    Overall Loss 4.608565    Objective Loss 4.608565                                        LR 0.001298    Time 0.130362    
2024-02-17 13:55:53,103 - Epoch: [249][  300/  391]    Overall Loss 4.608171    Objective Loss 4.608171                                        LR 0.001298    Time 0.127916    
2024-02-17 13:56:05,057 - Epoch: [249][  391/  391]    Overall Loss 4.608497    Objective Loss 4.608497    Top1 1.923077    Top5 5.769231    LR 0.001298    Time 0.128706    
2024-02-17 13:56:05,173 - --- validate (epoch=249)-----------
2024-02-17 13:56:05,174 - 10000 samples (128 per mini-batch)
2024-02-17 13:56:11,498 - Epoch: [249][   79/   79]    Loss 4.606250    Top1 0.990000    Top5 5.650000    
2024-02-17 13:56:11,617 - ==> Top1: 0.990    Top5: 5.650    Loss: 4.606

2024-02-17 13:56:11,638 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:56:11,638 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:56:11,702 - 

2024-02-17 13:56:11,703 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:56:25,558 - Epoch: [250][  100/  391]    Overall Loss 4.607528    Objective Loss 4.607528                                        LR 0.001298    Time 0.138469    
2024-02-17 13:56:38,596 - Epoch: [250][  200/  391]    Overall Loss 4.607908    Objective Loss 4.607908                                        LR 0.001298    Time 0.134400    
2024-02-17 13:56:51,814 - Epoch: [250][  300/  391]    Overall Loss 4.607817    Objective Loss 4.607817                                        LR 0.001298    Time 0.133644    
2024-02-17 13:57:02,367 - Epoch: [250][  391/  391]    Overall Loss 4.608249    Objective Loss 4.608249    Top1 1.442308    Top5 2.884615    LR 0.001298    Time 0.129519    
2024-02-17 13:57:02,477 - --- validate (epoch=250)-----------
2024-02-17 13:57:02,478 - 10000 samples (128 per mini-batch)
2024-02-17 13:57:08,058 - Epoch: [250][   79/   79]    Loss 4.607853    Top1 1.000000    Top5 4.960000    
2024-02-17 13:57:08,208 - ==> Top1: 1.000    Top5: 4.960    Loss: 4.608

2024-02-17 13:57:08,222 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:57:08,222 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:57:08,293 - 

2024-02-17 13:57:08,293 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:57:22,169 - Epoch: [251][  100/  391]    Overall Loss 4.607649    Objective Loss 4.607649                                        LR 0.001298    Time 0.138667    
2024-02-17 13:57:35,413 - Epoch: [251][  200/  391]    Overall Loss 4.607819    Objective Loss 4.607819                                        LR 0.001298    Time 0.135529    
2024-02-17 13:57:47,874 - Epoch: [251][  300/  391]    Overall Loss 4.607913    Objective Loss 4.607913                                        LR 0.001298    Time 0.131871    
2024-02-17 13:57:59,651 - Epoch: [251][  391/  391]    Overall Loss 4.608009    Objective Loss 4.608009    Top1 0.480769    Top5 2.403846    LR 0.001298    Time 0.131289    
2024-02-17 13:57:59,777 - --- validate (epoch=251)-----------
2024-02-17 13:57:59,778 - 10000 samples (128 per mini-batch)
2024-02-17 13:58:05,512 - Epoch: [251][   79/   79]    Loss 4.605822    Top1 0.980000    Top5 5.160000    
2024-02-17 13:58:05,622 - ==> Top1: 0.980    Top5: 5.160    Loss: 4.606

2024-02-17 13:58:05,641 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:58:05,641 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:58:05,713 - 

2024-02-17 13:58:05,713 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:58:18,930 - Epoch: [252][  100/  391]    Overall Loss 4.606534    Objective Loss 4.606534                                        LR 0.001298    Time 0.132094    
2024-02-17 13:58:31,073 - Epoch: [252][  200/  391]    Overall Loss 4.606830    Objective Loss 4.606830                                        LR 0.001298    Time 0.126737    
2024-02-17 13:58:44,203 - Epoch: [252][  300/  391]    Overall Loss 4.607049    Objective Loss 4.607049                                        LR 0.001298    Time 0.128241    
2024-02-17 13:58:56,185 - Epoch: [252][  391/  391]    Overall Loss 4.607187    Objective Loss 4.607187    Top1 0.480769    Top5 3.365385    LR 0.001298    Time 0.129026    
2024-02-17 13:58:56,339 - --- validate (epoch=252)-----------
2024-02-17 13:58:56,340 - 10000 samples (128 per mini-batch)
2024-02-17 13:59:01,730 - Epoch: [252][   79/   79]    Loss 4.606277    Top1 1.000000    Top5 5.030000    
2024-02-17 13:59:01,818 - ==> Top1: 1.000    Top5: 5.030    Loss: 4.606

2024-02-17 13:59:01,835 - ==> Best [Top1: 1.150   Top5: 4.880   Sparsity:0.00   Params: 1341960 on epoch: 227]
2024-02-17 13:59:01,835 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 13:59:01,893 - 

2024-02-17 13:59:01,894 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:59:15,684 - Epoch: [253][  100/  391]    Overall Loss 4.606849    Objective Loss 4.606849                                        LR 0.001298    Time 0.137827    
2024-02-17 13:59:28,724 - Epoch: [253][  200/  391]    Overall Loss 4.607903    Objective Loss 4.607903                                        LR 0.001298    Time 0.134087    
2024-02-17 13:59:41,950 - Epoch: [253][  300/  391]    Overall Loss 4.607482    Objective Loss 4.607482                                        LR 0.001298    Time 0.133462    
2024-02-17 13:59:54,078 - Epoch: [253][  391/  391]    Overall Loss 4.606967    Objective Loss 4.606967    Top1 2.884615    Top5 6.730769    LR 0.001298    Time 0.133406    
2024-02-17 13:59:54,232 - --- validate (epoch=253)-----------
2024-02-17 13:59:54,233 - 10000 samples (128 per mini-batch)
2024-02-17 14:00:00,130 - Epoch: [253][   79/   79]    Loss 4.605921    Top1 1.370000    Top5 5.140000    
2024-02-17 14:00:00,246 - ==> Top1: 1.370    Top5: 5.140    Loss: 4.606

2024-02-17 14:00:00,265 - ==> Best [Top1: 1.370   Top5: 5.140   Sparsity:0.00   Params: 1341960 on epoch: 253]
2024-02-17 14:00:00,265 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:00:00,346 - 

2024-02-17 14:00:00,347 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:00:14,353 - Epoch: [254][  100/  391]    Overall Loss 4.608208    Objective Loss 4.608208                                        LR 0.001298    Time 0.139988    
2024-02-17 14:00:27,878 - Epoch: [254][  200/  391]    Overall Loss 4.607763    Objective Loss 4.607763                                        LR 0.001298    Time 0.137592    
2024-02-17 14:00:40,852 - Epoch: [254][  300/  391]    Overall Loss 4.607892    Objective Loss 4.607892                                        LR 0.001298    Time 0.134957    
2024-02-17 14:00:52,934 - Epoch: [254][  391/  391]    Overall Loss 4.607694    Objective Loss 4.607694    Top1 1.442308    Top5 6.730769    LR 0.001298    Time 0.134436    
2024-02-17 14:00:53,050 - --- validate (epoch=254)-----------
2024-02-17 14:00:53,051 - 10000 samples (128 per mini-batch)
2024-02-17 14:00:59,179 - Epoch: [254][   79/   79]    Loss 4.606524    Top1 1.380000    Top5 5.490000    
2024-02-17 14:00:59,278 - ==> Top1: 1.380    Top5: 5.490    Loss: 4.607

2024-02-17 14:00:59,296 - ==> Best [Top1: 1.380   Top5: 5.490   Sparsity:0.00   Params: 1341960 on epoch: 254]
2024-02-17 14:00:59,296 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:00:59,376 - 

2024-02-17 14:00:59,377 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:01:13,186 - Epoch: [255][  100/  391]    Overall Loss 4.606853    Objective Loss 4.606853                                        LR 0.001298    Time 0.138008    
2024-02-17 14:01:26,591 - Epoch: [255][  200/  391]    Overall Loss 4.607479    Objective Loss 4.607479                                        LR 0.001298    Time 0.136006    
2024-02-17 14:01:40,105 - Epoch: [255][  300/  391]    Overall Loss 4.607660    Objective Loss 4.607660                                        LR 0.001298    Time 0.135700    
2024-02-17 14:01:52,180 - Epoch: [255][  391/  391]    Overall Loss 4.607579    Objective Loss 4.607579    Top1 0.480769    Top5 3.365385    LR 0.001298    Time 0.134987    
2024-02-17 14:01:52,311 - --- validate (epoch=255)-----------
2024-02-17 14:01:52,312 - 10000 samples (128 per mini-batch)
2024-02-17 14:01:58,164 - Epoch: [255][   79/   79]    Loss 4.606682    Top1 1.000000    Top5 5.140000    
2024-02-17 14:01:58,343 - ==> Top1: 1.000    Top5: 5.140    Loss: 4.607

2024-02-17 14:01:58,361 - ==> Best [Top1: 1.380   Top5: 5.490   Sparsity:0.00   Params: 1341960 on epoch: 254]
2024-02-17 14:01:58,361 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:01:58,427 - 

2024-02-17 14:01:58,427 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:02:11,644 - Epoch: [256][  100/  391]    Overall Loss 4.607459    Objective Loss 4.607459                                        LR 0.001298    Time 0.132097    
2024-02-17 14:02:25,032 - Epoch: [256][  200/  391]    Overall Loss 4.608099    Objective Loss 4.608099                                        LR 0.001298    Time 0.132964    
2024-02-17 14:02:37,860 - Epoch: [256][  300/  391]    Overall Loss 4.607853    Objective Loss 4.607853                                        LR 0.001298    Time 0.131385    
2024-02-17 14:02:47,725 - Epoch: [256][  391/  391]    Overall Loss 4.607696    Objective Loss 4.607696    Top1 0.480769    Top5 4.326923    LR 0.001298    Time 0.126030    
2024-02-17 14:02:47,881 - --- validate (epoch=256)-----------
2024-02-17 14:02:47,881 - 10000 samples (128 per mini-batch)
2024-02-17 14:02:53,764 - Epoch: [256][   79/   79]    Loss 4.610002    Top1 1.040000    Top5 5.030000    
2024-02-17 14:02:53,883 - ==> Top1: 1.040    Top5: 5.030    Loss: 4.610

2024-02-17 14:02:53,899 - ==> Best [Top1: 1.380   Top5: 5.490   Sparsity:0.00   Params: 1341960 on epoch: 254]
2024-02-17 14:02:53,899 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:02:53,964 - 

2024-02-17 14:02:53,964 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:03:07,649 - Epoch: [257][  100/  391]    Overall Loss 4.606754    Objective Loss 4.606754                                        LR 0.001298    Time 0.136766    
2024-02-17 14:03:21,064 - Epoch: [257][  200/  391]    Overall Loss 4.607303    Objective Loss 4.607303                                        LR 0.001298    Time 0.135434    
2024-02-17 14:03:34,493 - Epoch: [257][  300/  391]    Overall Loss 4.607561    Objective Loss 4.607561                                        LR 0.001298    Time 0.135033    
2024-02-17 14:03:46,645 - Epoch: [257][  391/  391]    Overall Loss 4.607526    Objective Loss 4.607526    Top1 1.923077    Top5 6.730769    LR 0.001298    Time 0.134675    
2024-02-17 14:03:46,780 - --- validate (epoch=257)-----------
2024-02-17 14:03:46,781 - 10000 samples (128 per mini-batch)
2024-02-17 14:03:53,387 - Epoch: [257][   79/   79]    Loss 4.607851    Top1 1.000000    Top5 5.160000    
2024-02-17 14:03:53,505 - ==> Top1: 1.000    Top5: 5.160    Loss: 4.608

2024-02-17 14:03:53,524 - ==> Best [Top1: 1.380   Top5: 5.490   Sparsity:0.00   Params: 1341960 on epoch: 254]
2024-02-17 14:03:53,524 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:03:53,591 - 

2024-02-17 14:03:53,592 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:04:07,381 - Epoch: [258][  100/  391]    Overall Loss 4.604604    Objective Loss 4.604604                                        LR 0.001298    Time 0.137751    
2024-02-17 14:04:20,425 - Epoch: [258][  200/  391]    Overall Loss 4.603459    Objective Loss 4.603459                                        LR 0.001298    Time 0.134073    
2024-02-17 14:04:33,486 - Epoch: [258][  300/  391]    Overall Loss 4.602681    Objective Loss 4.602681                                        LR 0.001298    Time 0.132902    
2024-02-17 14:04:45,512 - Epoch: [258][  391/  391]    Overall Loss 4.602195    Objective Loss 4.602195    Top1 1.923077    Top5 6.730769    LR 0.001298    Time 0.132717    
2024-02-17 14:04:45,663 - --- validate (epoch=258)-----------
2024-02-17 14:04:45,664 - 10000 samples (128 per mini-batch)
2024-02-17 14:04:52,270 - Epoch: [258][   79/   79]    Loss 4.596410    Top1 1.480000    Top5 6.670000    
2024-02-17 14:04:52,372 - ==> Top1: 1.480    Top5: 6.670    Loss: 4.596

2024-02-17 14:04:52,390 - ==> Best [Top1: 1.480   Top5: 6.670   Sparsity:0.00   Params: 1341960 on epoch: 258]
2024-02-17 14:04:52,390 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:04:52,473 - 

2024-02-17 14:04:52,474 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:05:06,364 - Epoch: [259][  100/  391]    Overall Loss 4.597012    Objective Loss 4.597012                                        LR 0.001298    Time 0.138816    
2024-02-17 14:05:19,782 - Epoch: [259][  200/  391]    Overall Loss 4.597668    Objective Loss 4.597668                                        LR 0.001298    Time 0.136471    
2024-02-17 14:05:32,961 - Epoch: [259][  300/  391]    Overall Loss 4.597440    Objective Loss 4.597440                                        LR 0.001298    Time 0.134893    
2024-02-17 14:05:44,341 - Epoch: [259][  391/  391]    Overall Loss 4.597365    Objective Loss 4.597365    Top1 0.480769    Top5 4.807692    LR 0.001298    Time 0.132595    
2024-02-17 14:05:44,460 - --- validate (epoch=259)-----------
2024-02-17 14:05:44,461 - 10000 samples (128 per mini-batch)
2024-02-17 14:05:50,283 - Epoch: [259][   79/   79]    Loss 4.593542    Top1 1.250000    Top5 6.400000    
2024-02-17 14:05:50,384 - ==> Top1: 1.250    Top5: 6.400    Loss: 4.594

2024-02-17 14:05:50,402 - ==> Best [Top1: 1.480   Top5: 6.670   Sparsity:0.00   Params: 1341960 on epoch: 258]
2024-02-17 14:05:50,402 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:05:50,453 - 

2024-02-17 14:05:50,454 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:06:03,245 - Epoch: [260][  100/  391]    Overall Loss 4.593300    Objective Loss 4.593300                                        LR 0.001298    Time 0.127848    
2024-02-17 14:06:15,144 - Epoch: [260][  200/  391]    Overall Loss 4.592435    Objective Loss 4.592435                                        LR 0.001298    Time 0.123398    
2024-02-17 14:06:26,356 - Epoch: [260][  300/  391]    Overall Loss 4.592108    Objective Loss 4.592108                                        LR 0.001298    Time 0.119626    
2024-02-17 14:06:38,460 - Epoch: [260][  391/  391]    Overall Loss 4.591720    Objective Loss 4.591720    Top1 2.403846    Top5 6.730769    LR 0.001298    Time 0.122732    
2024-02-17 14:06:38,584 - --- validate (epoch=260)-----------
2024-02-17 14:06:38,585 - 10000 samples (128 per mini-batch)
2024-02-17 14:06:44,287 - Epoch: [260][   79/   79]    Loss 4.588867    Top1 1.640000    Top5 7.190000    
2024-02-17 14:06:44,392 - ==> Top1: 1.640    Top5: 7.190    Loss: 4.589

2024-02-17 14:06:44,408 - ==> Best [Top1: 1.640   Top5: 7.190   Sparsity:0.00   Params: 1341960 on epoch: 260]
2024-02-17 14:06:44,409 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:06:44,471 - 

2024-02-17 14:06:44,471 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:06:56,340 - Epoch: [261][  100/  391]    Overall Loss 4.589693    Objective Loss 4.589693                                        LR 0.001298    Time 0.118626    
2024-02-17 14:07:09,372 - Epoch: [261][  200/  391]    Overall Loss 4.590886    Objective Loss 4.590886                                        LR 0.001298    Time 0.124451    
2024-02-17 14:07:23,021 - Epoch: [261][  300/  391]    Overall Loss 4.590704    Objective Loss 4.590704                                        LR 0.001298    Time 0.128448    
2024-02-17 14:07:35,042 - Epoch: [261][  391/  391]    Overall Loss 4.589670    Objective Loss 4.589670    Top1 0.480769    Top5 8.173077    LR 0.001298    Time 0.129286    
2024-02-17 14:07:35,205 - --- validate (epoch=261)-----------
2024-02-17 14:07:35,206 - 10000 samples (128 per mini-batch)
2024-02-17 14:07:41,449 - Epoch: [261][   79/   79]    Loss 4.583055    Top1 1.430000    Top5 7.050000    
2024-02-17 14:07:41,601 - ==> Top1: 1.430    Top5: 7.050    Loss: 4.583

2024-02-17 14:07:41,613 - ==> Best [Top1: 1.640   Top5: 7.190   Sparsity:0.00   Params: 1341960 on epoch: 260]
2024-02-17 14:07:41,613 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:07:41,694 - 

2024-02-17 14:07:41,694 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:07:55,471 - Epoch: [262][  100/  391]    Overall Loss 4.584955    Objective Loss 4.584955                                        LR 0.001298    Time 0.137669    
2024-02-17 14:08:06,617 - Epoch: [262][  200/  391]    Overall Loss 4.586423    Objective Loss 4.586423                                        LR 0.001298    Time 0.124547    
2024-02-17 14:08:18,399 - Epoch: [262][  300/  391]    Overall Loss 4.586441    Objective Loss 4.586441                                        LR 0.001298    Time 0.122288    
2024-02-17 14:08:29,974 - Epoch: [262][  391/  391]    Overall Loss 4.585832    Objective Loss 4.585832    Top1 1.923077    Top5 5.288462    LR 0.001298    Time 0.123420    
2024-02-17 14:08:30,091 - --- validate (epoch=262)-----------
2024-02-17 14:08:30,092 - 10000 samples (128 per mini-batch)
2024-02-17 14:08:35,559 - Epoch: [262][   79/   79]    Loss 4.614627    Top1 1.180000    Top5 5.820000    
2024-02-17 14:08:35,667 - ==> Top1: 1.180    Top5: 5.820    Loss: 4.615

2024-02-17 14:08:35,684 - ==> Best [Top1: 1.640   Top5: 7.190   Sparsity:0.00   Params: 1341960 on epoch: 260]
2024-02-17 14:08:35,684 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:08:35,748 - 

2024-02-17 14:08:35,748 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:08:49,252 - Epoch: [263][  100/  391]    Overall Loss 4.583093    Objective Loss 4.583093                                        LR 0.001298    Time 0.134958    
2024-02-17 14:09:02,573 - Epoch: [263][  200/  391]    Overall Loss 4.581942    Objective Loss 4.581942                                        LR 0.001298    Time 0.134058    
2024-02-17 14:09:15,775 - Epoch: [263][  300/  391]    Overall Loss 4.580260    Objective Loss 4.580260                                        LR 0.001298    Time 0.133361    
2024-02-17 14:09:28,212 - Epoch: [263][  391/  391]    Overall Loss 4.579715    Objective Loss 4.579715    Top1 1.923077    Top5 11.057692    LR 0.001298    Time 0.134119    
2024-02-17 14:09:28,352 - --- validate (epoch=263)-----------
2024-02-17 14:09:28,353 - 10000 samples (128 per mini-batch)
2024-02-17 14:09:34,047 - Epoch: [263][   79/   79]    Loss 4.576033    Top1 1.810000    Top5 8.040000    
2024-02-17 14:09:34,137 - ==> Top1: 1.810    Top5: 8.040    Loss: 4.576

2024-02-17 14:09:34,152 - ==> Best [Top1: 1.810   Top5: 8.040   Sparsity:0.00   Params: 1341960 on epoch: 263]
2024-02-17 14:09:34,153 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:09:34,229 - 

2024-02-17 14:09:34,229 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:09:48,224 - Epoch: [264][  100/  391]    Overall Loss 4.574981    Objective Loss 4.574981                                        LR 0.001298    Time 0.139875    
2024-02-17 14:10:01,432 - Epoch: [264][  200/  391]    Overall Loss 4.573983    Objective Loss 4.573983                                        LR 0.001298    Time 0.135949    
2024-02-17 14:10:14,518 - Epoch: [264][  300/  391]    Overall Loss 4.571759    Objective Loss 4.571759                                        LR 0.001298    Time 0.134238    
2024-02-17 14:10:26,800 - Epoch: [264][  391/  391]    Overall Loss 4.570177    Objective Loss 4.570177    Top1 0.480769    Top5 6.250000    LR 0.001298    Time 0.134394    
2024-02-17 14:10:26,963 - --- validate (epoch=264)-----------
2024-02-17 14:10:26,964 - 10000 samples (128 per mini-batch)
2024-02-17 14:10:33,002 - Epoch: [264][   79/   79]    Loss 4.561622    Top1 1.700000    Top5 8.160000    
2024-02-17 14:10:33,136 - ==> Top1: 1.700    Top5: 8.160    Loss: 4.562

2024-02-17 14:10:33,154 - ==> Best [Top1: 1.810   Top5: 8.040   Sparsity:0.00   Params: 1341960 on epoch: 263]
2024-02-17 14:10:33,154 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:10:33,216 - 

2024-02-17 14:10:33,216 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:10:46,571 - Epoch: [265][  100/  391]    Overall Loss 4.563881    Objective Loss 4.563881                                        LR 0.001298    Time 0.133468    
2024-02-17 14:10:59,884 - Epoch: [265][  200/  391]    Overall Loss 4.564431    Objective Loss 4.564431                                        LR 0.001298    Time 0.133275    
2024-02-17 14:11:12,799 - Epoch: [265][  300/  391]    Overall Loss 4.564059    Objective Loss 4.564059                                        LR 0.001298    Time 0.131884    
2024-02-17 14:11:24,553 - Epoch: [265][  391/  391]    Overall Loss 4.563659    Objective Loss 4.563659    Top1 1.923077    Top5 11.057692    LR 0.001298    Time 0.131240    
2024-02-17 14:11:24,733 - --- validate (epoch=265)-----------
2024-02-17 14:11:24,735 - 10000 samples (128 per mini-batch)
2024-02-17 14:11:29,983 - Epoch: [265][   79/   79]    Loss 4.554293    Top1 1.880000    Top5 7.970000    
2024-02-17 14:11:30,081 - ==> Top1: 1.880    Top5: 7.970    Loss: 4.554

2024-02-17 14:11:30,098 - ==> Best [Top1: 1.880   Top5: 7.970   Sparsity:0.00   Params: 1341960 on epoch: 265]
2024-02-17 14:11:30,099 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:11:30,167 - 

2024-02-17 14:11:30,168 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:11:43,398 - Epoch: [266][  100/  391]    Overall Loss 4.558457    Objective Loss 4.558457                                        LR 0.001298    Time 0.132229    
2024-02-17 14:11:56,736 - Epoch: [266][  200/  391]    Overall Loss 4.557856    Objective Loss 4.557856                                        LR 0.001298    Time 0.132778    
2024-02-17 14:12:08,735 - Epoch: [266][  300/  391]    Overall Loss 4.563262    Objective Loss 4.563262                                        LR 0.001298    Time 0.128500    
2024-02-17 14:12:19,714 - Epoch: [266][  391/  391]    Overall Loss 4.561730    Objective Loss 4.561730    Top1 1.442308    Top5 6.250000    LR 0.001298    Time 0.126661    
2024-02-17 14:12:19,837 - --- validate (epoch=266)-----------
2024-02-17 14:12:19,839 - 10000 samples (128 per mini-batch)
2024-02-17 14:12:25,347 - Epoch: [266][   79/   79]    Loss 4.578859    Top1 1.950000    Top5 7.610000    
2024-02-17 14:12:25,511 - ==> Top1: 1.950    Top5: 7.610    Loss: 4.579

2024-02-17 14:12:25,530 - ==> Best [Top1: 1.950   Top5: 7.610   Sparsity:0.00   Params: 1341960 on epoch: 266]
2024-02-17 14:12:25,531 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:12:25,611 - 

2024-02-17 14:12:25,612 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:12:39,305 - Epoch: [267][  100/  391]    Overall Loss 4.557205    Objective Loss 4.557205                                        LR 0.001298    Time 0.136854    
2024-02-17 14:12:52,158 - Epoch: [267][  200/  391]    Overall Loss 4.554598    Objective Loss 4.554598                                        LR 0.001298    Time 0.132671    
2024-02-17 14:13:05,356 - Epoch: [267][  300/  391]    Overall Loss 4.555220    Objective Loss 4.555220                                        LR 0.001298    Time 0.132424    
2024-02-17 14:13:17,586 - Epoch: [267][  391/  391]    Overall Loss 4.553873    Objective Loss 4.553873    Top1 0.480769    Top5 7.211538    LR 0.001298    Time 0.132871    
2024-02-17 14:13:17,765 - --- validate (epoch=267)-----------
2024-02-17 14:13:17,765 - 10000 samples (128 per mini-batch)
2024-02-17 14:13:23,708 - Epoch: [267][   79/   79]    Loss 4.549033    Top1 2.000000    Top5 8.680000    
2024-02-17 14:13:23,886 - ==> Top1: 2.000    Top5: 8.680    Loss: 4.549

2024-02-17 14:13:23,903 - ==> Best [Top1: 2.000   Top5: 8.680   Sparsity:0.00   Params: 1341960 on epoch: 267]
2024-02-17 14:13:23,904 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:13:23,983 - 

2024-02-17 14:13:23,983 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:13:37,967 - Epoch: [268][  100/  391]    Overall Loss 4.547956    Objective Loss 4.547956                                        LR 0.001298    Time 0.139758    
2024-02-17 14:13:51,334 - Epoch: [268][  200/  391]    Overall Loss 4.547511    Objective Loss 4.547511                                        LR 0.001298    Time 0.136686    
2024-02-17 14:14:04,286 - Epoch: [268][  300/  391]    Overall Loss 4.547602    Objective Loss 4.547602                                        LR 0.001298    Time 0.134281    
2024-02-17 14:14:15,895 - Epoch: [268][  391/  391]    Overall Loss 4.546458    Objective Loss 4.546458    Top1 0.961538    Top5 8.653846    LR 0.001298    Time 0.132708    
2024-02-17 14:14:16,027 - --- validate (epoch=268)-----------
2024-02-17 14:14:16,028 - 10000 samples (128 per mini-batch)
2024-02-17 14:14:22,398 - Epoch: [268][   79/   79]    Loss 4.533298    Top1 2.130000    Top5 9.400000    
2024-02-17 14:14:22,516 - ==> Top1: 2.130    Top5: 9.400    Loss: 4.533

2024-02-17 14:14:22,536 - ==> Best [Top1: 2.130   Top5: 9.400   Sparsity:0.00   Params: 1341960 on epoch: 268]
2024-02-17 14:14:22,536 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:14:22,623 - 

2024-02-17 14:14:22,623 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:14:36,750 - Epoch: [269][  100/  391]    Overall Loss 4.534682    Objective Loss 4.534682                                        LR 0.001298    Time 0.141143    
2024-02-17 14:14:48,326 - Epoch: [269][  200/  391]    Overall Loss 4.532444    Objective Loss 4.532444                                        LR 0.001298    Time 0.128431    
2024-02-17 14:15:01,073 - Epoch: [269][  300/  391]    Overall Loss 4.530050    Objective Loss 4.530050                                        LR 0.001298    Time 0.128095    
2024-02-17 14:15:12,941 - Epoch: [269][  391/  391]    Overall Loss 4.527326    Objective Loss 4.527326    Top1 2.403846    Top5 8.173077    LR 0.001298    Time 0.128625    
2024-02-17 14:15:13,076 - --- validate (epoch=269)-----------
2024-02-17 14:15:13,077 - 10000 samples (128 per mini-batch)
2024-02-17 14:15:18,849 - Epoch: [269][   79/   79]    Loss 4.508794    Top1 2.180000    Top5 10.490000    
2024-02-17 14:15:18,969 - ==> Top1: 2.180    Top5: 10.490    Loss: 4.509

2024-02-17 14:15:18,983 - ==> Best [Top1: 2.180   Top5: 10.490   Sparsity:0.00   Params: 1341960 on epoch: 269]
2024-02-17 14:15:18,984 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:15:19,062 - 

2024-02-17 14:15:19,062 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:15:32,908 - Epoch: [270][  100/  391]    Overall Loss 4.522503    Objective Loss 4.522503                                        LR 0.001298    Time 0.138379    
2024-02-17 14:15:46,232 - Epoch: [270][  200/  391]    Overall Loss 4.513876    Objective Loss 4.513876                                        LR 0.001298    Time 0.135786    
2024-02-17 14:15:59,336 - Epoch: [270][  300/  391]    Overall Loss 4.509505    Objective Loss 4.509505                                        LR 0.001298    Time 0.134186    
2024-02-17 14:16:11,198 - Epoch: [270][  391/  391]    Overall Loss 4.506465    Objective Loss 4.506465    Top1 4.326923    Top5 13.942308    LR 0.001298    Time 0.133283    
2024-02-17 14:16:11,330 - --- validate (epoch=270)-----------
2024-02-17 14:16:11,331 - 10000 samples (128 per mini-batch)
2024-02-17 14:16:17,643 - Epoch: [270][   79/   79]    Loss 4.505040    Top1 2.610000    Top5 10.820000    
2024-02-17 14:16:17,822 - ==> Top1: 2.610    Top5: 10.820    Loss: 4.505

2024-02-17 14:16:18,061 - ==> Best [Top1: 2.610   Top5: 10.820   Sparsity:0.00   Params: 1341960 on epoch: 270]
2024-02-17 14:16:18,061 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:16:18,137 - 

2024-02-17 14:16:18,137 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:16:31,455 - Epoch: [271][  100/  391]    Overall Loss 4.490960    Objective Loss 4.490960                                        LR 0.001298    Time 0.133100    
2024-02-17 14:16:44,179 - Epoch: [271][  200/  391]    Overall Loss 4.491574    Objective Loss 4.491574                                        LR 0.001298    Time 0.130146    
2024-02-17 14:16:56,915 - Epoch: [271][  300/  391]    Overall Loss 4.488567    Objective Loss 4.488567                                        LR 0.001298    Time 0.129201    
2024-02-17 14:17:08,743 - Epoch: [271][  391/  391]    Overall Loss 4.485994    Objective Loss 4.485994    Top1 0.961538    Top5 7.692308    LR 0.001298    Time 0.129372    
2024-02-17 14:17:08,911 - --- validate (epoch=271)-----------
2024-02-17 14:17:08,912 - 10000 samples (128 per mini-batch)
2024-02-17 14:17:14,580 - Epoch: [271][   79/   79]    Loss 4.486438    Top1 2.000000    Top5 10.010000    
2024-02-17 14:17:14,762 - ==> Top1: 2.000    Top5: 10.010    Loss: 4.486

2024-02-17 14:17:14,781 - ==> Best [Top1: 2.610   Top5: 10.820   Sparsity:0.00   Params: 1341960 on epoch: 270]
2024-02-17 14:17:14,781 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:17:14,845 - 

2024-02-17 14:17:14,846 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:17:28,785 - Epoch: [272][  100/  391]    Overall Loss 4.474716    Objective Loss 4.474716                                        LR 0.001298    Time 0.139323    
2024-02-17 14:17:41,648 - Epoch: [272][  200/  391]    Overall Loss 4.473344    Objective Loss 4.473344                                        LR 0.001298    Time 0.133950    
2024-02-17 14:17:54,787 - Epoch: [272][  300/  391]    Overall Loss 4.470042    Objective Loss 4.470042                                        LR 0.001298    Time 0.133081    
2024-02-17 14:18:07,251 - Epoch: [272][  391/  391]    Overall Loss 4.468297    Objective Loss 4.468297    Top1 2.403846    Top5 9.615385    LR 0.001298    Time 0.133974    
2024-02-17 14:18:07,419 - --- validate (epoch=272)-----------
2024-02-17 14:18:07,420 - 10000 samples (128 per mini-batch)
2024-02-17 14:18:13,154 - Epoch: [272][   79/   79]    Loss 4.442082    Top1 2.570000    Top5 11.920000    
2024-02-17 14:18:13,245 - ==> Top1: 2.570    Top5: 11.920    Loss: 4.442

2024-02-17 14:18:13,263 - ==> Best [Top1: 2.610   Top5: 10.820   Sparsity:0.00   Params: 1341960 on epoch: 270]
2024-02-17 14:18:13,263 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:18:13,314 - 

2024-02-17 14:18:13,315 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:18:26,637 - Epoch: [273][  100/  391]    Overall Loss 4.454962    Objective Loss 4.454962                                        LR 0.001298    Time 0.133119    
2024-02-17 14:18:39,698 - Epoch: [273][  200/  391]    Overall Loss 4.454848    Objective Loss 4.454848                                        LR 0.001298    Time 0.131841    
2024-02-17 14:18:52,887 - Epoch: [273][  300/  391]    Overall Loss 4.451272    Objective Loss 4.451272                                        LR 0.001298    Time 0.131838    
2024-02-17 14:19:05,094 - Epoch: [273][  391/  391]    Overall Loss 4.446890    Objective Loss 4.446890    Top1 2.403846    Top5 12.980769    LR 0.001298    Time 0.132364    
2024-02-17 14:19:05,212 - --- validate (epoch=273)-----------
2024-02-17 14:19:05,213 - 10000 samples (128 per mini-batch)
2024-02-17 14:19:10,259 - Epoch: [273][   79/   79]    Loss 4.413025    Top1 3.100000    Top5 13.470000    
2024-02-17 14:19:10,351 - ==> Top1: 3.100    Top5: 13.470    Loss: 4.413

2024-02-17 14:19:10,368 - ==> Best [Top1: 3.100   Top5: 13.470   Sparsity:0.00   Params: 1341960 on epoch: 273]
2024-02-17 14:19:10,368 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:19:10,430 - 

2024-02-17 14:19:10,430 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:19:23,067 - Epoch: [274][  100/  391]    Overall Loss 4.431215    Objective Loss 4.431215                                        LR 0.001298    Time 0.126306    
2024-02-17 14:19:36,067 - Epoch: [274][  200/  391]    Overall Loss 4.430238    Objective Loss 4.430238                                        LR 0.001298    Time 0.128131    
2024-02-17 14:19:49,294 - Epoch: [274][  300/  391]    Overall Loss 4.426323    Objective Loss 4.426323                                        LR 0.001298    Time 0.129495    
2024-02-17 14:20:01,404 - Epoch: [274][  391/  391]    Overall Loss 4.424844    Objective Loss 4.424844    Top1 2.884615    Top5 10.096154    LR 0.001298    Time 0.130317    
2024-02-17 14:20:01,582 - --- validate (epoch=274)-----------
2024-02-17 14:20:01,583 - 10000 samples (128 per mini-batch)
2024-02-17 14:20:06,613 - Epoch: [274][   79/   79]    Loss 4.409709    Top1 3.060000    Top5 13.270000    
2024-02-17 14:20:06,703 - ==> Top1: 3.060    Top5: 13.270    Loss: 4.410

2024-02-17 14:20:06,720 - ==> Best [Top1: 3.100   Top5: 13.470   Sparsity:0.00   Params: 1341960 on epoch: 273]
2024-02-17 14:20:06,721 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:20:06,770 - 

2024-02-17 14:20:06,770 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:20:19,236 - Epoch: [275][  100/  391]    Overall Loss 4.412058    Objective Loss 4.412058                                        LR 0.001298    Time 0.124597    
2024-02-17 14:20:32,605 - Epoch: [275][  200/  391]    Overall Loss 4.403921    Objective Loss 4.403921                                        LR 0.001298    Time 0.129122    
2024-02-17 14:20:45,979 - Epoch: [275][  300/  391]    Overall Loss 4.400355    Objective Loss 4.400355                                        LR 0.001298    Time 0.130642    
2024-02-17 14:20:58,082 - Epoch: [275][  391/  391]    Overall Loss 4.397814    Objective Loss 4.397814    Top1 2.403846    Top5 12.500000    LR 0.001298    Time 0.131178    
2024-02-17 14:20:58,199 - --- validate (epoch=275)-----------
2024-02-17 14:20:58,200 - 10000 samples (128 per mini-batch)
2024-02-17 14:21:02,881 - Epoch: [275][   79/   79]    Loss 4.381921    Top1 2.950000    Top5 13.330000    
2024-02-17 14:21:02,966 - ==> Top1: 2.950    Top5: 13.330    Loss: 4.382

2024-02-17 14:21:02,983 - ==> Best [Top1: 3.100   Top5: 13.470   Sparsity:0.00   Params: 1341960 on epoch: 273]
2024-02-17 14:21:02,983 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:21:03,033 - 

2024-02-17 14:21:03,033 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:21:16,533 - Epoch: [276][  100/  391]    Overall Loss 4.387897    Objective Loss 4.387897                                        LR 0.001298    Time 0.134940    
2024-02-17 14:21:29,642 - Epoch: [276][  200/  391]    Overall Loss 4.378122    Objective Loss 4.378122                                        LR 0.001298    Time 0.132990    
2024-02-17 14:21:42,540 - Epoch: [276][  300/  391]    Overall Loss 4.378027    Objective Loss 4.378027                                        LR 0.001298    Time 0.131637    
2024-02-17 14:21:54,276 - Epoch: [276][  391/  391]    Overall Loss 4.376005    Objective Loss 4.376005    Top1 2.403846    Top5 13.942308    LR 0.001298    Time 0.131005    
2024-02-17 14:21:54,399 - --- validate (epoch=276)-----------
2024-02-17 14:21:54,400 - 10000 samples (128 per mini-batch)
2024-02-17 14:22:00,761 - Epoch: [276][   79/   79]    Loss 4.354402    Top1 2.830000    Top5 14.640000    
2024-02-17 14:22:00,864 - ==> Top1: 2.830    Top5: 14.640    Loss: 4.354

2024-02-17 14:22:00,875 - ==> Best [Top1: 3.100   Top5: 13.470   Sparsity:0.00   Params: 1341960 on epoch: 273]
2024-02-17 14:22:00,875 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:22:00,938 - 

2024-02-17 14:22:00,938 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:22:14,723 - Epoch: [277][  100/  391]    Overall Loss 4.372206    Objective Loss 4.372206                                        LR 0.001298    Time 0.137748    
2024-02-17 14:22:28,071 - Epoch: [277][  200/  391]    Overall Loss 4.366708    Objective Loss 4.366708                                        LR 0.001298    Time 0.135586    
2024-02-17 14:22:40,630 - Epoch: [277][  300/  391]    Overall Loss 4.363192    Objective Loss 4.363192                                        LR 0.001298    Time 0.132237    
2024-02-17 14:22:52,386 - Epoch: [277][  391/  391]    Overall Loss 4.362585    Objective Loss 4.362585    Top1 4.807692    Top5 16.346154    LR 0.001298    Time 0.131518    
2024-02-17 14:22:52,500 - --- validate (epoch=277)-----------
2024-02-17 14:22:52,501 - 10000 samples (128 per mini-batch)
2024-02-17 14:22:58,973 - Epoch: [277][   79/   79]    Loss 4.347608    Top1 3.710000    Top5 14.840000    
2024-02-17 14:22:59,100 - ==> Top1: 3.710    Top5: 14.840    Loss: 4.348

2024-02-17 14:22:59,128 - ==> Best [Top1: 3.710   Top5: 14.840   Sparsity:0.00   Params: 1341960 on epoch: 277]
2024-02-17 14:22:59,128 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:22:59,226 - 

2024-02-17 14:22:59,226 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:23:13,080 - Epoch: [278][  100/  391]    Overall Loss 4.347290    Objective Loss 4.347290                                        LR 0.001298    Time 0.138463    
2024-02-17 14:23:26,380 - Epoch: [278][  200/  391]    Overall Loss 4.352776    Objective Loss 4.352776                                        LR 0.001298    Time 0.135703    
2024-02-17 14:23:39,142 - Epoch: [278][  300/  391]    Overall Loss 4.351920    Objective Loss 4.351920                                        LR 0.001298    Time 0.132992    
2024-02-17 14:23:50,991 - Epoch: [278][  391/  391]    Overall Loss 4.350029    Objective Loss 4.350029    Top1 2.403846    Top5 12.500000    LR 0.001298    Time 0.132334    
2024-02-17 14:23:51,101 - --- validate (epoch=278)-----------
2024-02-17 14:23:51,102 - 10000 samples (128 per mini-batch)
2024-02-17 14:23:57,081 - Epoch: [278][   79/   79]    Loss 4.333380    Top1 3.830000    Top5 14.280000    
2024-02-17 14:23:57,181 - ==> Top1: 3.830    Top5: 14.280    Loss: 4.333

2024-02-17 14:23:57,202 - ==> Best [Top1: 3.830   Top5: 14.280   Sparsity:0.00   Params: 1341960 on epoch: 278]
2024-02-17 14:23:57,202 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:23:57,283 - 

2024-02-17 14:23:57,284 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:24:10,341 - Epoch: [279][  100/  391]    Overall Loss 4.347185    Objective Loss 4.347185                                        LR 0.001298    Time 0.130494    
2024-02-17 14:24:23,476 - Epoch: [279][  200/  391]    Overall Loss 4.348021    Objective Loss 4.348021                                        LR 0.001298    Time 0.130900    
2024-02-17 14:24:36,496 - Epoch: [279][  300/  391]    Overall Loss 4.344835    Objective Loss 4.344835                                        LR 0.001298    Time 0.130650    
2024-02-17 14:24:46,875 - Epoch: [279][  391/  391]    Overall Loss 4.342450    Objective Loss 4.342450    Top1 4.326923    Top5 17.788462    LR 0.001298    Time 0.126779    
2024-02-17 14:24:47,028 - --- validate (epoch=279)-----------
2024-02-17 14:24:47,029 - 10000 samples (128 per mini-batch)
2024-02-17 14:24:52,254 - Epoch: [279][   79/   79]    Loss 4.327073    Top1 3.580000    Top5 15.310000    
2024-02-17 14:24:52,411 - ==> Top1: 3.580    Top5: 15.310    Loss: 4.327

2024-02-17 14:24:52,429 - ==> Best [Top1: 3.830   Top5: 14.280   Sparsity:0.00   Params: 1341960 on epoch: 278]
2024-02-17 14:24:52,429 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:24:52,491 - 

2024-02-17 14:24:52,492 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:25:06,496 - Epoch: [280][  100/  391]    Overall Loss 4.332715    Objective Loss 4.332715                                        LR 0.001298    Time 0.139970    
2024-02-17 14:25:19,668 - Epoch: [280][  200/  391]    Overall Loss 4.332573    Objective Loss 4.332573                                        LR 0.001298    Time 0.135819    
2024-02-17 14:25:33,010 - Epoch: [280][  300/  391]    Overall Loss 4.332189    Objective Loss 4.332189                                        LR 0.001298    Time 0.135000    
2024-02-17 14:25:45,047 - Epoch: [280][  391/  391]    Overall Loss 4.332532    Objective Loss 4.332532    Top1 1.923077    Top5 10.096154    LR 0.001298    Time 0.134354    
2024-02-17 14:25:45,187 - --- validate (epoch=280)-----------
2024-02-17 14:25:45,188 - 10000 samples (128 per mini-batch)
2024-02-17 14:25:50,925 - Epoch: [280][   79/   79]    Loss 4.314670    Top1 3.580000    Top5 15.480000    
2024-02-17 14:25:51,042 - ==> Top1: 3.580    Top5: 15.480    Loss: 4.315

2024-02-17 14:25:51,055 - ==> Best [Top1: 3.830   Top5: 14.280   Sparsity:0.00   Params: 1341960 on epoch: 278]
2024-02-17 14:25:51,056 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:25:51,122 - 

2024-02-17 14:25:51,122 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:26:04,594 - Epoch: [281][  100/  391]    Overall Loss 4.325333    Objective Loss 4.325333                                        LR 0.001298    Time 0.134642    
2024-02-17 14:26:17,597 - Epoch: [281][  200/  391]    Overall Loss 4.324877    Objective Loss 4.324877                                        LR 0.001298    Time 0.132317    
2024-02-17 14:26:30,384 - Epoch: [281][  300/  391]    Overall Loss 4.324567    Objective Loss 4.324567                                        LR 0.001298    Time 0.130819    
2024-02-17 14:26:41,437 - Epoch: [281][  391/  391]    Overall Loss 4.328782    Objective Loss 4.328782    Top1 2.403846    Top5 9.134615    LR 0.001298    Time 0.128630    
2024-02-17 14:26:41,625 - --- validate (epoch=281)-----------
2024-02-17 14:26:41,626 - 10000 samples (128 per mini-batch)
2024-02-17 14:26:47,434 - Epoch: [281][   79/   79]    Loss 4.319530    Top1 3.480000    Top5 14.990000    
2024-02-17 14:26:47,554 - ==> Top1: 3.480    Top5: 14.990    Loss: 4.320

2024-02-17 14:26:47,575 - ==> Best [Top1: 3.830   Top5: 14.280   Sparsity:0.00   Params: 1341960 on epoch: 278]
2024-02-17 14:26:47,576 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:26:47,641 - 

2024-02-17 14:26:47,641 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:27:01,081 - Epoch: [282][  100/  391]    Overall Loss 4.329633    Objective Loss 4.329633                                        LR 0.001298    Time 0.134318    
2024-02-17 14:27:13,339 - Epoch: [282][  200/  391]    Overall Loss 4.322338    Objective Loss 4.322338                                        LR 0.001298    Time 0.128428    
2024-02-17 14:27:23,972 - Epoch: [282][  300/  391]    Overall Loss 4.322798    Objective Loss 4.322798                                        LR 0.001298    Time 0.121051    
2024-02-17 14:27:33,490 - Epoch: [282][  391/  391]    Overall Loss 4.325456    Objective Loss 4.325456    Top1 5.288462    Top5 21.153846    LR 0.001298    Time 0.117214    
2024-02-17 14:27:33,659 - --- validate (epoch=282)-----------
2024-02-17 14:27:33,660 - 10000 samples (128 per mini-batch)
2024-02-17 14:27:39,282 - Epoch: [282][   79/   79]    Loss 4.312025    Top1 3.590000    Top5 14.750000    
2024-02-17 14:27:39,382 - ==> Top1: 3.590    Top5: 14.750    Loss: 4.312

2024-02-17 14:27:39,401 - ==> Best [Top1: 3.830   Top5: 14.280   Sparsity:0.00   Params: 1341960 on epoch: 278]
2024-02-17 14:27:39,402 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:27:39,449 - 

2024-02-17 14:27:39,449 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:27:50,197 - Epoch: [283][  100/  391]    Overall Loss 4.308714    Objective Loss 4.308714                                        LR 0.001298    Time 0.107419    
2024-02-17 14:28:03,327 - Epoch: [283][  200/  391]    Overall Loss 4.310786    Objective Loss 4.310786                                        LR 0.001298    Time 0.119336    
2024-02-17 14:28:16,636 - Epoch: [283][  300/  391]    Overall Loss 4.316046    Objective Loss 4.316046                                        LR 0.001298    Time 0.123903    
2024-02-17 14:28:28,490 - Epoch: [283][  391/  391]    Overall Loss 4.312356    Objective Loss 4.312356    Top1 5.288462    Top5 14.903846    LR 0.001298    Time 0.125373    
2024-02-17 14:28:28,673 - --- validate (epoch=283)-----------
2024-02-17 14:28:28,673 - 10000 samples (128 per mini-batch)
2024-02-17 14:28:34,338 - Epoch: [283][   79/   79]    Loss 4.307550    Top1 3.760000    Top5 15.750000    
2024-02-17 14:28:34,455 - ==> Top1: 3.760    Top5: 15.750    Loss: 4.308

2024-02-17 14:28:34,472 - ==> Best [Top1: 3.830   Top5: 14.280   Sparsity:0.00   Params: 1341960 on epoch: 278]
2024-02-17 14:28:34,472 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:28:34,525 - 

2024-02-17 14:28:34,525 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:28:46,848 - Epoch: [284][  100/  391]    Overall Loss 4.313981    Objective Loss 4.313981                                        LR 0.001298    Time 0.123169    
2024-02-17 14:28:57,115 - Epoch: [284][  200/  391]    Overall Loss 4.316544    Objective Loss 4.316544                                        LR 0.001298    Time 0.112904    
2024-02-17 14:29:10,142 - Epoch: [284][  300/  391]    Overall Loss 4.313367    Objective Loss 4.313367                                        LR 0.001298    Time 0.118676    
2024-02-17 14:29:22,510 - Epoch: [284][  391/  391]    Overall Loss 4.314963    Objective Loss 4.314963    Top1 1.923077    Top5 12.980769    LR 0.001298    Time 0.122678    
2024-02-17 14:29:22,697 - --- validate (epoch=284)-----------
2024-02-17 14:29:22,699 - 10000 samples (128 per mini-batch)
2024-02-17 14:29:28,850 - Epoch: [284][   79/   79]    Loss 4.298157    Top1 3.550000    Top5 15.840000    
2024-02-17 14:29:29,003 - ==> Top1: 3.550    Top5: 15.840    Loss: 4.298

2024-02-17 14:29:29,017 - ==> Best [Top1: 3.830   Top5: 14.280   Sparsity:0.00   Params: 1341960 on epoch: 278]
2024-02-17 14:29:29,017 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:29:29,082 - 

2024-02-17 14:29:29,083 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:29:42,521 - Epoch: [285][  100/  391]    Overall Loss 4.298362    Objective Loss 4.298362                                        LR 0.001298    Time 0.134313    
2024-02-17 14:29:55,567 - Epoch: [285][  200/  391]    Overall Loss 4.302286    Objective Loss 4.302286                                        LR 0.001298    Time 0.132362    
2024-02-17 14:30:08,783 - Epoch: [285][  300/  391]    Overall Loss 4.302977    Objective Loss 4.302977                                        LR 0.001298    Time 0.132277    
2024-02-17 14:30:20,754 - Epoch: [285][  391/  391]    Overall Loss 4.302362    Objective Loss 4.302362    Top1 3.846154    Top5 13.942308    LR 0.001298    Time 0.132097    
2024-02-17 14:30:20,936 - --- validate (epoch=285)-----------
2024-02-17 14:30:20,937 - 10000 samples (128 per mini-batch)
2024-02-17 14:30:27,338 - Epoch: [285][   79/   79]    Loss 4.308822    Top1 3.720000    Top5 15.560000    
2024-02-17 14:30:27,457 - ==> Top1: 3.720    Top5: 15.560    Loss: 4.309

2024-02-17 14:30:27,476 - ==> Best [Top1: 3.830   Top5: 14.280   Sparsity:0.00   Params: 1341960 on epoch: 278]
2024-02-17 14:30:27,476 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:30:27,543 - 

2024-02-17 14:30:27,543 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:30:41,796 - Epoch: [286][  100/  391]    Overall Loss 4.308367    Objective Loss 4.308367                                        LR 0.001298    Time 0.142451    
2024-02-17 14:30:55,040 - Epoch: [286][  200/  391]    Overall Loss 4.309236    Objective Loss 4.309236                                        LR 0.001298    Time 0.137419    
2024-02-17 14:31:07,610 - Epoch: [286][  300/  391]    Overall Loss 4.302757    Objective Loss 4.302757                                        LR 0.001298    Time 0.133497    
2024-02-17 14:31:19,646 - Epoch: [286][  391/  391]    Overall Loss 4.299799    Objective Loss 4.299799    Top1 2.884615    Top5 12.019231    LR 0.001298    Time 0.133197    
2024-02-17 14:31:19,776 - --- validate (epoch=286)-----------
2024-02-17 14:31:19,777 - 10000 samples (128 per mini-batch)
2024-02-17 14:31:25,972 - Epoch: [286][   79/   79]    Loss 4.305651    Top1 3.980000    Top5 15.910000    
2024-02-17 14:31:26,086 - ==> Top1: 3.980    Top5: 15.910    Loss: 4.306

2024-02-17 14:31:26,100 - ==> Best [Top1: 3.980   Top5: 15.910   Sparsity:0.00   Params: 1341960 on epoch: 286]
2024-02-17 14:31:26,100 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:31:26,179 - 

2024-02-17 14:31:26,179 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:31:40,271 - Epoch: [287][  100/  391]    Overall Loss 4.291595    Objective Loss 4.291595                                        LR 0.001298    Time 0.140839    
2024-02-17 14:31:53,634 - Epoch: [287][  200/  391]    Overall Loss 4.295506    Objective Loss 4.295506                                        LR 0.001298    Time 0.137211    
2024-02-17 14:32:07,003 - Epoch: [287][  300/  391]    Overall Loss 4.291699    Objective Loss 4.291699                                        LR 0.001298    Time 0.136020    
2024-02-17 14:32:18,821 - Epoch: [287][  391/  391]    Overall Loss 4.293838    Objective Loss 4.293838    Top1 3.846154    Top5 16.826923    LR 0.001298    Time 0.134576    
2024-02-17 14:32:18,920 - --- validate (epoch=287)-----------
2024-02-17 14:32:18,921 - 10000 samples (128 per mini-batch)
2024-02-17 14:32:24,538 - Epoch: [287][   79/   79]    Loss 4.299311    Top1 4.110000    Top5 16.680000    
2024-02-17 14:32:24,661 - ==> Top1: 4.110    Top5: 16.680    Loss: 4.299

2024-02-17 14:32:24,682 - ==> Best [Top1: 4.110   Top5: 16.680   Sparsity:0.00   Params: 1341960 on epoch: 287]
2024-02-17 14:32:24,682 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:32:24,764 - 

2024-02-17 14:32:24,765 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:32:38,306 - Epoch: [288][  100/  391]    Overall Loss 4.284823    Objective Loss 4.284823                                        LR 0.001298    Time 0.135333    
2024-02-17 14:32:51,036 - Epoch: [288][  200/  391]    Overall Loss 4.305186    Objective Loss 4.305186                                        LR 0.001298    Time 0.131291    
2024-02-17 14:33:03,935 - Epoch: [288][  300/  391]    Overall Loss 4.299295    Objective Loss 4.299295                                        LR 0.001298    Time 0.130510    
2024-02-17 14:33:15,933 - Epoch: [288][  391/  391]    Overall Loss 4.299102    Objective Loss 4.299102    Top1 2.884615    Top5 15.865385    LR 0.001298    Time 0.130808    
2024-02-17 14:33:16,091 - --- validate (epoch=288)-----------
2024-02-17 14:33:16,092 - 10000 samples (128 per mini-batch)
2024-02-17 14:33:22,242 - Epoch: [288][   79/   79]    Loss 4.305196    Top1 3.720000    Top5 16.330000    
2024-02-17 14:33:22,401 - ==> Top1: 3.720    Top5: 16.330    Loss: 4.305

2024-02-17 14:33:22,646 - ==> Best [Top1: 4.110   Top5: 16.680   Sparsity:0.00   Params: 1341960 on epoch: 287]
2024-02-17 14:33:22,646 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:33:22,707 - 

2024-02-17 14:33:22,708 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:33:36,350 - Epoch: [289][  100/  391]    Overall Loss 4.282993    Objective Loss 4.282993                                        LR 0.001298    Time 0.136302    
2024-02-17 14:33:49,040 - Epoch: [289][  200/  391]    Overall Loss 4.282835    Objective Loss 4.282835                                        LR 0.001298    Time 0.131582    
2024-02-17 14:34:02,451 - Epoch: [289][  300/  391]    Overall Loss 4.284908    Objective Loss 4.284908                                        LR 0.001298    Time 0.132408    
2024-02-17 14:34:14,485 - Epoch: [289][  391/  391]    Overall Loss 4.289125    Objective Loss 4.289125    Top1 5.769231    Top5 20.192308    LR 0.001298    Time 0.132357    
2024-02-17 14:34:14,665 - --- validate (epoch=289)-----------
2024-02-17 14:34:14,666 - 10000 samples (128 per mini-batch)
2024-02-17 14:34:20,545 - Epoch: [289][   79/   79]    Loss 4.298512    Top1 4.210000    Top5 16.470000    
2024-02-17 14:34:20,641 - ==> Top1: 4.210    Top5: 16.470    Loss: 4.299

2024-02-17 14:34:20,660 - ==> Best [Top1: 4.210   Top5: 16.470   Sparsity:0.00   Params: 1341960 on epoch: 289]
2024-02-17 14:34:20,661 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:34:20,740 - 

2024-02-17 14:34:20,740 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:34:34,275 - Epoch: [290][  100/  391]    Overall Loss 4.277660    Objective Loss 4.277660                                        LR 0.001298    Time 0.135277    
2024-02-17 14:34:47,534 - Epoch: [290][  200/  391]    Overall Loss 4.280394    Objective Loss 4.280394                                        LR 0.001298    Time 0.133906    
2024-02-17 14:35:00,774 - Epoch: [290][  300/  391]    Overall Loss 4.280912    Objective Loss 4.280912                                        LR 0.001298    Time 0.133388    
2024-02-17 14:35:11,818 - Epoch: [290][  391/  391]    Overall Loss 4.279832    Objective Loss 4.279832    Top1 5.769231    Top5 18.750000    LR 0.001298    Time 0.130579    
2024-02-17 14:35:11,907 - --- validate (epoch=290)-----------
2024-02-17 14:35:11,907 - 10000 samples (128 per mini-batch)
2024-02-17 14:35:16,926 - Epoch: [290][   79/   79]    Loss 4.272063    Top1 4.110000    Top5 16.300000    
2024-02-17 14:35:17,059 - ==> Top1: 4.110    Top5: 16.300    Loss: 4.272

2024-02-17 14:35:17,077 - ==> Best [Top1: 4.210   Top5: 16.470   Sparsity:0.00   Params: 1341960 on epoch: 289]
2024-02-17 14:35:17,078 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:35:17,142 - 

2024-02-17 14:35:17,143 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:35:31,109 - Epoch: [291][  100/  391]    Overall Loss 4.274208    Objective Loss 4.274208                                        LR 0.001298    Time 0.139591    
2024-02-17 14:35:44,360 - Epoch: [291][  200/  391]    Overall Loss 4.277820    Objective Loss 4.277820                                        LR 0.001298    Time 0.136027    
2024-02-17 14:35:55,173 - Epoch: [291][  300/  391]    Overall Loss 4.275518    Objective Loss 4.275518                                        LR 0.001298    Time 0.126713    
2024-02-17 14:36:06,965 - Epoch: [291][  391/  391]    Overall Loss 4.278328    Objective Loss 4.278328    Top1 3.846154    Top5 15.865385    LR 0.001298    Time 0.127370    
2024-02-17 14:36:07,119 - --- validate (epoch=291)-----------
2024-02-17 14:36:07,120 - 10000 samples (128 per mini-batch)
2024-02-17 14:36:12,278 - Epoch: [291][   79/   79]    Loss 4.278125    Top1 4.360000    Top5 16.970000    
2024-02-17 14:36:12,432 - ==> Top1: 4.360    Top5: 16.970    Loss: 4.278

2024-02-17 14:36:12,448 - ==> Best [Top1: 4.360   Top5: 16.970   Sparsity:0.00   Params: 1341960 on epoch: 291]
2024-02-17 14:36:12,449 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:36:12,513 - 

2024-02-17 14:36:12,514 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:36:24,460 - Epoch: [292][  100/  391]    Overall Loss 4.288812    Objective Loss 4.288812                                        LR 0.001298    Time 0.119404    
2024-02-17 14:36:37,770 - Epoch: [292][  200/  391]    Overall Loss 4.282569    Objective Loss 4.282569                                        LR 0.001298    Time 0.126227    
2024-02-17 14:36:51,046 - Epoch: [292][  300/  391]    Overall Loss 4.277965    Objective Loss 4.277965                                        LR 0.001298    Time 0.128386    
2024-02-17 14:37:03,077 - Epoch: [292][  391/  391]    Overall Loss 4.273590    Objective Loss 4.273590    Top1 5.769231    Top5 16.826923    LR 0.001298    Time 0.129265    
2024-02-17 14:37:03,216 - --- validate (epoch=292)-----------
2024-02-17 14:37:03,216 - 10000 samples (128 per mini-batch)
2024-02-17 14:37:08,475 - Epoch: [292][   79/   79]    Loss 4.252470    Top1 4.740000    Top5 16.980000    
2024-02-17 14:37:08,580 - ==> Top1: 4.740    Top5: 16.980    Loss: 4.252

2024-02-17 14:37:08,597 - ==> Best [Top1: 4.740   Top5: 16.980   Sparsity:0.00   Params: 1341960 on epoch: 292]
2024-02-17 14:37:08,597 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:37:08,668 - 

2024-02-17 14:37:08,669 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:37:22,177 - Epoch: [293][  100/  391]    Overall Loss 4.258619    Objective Loss 4.258619                                        LR 0.001298    Time 0.135007    
2024-02-17 14:37:35,336 - Epoch: [293][  200/  391]    Overall Loss 4.266673    Objective Loss 4.266673                                        LR 0.001298    Time 0.133277    
2024-02-17 14:37:48,581 - Epoch: [293][  300/  391]    Overall Loss 4.268446    Objective Loss 4.268446                                        LR 0.001298    Time 0.132986    
2024-02-17 14:38:00,364 - Epoch: [293][  391/  391]    Overall Loss 4.268466    Objective Loss 4.268466    Top1 5.288462    Top5 18.269231    LR 0.001298    Time 0.132159    
2024-02-17 14:38:00,536 - --- validate (epoch=293)-----------
2024-02-17 14:38:00,537 - 10000 samples (128 per mini-batch)
2024-02-17 14:38:06,248 - Epoch: [293][   79/   79]    Loss 4.251655    Top1 4.190000    Top5 17.560000    
2024-02-17 14:38:06,366 - ==> Top1: 4.190    Top5: 17.560    Loss: 4.252

2024-02-17 14:38:06,383 - ==> Best [Top1: 4.740   Top5: 16.980   Sparsity:0.00   Params: 1341960 on epoch: 292]
2024-02-17 14:38:06,383 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:38:06,447 - 

2024-02-17 14:38:06,447 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:38:20,304 - Epoch: [294][  100/  391]    Overall Loss 4.277996    Objective Loss 4.277996                                        LR 0.001298    Time 0.138499    
2024-02-17 14:38:33,351 - Epoch: [294][  200/  391]    Overall Loss 4.274657    Objective Loss 4.274657                                        LR 0.001298    Time 0.134457    
2024-02-17 14:38:47,029 - Epoch: [294][  300/  391]    Overall Loss 4.273413    Objective Loss 4.273413                                        LR 0.001298    Time 0.135215    
2024-02-17 14:38:59,341 - Epoch: [294][  391/  391]    Overall Loss 4.273303    Objective Loss 4.273303    Top1 4.326923    Top5 17.788462    LR 0.001298    Time 0.135222    
2024-02-17 14:38:59,439 - --- validate (epoch=294)-----------
2024-02-17 14:38:59,440 - 10000 samples (128 per mini-batch)
2024-02-17 14:39:05,244 - Epoch: [294][   79/   79]    Loss 4.339880    Top1 3.860000    Top5 14.640000    
2024-02-17 14:39:05,345 - ==> Top1: 3.860    Top5: 14.640    Loss: 4.340

2024-02-17 14:39:05,362 - ==> Best [Top1: 4.740   Top5: 16.980   Sparsity:0.00   Params: 1341960 on epoch: 292]
2024-02-17 14:39:05,362 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:39:05,432 - 

2024-02-17 14:39:05,432 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:39:18,927 - Epoch: [295][  100/  391]    Overall Loss 4.272182    Objective Loss 4.272182                                        LR 0.001298    Time 0.134836    
2024-02-17 14:39:32,085 - Epoch: [295][  200/  391]    Overall Loss 4.265836    Objective Loss 4.265836                                        LR 0.001298    Time 0.133182    
2024-02-17 14:39:45,570 - Epoch: [295][  300/  391]    Overall Loss 4.268629    Objective Loss 4.268629                                        LR 0.001298    Time 0.133721    
2024-02-17 14:39:57,761 - Epoch: [295][  391/  391]    Overall Loss 4.271457    Objective Loss 4.271457    Top1 3.365385    Top5 13.942308    LR 0.001298    Time 0.133765    
2024-02-17 14:39:57,875 - --- validate (epoch=295)-----------
2024-02-17 14:39:57,876 - 10000 samples (128 per mini-batch)
2024-02-17 14:40:03,419 - Epoch: [295][   79/   79]    Loss 4.291325    Top1 4.440000    Top5 16.620000    
2024-02-17 14:40:03,541 - ==> Top1: 4.440    Top5: 16.620    Loss: 4.291

2024-02-17 14:40:03,560 - ==> Best [Top1: 4.740   Top5: 16.980   Sparsity:0.00   Params: 1341960 on epoch: 292]
2024-02-17 14:40:03,560 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:40:03,623 - 

2024-02-17 14:40:03,623 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:40:17,793 - Epoch: [296][  100/  391]    Overall Loss 4.281449    Objective Loss 4.281449                                        LR 0.001298    Time 0.141616    
2024-02-17 14:40:30,695 - Epoch: [296][  200/  391]    Overall Loss 4.278062    Objective Loss 4.278062                                        LR 0.001298    Time 0.135298    
2024-02-17 14:40:43,988 - Epoch: [296][  300/  391]    Overall Loss 4.282172    Objective Loss 4.282172                                        LR 0.001298    Time 0.134490    
2024-02-17 14:40:55,828 - Epoch: [296][  391/  391]    Overall Loss 4.284658    Objective Loss 4.284658    Top1 6.250000    Top5 16.826923    LR 0.001298    Time 0.133459    
2024-02-17 14:40:55,931 - --- validate (epoch=296)-----------
2024-02-17 14:40:55,932 - 10000 samples (128 per mini-batch)
2024-02-17 14:41:01,955 - Epoch: [296][   79/   79]    Loss 4.253554    Top1 4.710000    Top5 17.240000    
2024-02-17 14:41:02,110 - ==> Top1: 4.710    Top5: 17.240    Loss: 4.254

2024-02-17 14:41:02,128 - ==> Best [Top1: 4.740   Top5: 16.980   Sparsity:0.00   Params: 1341960 on epoch: 292]
2024-02-17 14:41:02,129 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:41:02,194 - 

2024-02-17 14:41:02,194 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:41:15,566 - Epoch: [297][  100/  391]    Overall Loss 4.277765    Objective Loss 4.277765                                        LR 0.001298    Time 0.133651    
2024-02-17 14:41:28,335 - Epoch: [297][  200/  391]    Overall Loss 4.271418    Objective Loss 4.271418                                        LR 0.001298    Time 0.130642    
2024-02-17 14:41:41,503 - Epoch: [297][  300/  391]    Overall Loss 4.273183    Objective Loss 4.273183                                        LR 0.001298    Time 0.130972    
2024-02-17 14:41:53,537 - Epoch: [297][  391/  391]    Overall Loss 4.274655    Objective Loss 4.274655    Top1 3.365385    Top5 15.384615    LR 0.001298    Time 0.131256    
2024-02-17 14:41:53,716 - --- validate (epoch=297)-----------
2024-02-17 14:41:53,717 - 10000 samples (128 per mini-batch)
2024-02-17 14:41:59,376 - Epoch: [297][   79/   79]    Loss 4.284575    Top1 4.050000    Top5 16.490000    
2024-02-17 14:41:59,577 - ==> Top1: 4.050    Top5: 16.490    Loss: 4.285

2024-02-17 14:41:59,596 - ==> Best [Top1: 4.740   Top5: 16.980   Sparsity:0.00   Params: 1341960 on epoch: 292]
2024-02-17 14:41:59,596 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:41:59,667 - 

2024-02-17 14:41:59,667 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:42:13,388 - Epoch: [298][  100/  391]    Overall Loss 4.260505    Objective Loss 4.260505                                        LR 0.001298    Time 0.137128    
2024-02-17 14:42:26,402 - Epoch: [298][  200/  391]    Overall Loss 4.259015    Objective Loss 4.259015                                        LR 0.001298    Time 0.133604    
2024-02-17 14:42:39,595 - Epoch: [298][  300/  391]    Overall Loss 4.264496    Objective Loss 4.264496                                        LR 0.001298    Time 0.133029    
2024-02-17 14:42:51,571 - Epoch: [298][  391/  391]    Overall Loss 4.266327    Objective Loss 4.266327    Top1 3.846154    Top5 17.307692    LR 0.001298    Time 0.132686    
2024-02-17 14:42:51,702 - --- validate (epoch=298)-----------
2024-02-17 14:42:51,703 - 10000 samples (128 per mini-batch)
2024-02-17 14:42:57,299 - Epoch: [298][   79/   79]    Loss 4.239642    Top1 4.690000    Top5 17.770000    
2024-02-17 14:42:57,396 - ==> Top1: 4.690    Top5: 17.770    Loss: 4.240

2024-02-17 14:42:57,414 - ==> Best [Top1: 4.740   Top5: 16.980   Sparsity:0.00   Params: 1341960 on epoch: 292]
2024-02-17 14:42:57,414 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:42:57,465 - 

2024-02-17 14:42:57,465 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:43:11,037 - Epoch: [299][  100/  391]    Overall Loss 4.264206    Objective Loss 4.264206                                        LR 0.001298    Time 0.135657    
2024-02-17 14:43:24,279 - Epoch: [299][  200/  391]    Overall Loss 4.269060    Objective Loss 4.269060                                        LR 0.001298    Time 0.134008    
2024-02-17 14:43:37,479 - Epoch: [299][  300/  391]    Overall Loss 4.268389    Objective Loss 4.268389                                        LR 0.001298    Time 0.133321    
2024-02-17 14:43:49,463 - Epoch: [299][  391/  391]    Overall Loss 4.266753    Objective Loss 4.266753    Top1 4.326923    Top5 15.384615    LR 0.001298    Time 0.132930    
2024-02-17 14:43:49,644 - --- validate (epoch=299)-----------
2024-02-17 14:43:49,645 - 10000 samples (128 per mini-batch)
2024-02-17 14:43:54,399 - Epoch: [299][   79/   79]    Loss 4.237305    Top1 4.780000    Top5 17.950000    
2024-02-17 14:43:54,502 - ==> Top1: 4.780    Top5: 17.950    Loss: 4.237

2024-02-17 14:43:54,519 - ==> Best [Top1: 4.780   Top5: 17.950   Sparsity:0.00   Params: 1341960 on epoch: 299]
2024-02-17 14:43:54,519 - Saving checkpoint to: logs/2024.02.17-105231/qat_checkpoint.pth.tar
2024-02-17 14:43:54,580 - --- test ---------------------
2024-02-17 14:43:54,580 - 10000 samples (128 per mini-batch)
2024-02-17 14:43:58,290 - Test: [   79/   79]    Loss 4.242335    Top1 4.780000    Top5 17.950000    
2024-02-17 14:43:58,377 - ==> Top1: 4.780    Top5: 17.950    Loss: 4.242

2024-02-17 14:43:58,390 - 
2024-02-17 14:43:58,391 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.02.17-105231/2024.02.17-105231.log
