2024-02-17 10:49:53,639 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.02.17-104953/2024.02.17-104953.log
2024-02-17 10:49:56,881 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2024-02-17 10:49:56,881 - Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}
2024-02-17 10:49:58,393 - Dataset sizes:
	training=50000
	validation=10000
	test=10000
2024-02-17 10:49:58,393 - Reading compression schedule from: policies/schedule-cifar100-mobilenetv2.yaml
2024-02-17 10:49:58,411 - 

2024-02-17 10:49:58,411 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:50:08,723 - Epoch: [0][  100/  391]    Overall Loss 4.416315    Objective Loss 4.416315                                        LR 0.100000    Time 0.103055    
2024-02-17 10:50:17,958 - Epoch: [0][  200/  391]    Overall Loss 4.286143    Objective Loss 4.286143                                        LR 0.100000    Time 0.097676    
2024-02-17 10:50:25,446 - Epoch: [0][  300/  391]    Overall Loss 4.195390    Objective Loss 4.195390                                        LR 0.100000    Time 0.090066    
2024-02-17 10:50:33,893 - Epoch: [0][  391/  391]    Overall Loss 4.128643    Objective Loss 4.128643    Top1 8.173077    Top5 30.769231    LR 0.100000    Time 0.090695    
2024-02-17 10:50:33,999 - --- validate (epoch=0)-----------
2024-02-17 10:50:34,001 - 10000 samples (128 per mini-batch)
2024-02-17 10:50:36,612 - Epoch: [0][   79/   79]    Loss 4.294045    Top1 5.940000    Top5 21.100000    
2024-02-17 10:50:36,771 - ==> Top1: 5.940    Top5: 21.100    Loss: 4.294

2024-02-17 10:50:37,013 - ==> Best [Top1: 5.940   Top5: 21.100   Sparsity:0.00   Params: 1341960 on epoch: 0]
2024-02-17 10:50:37,013 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:50:37,087 - 

2024-02-17 10:50:37,088 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:50:46,783 - Epoch: [1][  100/  391]    Overall Loss 3.804707    Objective Loss 3.804707                                        LR 0.100000    Time 0.096881    
2024-02-17 10:50:55,993 - Epoch: [1][  200/  391]    Overall Loss 3.776933    Objective Loss 3.776933                                        LR 0.100000    Time 0.094473    
2024-02-17 10:51:05,186 - Epoch: [1][  300/  391]    Overall Loss 3.746371    Objective Loss 3.746371                                        LR 0.100000    Time 0.093611    
2024-02-17 10:51:13,546 - Epoch: [1][  391/  391]    Overall Loss 3.722566    Objective Loss 3.722566    Top1 11.538462    Top5 36.538462    LR 0.100000    Time 0.093196    
2024-02-17 10:51:13,743 - --- validate (epoch=1)-----------
2024-02-17 10:51:13,744 - 10000 samples (128 per mini-batch)
2024-02-17 10:51:16,324 - Epoch: [1][   79/   79]    Loss 3.612136    Top1 11.770000    Top5 36.110000    
2024-02-17 10:51:16,516 - ==> Top1: 11.770    Top5: 36.110    Loss: 3.612

2024-02-17 10:51:16,533 - ==> Best [Top1: 11.770   Top5: 36.110   Sparsity:0.00   Params: 1341960 on epoch: 1]
2024-02-17 10:51:16,533 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:51:16,621 - 

2024-02-17 10:51:16,621 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:51:24,055 - Epoch: [2][  100/  391]    Overall Loss 3.567376    Objective Loss 3.567376                                        LR 0.100000    Time 0.074279    
2024-02-17 10:51:30,755 - Epoch: [2][  200/  391]    Overall Loss 3.529924    Objective Loss 3.529924                                        LR 0.100000    Time 0.070623    
2024-02-17 10:51:38,368 - Epoch: [2][  300/  391]    Overall Loss 3.513421    Objective Loss 3.513421                                        LR 0.100000    Time 0.072445    
2024-02-17 10:51:46,681 - Epoch: [2][  391/  391]    Overall Loss 3.484502    Objective Loss 3.484502    Top1 13.461538    Top5 45.192308    LR 0.100000    Time 0.076837    
2024-02-17 10:51:46,914 - --- validate (epoch=2)-----------
2024-02-17 10:51:46,915 - 10000 samples (128 per mini-batch)
2024-02-17 10:51:49,472 - Epoch: [2][   79/   79]    Loss 3.470122    Top1 14.090000    Top5 42.420000    
2024-02-17 10:51:49,657 - ==> Top1: 14.090    Top5: 42.420    Loss: 3.470

2024-02-17 10:51:49,667 - ==> Best [Top1: 14.090   Top5: 42.420   Sparsity:0.00   Params: 1341960 on epoch: 2]
2024-02-17 10:51:49,667 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:51:49,754 - 

2024-02-17 10:51:49,755 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:51:57,051 - Epoch: [3][  100/  391]    Overall Loss 3.324654    Objective Loss 3.324654                                        LR 0.100000    Time 0.072899    
2024-02-17 10:52:03,814 - Epoch: [3][  200/  391]    Overall Loss 3.292986    Objective Loss 3.292986                                        LR 0.100000    Time 0.070249    
2024-02-17 10:52:10,605 - Epoch: [3][  300/  391]    Overall Loss 3.274662    Objective Loss 3.274662                                        LR 0.100000    Time 0.069455    
2024-02-17 10:52:16,793 - Epoch: [3][  391/  391]    Overall Loss 3.251404    Objective Loss 3.251404    Top1 17.788462    Top5 50.000000    LR 0.100000    Time 0.069109    
2024-02-17 10:52:16,934 - --- validate (epoch=3)-----------
2024-02-17 10:52:16,935 - 10000 samples (128 per mini-batch)
2024-02-17 10:52:19,504 - Epoch: [3][   79/   79]    Loss 3.512974    Top1 16.070000    Top5 42.120000    
2024-02-17 10:52:19,675 - ==> Top1: 16.070    Top5: 42.120    Loss: 3.513

2024-02-17 10:52:19,692 - ==> Best [Top1: 16.070   Top5: 42.120   Sparsity:0.00   Params: 1341960 on epoch: 3]
2024-02-17 10:52:19,692 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:52:19,775 - 

2024-02-17 10:52:19,775 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:52:29,309 - Epoch: [4][  100/  391]    Overall Loss 3.115228    Objective Loss 3.115228                                        LR 0.100000    Time 0.095268    
2024-02-17 10:52:37,777 - Epoch: [4][  200/  391]    Overall Loss 3.098125    Objective Loss 3.098125                                        LR 0.100000    Time 0.089947    
2024-02-17 10:52:46,732 - Epoch: [4][  300/  391]    Overall Loss 3.082714    Objective Loss 3.082714                                        LR 0.100000    Time 0.089798    
2024-02-17 10:52:55,256 - Epoch: [4][  391/  391]    Overall Loss 3.057796    Objective Loss 3.057796    Top1 21.153846    Top5 56.730769    LR 0.100000    Time 0.090689    
2024-02-17 10:52:55,421 - --- validate (epoch=4)-----------
2024-02-17 10:52:55,422 - 10000 samples (128 per mini-batch)
2024-02-17 10:52:58,259 - Epoch: [4][   79/   79]    Loss 3.680597    Top1 15.720000    Top5 42.760000    
2024-02-17 10:52:58,420 - ==> Top1: 15.720    Top5: 42.760    Loss: 3.681

2024-02-17 10:52:58,439 - ==> Best [Top1: 16.070   Top5: 42.120   Sparsity:0.00   Params: 1341960 on epoch: 3]
2024-02-17 10:52:58,439 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:52:58,681 - 

2024-02-17 10:52:58,681 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:53:08,705 - Epoch: [5][  100/  391]    Overall Loss 2.921600    Objective Loss 2.921600                                        LR 0.100000    Time 0.100173    
2024-02-17 10:53:17,708 - Epoch: [5][  200/  391]    Overall Loss 2.925484    Objective Loss 2.925484                                        LR 0.100000    Time 0.095082    
2024-02-17 10:53:27,247 - Epoch: [5][  300/  391]    Overall Loss 2.903268    Objective Loss 2.903268                                        LR 0.100000    Time 0.095170    
2024-02-17 10:53:35,737 - Epoch: [5][  391/  391]    Overall Loss 2.888552    Objective Loss 2.888552    Top1 24.038462    Top5 55.769231    LR 0.100000    Time 0.094724    
2024-02-17 10:53:35,943 - --- validate (epoch=5)-----------
2024-02-17 10:53:35,944 - 10000 samples (128 per mini-batch)
2024-02-17 10:53:38,617 - Epoch: [5][   79/   79]    Loss 3.252491    Top1 20.500000    Top5 51.040000    
2024-02-17 10:53:38,804 - ==> Top1: 20.500    Top5: 51.040    Loss: 3.252

2024-02-17 10:53:38,823 - ==> Best [Top1: 20.500   Top5: 51.040   Sparsity:0.00   Params: 1341960 on epoch: 5]
2024-02-17 10:53:38,823 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:53:38,914 - 

2024-02-17 10:53:38,914 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:53:48,937 - Epoch: [6][  100/  391]    Overall Loss 2.782903    Objective Loss 2.782903                                        LR 0.100000    Time 0.100155    
2024-02-17 10:53:58,105 - Epoch: [6][  200/  391]    Overall Loss 2.783424    Objective Loss 2.783424                                        LR 0.100000    Time 0.095898    
2024-02-17 10:54:05,821 - Epoch: [6][  300/  391]    Overall Loss 2.762626    Objective Loss 2.762626                                        LR 0.100000    Time 0.089638    
2024-02-17 10:54:14,276 - Epoch: [6][  391/  391]    Overall Loss 2.749103    Objective Loss 2.749103    Top1 28.365385    Top5 61.057692    LR 0.100000    Time 0.090391    
2024-02-17 10:54:14,479 - --- validate (epoch=6)-----------
2024-02-17 10:54:14,480 - 10000 samples (128 per mini-batch)
2024-02-17 10:54:17,074 - Epoch: [6][   79/   79]    Loss 2.988540    Top1 23.640000    Top5 56.320000    
2024-02-17 10:54:17,197 - ==> Top1: 23.640    Top5: 56.320    Loss: 2.989

2024-02-17 10:54:17,215 - ==> Best [Top1: 23.640   Top5: 56.320   Sparsity:0.00   Params: 1341960 on epoch: 6]
2024-02-17 10:54:17,215 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:54:17,304 - 

2024-02-17 10:54:17,305 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:54:27,233 - Epoch: [7][  100/  391]    Overall Loss 2.656388    Objective Loss 2.656388                                        LR 0.100000    Time 0.099212    
2024-02-17 10:54:36,531 - Epoch: [7][  200/  391]    Overall Loss 2.651310    Objective Loss 2.651310                                        LR 0.100000    Time 0.096078    
2024-02-17 10:54:45,557 - Epoch: [7][  300/  391]    Overall Loss 2.638051    Objective Loss 2.638051                                        LR 0.100000    Time 0.094123    
2024-02-17 10:54:54,113 - Epoch: [7][  391/  391]    Overall Loss 2.628637    Objective Loss 2.628637    Top1 29.807692    Top5 65.865385    LR 0.100000    Time 0.094090    
2024-02-17 10:54:54,296 - --- validate (epoch=7)-----------
2024-02-17 10:54:54,297 - 10000 samples (128 per mini-batch)
2024-02-17 10:54:56,952 - Epoch: [7][   79/   79]    Loss 2.928358    Top1 26.420000    Top5 58.360000    
2024-02-17 10:54:57,097 - ==> Top1: 26.420    Top5: 58.360    Loss: 2.928

2024-02-17 10:54:57,116 - ==> Best [Top1: 26.420   Top5: 58.360   Sparsity:0.00   Params: 1341960 on epoch: 7]
2024-02-17 10:54:57,116 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:54:57,205 - 

2024-02-17 10:54:57,205 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:55:07,247 - Epoch: [8][  100/  391]    Overall Loss 2.557369    Objective Loss 2.557369                                        LR 0.100000    Time 0.100345    
2024-02-17 10:55:16,484 - Epoch: [8][  200/  391]    Overall Loss 2.552592    Objective Loss 2.552592                                        LR 0.100000    Time 0.096338    
2024-02-17 10:55:25,914 - Epoch: [8][  300/  391]    Overall Loss 2.547581    Objective Loss 2.547581                                        LR 0.100000    Time 0.095640    
2024-02-17 10:55:34,358 - Epoch: [8][  391/  391]    Overall Loss 2.535819    Objective Loss 2.535819    Top1 30.769231    Top5 67.788462    LR 0.100000    Time 0.094968    
2024-02-17 10:55:34,575 - --- validate (epoch=8)-----------
2024-02-17 10:55:34,576 - 10000 samples (128 per mini-batch)
2024-02-17 10:55:37,136 - Epoch: [8][   79/   79]    Loss 2.706972    Top1 29.560000    Top5 62.900000    
2024-02-17 10:55:37,266 - ==> Top1: 29.560    Top5: 62.900    Loss: 2.707

2024-02-17 10:55:37,284 - ==> Best [Top1: 29.560   Top5: 62.900   Sparsity:0.00   Params: 1341960 on epoch: 8]
2024-02-17 10:55:37,284 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:55:37,375 - 

2024-02-17 10:55:37,375 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:55:47,236 - Epoch: [9][  100/  391]    Overall Loss 2.443809    Objective Loss 2.443809                                        LR 0.100000    Time 0.098540    
2024-02-17 10:55:56,539 - Epoch: [9][  200/  391]    Overall Loss 2.461339    Objective Loss 2.461339                                        LR 0.100000    Time 0.095764    
2024-02-17 10:56:05,809 - Epoch: [9][  300/  391]    Overall Loss 2.454683    Objective Loss 2.454683                                        LR 0.100000    Time 0.094729    
2024-02-17 10:56:14,229 - Epoch: [9][  391/  391]    Overall Loss 2.455477    Objective Loss 2.455477    Top1 37.019231    Top5 63.942308    LR 0.100000    Time 0.094207    
2024-02-17 10:56:14,424 - --- validate (epoch=9)-----------
2024-02-17 10:56:14,425 - 10000 samples (128 per mini-batch)
2024-02-17 10:56:17,130 - Epoch: [9][   79/   79]    Loss 3.159537    Top1 24.220000    Top5 55.490000    
2024-02-17 10:56:17,413 - ==> Top1: 24.220    Top5: 55.490    Loss: 3.160

2024-02-17 10:56:17,433 - ==> Best [Top1: 29.560   Top5: 62.900   Sparsity:0.00   Params: 1341960 on epoch: 8]
2024-02-17 10:56:17,433 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:56:17,507 - 

2024-02-17 10:56:17,507 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:56:27,303 - Epoch: [10][  100/  391]    Overall Loss 2.413701    Objective Loss 2.413701                                        LR 0.100000    Time 0.097884    
2024-02-17 10:56:36,614 - Epoch: [10][  200/  391]    Overall Loss 2.394479    Objective Loss 2.394479                                        LR 0.100000    Time 0.095479    
2024-02-17 10:56:45,902 - Epoch: [10][  300/  391]    Overall Loss 2.385760    Objective Loss 2.385760                                        LR 0.100000    Time 0.094599    
2024-02-17 10:56:54,320 - Epoch: [10][  391/  391]    Overall Loss 2.385589    Objective Loss 2.385589    Top1 31.250000    Top5 69.711538    LR 0.100000    Time 0.094103    
2024-02-17 10:56:54,522 - --- validate (epoch=10)-----------
2024-02-17 10:56:54,523 - 10000 samples (128 per mini-batch)
2024-02-17 10:56:57,203 - Epoch: [10][   79/   79]    Loss 2.457716    Top1 33.660000    Top5 68.120000    
2024-02-17 10:56:57,379 - ==> Top1: 33.660    Top5: 68.120    Loss: 2.458

2024-02-17 10:56:57,398 - ==> Best [Top1: 33.660   Top5: 68.120   Sparsity:0.00   Params: 1341960 on epoch: 10]
2024-02-17 10:56:57,399 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:56:57,490 - 

2024-02-17 10:56:57,490 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:57:07,291 - Epoch: [11][  100/  391]    Overall Loss 2.344371    Objective Loss 2.344371                                        LR 0.100000    Time 0.097935    
2024-02-17 10:57:16,448 - Epoch: [11][  200/  391]    Overall Loss 2.329532    Objective Loss 2.329532                                        LR 0.100000    Time 0.094733    
2024-02-17 10:57:25,800 - Epoch: [11][  300/  391]    Overall Loss 2.320764    Objective Loss 2.320764                                        LR 0.100000    Time 0.094317    
2024-02-17 10:57:34,215 - Epoch: [11][  391/  391]    Overall Loss 2.318187    Objective Loss 2.318187    Top1 40.384615    Top5 67.307692    LR 0.100000    Time 0.093877    
2024-02-17 10:57:34,402 - --- validate (epoch=11)-----------
2024-02-17 10:57:34,402 - 10000 samples (128 per mini-batch)
2024-02-17 10:57:37,159 - Epoch: [11][   79/   79]    Loss 2.570210    Top1 32.130000    Top5 66.130000    
2024-02-17 10:57:37,326 - ==> Top1: 32.130    Top5: 66.130    Loss: 2.570

2024-02-17 10:57:37,344 - ==> Best [Top1: 33.660   Top5: 68.120   Sparsity:0.00   Params: 1341960 on epoch: 10]
2024-02-17 10:57:37,344 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:57:37,419 - 

2024-02-17 10:57:37,419 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:57:47,218 - Epoch: [12][  100/  391]    Overall Loss 2.279788    Objective Loss 2.279788                                        LR 0.100000    Time 0.097924    
2024-02-17 10:57:56,472 - Epoch: [12][  200/  391]    Overall Loss 2.269952    Objective Loss 2.269952                                        LR 0.100000    Time 0.095209    
2024-02-17 10:58:05,824 - Epoch: [12][  300/  391]    Overall Loss 2.270224    Objective Loss 2.270224                                        LR 0.100000    Time 0.094633    
2024-02-17 10:58:14,296 - Epoch: [12][  391/  391]    Overall Loss 2.271273    Objective Loss 2.271273    Top1 36.057692    Top5 73.076923    LR 0.100000    Time 0.094268    
2024-02-17 10:58:14,467 - --- validate (epoch=12)-----------
2024-02-17 10:58:14,468 - 10000 samples (128 per mini-batch)
2024-02-17 10:58:17,452 - Epoch: [12][   79/   79]    Loss 2.388325    Top1 36.210000    Top5 69.180000    
2024-02-17 10:58:17,612 - ==> Top1: 36.210    Top5: 69.180    Loss: 2.388

2024-02-17 10:58:17,631 - ==> Best [Top1: 36.210   Top5: 69.180   Sparsity:0.00   Params: 1341960 on epoch: 12]
2024-02-17 10:58:17,632 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:58:17,723 - 

2024-02-17 10:58:17,723 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:58:27,507 - Epoch: [13][  100/  391]    Overall Loss 2.193745    Objective Loss 2.193745                                        LR 0.100000    Time 0.097765    
2024-02-17 10:58:36,756 - Epoch: [13][  200/  391]    Overall Loss 2.201809    Objective Loss 2.201809                                        LR 0.100000    Time 0.095105    
2024-02-17 10:58:46,144 - Epoch: [13][  300/  391]    Overall Loss 2.204183    Objective Loss 2.204183                                        LR 0.100000    Time 0.094685    
2024-02-17 10:58:54,729 - Epoch: [13][  391/  391]    Overall Loss 2.202856    Objective Loss 2.202856    Top1 39.903846    Top5 68.750000    LR 0.100000    Time 0.094593    
2024-02-17 10:58:54,980 - --- validate (epoch=13)-----------
2024-02-17 10:58:54,981 - 10000 samples (128 per mini-batch)
2024-02-17 10:58:57,649 - Epoch: [13][   79/   79]    Loss 3.709286    Top1 22.120000    Top5 47.120000    
2024-02-17 10:58:57,831 - ==> Top1: 22.120    Top5: 47.120    Loss: 3.709

2024-02-17 10:58:57,851 - ==> Best [Top1: 36.210   Top5: 69.180   Sparsity:0.00   Params: 1341960 on epoch: 12]
2024-02-17 10:58:57,851 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:58:57,927 - 

2024-02-17 10:58:57,927 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:59:07,925 - Epoch: [14][  100/  391]    Overall Loss 2.149988    Objective Loss 2.149988                                        LR 0.100000    Time 0.099910    
2024-02-17 10:59:17,289 - Epoch: [14][  200/  391]    Overall Loss 2.156148    Objective Loss 2.156148                                        LR 0.100000    Time 0.096757    
2024-02-17 10:59:26,682 - Epoch: [14][  300/  391]    Overall Loss 2.161825    Objective Loss 2.161825                                        LR 0.100000    Time 0.095798    
2024-02-17 10:59:35,163 - Epoch: [14][  391/  391]    Overall Loss 2.157114    Objective Loss 2.157114    Top1 41.826923    Top5 75.000000    LR 0.100000    Time 0.095185    
2024-02-17 10:59:35,462 - --- validate (epoch=14)-----------
2024-02-17 10:59:35,463 - 10000 samples (128 per mini-batch)
2024-02-17 10:59:38,189 - Epoch: [14][   79/   79]    Loss 2.429733    Top1 36.470000    Top5 68.480000    
2024-02-17 10:59:38,439 - ==> Top1: 36.470    Top5: 68.480    Loss: 2.430

2024-02-17 10:59:38,458 - ==> Best [Top1: 36.470   Top5: 68.480   Sparsity:0.00   Params: 1341960 on epoch: 14]
2024-02-17 10:59:38,458 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 10:59:38,549 - 

2024-02-17 10:59:38,549 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 10:59:48,367 - Epoch: [15][  100/  391]    Overall Loss 2.120016    Objective Loss 2.120016                                        LR 0.100000    Time 0.098111    
2024-02-17 10:59:57,736 - Epoch: [15][  200/  391]    Overall Loss 2.116726    Objective Loss 2.116726                                        LR 0.100000    Time 0.095875    
2024-02-17 11:00:07,087 - Epoch: [15][  300/  391]    Overall Loss 2.114391    Objective Loss 2.114391                                        LR 0.100000    Time 0.095073    
2024-02-17 11:00:15,553 - Epoch: [15][  391/  391]    Overall Loss 2.116584    Objective Loss 2.116584    Top1 47.596154    Top5 78.846154    LR 0.100000    Time 0.094589    
2024-02-17 11:00:15,740 - --- validate (epoch=15)-----------
2024-02-17 11:00:15,741 - 10000 samples (128 per mini-batch)
2024-02-17 11:00:18,462 - Epoch: [15][   79/   79]    Loss 2.365633    Top1 37.020000    Top5 70.350000    
2024-02-17 11:00:18,671 - ==> Top1: 37.020    Top5: 70.350    Loss: 2.366

2024-02-17 11:00:18,689 - ==> Best [Top1: 37.020   Top5: 70.350   Sparsity:0.00   Params: 1341960 on epoch: 15]
2024-02-17 11:00:18,689 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:00:18,776 - 

2024-02-17 11:00:18,777 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:00:28,966 - Epoch: [16][  100/  391]    Overall Loss 2.047925    Objective Loss 2.047925                                        LR 0.100000    Time 0.101817    
2024-02-17 11:00:38,267 - Epoch: [16][  200/  391]    Overall Loss 2.072812    Objective Loss 2.072812                                        LR 0.100000    Time 0.097393    
2024-02-17 11:00:47,722 - Epoch: [16][  300/  391]    Overall Loss 2.064370    Objective Loss 2.064370                                        LR 0.100000    Time 0.096433    
2024-02-17 11:00:54,695 - Epoch: [16][  391/  391]    Overall Loss 2.064254    Objective Loss 2.064254    Top1 44.230769    Top5 77.403846    LR 0.100000    Time 0.091814    
2024-02-17 11:00:54,837 - --- validate (epoch=16)-----------
2024-02-17 11:00:54,837 - 10000 samples (128 per mini-batch)
2024-02-17 11:00:57,612 - Epoch: [16][   79/   79]    Loss 2.320833    Top1 38.740000    Top5 71.180000    
2024-02-17 11:00:57,757 - ==> Top1: 38.740    Top5: 71.180    Loss: 2.321

2024-02-17 11:00:57,775 - ==> Best [Top1: 38.740   Top5: 71.180   Sparsity:0.00   Params: 1341960 on epoch: 16]
2024-02-17 11:00:57,776 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:00:57,864 - 

2024-02-17 11:00:57,864 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:01:07,794 - Epoch: [17][  100/  391]    Overall Loss 2.010597    Objective Loss 2.010597                                        LR 0.100000    Time 0.099232    
2024-02-17 11:01:17,159 - Epoch: [17][  200/  391]    Overall Loss 2.013927    Objective Loss 2.013927                                        LR 0.100000    Time 0.096417    
2024-02-17 11:01:25,503 - Epoch: [17][  300/  391]    Overall Loss 2.022046    Objective Loss 2.022046                                        LR 0.100000    Time 0.092079    
2024-02-17 11:01:33,174 - Epoch: [17][  391/  391]    Overall Loss 2.025805    Objective Loss 2.025805    Top1 44.230769    Top5 77.403846    LR 0.100000    Time 0.090260    
2024-02-17 11:01:33,356 - --- validate (epoch=17)-----------
2024-02-17 11:01:33,357 - 10000 samples (128 per mini-batch)
2024-02-17 11:01:35,561 - Epoch: [17][   79/   79]    Loss 2.210307    Top1 40.720000    Top5 73.640000    
2024-02-17 11:01:35,671 - ==> Top1: 40.720    Top5: 73.640    Loss: 2.210

2024-02-17 11:01:35,688 - ==> Best [Top1: 40.720   Top5: 73.640   Sparsity:0.00   Params: 1341960 on epoch: 17]
2024-02-17 11:01:35,688 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:01:35,768 - 

2024-02-17 11:01:35,768 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:01:45,168 - Epoch: [18][  100/  391]    Overall Loss 2.002099    Objective Loss 2.002099                                        LR 0.100000    Time 0.093938    
2024-02-17 11:01:54,552 - Epoch: [18][  200/  391]    Overall Loss 1.996806    Objective Loss 1.996806                                        LR 0.100000    Time 0.093865    
2024-02-17 11:02:04,081 - Epoch: [18][  300/  391]    Overall Loss 2.003348    Objective Loss 2.003348                                        LR 0.100000    Time 0.094329    
2024-02-17 11:02:12,733 - Epoch: [18][  391/  391]    Overall Loss 2.002153    Objective Loss 2.002153    Top1 42.788462    Top5 75.000000    LR 0.100000    Time 0.094492    
2024-02-17 11:02:12,916 - --- validate (epoch=18)-----------
2024-02-17 11:02:12,916 - 10000 samples (128 per mini-batch)
2024-02-17 11:02:15,914 - Epoch: [18][   79/   79]    Loss 2.192672    Top1 41.320000    Top5 74.030000    
2024-02-17 11:02:16,098 - ==> Top1: 41.320    Top5: 74.030    Loss: 2.193

2024-02-17 11:02:16,116 - ==> Best [Top1: 41.320   Top5: 74.030   Sparsity:0.00   Params: 1341960 on epoch: 18]
2024-02-17 11:02:16,116 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:02:16,208 - 

2024-02-17 11:02:16,208 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:02:25,926 - Epoch: [19][  100/  391]    Overall Loss 1.957174    Objective Loss 1.957174                                        LR 0.100000    Time 0.097109    
2024-02-17 11:02:35,077 - Epoch: [19][  200/  391]    Overall Loss 1.959424    Objective Loss 1.959424                                        LR 0.100000    Time 0.094288    
2024-02-17 11:02:44,803 - Epoch: [19][  300/  391]    Overall Loss 1.953811    Objective Loss 1.953811                                        LR 0.100000    Time 0.095262    
2024-02-17 11:02:53,814 - Epoch: [19][  391/  391]    Overall Loss 1.957145    Objective Loss 1.957145    Top1 46.634615    Top5 79.807692    LR 0.100000    Time 0.096127    
2024-02-17 11:02:53,983 - --- validate (epoch=19)-----------
2024-02-17 11:02:53,984 - 10000 samples (128 per mini-batch)
2024-02-17 11:02:56,890 - Epoch: [19][   79/   79]    Loss 2.076991    Top1 42.880000    Top5 75.840000    
2024-02-17 11:02:57,059 - ==> Top1: 42.880    Top5: 75.840    Loss: 2.077

2024-02-17 11:02:57,078 - ==> Best [Top1: 42.880   Top5: 75.840   Sparsity:0.00   Params: 1341960 on epoch: 19]
2024-02-17 11:02:57,079 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:02:57,172 - 

2024-02-17 11:02:57,173 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:03:07,521 - Epoch: [20][  100/  391]    Overall Loss 1.917232    Objective Loss 1.917232                                        LR 0.100000    Time 0.103404    
2024-02-17 11:03:16,845 - Epoch: [20][  200/  391]    Overall Loss 1.918055    Objective Loss 1.918055                                        LR 0.100000    Time 0.098301    
2024-02-17 11:03:26,503 - Epoch: [20][  300/  391]    Overall Loss 1.924079    Objective Loss 1.924079                                        LR 0.100000    Time 0.097712    
2024-02-17 11:03:35,097 - Epoch: [20][  391/  391]    Overall Loss 1.920631    Objective Loss 1.920631    Top1 50.480769    Top5 82.692308    LR 0.100000    Time 0.096942    
2024-02-17 11:03:35,323 - --- validate (epoch=20)-----------
2024-02-17 11:03:35,324 - 10000 samples (128 per mini-batch)
2024-02-17 11:03:38,326 - Epoch: [20][   79/   79]    Loss 2.159016    Top1 40.930000    Top5 75.230000    
2024-02-17 11:03:38,476 - ==> Top1: 40.930    Top5: 75.230    Loss: 2.159

2024-02-17 11:03:38,494 - ==> Best [Top1: 42.880   Top5: 75.840   Sparsity:0.00   Params: 1341960 on epoch: 19]
2024-02-17 11:03:38,494 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:03:38,568 - 

2024-02-17 11:03:38,569 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:03:48,929 - Epoch: [21][  100/  391]    Overall Loss 1.908032    Objective Loss 1.908032                                        LR 0.100000    Time 0.103526    
2024-02-17 11:03:58,197 - Epoch: [21][  200/  391]    Overall Loss 1.902803    Objective Loss 1.902803                                        LR 0.100000    Time 0.098082    
2024-02-17 11:04:08,096 - Epoch: [21][  300/  391]    Overall Loss 1.902629    Objective Loss 1.902629                                        LR 0.100000    Time 0.098371    
2024-02-17 11:04:16,970 - Epoch: [21][  391/  391]    Overall Loss 1.899149    Objective Loss 1.899149    Top1 44.230769    Top5 75.961538    LR 0.100000    Time 0.098162    
2024-02-17 11:04:17,148 - --- validate (epoch=21)-----------
2024-02-17 11:04:17,149 - 10000 samples (128 per mini-batch)
2024-02-17 11:04:19,868 - Epoch: [21][   79/   79]    Loss 2.036169    Top1 44.570000    Top5 76.830000    
2024-02-17 11:04:20,012 - ==> Top1: 44.570    Top5: 76.830    Loss: 2.036

2024-02-17 11:04:20,023 - ==> Best [Top1: 44.570   Top5: 76.830   Sparsity:0.00   Params: 1341960 on epoch: 21]
2024-02-17 11:04:20,023 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:04:20,123 - 

2024-02-17 11:04:20,123 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:04:30,401 - Epoch: [22][  100/  391]    Overall Loss 1.867678    Objective Loss 1.867678                                        LR 0.100000    Time 0.102698    
2024-02-17 11:04:39,841 - Epoch: [22][  200/  391]    Overall Loss 1.858907    Objective Loss 1.858907                                        LR 0.100000    Time 0.098526    
2024-02-17 11:04:49,069 - Epoch: [22][  300/  391]    Overall Loss 1.868168    Objective Loss 1.868168                                        LR 0.100000    Time 0.096431    
2024-02-17 11:04:57,610 - Epoch: [22][  391/  391]    Overall Loss 1.867814    Objective Loss 1.867814    Top1 46.634615    Top5 80.288462    LR 0.100000    Time 0.095822    
2024-02-17 11:04:57,909 - --- validate (epoch=22)-----------
2024-02-17 11:04:57,910 - 10000 samples (128 per mini-batch)
2024-02-17 11:05:00,786 - Epoch: [22][   79/   79]    Loss 2.015076    Top1 44.960000    Top5 77.410000    
2024-02-17 11:05:00,967 - ==> Top1: 44.960    Top5: 77.410    Loss: 2.015

2024-02-17 11:05:00,985 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:05:00,985 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:05:01,078 - 

2024-02-17 11:05:01,079 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:05:11,156 - Epoch: [23][  100/  391]    Overall Loss 1.836784    Objective Loss 1.836784                                        LR 0.100000    Time 0.100702    
2024-02-17 11:05:20,353 - Epoch: [23][  200/  391]    Overall Loss 1.845296    Objective Loss 1.845296                                        LR 0.100000    Time 0.096316    
2024-02-17 11:05:29,736 - Epoch: [23][  300/  391]    Overall Loss 1.845424    Objective Loss 1.845424                                        LR 0.100000    Time 0.095474    
2024-02-17 11:05:38,349 - Epoch: [23][  391/  391]    Overall Loss 1.846385    Objective Loss 1.846385    Top1 42.788462    Top5 74.519231    LR 0.100000    Time 0.095269    
2024-02-17 11:05:38,516 - --- validate (epoch=23)-----------
2024-02-17 11:05:38,516 - 10000 samples (128 per mini-batch)
2024-02-17 11:05:41,204 - Epoch: [23][   79/   79]    Loss 2.083110    Top1 44.040000    Top5 76.330000    
2024-02-17 11:05:41,340 - ==> Top1: 44.040    Top5: 76.330    Loss: 2.083

2024-02-17 11:05:41,352 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:05:41,352 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:05:41,425 - 

2024-02-17 11:05:41,425 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:05:51,879 - Epoch: [24][  100/  391]    Overall Loss 1.795736    Objective Loss 1.795736                                        LR 0.100000    Time 0.104465    
2024-02-17 11:06:01,536 - Epoch: [24][  200/  391]    Overall Loss 1.800785    Objective Loss 1.800785                                        LR 0.100000    Time 0.100493    
2024-02-17 11:06:10,738 - Epoch: [24][  300/  391]    Overall Loss 1.814031    Objective Loss 1.814031                                        LR 0.100000    Time 0.097654    
2024-02-17 11:06:19,438 - Epoch: [24][  391/  391]    Overall Loss 1.819678    Objective Loss 1.819678    Top1 53.365385    Top5 83.653846    LR 0.100000    Time 0.097169    
2024-02-17 11:06:19,687 - --- validate (epoch=24)-----------
2024-02-17 11:06:19,688 - 10000 samples (128 per mini-batch)
2024-02-17 11:06:22,399 - Epoch: [24][   79/   79]    Loss 2.628696    Top1 35.390000    Top5 68.330000    
2024-02-17 11:06:22,547 - ==> Top1: 35.390    Top5: 68.330    Loss: 2.629

2024-02-17 11:06:22,569 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:06:22,569 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:06:22,648 - 

2024-02-17 11:06:22,649 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:06:32,820 - Epoch: [25][  100/  391]    Overall Loss 1.772238    Objective Loss 1.772238                                        LR 0.100000    Time 0.101604    
2024-02-17 11:06:42,059 - Epoch: [25][  200/  391]    Overall Loss 1.775472    Objective Loss 1.775472                                        LR 0.100000    Time 0.096973    
2024-02-17 11:06:51,561 - Epoch: [25][  300/  391]    Overall Loss 1.788799    Objective Loss 1.788799                                        LR 0.100000    Time 0.096308    
2024-02-17 11:06:59,129 - Epoch: [25][  391/  391]    Overall Loss 1.791222    Objective Loss 1.791222    Top1 48.076923    Top5 81.730769    LR 0.100000    Time 0.093241    
2024-02-17 11:06:59,330 - --- validate (epoch=25)-----------
2024-02-17 11:06:59,331 - 10000 samples (128 per mini-batch)
2024-02-17 11:07:02,113 - Epoch: [25][   79/   79]    Loss 2.358221    Top1 39.670000    Top5 71.360000    
2024-02-17 11:07:02,280 - ==> Top1: 39.670    Top5: 71.360    Loss: 2.358

2024-02-17 11:07:02,299 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:07:02,299 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:07:02,372 - 

2024-02-17 11:07:02,373 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:07:12,861 - Epoch: [26][  100/  391]    Overall Loss 1.733931    Objective Loss 1.733931                                        LR 0.100000    Time 0.104815    
2024-02-17 11:07:21,810 - Epoch: [26][  200/  391]    Overall Loss 1.755922    Objective Loss 1.755922                                        LR 0.100000    Time 0.097128    
2024-02-17 11:07:31,202 - Epoch: [26][  300/  391]    Overall Loss 1.762424    Objective Loss 1.762424                                        LR 0.100000    Time 0.096047    
2024-02-17 11:07:39,878 - Epoch: [26][  391/  391]    Overall Loss 1.766247    Objective Loss 1.766247    Top1 51.442308    Top5 82.692308    LR 0.100000    Time 0.095871    
2024-02-17 11:07:40,038 - --- validate (epoch=26)-----------
2024-02-17 11:07:40,039 - 10000 samples (128 per mini-batch)
2024-02-17 11:07:42,925 - Epoch: [26][   79/   79]    Loss 2.096929    Top1 43.990000    Top5 76.130000    
2024-02-17 11:07:43,077 - ==> Top1: 43.990    Top5: 76.130    Loss: 2.097

2024-02-17 11:07:43,096 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:07:43,096 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:07:43,175 - 

2024-02-17 11:07:43,175 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:07:53,451 - Epoch: [27][  100/  391]    Overall Loss 1.701968    Objective Loss 1.701968                                        LR 0.100000    Time 0.102684    
2024-02-17 11:08:01,699 - Epoch: [27][  200/  391]    Overall Loss 1.725346    Objective Loss 1.725346                                        LR 0.100000    Time 0.092560    
2024-02-17 11:08:11,215 - Epoch: [27][  300/  391]    Overall Loss 1.735624    Objective Loss 1.735624                                        LR 0.100000    Time 0.093413    
2024-02-17 11:08:19,994 - Epoch: [27][  391/  391]    Overall Loss 1.742071    Objective Loss 1.742071    Top1 48.076923    Top5 81.250000    LR 0.100000    Time 0.094116    
2024-02-17 11:08:20,165 - --- validate (epoch=27)-----------
2024-02-17 11:08:20,165 - 10000 samples (128 per mini-batch)
2024-02-17 11:08:22,931 - Epoch: [27][   79/   79]    Loss 2.224197    Top1 41.210000    Top5 73.400000    
2024-02-17 11:08:23,115 - ==> Top1: 41.210    Top5: 73.400    Loss: 2.224

2024-02-17 11:08:23,133 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:08:23,134 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:08:23,209 - 

2024-02-17 11:08:23,209 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:08:33,737 - Epoch: [28][  100/  391]    Overall Loss 1.722163    Objective Loss 1.722163                                        LR 0.100000    Time 0.105205    
2024-02-17 11:08:43,242 - Epoch: [28][  200/  391]    Overall Loss 1.715269    Objective Loss 1.715269                                        LR 0.100000    Time 0.100107    
2024-02-17 11:08:52,785 - Epoch: [28][  300/  391]    Overall Loss 1.721193    Objective Loss 1.721193                                        LR 0.100000    Time 0.098531    
2024-02-17 11:09:00,726 - Epoch: [28][  391/  391]    Overall Loss 1.723180    Objective Loss 1.723180    Top1 50.961538    Top5 80.769231    LR 0.100000    Time 0.095900    
2024-02-17 11:09:00,934 - --- validate (epoch=28)-----------
2024-02-17 11:09:00,935 - 10000 samples (128 per mini-batch)
2024-02-17 11:09:03,609 - Epoch: [28][   79/   79]    Loss 2.306765    Top1 40.140000    Top5 72.850000    
2024-02-17 11:09:03,757 - ==> Top1: 40.140    Top5: 72.850    Loss: 2.307

2024-02-17 11:09:03,775 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:09:03,775 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:09:03,851 - 

2024-02-17 11:09:03,851 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:09:14,111 - Epoch: [29][  100/  391]    Overall Loss 1.684613    Objective Loss 1.684613                                        LR 0.100000    Time 0.102532    
2024-02-17 11:09:23,317 - Epoch: [29][  200/  391]    Overall Loss 1.698510    Objective Loss 1.698510                                        LR 0.100000    Time 0.097272    
2024-02-17 11:09:32,874 - Epoch: [29][  300/  391]    Overall Loss 1.710720    Objective Loss 1.710720                                        LR 0.100000    Time 0.096688    
2024-02-17 11:09:41,789 - Epoch: [29][  391/  391]    Overall Loss 1.714815    Objective Loss 1.714815    Top1 50.000000    Top5 83.173077    LR 0.100000    Time 0.096976    
2024-02-17 11:09:41,971 - --- validate (epoch=29)-----------
2024-02-17 11:09:41,972 - 10000 samples (128 per mini-batch)
2024-02-17 11:09:44,729 - Epoch: [29][   79/   79]    Loss 2.234177    Top1 42.070000    Top5 73.000000    
2024-02-17 11:09:44,924 - ==> Top1: 42.070    Top5: 73.000    Loss: 2.234

2024-02-17 11:09:44,938 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:09:44,939 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:09:45,013 - 

2024-02-17 11:09:45,013 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:09:55,369 - Epoch: [30][  100/  391]    Overall Loss 1.644164    Objective Loss 1.644164                                        LR 0.100000    Time 0.103419    
2024-02-17 11:10:04,443 - Epoch: [30][  200/  391]    Overall Loss 1.668109    Objective Loss 1.668109                                        LR 0.100000    Time 0.097059    
2024-02-17 11:10:13,994 - Epoch: [30][  300/  391]    Overall Loss 1.679802    Objective Loss 1.679802                                        LR 0.100000    Time 0.096529    
2024-02-17 11:10:22,867 - Epoch: [30][  391/  391]    Overall Loss 1.683561    Objective Loss 1.683561    Top1 51.923077    Top5 85.096154    LR 0.100000    Time 0.096747    
2024-02-17 11:10:23,047 - --- validate (epoch=30)-----------
2024-02-17 11:10:23,048 - 10000 samples (128 per mini-batch)
2024-02-17 11:10:25,798 - Epoch: [30][   79/   79]    Loss 2.379153    Top1 39.670000    Top5 71.900000    
2024-02-17 11:10:25,970 - ==> Top1: 39.670    Top5: 71.900    Loss: 2.379

2024-02-17 11:10:25,988 - ==> Best [Top1: 44.960   Top5: 77.410   Sparsity:0.00   Params: 1341960 on epoch: 22]
2024-02-17 11:10:25,989 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:10:26,064 - 

2024-02-17 11:10:26,064 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:10:36,198 - Epoch: [31][  100/  391]    Overall Loss 1.661652    Objective Loss 1.661652                                        LR 0.100000    Time 0.101267    
2024-02-17 11:10:45,280 - Epoch: [31][  200/  391]    Overall Loss 1.659721    Objective Loss 1.659721                                        LR 0.100000    Time 0.095994    
2024-02-17 11:10:54,961 - Epoch: [31][  300/  391]    Overall Loss 1.663134    Objective Loss 1.663134                                        LR 0.100000    Time 0.096252    
2024-02-17 11:11:03,698 - Epoch: [31][  391/  391]    Overall Loss 1.668387    Objective Loss 1.668387    Top1 56.730769    Top5 82.692308    LR 0.100000    Time 0.096185    
2024-02-17 11:11:03,883 - --- validate (epoch=31)-----------
2024-02-17 11:11:03,883 - 10000 samples (128 per mini-batch)
2024-02-17 11:11:06,710 - Epoch: [31][   79/   79]    Loss 1.868021    Top1 48.600000    Top5 80.400000    
2024-02-17 11:11:06,847 - ==> Top1: 48.600    Top5: 80.400    Loss: 1.868

2024-02-17 11:11:06,865 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:11:06,865 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:11:06,958 - 

2024-02-17 11:11:06,958 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:11:17,193 - Epoch: [32][  100/  391]    Overall Loss 1.627926    Objective Loss 1.627926                                        LR 0.100000    Time 0.102278    
2024-02-17 11:11:26,574 - Epoch: [32][  200/  391]    Overall Loss 1.643109    Objective Loss 1.643109                                        LR 0.100000    Time 0.098017    
2024-02-17 11:11:36,520 - Epoch: [32][  300/  391]    Overall Loss 1.644081    Objective Loss 1.644081                                        LR 0.100000    Time 0.098483    
2024-02-17 11:11:45,465 - Epoch: [32][  391/  391]    Overall Loss 1.646557    Objective Loss 1.646557    Top1 48.076923    Top5 83.653846    LR 0.100000    Time 0.098429    
2024-02-17 11:11:45,699 - --- validate (epoch=32)-----------
2024-02-17 11:11:45,700 - 10000 samples (128 per mini-batch)
2024-02-17 11:11:48,719 - Epoch: [32][   79/   79]    Loss 1.976561    Top1 45.680000    Top5 78.180000    
2024-02-17 11:11:48,899 - ==> Top1: 45.680    Top5: 78.180    Loss: 1.977

2024-02-17 11:11:48,910 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:11:48,910 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:11:48,985 - 

2024-02-17 11:11:48,985 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:11:59,097 - Epoch: [33][  100/  391]    Overall Loss 1.595683    Objective Loss 1.595683                                        LR 0.100000    Time 0.101050    
2024-02-17 11:12:08,188 - Epoch: [33][  200/  391]    Overall Loss 1.606158    Objective Loss 1.606158                                        LR 0.100000    Time 0.095957    
2024-02-17 11:12:17,745 - Epoch: [33][  300/  391]    Overall Loss 1.621585    Objective Loss 1.621585                                        LR 0.100000    Time 0.095813    
2024-02-17 11:12:26,576 - Epoch: [33][  391/  391]    Overall Loss 1.629625    Objective Loss 1.629625    Top1 54.807692    Top5 82.211538    LR 0.100000    Time 0.096088    
2024-02-17 11:12:26,778 - --- validate (epoch=33)-----------
2024-02-17 11:12:26,779 - 10000 samples (128 per mini-batch)
2024-02-17 11:12:29,478 - Epoch: [33][   79/   79]    Loss 1.999365    Top1 46.460000    Top5 78.210000    
2024-02-17 11:12:29,631 - ==> Top1: 46.460    Top5: 78.210    Loss: 1.999

2024-02-17 11:12:29,651 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:12:29,651 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:12:29,727 - 

2024-02-17 11:12:29,728 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:12:40,078 - Epoch: [34][  100/  391]    Overall Loss 1.594835    Objective Loss 1.594835                                        LR 0.100000    Time 0.103432    
2024-02-17 11:12:49,330 - Epoch: [34][  200/  391]    Overall Loss 1.604233    Objective Loss 1.604233                                        LR 0.100000    Time 0.097952    
2024-02-17 11:12:58,982 - Epoch: [34][  300/  391]    Overall Loss 1.605438    Objective Loss 1.605438                                        LR 0.100000    Time 0.097458    
2024-02-17 11:13:07,693 - Epoch: [34][  391/  391]    Overall Loss 1.612352    Objective Loss 1.612352    Top1 54.807692    Top5 87.500000    LR 0.100000    Time 0.097044    
2024-02-17 11:13:07,895 - --- validate (epoch=34)-----------
2024-02-17 11:13:07,895 - 10000 samples (128 per mini-batch)
2024-02-17 11:13:10,538 - Epoch: [34][   79/   79]    Loss 2.018327    Top1 46.060000    Top5 77.830000    
2024-02-17 11:13:10,682 - ==> Top1: 46.060    Top5: 77.830    Loss: 2.018

2024-02-17 11:13:10,700 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:13:10,701 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:13:10,776 - 

2024-02-17 11:13:10,776 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:13:20,942 - Epoch: [35][  100/  391]    Overall Loss 1.575489    Objective Loss 1.575489                                        LR 0.100000    Time 0.101580    
2024-02-17 11:13:30,198 - Epoch: [35][  200/  391]    Overall Loss 1.586312    Objective Loss 1.586312                                        LR 0.100000    Time 0.097051    
2024-02-17 11:13:39,914 - Epoch: [35][  300/  391]    Overall Loss 1.602771    Objective Loss 1.602771                                        LR 0.100000    Time 0.097070    
2024-02-17 11:13:48,494 - Epoch: [35][  391/  391]    Overall Loss 1.606899    Objective Loss 1.606899    Top1 51.442308    Top5 86.057692    LR 0.100000    Time 0.096414    
2024-02-17 11:13:48,701 - --- validate (epoch=35)-----------
2024-02-17 11:13:48,703 - 10000 samples (128 per mini-batch)
2024-02-17 11:13:51,418 - Epoch: [35][   79/   79]    Loss 1.989054    Top1 45.770000    Top5 77.170000    
2024-02-17 11:13:51,627 - ==> Top1: 45.770    Top5: 77.170    Loss: 1.989

2024-02-17 11:13:51,645 - ==> Best [Top1: 48.600   Top5: 80.400   Sparsity:0.00   Params: 1341960 on epoch: 31]
2024-02-17 11:13:51,646 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:13:51,722 - 

2024-02-17 11:13:51,722 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:14:01,931 - Epoch: [36][  100/  391]    Overall Loss 1.539836    Objective Loss 1.539836                                        LR 0.100000    Time 0.102016    
2024-02-17 11:14:11,280 - Epoch: [36][  200/  391]    Overall Loss 1.562597    Objective Loss 1.562597                                        LR 0.100000    Time 0.097732    
2024-02-17 11:14:20,838 - Epoch: [36][  300/  391]    Overall Loss 1.576128    Objective Loss 1.576128                                        LR 0.100000    Time 0.096999    
2024-02-17 11:14:29,520 - Epoch: [36][  391/  391]    Overall Loss 1.586168    Objective Loss 1.586168    Top1 52.403846    Top5 86.057692    LR 0.100000    Time 0.096619    
2024-02-17 11:14:29,725 - --- validate (epoch=36)-----------
2024-02-17 11:14:29,725 - 10000 samples (128 per mini-batch)
2024-02-17 11:14:32,424 - Epoch: [36][   79/   79]    Loss 1.807605    Top1 50.040000    Top5 81.320000    
2024-02-17 11:14:32,551 - ==> Top1: 50.040    Top5: 81.320    Loss: 1.808

2024-02-17 11:14:32,569 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:14:32,569 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:14:32,662 - 

2024-02-17 11:14:32,662 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:14:42,511 - Epoch: [37][  100/  391]    Overall Loss 1.568262    Objective Loss 1.568262                                        LR 0.100000    Time 0.098412    
2024-02-17 11:14:52,025 - Epoch: [37][  200/  391]    Overall Loss 1.577324    Objective Loss 1.577324                                        LR 0.100000    Time 0.096752    
2024-02-17 11:15:01,712 - Epoch: [37][  300/  391]    Overall Loss 1.573395    Objective Loss 1.573395                                        LR 0.100000    Time 0.096779    
2024-02-17 11:15:10,358 - Epoch: [37][  391/  391]    Overall Loss 1.579745    Objective Loss 1.579745    Top1 52.884615    Top5 84.134615    LR 0.100000    Time 0.096358    
2024-02-17 11:15:10,566 - --- validate (epoch=37)-----------
2024-02-17 11:15:10,567 - 10000 samples (128 per mini-batch)
2024-02-17 11:15:13,202 - Epoch: [37][   79/   79]    Loss 2.064832    Top1 45.690000    Top5 76.750000    
2024-02-17 11:15:13,484 - ==> Top1: 45.690    Top5: 76.750    Loss: 2.065

2024-02-17 11:15:13,503 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:15:13,503 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:15:13,578 - 

2024-02-17 11:15:13,578 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:15:23,734 - Epoch: [38][  100/  391]    Overall Loss 1.527588    Objective Loss 1.527588                                        LR 0.100000    Time 0.101488    
2024-02-17 11:15:33,573 - Epoch: [38][  200/  391]    Overall Loss 1.537939    Objective Loss 1.537939                                        LR 0.100000    Time 0.099914    
2024-02-17 11:15:43,156 - Epoch: [38][  300/  391]    Overall Loss 1.550885    Objective Loss 1.550885                                        LR 0.100000    Time 0.098537    
2024-02-17 11:15:51,937 - Epoch: [38][  391/  391]    Overall Loss 1.551893    Objective Loss 1.551893    Top1 51.923077    Top5 85.096154    LR 0.100000    Time 0.098053    
2024-02-17 11:15:52,208 - --- validate (epoch=38)-----------
2024-02-17 11:15:52,208 - 10000 samples (128 per mini-batch)
2024-02-17 11:15:54,964 - Epoch: [38][   79/   79]    Loss 1.979522    Top1 47.710000    Top5 78.950000    
2024-02-17 11:15:55,109 - ==> Top1: 47.710    Top5: 78.950    Loss: 1.980

2024-02-17 11:15:55,120 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:15:55,120 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:15:55,194 - 

2024-02-17 11:15:55,194 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:16:04,996 - Epoch: [39][  100/  391]    Overall Loss 1.531712    Objective Loss 1.531712                                        LR 0.100000    Time 0.097941    
2024-02-17 11:16:14,432 - Epoch: [39][  200/  391]    Overall Loss 1.542680    Objective Loss 1.542680                                        LR 0.100000    Time 0.096129    
2024-02-17 11:16:24,078 - Epoch: [39][  300/  391]    Overall Loss 1.548814    Objective Loss 1.548814                                        LR 0.100000    Time 0.096227    
2024-02-17 11:16:32,821 - Epoch: [39][  391/  391]    Overall Loss 1.550180    Objective Loss 1.550180    Top1 59.134615    Top5 87.019231    LR 0.100000    Time 0.096181    
2024-02-17 11:16:33,083 - --- validate (epoch=39)-----------
2024-02-17 11:16:33,084 - 10000 samples (128 per mini-batch)
2024-02-17 11:16:35,857 - Epoch: [39][   79/   79]    Loss 1.908074    Top1 47.730000    Top5 79.360000    
2024-02-17 11:16:36,053 - ==> Top1: 47.730    Top5: 79.360    Loss: 1.908

2024-02-17 11:16:36,072 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:16:36,072 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:16:36,148 - 

2024-02-17 11:16:36,149 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:16:46,326 - Epoch: [40][  100/  391]    Overall Loss 1.499845    Objective Loss 1.499845                                        LR 0.100000    Time 0.101702    
2024-02-17 11:16:55,631 - Epoch: [40][  200/  391]    Overall Loss 1.531605    Objective Loss 1.531605                                        LR 0.100000    Time 0.097356    
2024-02-17 11:17:05,254 - Epoch: [40][  300/  391]    Overall Loss 1.545265    Objective Loss 1.545265                                        LR 0.100000    Time 0.096964    
2024-02-17 11:17:13,994 - Epoch: [40][  391/  391]    Overall Loss 1.539392    Objective Loss 1.539392    Top1 58.173077    Top5 89.903846    LR 0.100000    Time 0.096740    
2024-02-17 11:17:14,246 - --- validate (epoch=40)-----------
2024-02-17 11:17:14,247 - 10000 samples (128 per mini-batch)
2024-02-17 11:17:17,106 - Epoch: [40][   79/   79]    Loss 1.845986    Top1 48.800000    Top5 80.410000    
2024-02-17 11:17:17,281 - ==> Top1: 48.800    Top5: 80.410    Loss: 1.846

2024-02-17 11:17:17,299 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:17:17,300 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:17:17,376 - 

2024-02-17 11:17:17,377 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:17:27,493 - Epoch: [41][  100/  391]    Overall Loss 1.485648    Objective Loss 1.485648                                        LR 0.100000    Time 0.101085    
2024-02-17 11:17:36,989 - Epoch: [41][  200/  391]    Overall Loss 1.491010    Objective Loss 1.491010                                        LR 0.100000    Time 0.097999    
2024-02-17 11:17:46,740 - Epoch: [41][  300/  391]    Overall Loss 1.509338    Objective Loss 1.509338                                        LR 0.100000    Time 0.097822    
2024-02-17 11:17:55,421 - Epoch: [41][  391/  391]    Overall Loss 1.520214    Objective Loss 1.520214    Top1 59.134615    Top5 88.942308    LR 0.100000    Time 0.097249    
2024-02-17 11:17:55,603 - --- validate (epoch=41)-----------
2024-02-17 11:17:55,603 - 10000 samples (128 per mini-batch)
2024-02-17 11:17:58,361 - Epoch: [41][   79/   79]    Loss 1.984574    Top1 46.930000    Top5 77.890000    
2024-02-17 11:17:58,528 - ==> Top1: 46.930    Top5: 77.890    Loss: 1.985

2024-02-17 11:17:58,546 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:17:58,546 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:17:58,622 - 

2024-02-17 11:17:58,622 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:18:09,042 - Epoch: [42][  100/  391]    Overall Loss 1.489325    Objective Loss 1.489325                                        LR 0.100000    Time 0.104123    
2024-02-17 11:18:17,786 - Epoch: [42][  200/  391]    Overall Loss 1.483331    Objective Loss 1.483331                                        LR 0.100000    Time 0.095760    
2024-02-17 11:18:27,478 - Epoch: [42][  300/  391]    Overall Loss 1.500461    Objective Loss 1.500461                                        LR 0.100000    Time 0.096134    
2024-02-17 11:18:36,187 - Epoch: [42][  391/  391]    Overall Loss 1.510962    Objective Loss 1.510962    Top1 56.250000    Top5 87.019231    LR 0.100000    Time 0.096022    
2024-02-17 11:18:36,379 - --- validate (epoch=42)-----------
2024-02-17 11:18:36,380 - 10000 samples (128 per mini-batch)
2024-02-17 11:18:39,194 - Epoch: [42][   79/   79]    Loss 1.952823    Top1 47.340000    Top5 78.560000    
2024-02-17 11:18:39,418 - ==> Top1: 47.340    Top5: 78.560    Loss: 1.953

2024-02-17 11:18:39,437 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:18:39,438 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:18:39,514 - 

2024-02-17 11:18:39,514 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:18:49,698 - Epoch: [43][  100/  391]    Overall Loss 1.457503    Objective Loss 1.457503                                        LR 0.100000    Time 0.101765    
2024-02-17 11:18:59,410 - Epoch: [43][  200/  391]    Overall Loss 1.475390    Objective Loss 1.475390                                        LR 0.100000    Time 0.099419    
2024-02-17 11:19:09,021 - Epoch: [43][  300/  391]    Overall Loss 1.492215    Objective Loss 1.492215                                        LR 0.100000    Time 0.098300    
2024-02-17 11:19:17,816 - Epoch: [43][  391/  391]    Overall Loss 1.502405    Objective Loss 1.502405    Top1 54.807692    Top5 82.211538    LR 0.100000    Time 0.097904    
2024-02-17 11:19:17,956 - --- validate (epoch=43)-----------
2024-02-17 11:19:17,956 - 10000 samples (128 per mini-batch)
2024-02-17 11:19:20,710 - Epoch: [43][   79/   79]    Loss 1.839245    Top1 48.860000    Top5 81.150000    
2024-02-17 11:19:20,995 - ==> Top1: 48.860    Top5: 81.150    Loss: 1.839

2024-02-17 11:19:21,015 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:19:21,015 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:19:21,092 - 

2024-02-17 11:19:21,092 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:19:31,346 - Epoch: [44][  100/  391]    Overall Loss 1.465794    Objective Loss 1.465794                                        LR 0.100000    Time 0.102461    
2024-02-17 11:19:40,929 - Epoch: [44][  200/  391]    Overall Loss 1.475357    Objective Loss 1.475357                                        LR 0.100000    Time 0.099121    
2024-02-17 11:19:50,471 - Epoch: [44][  300/  391]    Overall Loss 1.490347    Objective Loss 1.490347                                        LR 0.100000    Time 0.097872    
2024-02-17 11:19:59,449 - Epoch: [44][  391/  391]    Overall Loss 1.492887    Objective Loss 1.492887    Top1 54.807692    Top5 79.326923    LR 0.100000    Time 0.098045    
2024-02-17 11:19:59,704 - --- validate (epoch=44)-----------
2024-02-17 11:19:59,705 - 10000 samples (128 per mini-batch)
2024-02-17 11:20:02,707 - Epoch: [44][   79/   79]    Loss 1.917619    Top1 48.810000    Top5 79.840000    
2024-02-17 11:20:02,876 - ==> Top1: 48.810    Top5: 79.840    Loss: 1.918

2024-02-17 11:20:02,895 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:20:02,896 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:20:02,973 - 

2024-02-17 11:20:02,974 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:20:13,462 - Epoch: [45][  100/  391]    Overall Loss 1.433537    Objective Loss 1.433537                                        LR 0.100000    Time 0.104800    
2024-02-17 11:20:22,847 - Epoch: [45][  200/  391]    Overall Loss 1.449299    Objective Loss 1.449299                                        LR 0.100000    Time 0.099302    
2024-02-17 11:20:32,377 - Epoch: [45][  300/  391]    Overall Loss 1.465140    Objective Loss 1.465140                                        LR 0.100000    Time 0.097952    
2024-02-17 11:20:39,979 - Epoch: [45][  391/  391]    Overall Loss 1.475613    Objective Loss 1.475613    Top1 58.173077    Top5 87.019231    LR 0.100000    Time 0.094589    
2024-02-17 11:20:40,171 - --- validate (epoch=45)-----------
2024-02-17 11:20:40,172 - 10000 samples (128 per mini-batch)
2024-02-17 11:20:43,194 - Epoch: [45][   79/   79]    Loss 1.833996    Top1 49.530000    Top5 80.410000    
2024-02-17 11:20:43,361 - ==> Top1: 49.530    Top5: 80.410    Loss: 1.834

2024-02-17 11:20:43,372 - ==> Best [Top1: 50.040   Top5: 81.320   Sparsity:0.00   Params: 1341960 on epoch: 36]
2024-02-17 11:20:43,372 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:20:43,454 - 

2024-02-17 11:20:43,454 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:20:54,296 - Epoch: [46][  100/  391]    Overall Loss 1.404987    Objective Loss 1.404987                                        LR 0.100000    Time 0.108341    
2024-02-17 11:21:03,606 - Epoch: [46][  200/  391]    Overall Loss 1.428167    Objective Loss 1.428167                                        LR 0.100000    Time 0.100697    
2024-02-17 11:21:13,144 - Epoch: [46][  300/  391]    Overall Loss 1.445878    Objective Loss 1.445878                                        LR 0.100000    Time 0.098912    
2024-02-17 11:21:20,779 - Epoch: [46][  391/  391]    Overall Loss 1.459151    Objective Loss 1.459151    Top1 62.019231    Top5 93.269231    LR 0.100000    Time 0.095408    
2024-02-17 11:21:20,951 - --- validate (epoch=46)-----------
2024-02-17 11:21:20,952 - 10000 samples (128 per mini-batch)
2024-02-17 11:21:23,791 - Epoch: [46][   79/   79]    Loss 1.885629    Top1 50.330000    Top5 80.910000    
2024-02-17 11:21:23,995 - ==> Top1: 50.330    Top5: 80.910    Loss: 1.886

2024-02-17 11:21:24,014 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:21:24,014 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:21:24,154 - 

2024-02-17 11:21:24,154 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:21:34,121 - Epoch: [47][  100/  391]    Overall Loss 1.427855    Objective Loss 1.427855                                        LR 0.100000    Time 0.099583    
2024-02-17 11:21:43,608 - Epoch: [47][  200/  391]    Overall Loss 1.431328    Objective Loss 1.431328                                        LR 0.100000    Time 0.097207    
2024-02-17 11:21:53,122 - Epoch: [47][  300/  391]    Overall Loss 1.449821    Objective Loss 1.449821                                        LR 0.100000    Time 0.096504    
2024-02-17 11:22:01,619 - Epoch: [47][  391/  391]    Overall Loss 1.456628    Objective Loss 1.456628    Top1 54.807692    Top5 90.384615    LR 0.100000    Time 0.095764    
2024-02-17 11:22:01,853 - --- validate (epoch=47)-----------
2024-02-17 11:22:01,853 - 10000 samples (128 per mini-batch)
2024-02-17 11:22:04,633 - Epoch: [47][   79/   79]    Loss 2.009425    Top1 47.200000    Top5 77.860000    
2024-02-17 11:22:04,794 - ==> Top1: 47.200    Top5: 77.860    Loss: 2.009

2024-02-17 11:22:04,813 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:22:04,813 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:22:04,891 - 

2024-02-17 11:22:04,892 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:22:15,077 - Epoch: [48][  100/  391]    Overall Loss 1.426738    Objective Loss 1.426738                                        LR 0.100000    Time 0.101773    
2024-02-17 11:22:24,794 - Epoch: [48][  200/  391]    Overall Loss 1.439196    Objective Loss 1.439196                                        LR 0.100000    Time 0.099444    
2024-02-17 11:22:34,246 - Epoch: [48][  300/  391]    Overall Loss 1.440461    Objective Loss 1.440461                                        LR 0.100000    Time 0.097791    
2024-02-17 11:22:43,154 - Epoch: [48][  391/  391]    Overall Loss 1.447062    Objective Loss 1.447062    Top1 54.807692    Top5 85.096154    LR 0.100000    Time 0.097804    
2024-02-17 11:22:43,341 - --- validate (epoch=48)-----------
2024-02-17 11:22:43,343 - 10000 samples (128 per mini-batch)
2024-02-17 11:22:46,100 - Epoch: [48][   79/   79]    Loss 1.940782    Top1 48.330000    Top5 79.390000    
2024-02-17 11:22:46,267 - ==> Top1: 48.330    Top5: 79.390    Loss: 1.941

2024-02-17 11:22:46,286 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:22:46,287 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:22:46,364 - 

2024-02-17 11:22:46,365 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:22:56,452 - Epoch: [49][  100/  391]    Overall Loss 1.422912    Objective Loss 1.422912                                        LR 0.100000    Time 0.100799    
2024-02-17 11:23:06,282 - Epoch: [49][  200/  391]    Overall Loss 1.418644    Objective Loss 1.418644                                        LR 0.100000    Time 0.099524    
2024-02-17 11:23:15,900 - Epoch: [49][  300/  391]    Overall Loss 1.426263    Objective Loss 1.426263                                        LR 0.100000    Time 0.098397    
2024-02-17 11:23:24,687 - Epoch: [49][  391/  391]    Overall Loss 1.433422    Objective Loss 1.433422    Top1 62.019231    Top5 87.019231    LR 0.100000    Time 0.097959    
2024-02-17 11:23:24,944 - --- validate (epoch=49)-----------
2024-02-17 11:23:24,944 - 10000 samples (128 per mini-batch)
2024-02-17 11:23:27,902 - Epoch: [49][   79/   79]    Loss 2.283702    Top1 42.440000    Top5 73.860000    
2024-02-17 11:23:28,101 - ==> Top1: 42.440    Top5: 73.860    Loss: 2.284

2024-02-17 11:23:28,119 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:23:28,120 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:23:28,201 - 

2024-02-17 11:23:28,201 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:23:38,646 - Epoch: [50][  100/  391]    Overall Loss 1.403427    Objective Loss 1.403427                                        LR 0.100000    Time 0.104361    
2024-02-17 11:23:48,017 - Epoch: [50][  200/  391]    Overall Loss 1.417837    Objective Loss 1.417837                                        LR 0.100000    Time 0.099012    
2024-02-17 11:23:57,631 - Epoch: [50][  300/  391]    Overall Loss 1.419865    Objective Loss 1.419865                                        LR 0.100000    Time 0.098041    
2024-02-17 11:24:06,540 - Epoch: [50][  391/  391]    Overall Loss 1.429393    Objective Loss 1.429393    Top1 57.211538    Top5 84.615385    LR 0.100000    Time 0.097996    
2024-02-17 11:24:06,704 - --- validate (epoch=50)-----------
2024-02-17 11:24:06,705 - 10000 samples (128 per mini-batch)
2024-02-17 11:24:09,780 - Epoch: [50][   79/   79]    Loss 1.889087    Top1 50.290000    Top5 79.830000    
2024-02-17 11:24:09,965 - ==> Top1: 50.290    Top5: 79.830    Loss: 1.889

2024-02-17 11:24:09,976 - ==> Best [Top1: 50.330   Top5: 80.910   Sparsity:0.00   Params: 1341960 on epoch: 46]
2024-02-17 11:24:09,977 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:24:10,058 - 

2024-02-17 11:24:10,059 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:24:20,215 - Epoch: [51][  100/  391]    Overall Loss 1.368844    Objective Loss 1.368844                                        LR 0.100000    Time 0.101488    
2024-02-17 11:24:29,862 - Epoch: [51][  200/  391]    Overall Loss 1.400591    Objective Loss 1.400591                                        LR 0.100000    Time 0.098956    
2024-02-17 11:24:39,449 - Epoch: [51][  300/  391]    Overall Loss 1.412388    Objective Loss 1.412388                                        LR 0.100000    Time 0.097912    
2024-02-17 11:24:48,236 - Epoch: [51][  391/  391]    Overall Loss 1.425627    Objective Loss 1.425627    Top1 52.403846    Top5 84.615385    LR 0.100000    Time 0.097587    
2024-02-17 11:24:48,477 - --- validate (epoch=51)-----------
2024-02-17 11:24:48,478 - 10000 samples (128 per mini-batch)
2024-02-17 11:24:51,426 - Epoch: [51][   79/   79]    Loss 1.779124    Top1 51.110000    Top5 81.550000    
2024-02-17 11:24:51,651 - ==> Top1: 51.110    Top5: 81.550    Loss: 1.779

2024-02-17 11:24:51,669 - ==> Best [Top1: 51.110   Top5: 81.550   Sparsity:0.00   Params: 1341960 on epoch: 51]
2024-02-17 11:24:51,670 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:24:51,762 - 

2024-02-17 11:24:51,762 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:25:02,022 - Epoch: [52][  100/  391]    Overall Loss 1.356018    Objective Loss 1.356018                                        LR 0.100000    Time 0.102523    
2024-02-17 11:25:11,525 - Epoch: [52][  200/  391]    Overall Loss 1.368702    Objective Loss 1.368702                                        LR 0.100000    Time 0.098753    
2024-02-17 11:25:21,188 - Epoch: [52][  300/  391]    Overall Loss 1.390229    Objective Loss 1.390229                                        LR 0.100000    Time 0.098032    
2024-02-17 11:25:29,499 - Epoch: [52][  391/  391]    Overall Loss 1.396170    Objective Loss 1.396170    Top1 62.980769    Top5 86.538462    LR 0.100000    Time 0.096460    
2024-02-17 11:25:29,689 - --- validate (epoch=52)-----------
2024-02-17 11:25:29,690 - 10000 samples (128 per mini-batch)
2024-02-17 11:25:32,513 - Epoch: [52][   79/   79]    Loss 1.752732    Top1 51.550000    Top5 81.970000    
2024-02-17 11:25:32,659 - ==> Top1: 51.550    Top5: 81.970    Loss: 1.753

2024-02-17 11:25:32,677 - ==> Best [Top1: 51.550   Top5: 81.970   Sparsity:0.00   Params: 1341960 on epoch: 52]
2024-02-17 11:25:32,678 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:25:32,769 - 

2024-02-17 11:25:32,769 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:25:42,000 - Epoch: [53][  100/  391]    Overall Loss 1.352209    Objective Loss 1.352209                                        LR 0.100000    Time 0.092237    
2024-02-17 11:25:51,531 - Epoch: [53][  200/  391]    Overall Loss 1.385189    Objective Loss 1.385189                                        LR 0.100000    Time 0.093751    
2024-02-17 11:26:01,010 - Epoch: [53][  300/  391]    Overall Loss 1.395419    Objective Loss 1.395419                                        LR 0.100000    Time 0.094083    
2024-02-17 11:26:09,803 - Epoch: [53][  391/  391]    Overall Loss 1.400876    Objective Loss 1.400876    Top1 61.057692    Top5 88.461538    LR 0.100000    Time 0.094663    
2024-02-17 11:26:09,982 - --- validate (epoch=53)-----------
2024-02-17 11:26:09,982 - 10000 samples (128 per mini-batch)
2024-02-17 11:26:12,844 - Epoch: [53][   79/   79]    Loss 1.756776    Top1 52.330000    Top5 82.320000    
2024-02-17 11:26:13,035 - ==> Top1: 52.330    Top5: 82.320    Loss: 1.757

2024-02-17 11:26:13,053 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:26:13,054 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:26:13,149 - 

2024-02-17 11:26:13,150 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:26:23,192 - Epoch: [54][  100/  391]    Overall Loss 1.365076    Objective Loss 1.365076                                        LR 0.100000    Time 0.100340    
2024-02-17 11:26:32,780 - Epoch: [54][  200/  391]    Overall Loss 1.377045    Objective Loss 1.377045                                        LR 0.100000    Time 0.098090    
2024-02-17 11:26:42,447 - Epoch: [54][  300/  391]    Overall Loss 1.386387    Objective Loss 1.386387                                        LR 0.100000    Time 0.097600    
2024-02-17 11:26:51,398 - Epoch: [54][  391/  391]    Overall Loss 1.388969    Objective Loss 1.388969    Top1 61.057692    Top5 86.538462    LR 0.100000    Time 0.097766    
2024-02-17 11:26:51,630 - --- validate (epoch=54)-----------
2024-02-17 11:26:51,631 - 10000 samples (128 per mini-batch)
2024-02-17 11:26:54,622 - Epoch: [54][   79/   79]    Loss 1.958111    Top1 48.140000    Top5 78.360000    
2024-02-17 11:26:54,820 - ==> Top1: 48.140    Top5: 78.360    Loss: 1.958

2024-02-17 11:26:54,840 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:26:54,840 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:26:54,915 - 

2024-02-17 11:26:54,916 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:27:04,732 - Epoch: [55][  100/  391]    Overall Loss 1.354112    Objective Loss 1.354112                                        LR 0.100000    Time 0.098086    
2024-02-17 11:27:14,304 - Epoch: [55][  200/  391]    Overall Loss 1.363370    Objective Loss 1.363370                                        LR 0.100000    Time 0.096883    
2024-02-17 11:27:23,926 - Epoch: [55][  300/  391]    Overall Loss 1.373081    Objective Loss 1.373081                                        LR 0.100000    Time 0.096646    
2024-02-17 11:27:32,787 - Epoch: [55][  391/  391]    Overall Loss 1.380583    Objective Loss 1.380583    Top1 61.538462    Top5 89.903846    LR 0.100000    Time 0.096806    
2024-02-17 11:27:33,034 - --- validate (epoch=55)-----------
2024-02-17 11:27:33,035 - 10000 samples (128 per mini-batch)
2024-02-17 11:27:36,002 - Epoch: [55][   79/   79]    Loss 1.765769    Top1 51.630000    Top5 82.110000    
2024-02-17 11:27:36,167 - ==> Top1: 51.630    Top5: 82.110    Loss: 1.766

2024-02-17 11:27:36,187 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:27:36,188 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:27:36,264 - 

2024-02-17 11:27:36,265 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:27:46,516 - Epoch: [56][  100/  391]    Overall Loss 1.326227    Objective Loss 1.326227                                        LR 0.100000    Time 0.102440    
2024-02-17 11:27:56,125 - Epoch: [56][  200/  391]    Overall Loss 1.348147    Objective Loss 1.348147                                        LR 0.100000    Time 0.099241    
2024-02-17 11:28:05,278 - Epoch: [56][  300/  391]    Overall Loss 1.367657    Objective Loss 1.367657                                        LR 0.100000    Time 0.096656    
2024-02-17 11:28:11,910 - Epoch: [56][  391/  391]    Overall Loss 1.379064    Objective Loss 1.379064    Top1 57.692308    Top5 86.538462    LR 0.100000    Time 0.091114    
2024-02-17 11:28:12,120 - --- validate (epoch=56)-----------
2024-02-17 11:28:12,121 - 10000 samples (128 per mini-batch)
2024-02-17 11:28:15,075 - Epoch: [56][   79/   79]    Loss 1.785426    Top1 51.600000    Top5 81.980000    
2024-02-17 11:28:15,282 - ==> Top1: 51.600    Top5: 81.980    Loss: 1.785

2024-02-17 11:28:15,305 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:28:15,306 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:28:15,389 - 

2024-02-17 11:28:15,390 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:28:25,939 - Epoch: [57][  100/  391]    Overall Loss 1.349260    Objective Loss 1.349260                                        LR 0.100000    Time 0.105413    
2024-02-17 11:28:35,620 - Epoch: [57][  200/  391]    Overall Loss 1.363172    Objective Loss 1.363172                                        LR 0.100000    Time 0.101091    
2024-02-17 11:28:45,317 - Epoch: [57][  300/  391]    Overall Loss 1.365457    Objective Loss 1.365457                                        LR 0.100000    Time 0.099700    
2024-02-17 11:28:54,144 - Epoch: [57][  391/  391]    Overall Loss 1.366706    Objective Loss 1.366706    Top1 62.019231    Top5 88.942308    LR 0.100000    Time 0.099061    
2024-02-17 11:28:54,321 - --- validate (epoch=57)-----------
2024-02-17 11:28:54,322 - 10000 samples (128 per mini-batch)
2024-02-17 11:28:57,298 - Epoch: [57][   79/   79]    Loss 1.894377    Top1 49.460000    Top5 80.050000    
2024-02-17 11:28:57,499 - ==> Top1: 49.460    Top5: 80.050    Loss: 1.894

2024-02-17 11:28:57,518 - ==> Best [Top1: 52.330   Top5: 82.320   Sparsity:0.00   Params: 1341960 on epoch: 53]
2024-02-17 11:28:57,519 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:28:57,597 - 

2024-02-17 11:28:57,597 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:29:08,256 - Epoch: [58][  100/  391]    Overall Loss 1.325014    Objective Loss 1.325014                                        LR 0.100000    Time 0.106510    
2024-02-17 11:29:17,720 - Epoch: [58][  200/  391]    Overall Loss 1.345946    Objective Loss 1.345946                                        LR 0.100000    Time 0.100552    
2024-02-17 11:29:27,313 - Epoch: [58][  300/  391]    Overall Loss 1.349931    Objective Loss 1.349931                                        LR 0.100000    Time 0.098997    
2024-02-17 11:29:36,449 - Epoch: [58][  391/  391]    Overall Loss 1.360621    Objective Loss 1.360621    Top1 62.980769    Top5 87.019231    LR 0.100000    Time 0.099311    
2024-02-17 11:29:36,650 - --- validate (epoch=58)-----------
2024-02-17 11:29:36,651 - 10000 samples (128 per mini-batch)
2024-02-17 11:29:39,659 - Epoch: [58][   79/   79]    Loss 1.667708    Top1 54.190000    Top5 83.330000    
2024-02-17 11:29:39,830 - ==> Top1: 54.190    Top5: 83.330    Loss: 1.668

2024-02-17 11:29:39,851 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:29:39,852 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:29:39,941 - 

2024-02-17 11:29:39,941 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:29:50,118 - Epoch: [59][  100/  391]    Overall Loss 1.308791    Objective Loss 1.308791                                        LR 0.100000    Time 0.101700    
2024-02-17 11:29:59,569 - Epoch: [59][  200/  391]    Overall Loss 1.326215    Objective Loss 1.326215                                        LR 0.100000    Time 0.098079    
2024-02-17 11:30:09,150 - Epoch: [59][  300/  391]    Overall Loss 1.341077    Objective Loss 1.341077                                        LR 0.100000    Time 0.097311    
2024-02-17 11:30:17,680 - Epoch: [59][  391/  391]    Overall Loss 1.348947    Objective Loss 1.348947    Top1 62.500000    Top5 88.942308    LR 0.100000    Time 0.096468    
2024-02-17 11:30:17,940 - --- validate (epoch=59)-----------
2024-02-17 11:30:17,941 - 10000 samples (128 per mini-batch)
2024-02-17 11:30:20,710 - Epoch: [59][   79/   79]    Loss 1.826966    Top1 50.750000    Top5 81.150000    
2024-02-17 11:30:20,979 - ==> Top1: 50.750    Top5: 81.150    Loss: 1.827

2024-02-17 11:30:20,997 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:30:20,998 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:30:21,077 - 

2024-02-17 11:30:21,077 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:30:31,186 - Epoch: [60][  100/  391]    Overall Loss 1.305665    Objective Loss 1.305665                                        LR 0.100000    Time 0.101015    
2024-02-17 11:30:40,722 - Epoch: [60][  200/  391]    Overall Loss 1.311440    Objective Loss 1.311440                                        LR 0.100000    Time 0.098166    
2024-02-17 11:30:50,369 - Epoch: [60][  300/  391]    Overall Loss 1.328394    Objective Loss 1.328394                                        LR 0.100000    Time 0.097588    
2024-02-17 11:30:58,875 - Epoch: [60][  391/  391]    Overall Loss 1.339107    Objective Loss 1.339107    Top1 63.942308    Top5 88.461538    LR 0.100000    Time 0.096619    
2024-02-17 11:30:59,020 - --- validate (epoch=60)-----------
2024-02-17 11:30:59,021 - 10000 samples (128 per mini-batch)
2024-02-17 11:31:01,795 - Epoch: [60][   79/   79]    Loss 1.691084    Top1 52.590000    Top5 83.400000    
2024-02-17 11:31:01,986 - ==> Top1: 52.590    Top5: 83.400    Loss: 1.691

2024-02-17 11:31:02,005 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:31:02,005 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:31:02,082 - 

2024-02-17 11:31:02,082 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:31:12,652 - Epoch: [61][  100/  391]    Overall Loss 1.296610    Objective Loss 1.296610                                        LR 0.100000    Time 0.105621    
2024-02-17 11:31:22,280 - Epoch: [61][  200/  391]    Overall Loss 1.326394    Objective Loss 1.326394                                        LR 0.100000    Time 0.100931    
2024-02-17 11:31:31,949 - Epoch: [61][  300/  391]    Overall Loss 1.333381    Objective Loss 1.333381                                        LR 0.100000    Time 0.099502    
2024-02-17 11:31:40,482 - Epoch: [61][  391/  391]    Overall Loss 1.339205    Objective Loss 1.339205    Top1 61.538462    Top5 86.538462    LR 0.100000    Time 0.098158    
2024-02-17 11:31:40,638 - --- validate (epoch=61)-----------
2024-02-17 11:31:40,639 - 10000 samples (128 per mini-batch)
2024-02-17 11:31:43,361 - Epoch: [61][   79/   79]    Loss 1.818900    Top1 50.560000    Top5 81.340000    
2024-02-17 11:31:43,539 - ==> Top1: 50.560    Top5: 81.340    Loss: 1.819

2024-02-17 11:31:43,561 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:31:43,561 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:31:43,637 - 

2024-02-17 11:31:43,638 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:31:53,958 - Epoch: [62][  100/  391]    Overall Loss 1.288146    Objective Loss 1.288146                                        LR 0.100000    Time 0.103131    
2024-02-17 11:32:03,650 - Epoch: [62][  200/  391]    Overall Loss 1.302987    Objective Loss 1.302987                                        LR 0.100000    Time 0.100006    
2024-02-17 11:32:13,150 - Epoch: [62][  300/  391]    Overall Loss 1.328770    Objective Loss 1.328770                                        LR 0.100000    Time 0.098322    
2024-02-17 11:32:22,017 - Epoch: [62][  391/  391]    Overall Loss 1.332459    Objective Loss 1.332459    Top1 58.173077    Top5 86.538462    LR 0.100000    Time 0.098106    
2024-02-17 11:32:22,224 - --- validate (epoch=62)-----------
2024-02-17 11:32:22,225 - 10000 samples (128 per mini-batch)
2024-02-17 11:32:24,910 - Epoch: [62][   79/   79]    Loss 1.716351    Top1 52.900000    Top5 82.810000    
2024-02-17 11:32:25,046 - ==> Top1: 52.900    Top5: 82.810    Loss: 1.716

2024-02-17 11:32:25,064 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:32:25,064 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:32:25,161 - 

2024-02-17 11:32:25,161 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:32:35,045 - Epoch: [63][  100/  391]    Overall Loss 1.285306    Objective Loss 1.285306                                        LR 0.100000    Time 0.098768    
2024-02-17 11:32:44,289 - Epoch: [63][  200/  391]    Overall Loss 1.292782    Objective Loss 1.292782                                        LR 0.100000    Time 0.095581    
2024-02-17 11:32:51,468 - Epoch: [63][  300/  391]    Overall Loss 1.312682    Objective Loss 1.312682                                        LR 0.100000    Time 0.087639    
2024-02-17 11:32:58,980 - Epoch: [63][  391/  391]    Overall Loss 1.317608    Objective Loss 1.317608    Top1 61.057692    Top5 87.980769    LR 0.100000    Time 0.086446    
2024-02-17 11:32:59,181 - --- validate (epoch=63)-----------
2024-02-17 11:32:59,181 - 10000 samples (128 per mini-batch)
2024-02-17 11:33:02,165 - Epoch: [63][   79/   79]    Loss 1.956995    Top1 48.160000    Top5 78.860000    
2024-02-17 11:33:02,302 - ==> Top1: 48.160    Top5: 78.860    Loss: 1.957

2024-02-17 11:33:02,312 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:33:02,313 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:33:02,384 - 

2024-02-17 11:33:02,384 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:33:12,874 - Epoch: [64][  100/  391]    Overall Loss 1.297899    Objective Loss 1.297899                                        LR 0.100000    Time 0.104822    
2024-02-17 11:33:22,206 - Epoch: [64][  200/  391]    Overall Loss 1.311936    Objective Loss 1.311936                                        LR 0.100000    Time 0.099047    
2024-02-17 11:33:31,772 - Epoch: [64][  300/  391]    Overall Loss 1.316976    Objective Loss 1.316976                                        LR 0.100000    Time 0.097904    
2024-02-17 11:33:40,601 - Epoch: [64][  391/  391]    Overall Loss 1.320537    Objective Loss 1.320537    Top1 63.461538    Top5 92.307692    LR 0.100000    Time 0.097687    
2024-02-17 11:33:40,739 - --- validate (epoch=64)-----------
2024-02-17 11:33:40,740 - 10000 samples (128 per mini-batch)
2024-02-17 11:33:43,629 - Epoch: [64][   79/   79]    Loss 1.750812    Top1 51.930000    Top5 82.150000    
2024-02-17 11:33:43,762 - ==> Top1: 51.930    Top5: 82.150    Loss: 1.751

2024-02-17 11:33:43,781 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:33:43,781 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:33:43,858 - 

2024-02-17 11:33:43,858 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:33:53,950 - Epoch: [65][  100/  391]    Overall Loss 1.261279    Objective Loss 1.261279                                        LR 0.100000    Time 0.100849    
2024-02-17 11:34:03,937 - Epoch: [65][  200/  391]    Overall Loss 1.285149    Objective Loss 1.285149                                        LR 0.100000    Time 0.100333    
2024-02-17 11:34:13,477 - Epoch: [65][  300/  391]    Overall Loss 1.301239    Objective Loss 1.301239                                        LR 0.100000    Time 0.098677    
2024-02-17 11:34:21,475 - Epoch: [65][  391/  391]    Overall Loss 1.310330    Objective Loss 1.310330    Top1 60.096154    Top5 87.019231    LR 0.100000    Time 0.096155    
2024-02-17 11:34:21,686 - --- validate (epoch=65)-----------
2024-02-17 11:34:21,687 - 10000 samples (128 per mini-batch)
2024-02-17 11:34:24,621 - Epoch: [65][   79/   79]    Loss 1.815695    Top1 51.080000    Top5 80.620000    
2024-02-17 11:34:24,775 - ==> Top1: 51.080    Top5: 80.620    Loss: 1.816

2024-02-17 11:34:24,798 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:34:24,798 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:34:24,884 - 

2024-02-17 11:34:24,885 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:34:35,536 - Epoch: [66][  100/  391]    Overall Loss 1.250810    Objective Loss 1.250810                                        LR 0.100000    Time 0.106424    
2024-02-17 11:34:45,158 - Epoch: [66][  200/  391]    Overall Loss 1.267830    Objective Loss 1.267830                                        LR 0.100000    Time 0.101303    
2024-02-17 11:34:54,806 - Epoch: [66][  300/  391]    Overall Loss 1.283463    Objective Loss 1.283463                                        LR 0.100000    Time 0.099680    
2024-02-17 11:35:03,622 - Epoch: [66][  391/  391]    Overall Loss 1.292746    Objective Loss 1.292746    Top1 49.519231    Top5 80.769231    LR 0.100000    Time 0.099015    
2024-02-17 11:35:03,838 - --- validate (epoch=66)-----------
2024-02-17 11:35:03,839 - 10000 samples (128 per mini-batch)
2024-02-17 11:35:06,895 - Epoch: [66][   79/   79]    Loss 1.934755    Top1 49.240000    Top5 80.590000    
2024-02-17 11:35:07,063 - ==> Top1: 49.240    Top5: 80.590    Loss: 1.935

2024-02-17 11:35:07,078 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:35:07,078 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:35:07,152 - 

2024-02-17 11:35:07,152 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:35:17,068 - Epoch: [67][  100/  391]    Overall Loss 1.269191    Objective Loss 1.269191                                        LR 0.100000    Time 0.099080    
2024-02-17 11:35:25,835 - Epoch: [67][  200/  391]    Overall Loss 1.276007    Objective Loss 1.276007                                        LR 0.100000    Time 0.093354    
2024-02-17 11:35:35,410 - Epoch: [67][  300/  391]    Overall Loss 1.285233    Objective Loss 1.285233                                        LR 0.100000    Time 0.094139    
2024-02-17 11:35:44,054 - Epoch: [67][  391/  391]    Overall Loss 1.298565    Objective Loss 1.298565    Top1 66.346154    Top5 89.423077    LR 0.100000    Time 0.094326    
2024-02-17 11:35:44,246 - --- validate (epoch=67)-----------
2024-02-17 11:35:44,246 - 10000 samples (128 per mini-batch)
2024-02-17 11:35:47,374 - Epoch: [67][   79/   79]    Loss 1.758080    Top1 52.600000    Top5 82.260000    
2024-02-17 11:35:47,505 - ==> Top1: 52.600    Top5: 82.260    Loss: 1.758

2024-02-17 11:35:47,526 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:35:47,526 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:35:47,608 - 

2024-02-17 11:35:47,609 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:35:57,861 - Epoch: [68][  100/  391]    Overall Loss 1.250863    Objective Loss 1.250863                                        LR 0.100000    Time 0.102404    
2024-02-17 11:36:07,424 - Epoch: [68][  200/  391]    Overall Loss 1.268339    Objective Loss 1.268339                                        LR 0.100000    Time 0.098994    
2024-02-17 11:36:17,035 - Epoch: [68][  300/  391]    Overall Loss 1.273696    Objective Loss 1.273696                                        LR 0.100000    Time 0.098019    
2024-02-17 11:36:25,753 - Epoch: [68][  391/  391]    Overall Loss 1.285888    Objective Loss 1.285888    Top1 61.538462    Top5 90.865385    LR 0.100000    Time 0.097494    
2024-02-17 11:36:25,927 - --- validate (epoch=68)-----------
2024-02-17 11:36:25,929 - 10000 samples (128 per mini-batch)
2024-02-17 11:36:29,324 - Epoch: [68][   79/   79]    Loss 1.862298    Top1 50.420000    Top5 80.730000    
2024-02-17 11:36:29,529 - ==> Top1: 50.420    Top5: 80.730    Loss: 1.862

2024-02-17 11:36:29,547 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:36:29,548 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:36:29,620 - 

2024-02-17 11:36:29,620 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:36:39,765 - Epoch: [69][  100/  391]    Overall Loss 1.257213    Objective Loss 1.257213                                        LR 0.100000    Time 0.101380    
2024-02-17 11:36:49,304 - Epoch: [69][  200/  391]    Overall Loss 1.259300    Objective Loss 1.259300                                        LR 0.100000    Time 0.098364    
2024-02-17 11:36:58,855 - Epoch: [69][  300/  391]    Overall Loss 1.275271    Objective Loss 1.275271                                        LR 0.100000    Time 0.097395    
2024-02-17 11:37:07,448 - Epoch: [69][  391/  391]    Overall Loss 1.284235    Objective Loss 1.284235    Top1 63.942308    Top5 87.980769    LR 0.100000    Time 0.096694    
2024-02-17 11:37:07,653 - --- validate (epoch=69)-----------
2024-02-17 11:37:07,654 - 10000 samples (128 per mini-batch)
2024-02-17 11:37:10,558 - Epoch: [69][   79/   79]    Loss 1.776082    Top1 51.820000    Top5 82.050000    
2024-02-17 11:37:10,768 - ==> Top1: 51.820    Top5: 82.050    Loss: 1.776

2024-02-17 11:37:10,789 - ==> Best [Top1: 54.190   Top5: 83.330   Sparsity:0.00   Params: 1341960 on epoch: 58]
2024-02-17 11:37:10,789 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:37:10,879 - 

2024-02-17 11:37:10,880 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:37:21,345 - Epoch: [70][  100/  391]    Overall Loss 1.246878    Objective Loss 1.246878                                        LR 0.100000    Time 0.104572    
2024-02-17 11:37:30,891 - Epoch: [70][  200/  391]    Overall Loss 1.248874    Objective Loss 1.248874                                        LR 0.100000    Time 0.099994    
2024-02-17 11:37:40,494 - Epoch: [70][  300/  391]    Overall Loss 1.265239    Objective Loss 1.265239                                        LR 0.100000    Time 0.098656    
2024-02-17 11:37:48,981 - Epoch: [70][  391/  391]    Overall Loss 1.268964    Objective Loss 1.268964    Top1 63.461538    Top5 88.461538    LR 0.100000    Time 0.097390    
2024-02-17 11:37:49,153 - --- validate (epoch=70)-----------
2024-02-17 11:37:49,154 - 10000 samples (128 per mini-batch)
2024-02-17 11:37:52,240 - Epoch: [70][   79/   79]    Loss 1.651100    Top1 54.200000    Top5 84.270000    
2024-02-17 11:37:52,397 - ==> Top1: 54.200    Top5: 84.270    Loss: 1.651

2024-02-17 11:37:52,415 - ==> Best [Top1: 54.200   Top5: 84.270   Sparsity:0.00   Params: 1341960 on epoch: 70]
2024-02-17 11:37:52,415 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:37:52,508 - 

2024-02-17 11:37:52,508 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:38:02,694 - Epoch: [71][  100/  391]    Overall Loss 1.214183    Objective Loss 1.214183                                        LR 0.100000    Time 0.101789    
2024-02-17 11:38:12,241 - Epoch: [71][  200/  391]    Overall Loss 1.237423    Objective Loss 1.237423                                        LR 0.100000    Time 0.098607    
2024-02-17 11:38:21,887 - Epoch: [71][  300/  391]    Overall Loss 1.252054    Objective Loss 1.252054                                        LR 0.100000    Time 0.097876    
2024-02-17 11:38:30,394 - Epoch: [71][  391/  391]    Overall Loss 1.269942    Objective Loss 1.269942    Top1 62.500000    Top5 92.307692    LR 0.100000    Time 0.096843    
2024-02-17 11:38:30,582 - --- validate (epoch=71)-----------
2024-02-17 11:38:30,583 - 10000 samples (128 per mini-batch)
2024-02-17 11:38:33,433 - Epoch: [71][   79/   79]    Loss 1.664598    Top1 54.580000    Top5 83.680000    
2024-02-17 11:38:33,596 - ==> Top1: 54.580    Top5: 83.680    Loss: 1.665

2024-02-17 11:38:33,615 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:38:33,615 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:38:33,712 - 

2024-02-17 11:38:33,712 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:38:44,312 - Epoch: [72][  100/  391]    Overall Loss 1.228445    Objective Loss 1.228445                                        LR 0.100000    Time 0.105918    
2024-02-17 11:38:53,906 - Epoch: [72][  200/  391]    Overall Loss 1.231338    Objective Loss 1.231338                                        LR 0.100000    Time 0.100909    
2024-02-17 11:39:03,191 - Epoch: [72][  300/  391]    Overall Loss 1.256302    Objective Loss 1.256302                                        LR 0.100000    Time 0.098206    
2024-02-17 11:39:11,599 - Epoch: [72][  391/  391]    Overall Loss 1.258400    Objective Loss 1.258400    Top1 60.576923    Top5 88.461538    LR 0.100000    Time 0.096842    
2024-02-17 11:39:11,807 - --- validate (epoch=72)-----------
2024-02-17 11:39:11,807 - 10000 samples (128 per mini-batch)
2024-02-17 11:39:14,842 - Epoch: [72][   79/   79]    Loss 1.766127    Top1 52.280000    Top5 81.960000    
2024-02-17 11:39:14,988 - ==> Top1: 52.280    Top5: 81.960    Loss: 1.766

2024-02-17 11:39:15,008 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:39:15,008 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:39:15,084 - 

2024-02-17 11:39:15,085 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:39:25,338 - Epoch: [73][  100/  391]    Overall Loss 1.235244    Objective Loss 1.235244                                        LR 0.100000    Time 0.102458    
2024-02-17 11:39:34,945 - Epoch: [73][  200/  391]    Overall Loss 1.225271    Objective Loss 1.225271                                        LR 0.100000    Time 0.099245    
2024-02-17 11:39:44,483 - Epoch: [73][  300/  391]    Overall Loss 1.248749    Objective Loss 1.248749                                        LR 0.100000    Time 0.097940    
2024-02-17 11:39:52,980 - Epoch: [73][  391/  391]    Overall Loss 1.258203    Objective Loss 1.258203    Top1 62.019231    Top5 92.307692    LR 0.100000    Time 0.096866    
2024-02-17 11:39:53,156 - --- validate (epoch=73)-----------
2024-02-17 11:39:53,156 - 10000 samples (128 per mini-batch)
2024-02-17 11:39:55,960 - Epoch: [73][   79/   79]    Loss 1.792526    Top1 52.330000    Top5 81.370000    
2024-02-17 11:39:56,175 - ==> Top1: 52.330    Top5: 81.370    Loss: 1.793

2024-02-17 11:39:56,193 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:39:56,193 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:39:56,268 - 

2024-02-17 11:39:56,268 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:40:06,758 - Epoch: [74][  100/  391]    Overall Loss 1.213928    Objective Loss 1.213928                                        LR 0.100000    Time 0.104823    
2024-02-17 11:40:16,262 - Epoch: [74][  200/  391]    Overall Loss 1.237431    Objective Loss 1.237431                                        LR 0.100000    Time 0.099910    
2024-02-17 11:40:25,960 - Epoch: [74][  300/  391]    Overall Loss 1.250356    Objective Loss 1.250356                                        LR 0.100000    Time 0.098918    
2024-02-17 11:40:34,852 - Epoch: [74][  391/  391]    Overall Loss 1.257349    Objective Loss 1.257349    Top1 62.980769    Top5 90.865385    LR 0.100000    Time 0.098627    
2024-02-17 11:40:35,024 - --- validate (epoch=74)-----------
2024-02-17 11:40:35,025 - 10000 samples (128 per mini-batch)
2024-02-17 11:40:37,810 - Epoch: [74][   79/   79]    Loss 1.893712    Top1 50.480000    Top5 80.250000    
2024-02-17 11:40:37,947 - ==> Top1: 50.480    Top5: 80.250    Loss: 1.894

2024-02-17 11:40:37,971 - ==> Best [Top1: 54.580   Top5: 83.680   Sparsity:0.00   Params: 1341960 on epoch: 71]
2024-02-17 11:40:37,971 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:40:38,046 - 

2024-02-17 11:40:38,047 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:40:48,213 - Epoch: [75][  100/  391]    Overall Loss 1.223472    Objective Loss 1.223472                                        LR 0.100000    Time 0.101589    
2024-02-17 11:40:57,711 - Epoch: [75][  200/  391]    Overall Loss 1.242622    Objective Loss 1.242622                                        LR 0.100000    Time 0.098265    
2024-02-17 11:41:07,497 - Epoch: [75][  300/  391]    Overall Loss 1.249421    Objective Loss 1.249421                                        LR 0.100000    Time 0.098114    
2024-02-17 11:41:15,545 - Epoch: [75][  391/  391]    Overall Loss 1.251632    Objective Loss 1.251632    Top1 62.019231    Top5 87.500000    LR 0.100000    Time 0.095853    
2024-02-17 11:41:15,738 - --- validate (epoch=75)-----------
2024-02-17 11:41:15,739 - 10000 samples (128 per mini-batch)
2024-02-17 11:41:18,479 - Epoch: [75][   79/   79]    Loss 1.653270    Top1 54.870000    Top5 83.890000    
2024-02-17 11:41:18,644 - ==> Top1: 54.870    Top5: 83.890    Loss: 1.653

2024-02-17 11:41:18,668 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:41:18,668 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:41:18,759 - 

2024-02-17 11:41:18,759 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:41:29,274 - Epoch: [76][  100/  391]    Overall Loss 1.170750    Objective Loss 1.170750                                        LR 0.100000    Time 0.105070    
2024-02-17 11:41:38,746 - Epoch: [76][  200/  391]    Overall Loss 1.213522    Objective Loss 1.213522                                        LR 0.100000    Time 0.099875    
2024-02-17 11:41:48,365 - Epoch: [76][  300/  391]    Overall Loss 1.231686    Objective Loss 1.231686                                        LR 0.100000    Time 0.098629    
2024-02-17 11:41:56,381 - Epoch: [76][  391/  391]    Overall Loss 1.240826    Objective Loss 1.240826    Top1 67.788462    Top5 90.865385    LR 0.100000    Time 0.096168    
2024-02-17 11:41:56,564 - --- validate (epoch=76)-----------
2024-02-17 11:41:56,565 - 10000 samples (128 per mini-batch)
2024-02-17 11:41:59,679 - Epoch: [76][   79/   79]    Loss 1.814758    Top1 51.530000    Top5 81.440000    
2024-02-17 11:41:59,822 - ==> Top1: 51.530    Top5: 81.440    Loss: 1.815

2024-02-17 11:41:59,841 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:41:59,842 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:41:59,921 - 

2024-02-17 11:41:59,922 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:42:10,264 - Epoch: [77][  100/  391]    Overall Loss 1.180127    Objective Loss 1.180127                                        LR 0.100000    Time 0.103354    
2024-02-17 11:42:19,852 - Epoch: [77][  200/  391]    Overall Loss 1.209763    Objective Loss 1.209763                                        LR 0.100000    Time 0.099593    
2024-02-17 11:42:29,991 - Epoch: [77][  300/  391]    Overall Loss 1.223902    Objective Loss 1.223902                                        LR 0.100000    Time 0.100176    
2024-02-17 11:42:38,737 - Epoch: [77][  391/  391]    Overall Loss 1.233894    Objective Loss 1.233894    Top1 61.538462    Top5 92.307692    LR 0.100000    Time 0.099220    
2024-02-17 11:42:38,875 - --- validate (epoch=77)-----------
2024-02-17 11:42:38,876 - 10000 samples (128 per mini-batch)
2024-02-17 11:42:41,530 - Epoch: [77][   79/   79]    Loss 1.657645    Top1 54.100000    Top5 83.660000    
2024-02-17 11:42:41,667 - ==> Top1: 54.100    Top5: 83.660    Loss: 1.658

2024-02-17 11:42:41,685 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:42:41,686 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:42:41,762 - 

2024-02-17 11:42:41,762 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:42:52,077 - Epoch: [78][  100/  391]    Overall Loss 1.201644    Objective Loss 1.201644                                        LR 0.100000    Time 0.103076    
2024-02-17 11:43:01,613 - Epoch: [78][  200/  391]    Overall Loss 1.203288    Objective Loss 1.203288                                        LR 0.100000    Time 0.099193    
2024-02-17 11:43:10,938 - Epoch: [78][  300/  391]    Overall Loss 1.218297    Objective Loss 1.218297                                        LR 0.100000    Time 0.097200    
2024-02-17 11:43:19,505 - Epoch: [78][  391/  391]    Overall Loss 1.222866    Objective Loss 1.222866    Top1 61.057692    Top5 92.307692    LR 0.100000    Time 0.096479    
2024-02-17 11:43:19,651 - --- validate (epoch=78)-----------
2024-02-17 11:43:19,651 - 10000 samples (128 per mini-batch)
2024-02-17 11:43:22,273 - Epoch: [78][   79/   79]    Loss 1.797887    Top1 52.410000    Top5 81.490000    
2024-02-17 11:43:22,474 - ==> Top1: 52.410    Top5: 81.490    Loss: 1.798

2024-02-17 11:43:22,492 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:43:22,493 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:43:22,567 - 

2024-02-17 11:43:22,567 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:43:32,683 - Epoch: [79][  100/  391]    Overall Loss 1.185781    Objective Loss 1.185781                                        LR 0.100000    Time 0.101090    
2024-02-17 11:43:42,194 - Epoch: [79][  200/  391]    Overall Loss 1.206895    Objective Loss 1.206895                                        LR 0.100000    Time 0.098075    
2024-02-17 11:43:51,311 - Epoch: [79][  300/  391]    Overall Loss 1.223145    Objective Loss 1.223145                                        LR 0.100000    Time 0.095761    
2024-02-17 11:43:59,716 - Epoch: [79][  391/  391]    Overall Loss 1.228430    Objective Loss 1.228430    Top1 60.576923    Top5 89.903846    LR 0.100000    Time 0.094960    
2024-02-17 11:43:59,961 - --- validate (epoch=79)-----------
2024-02-17 11:43:59,961 - 10000 samples (128 per mini-batch)
2024-02-17 11:44:02,631 - Epoch: [79][   79/   79]    Loss 1.761844    Top1 52.140000    Top5 82.710000    
2024-02-17 11:44:02,751 - ==> Top1: 52.140    Top5: 82.710    Loss: 1.762

2024-02-17 11:44:02,770 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:44:02,770 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:44:02,847 - 

2024-02-17 11:44:02,848 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:44:13,058 - Epoch: [80][  100/  391]    Overall Loss 1.183672    Objective Loss 1.183672                                        LR 0.100000    Time 0.102026    
2024-02-17 11:44:22,575 - Epoch: [80][  200/  391]    Overall Loss 1.188290    Objective Loss 1.188290                                        LR 0.100000    Time 0.098576    
2024-02-17 11:44:31,726 - Epoch: [80][  300/  391]    Overall Loss 1.207868    Objective Loss 1.207868                                        LR 0.100000    Time 0.096208    
2024-02-17 11:44:40,004 - Epoch: [80][  391/  391]    Overall Loss 1.216519    Objective Loss 1.216519    Top1 59.615385    Top5 87.019231    LR 0.100000    Time 0.094977    
2024-02-17 11:44:40,228 - --- validate (epoch=80)-----------
2024-02-17 11:44:40,230 - 10000 samples (128 per mini-batch)
2024-02-17 11:44:43,138 - Epoch: [80][   79/   79]    Loss 1.780596    Top1 51.910000    Top5 82.270000    
2024-02-17 11:44:43,347 - ==> Top1: 51.910    Top5: 82.270    Loss: 1.781

2024-02-17 11:44:43,365 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:44:43,366 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:44:43,443 - 

2024-02-17 11:44:43,443 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:44:53,831 - Epoch: [81][  100/  391]    Overall Loss 1.154308    Objective Loss 1.154308                                        LR 0.100000    Time 0.103802    
2024-02-17 11:45:03,338 - Epoch: [81][  200/  391]    Overall Loss 1.192567    Objective Loss 1.192567                                        LR 0.100000    Time 0.099413    
2024-02-17 11:45:12,864 - Epoch: [81][  300/  391]    Overall Loss 1.212697    Objective Loss 1.212697                                        LR 0.100000    Time 0.098011    
2024-02-17 11:45:21,194 - Epoch: [81][  391/  391]    Overall Loss 1.216576    Objective Loss 1.216576    Top1 65.865385    Top5 88.461538    LR 0.100000    Time 0.096496    
2024-02-17 11:45:21,407 - --- validate (epoch=81)-----------
2024-02-17 11:45:21,408 - 10000 samples (128 per mini-batch)
2024-02-17 11:45:24,275 - Epoch: [81][   79/   79]    Loss 1.901255    Top1 50.050000    Top5 80.380000    
2024-02-17 11:45:24,493 - ==> Top1: 50.050    Top5: 80.380    Loss: 1.901

2024-02-17 11:45:24,512 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:45:24,512 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:45:24,586 - 

2024-02-17 11:45:24,587 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:45:35,060 - Epoch: [82][  100/  391]    Overall Loss 1.181685    Objective Loss 1.181685                                        LR 0.100000    Time 0.104663    
2024-02-17 11:45:44,549 - Epoch: [82][  200/  391]    Overall Loss 1.191637    Objective Loss 1.191637                                        LR 0.100000    Time 0.099752    
2024-02-17 11:45:54,058 - Epoch: [82][  300/  391]    Overall Loss 1.195987    Objective Loss 1.195987                                        LR 0.100000    Time 0.098183    
2024-02-17 11:46:02,242 - Epoch: [82][  391/  391]    Overall Loss 1.205439    Objective Loss 1.205439    Top1 65.384615    Top5 88.461538    LR 0.100000    Time 0.096255    
2024-02-17 11:46:02,462 - --- validate (epoch=82)-----------
2024-02-17 11:46:02,463 - 10000 samples (128 per mini-batch)
2024-02-17 11:46:05,224 - Epoch: [82][   79/   79]    Loss 1.780336    Top1 52.640000    Top5 82.950000    
2024-02-17 11:46:05,442 - ==> Top1: 52.640    Top5: 82.950    Loss: 1.780

2024-02-17 11:46:05,452 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:46:05,453 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:46:05,529 - 

2024-02-17 11:46:05,529 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:46:15,647 - Epoch: [83][  100/  391]    Overall Loss 1.189221    Objective Loss 1.189221                                        LR 0.100000    Time 0.101104    
2024-02-17 11:46:25,264 - Epoch: [83][  200/  391]    Overall Loss 1.189462    Objective Loss 1.189462                                        LR 0.100000    Time 0.098617    
2024-02-17 11:46:34,928 - Epoch: [83][  300/  391]    Overall Loss 1.210404    Objective Loss 1.210404                                        LR 0.100000    Time 0.097941    
2024-02-17 11:46:42,745 - Epoch: [83][  391/  391]    Overall Loss 1.212851    Objective Loss 1.212851    Top1 72.596154    Top5 90.865385    LR 0.100000    Time 0.095129    
2024-02-17 11:46:42,945 - --- validate (epoch=83)-----------
2024-02-17 11:46:42,946 - 10000 samples (128 per mini-batch)
2024-02-17 11:46:45,568 - Epoch: [83][   79/   79]    Loss 1.912212    Top1 49.810000    Top5 80.350000    
2024-02-17 11:46:45,713 - ==> Top1: 49.810    Top5: 80.350    Loss: 1.912

2024-02-17 11:46:45,732 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:46:45,733 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:46:45,809 - 

2024-02-17 11:46:45,809 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:46:56,124 - Epoch: [84][  100/  391]    Overall Loss 1.144011    Objective Loss 1.144011                                        LR 0.100000    Time 0.103079    
2024-02-17 11:47:05,985 - Epoch: [84][  200/  391]    Overall Loss 1.156792    Objective Loss 1.156792                                        LR 0.100000    Time 0.100820    
2024-02-17 11:47:15,701 - Epoch: [84][  300/  391]    Overall Loss 1.189619    Objective Loss 1.189619                                        LR 0.100000    Time 0.099586    
2024-02-17 11:47:24,199 - Epoch: [84][  391/  391]    Overall Loss 1.205060    Objective Loss 1.205060    Top1 65.384615    Top5 91.346154    LR 0.100000    Time 0.098133    
2024-02-17 11:47:24,386 - --- validate (epoch=84)-----------
2024-02-17 11:47:24,388 - 10000 samples (128 per mini-batch)
2024-02-17 11:47:27,265 - Epoch: [84][   79/   79]    Loss 1.722928    Top1 53.200000    Top5 83.280000    
2024-02-17 11:47:27,431 - ==> Top1: 53.200    Top5: 83.280    Loss: 1.723

2024-02-17 11:47:27,446 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:47:27,446 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:47:27,526 - 

2024-02-17 11:47:27,527 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:47:37,625 - Epoch: [85][  100/  391]    Overall Loss 1.142040    Objective Loss 1.142040                                        LR 0.100000    Time 0.100914    
2024-02-17 11:47:46,414 - Epoch: [85][  200/  391]    Overall Loss 1.177139    Objective Loss 1.177139                                        LR 0.100000    Time 0.094381    
2024-02-17 11:47:55,732 - Epoch: [85][  300/  391]    Overall Loss 1.182821    Objective Loss 1.182821                                        LR 0.100000    Time 0.093967    
2024-02-17 11:48:04,274 - Epoch: [85][  391/  391]    Overall Loss 1.191620    Objective Loss 1.191620    Top1 67.788462    Top5 89.423077    LR 0.100000    Time 0.093932    
2024-02-17 11:48:04,478 - --- validate (epoch=85)-----------
2024-02-17 11:48:04,479 - 10000 samples (128 per mini-batch)
2024-02-17 11:48:07,192 - Epoch: [85][   79/   79]    Loss 1.687304    Top1 54.170000    Top5 83.350000    
2024-02-17 11:48:07,316 - ==> Top1: 54.170    Top5: 83.350    Loss: 1.687

2024-02-17 11:48:07,333 - ==> Best [Top1: 54.870   Top5: 83.890   Sparsity:0.00   Params: 1341960 on epoch: 75]
2024-02-17 11:48:07,334 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:48:07,409 - 

2024-02-17 11:48:07,410 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:48:17,524 - Epoch: [86][  100/  391]    Overall Loss 1.130520    Objective Loss 1.130520                                        LR 0.100000    Time 0.101064    
2024-02-17 11:48:27,180 - Epoch: [86][  200/  391]    Overall Loss 1.160144    Objective Loss 1.160144                                        LR 0.100000    Time 0.098789    
2024-02-17 11:48:37,053 - Epoch: [86][  300/  391]    Overall Loss 1.178367    Objective Loss 1.178367                                        LR 0.100000    Time 0.098757    
2024-02-17 11:48:45,520 - Epoch: [86][  391/  391]    Overall Loss 1.190103    Objective Loss 1.190103    Top1 63.942308    Top5 87.980769    LR 0.100000    Time 0.097416    
2024-02-17 11:48:45,719 - --- validate (epoch=86)-----------
2024-02-17 11:48:45,720 - 10000 samples (128 per mini-batch)
2024-02-17 11:48:48,657 - Epoch: [86][   79/   79]    Loss 1.617547    Top1 55.120000    Top5 84.970000    
2024-02-17 11:48:48,867 - ==> Top1: 55.120    Top5: 84.970    Loss: 1.618

2024-02-17 11:48:48,885 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:48:48,885 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:48:48,977 - 

2024-02-17 11:48:48,978 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:48:59,117 - Epoch: [87][  100/  391]    Overall Loss 1.147955    Objective Loss 1.147955                                        LR 0.100000    Time 0.101320    
2024-02-17 11:49:08,960 - Epoch: [87][  200/  391]    Overall Loss 1.160226    Objective Loss 1.160226                                        LR 0.100000    Time 0.099852    
2024-02-17 11:49:18,443 - Epoch: [87][  300/  391]    Overall Loss 1.174703    Objective Loss 1.174703                                        LR 0.100000    Time 0.098162    
2024-02-17 11:49:27,009 - Epoch: [87][  391/  391]    Overall Loss 1.181049    Objective Loss 1.181049    Top1 60.096154    Top5 89.423077    LR 0.100000    Time 0.097214    
2024-02-17 11:49:27,285 - --- validate (epoch=87)-----------
2024-02-17 11:49:27,286 - 10000 samples (128 per mini-batch)
2024-02-17 11:49:30,051 - Epoch: [87][   79/   79]    Loss 1.951335    Top1 49.510000    Top5 80.040000    
2024-02-17 11:49:30,243 - ==> Top1: 49.510    Top5: 80.040    Loss: 1.951

2024-02-17 11:49:30,264 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:49:30,264 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:49:30,349 - 

2024-02-17 11:49:30,349 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:49:38,991 - Epoch: [88][  100/  391]    Overall Loss 1.139977    Objective Loss 1.139977                                        LR 0.100000    Time 0.086351    
2024-02-17 11:49:48,221 - Epoch: [88][  200/  391]    Overall Loss 1.153389    Objective Loss 1.153389                                        LR 0.100000    Time 0.089304    
2024-02-17 11:49:57,884 - Epoch: [88][  300/  391]    Overall Loss 1.167938    Objective Loss 1.167938                                        LR 0.100000    Time 0.091731    
2024-02-17 11:50:06,540 - Epoch: [88][  391/  391]    Overall Loss 1.181036    Objective Loss 1.181036    Top1 69.230769    Top5 93.269231    LR 0.100000    Time 0.092509    
2024-02-17 11:50:06,752 - --- validate (epoch=88)-----------
2024-02-17 11:50:06,753 - 10000 samples (128 per mini-batch)
2024-02-17 11:50:09,436 - Epoch: [88][   79/   79]    Loss 1.785244    Top1 52.210000    Top5 82.760000    
2024-02-17 11:50:09,583 - ==> Top1: 52.210    Top5: 82.760    Loss: 1.785

2024-02-17 11:50:09,602 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:50:09,603 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:50:09,677 - 

2024-02-17 11:50:09,677 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:50:19,926 - Epoch: [89][  100/  391]    Overall Loss 1.127520    Objective Loss 1.127520                                        LR 0.100000    Time 0.102413    
2024-02-17 11:50:29,436 - Epoch: [89][  200/  391]    Overall Loss 1.147720    Objective Loss 1.147720                                        LR 0.100000    Time 0.098735    
2024-02-17 11:50:39,004 - Epoch: [89][  300/  391]    Overall Loss 1.166428    Objective Loss 1.166428                                        LR 0.100000    Time 0.097703    
2024-02-17 11:50:47,600 - Epoch: [89][  391/  391]    Overall Loss 1.177647    Objective Loss 1.177647    Top1 65.384615    Top5 89.423077    LR 0.100000    Time 0.096938    
2024-02-17 11:50:47,823 - --- validate (epoch=89)-----------
2024-02-17 11:50:47,824 - 10000 samples (128 per mini-batch)
2024-02-17 11:50:50,544 - Epoch: [89][   79/   79]    Loss 1.973575    Top1 50.370000    Top5 78.920000    
2024-02-17 11:50:50,746 - ==> Top1: 50.370    Top5: 78.920    Loss: 1.974

2024-02-17 11:50:50,765 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:50:50,766 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:50:50,840 - 

2024-02-17 11:50:50,841 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:51:00,985 - Epoch: [90][  100/  391]    Overall Loss 1.121073    Objective Loss 1.121073                                        LR 0.100000    Time 0.101368    
2024-02-17 11:51:10,417 - Epoch: [90][  200/  391]    Overall Loss 1.145805    Objective Loss 1.145805                                        LR 0.100000    Time 0.097824    
2024-02-17 11:51:19,957 - Epoch: [90][  300/  391]    Overall Loss 1.156484    Objective Loss 1.156484                                        LR 0.100000    Time 0.096999    
2024-02-17 11:51:28,628 - Epoch: [90][  391/  391]    Overall Loss 1.164850    Objective Loss 1.164850    Top1 64.423077    Top5 89.423077    LR 0.100000    Time 0.096592    
2024-02-17 11:51:28,920 - --- validate (epoch=90)-----------
2024-02-17 11:51:28,921 - 10000 samples (128 per mini-batch)
2024-02-17 11:51:32,016 - Epoch: [90][   79/   79]    Loss 1.671971    Top1 54.660000    Top5 83.970000    
2024-02-17 11:51:32,163 - ==> Top1: 54.660    Top5: 83.970    Loss: 1.672

2024-02-17 11:51:32,181 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:51:32,181 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:51:32,259 - 

2024-02-17 11:51:32,260 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:51:42,408 - Epoch: [91][  100/  391]    Overall Loss 1.106952    Objective Loss 1.106952                                        LR 0.100000    Time 0.101408    
2024-02-17 11:51:51,701 - Epoch: [91][  200/  391]    Overall Loss 1.141084    Objective Loss 1.141084                                        LR 0.100000    Time 0.097147    
2024-02-17 11:52:01,285 - Epoch: [91][  300/  391]    Overall Loss 1.156907    Objective Loss 1.156907                                        LR 0.100000    Time 0.096698    
2024-02-17 11:52:09,806 - Epoch: [91][  391/  391]    Overall Loss 1.162098    Objective Loss 1.162098    Top1 61.538462    Top5 86.538462    LR 0.100000    Time 0.095975    
2024-02-17 11:52:10,003 - --- validate (epoch=91)-----------
2024-02-17 11:52:10,004 - 10000 samples (128 per mini-batch)
2024-02-17 11:52:12,663 - Epoch: [91][   79/   79]    Loss 1.832772    Top1 52.330000    Top5 81.380000    
2024-02-17 11:52:12,819 - ==> Top1: 52.330    Top5: 81.380    Loss: 1.833

2024-02-17 11:52:12,830 - ==> Best [Top1: 55.120   Top5: 84.970   Sparsity:0.00   Params: 1341960 on epoch: 86]
2024-02-17 11:52:12,830 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:52:12,935 - 

2024-02-17 11:52:12,935 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:52:23,321 - Epoch: [92][  100/  391]    Overall Loss 1.128679    Objective Loss 1.128679                                        LR 0.100000    Time 0.103791    
2024-02-17 11:52:32,695 - Epoch: [92][  200/  391]    Overall Loss 1.136649    Objective Loss 1.136649                                        LR 0.100000    Time 0.098743    
2024-02-17 11:52:42,451 - Epoch: [92][  300/  391]    Overall Loss 1.152592    Objective Loss 1.152592                                        LR 0.100000    Time 0.098332    
2024-02-17 11:52:51,053 - Epoch: [92][  391/  391]    Overall Loss 1.165580    Objective Loss 1.165580    Top1 61.057692    Top5 88.461538    LR 0.100000    Time 0.097436    
2024-02-17 11:52:51,245 - --- validate (epoch=92)-----------
2024-02-17 11:52:51,246 - 10000 samples (128 per mini-batch)
2024-02-17 11:52:53,925 - Epoch: [92][   79/   79]    Loss 1.663866    Top1 55.390000    Top5 84.270000    
2024-02-17 11:52:54,147 - ==> Top1: 55.390    Top5: 84.270    Loss: 1.664

2024-02-17 11:52:54,167 - ==> Best [Top1: 55.390   Top5: 84.270   Sparsity:0.00   Params: 1341960 on epoch: 92]
2024-02-17 11:52:54,168 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:52:54,263 - 

2024-02-17 11:52:54,263 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:53:04,021 - Epoch: [93][  100/  391]    Overall Loss 1.132603    Objective Loss 1.132603                                        LR 0.100000    Time 0.097512    
2024-02-17 11:53:12,606 - Epoch: [93][  200/  391]    Overall Loss 1.157660    Objective Loss 1.157660                                        LR 0.100000    Time 0.091655    
2024-02-17 11:53:22,274 - Epoch: [93][  300/  391]    Overall Loss 1.163294    Objective Loss 1.163294                                        LR 0.100000    Time 0.093316    
2024-02-17 11:53:30,905 - Epoch: [93][  391/  391]    Overall Loss 1.168071    Objective Loss 1.168071    Top1 64.903846    Top5 92.307692    LR 0.100000    Time 0.093661    
2024-02-17 11:53:31,194 - --- validate (epoch=93)-----------
2024-02-17 11:53:31,194 - 10000 samples (128 per mini-batch)
2024-02-17 11:53:34,000 - Epoch: [93][   79/   79]    Loss 1.778660    Top1 53.460000    Top5 82.040000    
2024-02-17 11:53:34,130 - ==> Top1: 53.460    Top5: 82.040    Loss: 1.779

2024-02-17 11:53:34,148 - ==> Best [Top1: 55.390   Top5: 84.270   Sparsity:0.00   Params: 1341960 on epoch: 92]
2024-02-17 11:53:34,149 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:53:34,230 - 

2024-02-17 11:53:34,230 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:53:44,710 - Epoch: [94][  100/  391]    Overall Loss 1.124040    Objective Loss 1.124040                                        LR 0.100000    Time 0.104722    
2024-02-17 11:53:54,108 - Epoch: [94][  200/  391]    Overall Loss 1.130566    Objective Loss 1.130566                                        LR 0.100000    Time 0.099331    
2024-02-17 11:54:02,980 - Epoch: [94][  300/  391]    Overall Loss 1.136517    Objective Loss 1.136517                                        LR 0.100000    Time 0.095778    
2024-02-17 11:54:10,718 - Epoch: [94][  391/  391]    Overall Loss 1.153137    Objective Loss 1.153137    Top1 62.019231    Top5 90.865385    LR 0.100000    Time 0.093269    
2024-02-17 11:54:11,036 - --- validate (epoch=94)-----------
2024-02-17 11:54:11,037 - 10000 samples (128 per mini-batch)
2024-02-17 11:54:13,835 - Epoch: [94][   79/   79]    Loss 1.611510    Top1 55.400000    Top5 84.510000    
2024-02-17 11:54:13,971 - ==> Top1: 55.400    Top5: 84.510    Loss: 1.612

2024-02-17 11:54:13,990 - ==> Best [Top1: 55.400   Top5: 84.510   Sparsity:0.00   Params: 1341960 on epoch: 94]
2024-02-17 11:54:13,991 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:54:14,090 - 

2024-02-17 11:54:14,090 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:54:22,150 - Epoch: [95][  100/  391]    Overall Loss 1.102432    Objective Loss 1.102432                                        LR 0.100000    Time 0.080533    
2024-02-17 11:54:29,543 - Epoch: [95][  200/  391]    Overall Loss 1.122185    Objective Loss 1.122185                                        LR 0.100000    Time 0.077215    
2024-02-17 11:54:38,655 - Epoch: [95][  300/  391]    Overall Loss 1.136620    Objective Loss 1.136620                                        LR 0.100000    Time 0.081833    
2024-02-17 11:54:47,300 - Epoch: [95][  391/  391]    Overall Loss 1.147500    Objective Loss 1.147500    Top1 66.826923    Top5 92.307692    LR 0.100000    Time 0.084887    
2024-02-17 11:54:47,525 - --- validate (epoch=95)-----------
2024-02-17 11:54:47,526 - 10000 samples (128 per mini-batch)
2024-02-17 11:54:50,301 - Epoch: [95][   79/   79]    Loss 1.925001    Top1 49.310000    Top5 79.670000    
2024-02-17 11:54:50,463 - ==> Top1: 49.310    Top5: 79.670    Loss: 1.925

2024-02-17 11:54:50,482 - ==> Best [Top1: 55.400   Top5: 84.510   Sparsity:0.00   Params: 1341960 on epoch: 94]
2024-02-17 11:54:50,483 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:54:50,557 - 

2024-02-17 11:54:50,558 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:55:00,957 - Epoch: [96][  100/  391]    Overall Loss 1.119518    Objective Loss 1.119518                                        LR 0.100000    Time 0.103923    
2024-02-17 11:55:10,422 - Epoch: [96][  200/  391]    Overall Loss 1.122402    Objective Loss 1.122402                                        LR 0.100000    Time 0.099263    
2024-02-17 11:55:19,756 - Epoch: [96][  300/  391]    Overall Loss 1.134191    Objective Loss 1.134191                                        LR 0.100000    Time 0.097274    
2024-02-17 11:55:28,188 - Epoch: [96][  391/  391]    Overall Loss 1.143472    Objective Loss 1.143472    Top1 58.653846    Top5 87.980769    LR 0.100000    Time 0.096190    
2024-02-17 11:55:28,395 - --- validate (epoch=96)-----------
2024-02-17 11:55:28,395 - 10000 samples (128 per mini-batch)
2024-02-17 11:55:30,995 - Epoch: [96][   79/   79]    Loss 1.566625    Top1 57.290000    Top5 85.610000    
2024-02-17 11:55:31,158 - ==> Top1: 57.290    Top5: 85.610    Loss: 1.567

2024-02-17 11:55:31,179 - ==> Best [Top1: 57.290   Top5: 85.610   Sparsity:0.00   Params: 1341960 on epoch: 96]
2024-02-17 11:55:31,179 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:55:31,271 - 

2024-02-17 11:55:31,271 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:55:41,408 - Epoch: [97][  100/  391]    Overall Loss 1.097953    Objective Loss 1.097953                                        LR 0.100000    Time 0.101298    
2024-02-17 11:55:50,729 - Epoch: [97][  200/  391]    Overall Loss 1.116361    Objective Loss 1.116361                                        LR 0.100000    Time 0.097233    
2024-02-17 11:55:58,497 - Epoch: [97][  300/  391]    Overall Loss 1.132750    Objective Loss 1.132750                                        LR 0.100000    Time 0.090703    
2024-02-17 11:56:06,451 - Epoch: [97][  391/  391]    Overall Loss 1.143397    Objective Loss 1.143397    Top1 67.788462    Top5 91.826923    LR 0.100000    Time 0.089925    
2024-02-17 11:56:06,668 - --- validate (epoch=97)-----------
2024-02-17 11:56:06,669 - 10000 samples (128 per mini-batch)
2024-02-17 11:56:09,357 - Epoch: [97][   79/   79]    Loss 1.555645    Top1 57.460000    Top5 84.990000    
2024-02-17 11:56:09,648 - ==> Top1: 57.460    Top5: 84.990    Loss: 1.556

2024-02-17 11:56:09,667 - ==> Best [Top1: 57.460   Top5: 84.990   Sparsity:0.00   Params: 1341960 on epoch: 97]
2024-02-17 11:56:09,668 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:56:09,758 - 

2024-02-17 11:56:09,759 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:56:18,046 - Epoch: [98][  100/  391]    Overall Loss 1.112636    Objective Loss 1.112636                                        LR 0.100000    Time 0.082809    
2024-02-17 11:56:25,906 - Epoch: [98][  200/  391]    Overall Loss 1.116951    Objective Loss 1.116951                                        LR 0.100000    Time 0.080683    
2024-02-17 11:56:35,149 - Epoch: [98][  300/  391]    Overall Loss 1.128124    Objective Loss 1.128124                                        LR 0.100000    Time 0.084585    
2024-02-17 11:56:43,833 - Epoch: [98][  391/  391]    Overall Loss 1.137896    Objective Loss 1.137896    Top1 58.653846    Top5 89.903846    LR 0.100000    Time 0.087099    
2024-02-17 11:56:44,059 - --- validate (epoch=98)-----------
2024-02-17 11:56:44,060 - 10000 samples (128 per mini-batch)
2024-02-17 11:56:46,834 - Epoch: [98][   79/   79]    Loss 1.759463    Top1 53.370000    Top5 83.050000    
2024-02-17 11:56:47,031 - ==> Top1: 53.370    Top5: 83.050    Loss: 1.759

2024-02-17 11:56:47,051 - ==> Best [Top1: 57.460   Top5: 84.990   Sparsity:0.00   Params: 1341960 on epoch: 97]
2024-02-17 11:56:47,051 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:56:47,128 - 

2024-02-17 11:56:47,129 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:56:57,427 - Epoch: [99][  100/  391]    Overall Loss 1.093764    Objective Loss 1.093764                                        LR 0.100000    Time 0.102908    
2024-02-17 11:57:06,948 - Epoch: [99][  200/  391]    Overall Loss 1.125451    Objective Loss 1.125451                                        LR 0.100000    Time 0.099042    
2024-02-17 11:57:16,620 - Epoch: [99][  300/  391]    Overall Loss 1.129275    Objective Loss 1.129275                                        LR 0.100000    Time 0.098254    
2024-02-17 11:57:25,212 - Epoch: [99][  391/  391]    Overall Loss 1.134985    Objective Loss 1.134985    Top1 65.384615    Top5 91.346154    LR 0.100000    Time 0.097348    
2024-02-17 11:57:25,481 - --- validate (epoch=99)-----------
2024-02-17 11:57:25,482 - 10000 samples (128 per mini-batch)
2024-02-17 11:57:28,122 - Epoch: [99][   79/   79]    Loss 1.652775    Top1 55.010000    Top5 84.220000    
2024-02-17 11:57:28,285 - ==> Top1: 55.010    Top5: 84.220    Loss: 1.653

2024-02-17 11:57:28,304 - ==> Best [Top1: 57.460   Top5: 84.990   Sparsity:0.00   Params: 1341960 on epoch: 97]
2024-02-17 11:57:28,304 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:57:28,380 - 

2024-02-17 11:57:28,380 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:57:38,185 - Epoch: [100][  100/  391]    Overall Loss 0.903404    Objective Loss 0.903404                                        LR 0.023500    Time 0.097983    
2024-02-17 11:57:47,642 - Epoch: [100][  200/  391]    Overall Loss 0.855386    Objective Loss 0.855386                                        LR 0.023500    Time 0.096257    
2024-02-17 11:57:57,272 - Epoch: [100][  300/  391]    Overall Loss 0.837106    Objective Loss 0.837106                                        LR 0.023500    Time 0.096256    
2024-02-17 11:58:05,714 - Epoch: [100][  391/  391]    Overall Loss 0.827087    Objective Loss 0.827087    Top1 75.480769    Top5 95.673077    LR 0.023500    Time 0.095432    
2024-02-17 11:58:05,923 - --- validate (epoch=100)-----------
2024-02-17 11:58:05,924 - 10000 samples (128 per mini-batch)
2024-02-17 11:58:08,893 - Epoch: [100][   79/   79]    Loss 1.230817    Top1 65.550000    Top5 90.210000    
2024-02-17 11:58:09,056 - ==> Top1: 65.550    Top5: 90.210    Loss: 1.231

2024-02-17 11:58:09,070 - ==> Best [Top1: 65.550   Top5: 90.210   Sparsity:0.00   Params: 1341960 on epoch: 100]
2024-02-17 11:58:09,070 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:58:09,193 - 

2024-02-17 11:58:09,193 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:58:19,415 - Epoch: [101][  100/  391]    Overall Loss 0.744482    Objective Loss 0.744482                                        LR 0.023500    Time 0.102138    
2024-02-17 11:58:28,804 - Epoch: [101][  200/  391]    Overall Loss 0.752669    Objective Loss 0.752669                                        LR 0.023500    Time 0.097995    
2024-02-17 11:58:37,754 - Epoch: [101][  300/  391]    Overall Loss 0.749218    Objective Loss 0.749218                                        LR 0.023500    Time 0.095151    
2024-02-17 11:58:46,499 - Epoch: [101][  391/  391]    Overall Loss 0.749171    Objective Loss 0.749171    Top1 78.365385    Top5 96.153846    LR 0.023500    Time 0.095360    
2024-02-17 11:58:46,670 - --- validate (epoch=101)-----------
2024-02-17 11:58:46,672 - 10000 samples (128 per mini-batch)
2024-02-17 11:58:49,421 - Epoch: [101][   79/   79]    Loss 1.232268    Top1 65.080000    Top5 90.050000    
2024-02-17 11:58:49,586 - ==> Top1: 65.080    Top5: 90.050    Loss: 1.232

2024-02-17 11:58:49,607 - ==> Best [Top1: 65.550   Top5: 90.210   Sparsity:0.00   Params: 1341960 on epoch: 100]
2024-02-17 11:58:49,608 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:58:49,688 - 

2024-02-17 11:58:49,689 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:59:00,292 - Epoch: [102][  100/  391]    Overall Loss 0.727950    Objective Loss 0.727950                                        LR 0.023500    Time 0.105954    
2024-02-17 11:59:09,948 - Epoch: [102][  200/  391]    Overall Loss 0.724930    Objective Loss 0.724930                                        LR 0.023500    Time 0.101236    
2024-02-17 11:59:19,284 - Epoch: [102][  300/  391]    Overall Loss 0.721203    Objective Loss 0.721203                                        LR 0.023500    Time 0.098595    
2024-02-17 11:59:27,795 - Epoch: [102][  391/  391]    Overall Loss 0.721563    Objective Loss 0.721563    Top1 74.038462    Top5 95.192308    LR 0.023500    Time 0.097405    
2024-02-17 11:59:27,992 - --- validate (epoch=102)-----------
2024-02-17 11:59:27,993 - 10000 samples (128 per mini-batch)
2024-02-17 11:59:30,836 - Epoch: [102][   79/   79]    Loss 1.217137    Top1 65.640000    Top5 90.050000    
2024-02-17 11:59:31,059 - ==> Top1: 65.640    Top5: 90.050    Loss: 1.217

2024-02-17 11:59:31,078 - ==> Best [Top1: 65.640   Top5: 90.050   Sparsity:0.00   Params: 1341960 on epoch: 102]
2024-02-17 11:59:31,078 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 11:59:31,171 - 

2024-02-17 11:59:31,171 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 11:59:41,091 - Epoch: [103][  100/  391]    Overall Loss 0.680840    Objective Loss 0.680840                                        LR 0.023500    Time 0.099117    
2024-02-17 11:59:50,555 - Epoch: [103][  200/  391]    Overall Loss 0.682802    Objective Loss 0.682802                                        LR 0.023500    Time 0.096857    
2024-02-17 11:59:59,916 - Epoch: [103][  300/  391]    Overall Loss 0.691215    Objective Loss 0.691215                                        LR 0.023500    Time 0.095760    
2024-02-17 12:00:07,844 - Epoch: [103][  391/  391]    Overall Loss 0.697680    Objective Loss 0.697680    Top1 85.096154    Top5 95.673077    LR 0.023500    Time 0.093741    
2024-02-17 12:00:08,059 - --- validate (epoch=103)-----------
2024-02-17 12:00:08,060 - 10000 samples (128 per mini-batch)
2024-02-17 12:00:11,073 - Epoch: [103][   79/   79]    Loss 1.242191    Top1 65.060000    Top5 89.730000    
2024-02-17 12:00:11,285 - ==> Top1: 65.060    Top5: 89.730    Loss: 1.242

2024-02-17 12:00:11,304 - ==> Best [Top1: 65.640   Top5: 90.050   Sparsity:0.00   Params: 1341960 on epoch: 102]
2024-02-17 12:00:11,305 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:00:11,381 - 

2024-02-17 12:00:11,381 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:00:21,783 - Epoch: [104][  100/  391]    Overall Loss 0.668947    Objective Loss 0.668947                                        LR 0.023500    Time 0.103938    
2024-02-17 12:00:31,513 - Epoch: [104][  200/  391]    Overall Loss 0.680357    Objective Loss 0.680357                                        LR 0.023500    Time 0.100600    
2024-02-17 12:00:40,971 - Epoch: [104][  300/  391]    Overall Loss 0.679509    Objective Loss 0.679509                                        LR 0.023500    Time 0.098574    
2024-02-17 12:00:49,920 - Epoch: [104][  391/  391]    Overall Loss 0.687346    Objective Loss 0.687346    Top1 76.442308    Top5 95.192308    LR 0.023500    Time 0.098508    
2024-02-17 12:00:50,182 - --- validate (epoch=104)-----------
2024-02-17 12:00:50,182 - 10000 samples (128 per mini-batch)
2024-02-17 12:00:52,921 - Epoch: [104][   79/   79]    Loss 1.212613    Top1 66.040000    Top5 90.370000    
2024-02-17 12:00:53,037 - ==> Top1: 66.040    Top5: 90.370    Loss: 1.213

2024-02-17 12:00:53,056 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:00:53,057 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:00:53,150 - 

2024-02-17 12:00:53,150 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:01:03,262 - Epoch: [105][  100/  391]    Overall Loss 0.649788    Objective Loss 0.649788                                        LR 0.023500    Time 0.101041    
2024-02-17 12:01:12,860 - Epoch: [105][  200/  391]    Overall Loss 0.655619    Objective Loss 0.655619                                        LR 0.023500    Time 0.098490    
2024-02-17 12:01:22,114 - Epoch: [105][  300/  391]    Overall Loss 0.663308    Objective Loss 0.663308                                        LR 0.023500    Time 0.096491    
2024-02-17 12:01:30,586 - Epoch: [105][  391/  391]    Overall Loss 0.667088    Objective Loss 0.667088    Top1 77.884615    Top5 98.557692    LR 0.023500    Time 0.095691    
2024-02-17 12:01:30,723 - --- validate (epoch=105)-----------
2024-02-17 12:01:30,723 - 10000 samples (128 per mini-batch)
2024-02-17 12:01:33,346 - Epoch: [105][   79/   79]    Loss 1.222828    Top1 65.370000    Top5 90.160000    
2024-02-17 12:01:33,502 - ==> Top1: 65.370    Top5: 90.160    Loss: 1.223

2024-02-17 12:01:33,520 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:01:33,521 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:01:33,597 - 

2024-02-17 12:01:33,597 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:01:44,298 - Epoch: [106][  100/  391]    Overall Loss 0.647922    Objective Loss 0.647922                                        LR 0.023500    Time 0.106931    
2024-02-17 12:01:53,538 - Epoch: [106][  200/  391]    Overall Loss 0.656212    Objective Loss 0.656212                                        LR 0.023500    Time 0.099647    
2024-02-17 12:02:01,793 - Epoch: [106][  300/  391]    Overall Loss 0.650680    Objective Loss 0.650680                                        LR 0.023500    Time 0.093934    
2024-02-17 12:02:09,069 - Epoch: [106][  391/  391]    Overall Loss 0.652795    Objective Loss 0.652795    Top1 80.288462    Top5 96.634615    LR 0.023500    Time 0.090671    
2024-02-17 12:02:09,272 - --- validate (epoch=106)-----------
2024-02-17 12:02:09,273 - 10000 samples (128 per mini-batch)
2024-02-17 12:02:12,157 - Epoch: [106][   79/   79]    Loss 1.237146    Top1 65.390000    Top5 90.110000    
2024-02-17 12:02:12,294 - ==> Top1: 65.390    Top5: 90.110    Loss: 1.237

2024-02-17 12:02:12,315 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:02:12,316 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:02:12,391 - 

2024-02-17 12:02:12,391 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:02:22,452 - Epoch: [107][  100/  391]    Overall Loss 0.628644    Objective Loss 0.628644                                        LR 0.023500    Time 0.100534    
2024-02-17 12:02:32,197 - Epoch: [107][  200/  391]    Overall Loss 0.638219    Objective Loss 0.638219                                        LR 0.023500    Time 0.098973    
2024-02-17 12:02:41,258 - Epoch: [107][  300/  391]    Overall Loss 0.640107    Objective Loss 0.640107                                        LR 0.023500    Time 0.096170    
2024-02-17 12:02:49,048 - Epoch: [107][  391/  391]    Overall Loss 0.648960    Objective Loss 0.648960    Top1 79.326923    Top5 98.076923    LR 0.023500    Time 0.093701    
2024-02-17 12:02:49,255 - --- validate (epoch=107)-----------
2024-02-17 12:02:49,256 - 10000 samples (128 per mini-batch)
2024-02-17 12:02:52,195 - Epoch: [107][   79/   79]    Loss 1.260121    Top1 65.170000    Top5 89.580000    
2024-02-17 12:02:52,394 - ==> Top1: 65.170    Top5: 89.580    Loss: 1.260

2024-02-17 12:02:52,412 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:02:52,412 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:02:52,487 - 

2024-02-17 12:02:52,487 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:03:02,612 - Epoch: [108][  100/  391]    Overall Loss 0.618427    Objective Loss 0.618427                                        LR 0.023500    Time 0.101175    
2024-02-17 12:03:10,124 - Epoch: [108][  200/  391]    Overall Loss 0.626711    Objective Loss 0.626711                                        LR 0.023500    Time 0.088130    
2024-02-17 12:03:18,006 - Epoch: [108][  300/  391]    Overall Loss 0.635810    Objective Loss 0.635810                                        LR 0.023500    Time 0.085014    
2024-02-17 12:03:26,686 - Epoch: [108][  391/  391]    Overall Loss 0.640143    Objective Loss 0.640143    Top1 82.211538    Top5 99.519231    LR 0.023500    Time 0.087416    
2024-02-17 12:03:26,864 - --- validate (epoch=108)-----------
2024-02-17 12:03:26,865 - 10000 samples (128 per mini-batch)
2024-02-17 12:03:29,890 - Epoch: [108][   79/   79]    Loss 1.233425    Top1 65.550000    Top5 90.400000    
2024-02-17 12:03:30,131 - ==> Top1: 65.550    Top5: 90.400    Loss: 1.233

2024-02-17 12:03:30,142 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:03:30,142 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:03:30,222 - 

2024-02-17 12:03:30,223 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:03:40,598 - Epoch: [109][  100/  391]    Overall Loss 0.608839    Objective Loss 0.608839                                        LR 0.023500    Time 0.103682    
2024-02-17 12:03:50,245 - Epoch: [109][  200/  391]    Overall Loss 0.617831    Objective Loss 0.617831                                        LR 0.023500    Time 0.100052    
2024-02-17 12:03:59,743 - Epoch: [109][  300/  391]    Overall Loss 0.625404    Objective Loss 0.625404                                        LR 0.023500    Time 0.098344    
2024-02-17 12:04:08,572 - Epoch: [109][  391/  391]    Overall Loss 0.631449    Objective Loss 0.631449    Top1 78.365385    Top5 96.153846    LR 0.023500    Time 0.098026    
2024-02-17 12:04:08,786 - --- validate (epoch=109)-----------
2024-02-17 12:04:08,787 - 10000 samples (128 per mini-batch)
2024-02-17 12:04:11,736 - Epoch: [109][   79/   79]    Loss 1.257949    Top1 65.170000    Top5 89.880000    
2024-02-17 12:04:11,906 - ==> Top1: 65.170    Top5: 89.880    Loss: 1.258

2024-02-17 12:04:11,926 - ==> Best [Top1: 66.040   Top5: 90.370   Sparsity:0.00   Params: 1341960 on epoch: 104]
2024-02-17 12:04:11,926 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:04:12,006 - 

2024-02-17 12:04:12,007 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:04:22,164 - Epoch: [110][  100/  391]    Overall Loss 0.593725    Objective Loss 0.593725                                        LR 0.023500    Time 0.101500    
2024-02-17 12:04:31,491 - Epoch: [110][  200/  391]    Overall Loss 0.605866    Objective Loss 0.605866                                        LR 0.023500    Time 0.097362    
2024-02-17 12:04:41,178 - Epoch: [110][  300/  391]    Overall Loss 0.614432    Objective Loss 0.614432                                        LR 0.023500    Time 0.097183    
2024-02-17 12:04:49,884 - Epoch: [110][  391/  391]    Overall Loss 0.621633    Objective Loss 0.621633    Top1 75.480769    Top5 95.192308    LR 0.023500    Time 0.096820    
2024-02-17 12:04:50,039 - --- validate (epoch=110)-----------
2024-02-17 12:04:50,040 - 10000 samples (128 per mini-batch)
2024-02-17 12:04:53,238 - Epoch: [110][   79/   79]    Loss 1.219577    Top1 66.110000    Top5 90.090000    
2024-02-17 12:04:53,478 - ==> Top1: 66.110    Top5: 90.090    Loss: 1.220

2024-02-17 12:04:53,496 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:04:53,496 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:04:53,587 - 

2024-02-17 12:04:53,587 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:05:03,787 - Epoch: [111][  100/  391]    Overall Loss 0.599360    Objective Loss 0.599360                                        LR 0.023500    Time 0.101924    
2024-02-17 12:05:13,383 - Epoch: [111][  200/  391]    Overall Loss 0.600780    Objective Loss 0.600780                                        LR 0.023500    Time 0.098920    
2024-02-17 12:05:21,878 - Epoch: [111][  300/  391]    Overall Loss 0.612345    Objective Loss 0.612345                                        LR 0.023500    Time 0.094249    
2024-02-17 12:05:29,994 - Epoch: [111][  391/  391]    Overall Loss 0.621217    Objective Loss 0.621217    Top1 82.211538    Top5 97.115385    LR 0.023500    Time 0.093063    
2024-02-17 12:05:30,191 - --- validate (epoch=111)-----------
2024-02-17 12:05:30,192 - 10000 samples (128 per mini-batch)
2024-02-17 12:05:33,245 - Epoch: [111][   79/   79]    Loss 1.307443    Top1 64.350000    Top5 89.100000    
2024-02-17 12:05:33,384 - ==> Top1: 64.350    Top5: 89.100    Loss: 1.307

2024-02-17 12:05:33,406 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:05:33,407 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:05:33,486 - 

2024-02-17 12:05:33,486 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:05:43,970 - Epoch: [112][  100/  391]    Overall Loss 0.589680    Objective Loss 0.589680                                        LR 0.023500    Time 0.104757    
2024-02-17 12:05:53,513 - Epoch: [112][  200/  391]    Overall Loss 0.594518    Objective Loss 0.594518                                        LR 0.023500    Time 0.100072    
2024-02-17 12:06:03,088 - Epoch: [112][  300/  391]    Overall Loss 0.599510    Objective Loss 0.599510                                        LR 0.023500    Time 0.098618    
2024-02-17 12:06:11,585 - Epoch: [112][  391/  391]    Overall Loss 0.609625    Objective Loss 0.609625    Top1 76.442308    Top5 96.153846    LR 0.023500    Time 0.097388    
2024-02-17 12:06:11,862 - --- validate (epoch=112)-----------
2024-02-17 12:06:11,862 - 10000 samples (128 per mini-batch)
2024-02-17 12:06:14,810 - Epoch: [112][   79/   79]    Loss 1.298559    Top1 64.930000    Top5 89.460000    
2024-02-17 12:06:15,008 - ==> Top1: 64.930    Top5: 89.460    Loss: 1.299

2024-02-17 12:06:15,028 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:06:15,029 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:06:15,103 - 

2024-02-17 12:06:15,104 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:06:25,298 - Epoch: [113][  100/  391]    Overall Loss 0.592065    Objective Loss 0.592065                                        LR 0.023500    Time 0.101868    
2024-02-17 12:06:34,824 - Epoch: [113][  200/  391]    Overall Loss 0.593936    Objective Loss 0.593936                                        LR 0.023500    Time 0.098544    
2024-02-17 12:06:44,351 - Epoch: [113][  300/  391]    Overall Loss 0.600268    Objective Loss 0.600268                                        LR 0.023500    Time 0.097438    
2024-02-17 12:06:53,000 - Epoch: [113][  391/  391]    Overall Loss 0.606185    Objective Loss 0.606185    Top1 76.442308    Top5 98.076923    LR 0.023500    Time 0.096870    
2024-02-17 12:06:53,211 - --- validate (epoch=113)-----------
2024-02-17 12:06:53,212 - 10000 samples (128 per mini-batch)
2024-02-17 12:06:56,144 - Epoch: [113][   79/   79]    Loss 1.317711    Top1 64.190000    Top5 89.450000    
2024-02-17 12:06:56,332 - ==> Top1: 64.190    Top5: 89.450    Loss: 1.318

2024-02-17 12:06:56,350 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:06:56,351 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:06:56,443 - 

2024-02-17 12:06:56,443 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:07:06,988 - Epoch: [114][  100/  391]    Overall Loss 0.574343    Objective Loss 0.574343                                        LR 0.023500    Time 0.105376    
2024-02-17 12:07:16,522 - Epoch: [114][  200/  391]    Overall Loss 0.586967    Objective Loss 0.586967                                        LR 0.023500    Time 0.100340    
2024-02-17 12:07:26,478 - Epoch: [114][  300/  391]    Overall Loss 0.596132    Objective Loss 0.596132                                        LR 0.023500    Time 0.100064    
2024-02-17 12:07:35,151 - Epoch: [114][  391/  391]    Overall Loss 0.605783    Objective Loss 0.605783    Top1 80.769231    Top5 96.153846    LR 0.023500    Time 0.098947    
2024-02-17 12:07:35,316 - --- validate (epoch=114)-----------
2024-02-17 12:07:35,317 - 10000 samples (128 per mini-batch)
2024-02-17 12:07:38,247 - Epoch: [114][   79/   79]    Loss 1.309465    Top1 64.210000    Top5 88.980000    
2024-02-17 12:07:38,455 - ==> Top1: 64.210    Top5: 88.980    Loss: 1.309

2024-02-17 12:07:38,472 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:07:38,473 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:07:38,547 - 

2024-02-17 12:07:38,547 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:07:48,600 - Epoch: [115][  100/  391]    Overall Loss 0.574954    Objective Loss 0.574954                                        LR 0.023500    Time 0.100458    
2024-02-17 12:07:58,127 - Epoch: [115][  200/  391]    Overall Loss 0.579106    Objective Loss 0.579106                                        LR 0.023500    Time 0.097842    
2024-02-17 12:08:07,653 - Epoch: [115][  300/  391]    Overall Loss 0.592322    Objective Loss 0.592322                                        LR 0.023500    Time 0.096967    
2024-02-17 12:08:16,455 - Epoch: [115][  391/  391]    Overall Loss 0.601133    Objective Loss 0.601133    Top1 81.730769    Top5 96.153846    LR 0.023500    Time 0.096901    
2024-02-17 12:08:16,635 - --- validate (epoch=115)-----------
2024-02-17 12:08:16,635 - 10000 samples (128 per mini-batch)
2024-02-17 12:08:19,528 - Epoch: [115][   79/   79]    Loss 1.320970    Top1 64.180000    Top5 88.950000    
2024-02-17 12:08:19,703 - ==> Top1: 64.180    Top5: 88.950    Loss: 1.321

2024-02-17 12:08:19,723 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:08:19,724 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:08:19,808 - 

2024-02-17 12:08:19,808 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:08:30,131 - Epoch: [116][  100/  391]    Overall Loss 0.590266    Objective Loss 0.590266                                        LR 0.023500    Time 0.103151    
2024-02-17 12:08:39,757 - Epoch: [116][  200/  391]    Overall Loss 0.591161    Objective Loss 0.591161                                        LR 0.023500    Time 0.099686    
2024-02-17 12:08:49,175 - Epoch: [116][  300/  391]    Overall Loss 0.597120    Objective Loss 0.597120                                        LR 0.023500    Time 0.097833    
2024-02-17 12:08:57,888 - Epoch: [116][  391/  391]    Overall Loss 0.602517    Objective Loss 0.602517    Top1 77.403846    Top5 96.634615    LR 0.023500    Time 0.097338    
2024-02-17 12:08:58,109 - --- validate (epoch=116)-----------
2024-02-17 12:08:58,110 - 10000 samples (128 per mini-batch)
2024-02-17 12:09:01,033 - Epoch: [116][   79/   79]    Loss 1.299381    Top1 65.010000    Top5 89.490000    
2024-02-17 12:09:01,183 - ==> Top1: 65.010    Top5: 89.490    Loss: 1.299

2024-02-17 12:09:01,204 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:09:01,204 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:09:01,285 - 

2024-02-17 12:09:01,286 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:09:11,295 - Epoch: [117][  100/  391]    Overall Loss 0.571592    Objective Loss 0.571592                                        LR 0.023500    Time 0.100011    
2024-02-17 12:09:20,946 - Epoch: [117][  200/  391]    Overall Loss 0.580793    Objective Loss 0.580793                                        LR 0.023500    Time 0.098239    
2024-02-17 12:09:30,302 - Epoch: [117][  300/  391]    Overall Loss 0.587905    Objective Loss 0.587905                                        LR 0.023500    Time 0.096664    
2024-02-17 12:09:39,145 - Epoch: [117][  391/  391]    Overall Loss 0.594131    Objective Loss 0.594131    Top1 77.403846    Top5 95.673077    LR 0.023500    Time 0.096772    
2024-02-17 12:09:39,404 - --- validate (epoch=117)-----------
2024-02-17 12:09:39,404 - 10000 samples (128 per mini-batch)
2024-02-17 12:09:42,285 - Epoch: [117][   79/   79]    Loss 1.343025    Top1 64.040000    Top5 88.790000    
2024-02-17 12:09:42,463 - ==> Top1: 64.040    Top5: 88.790    Loss: 1.343

2024-02-17 12:09:42,483 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:09:42,483 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:09:42,560 - 

2024-02-17 12:09:42,560 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:09:52,661 - Epoch: [118][  100/  391]    Overall Loss 0.565083    Objective Loss 0.565083                                        LR 0.023500    Time 0.100933    
2024-02-17 12:10:02,132 - Epoch: [118][  200/  391]    Overall Loss 0.568931    Objective Loss 0.568931                                        LR 0.023500    Time 0.097798    
2024-02-17 12:10:11,344 - Epoch: [118][  300/  391]    Overall Loss 0.579081    Objective Loss 0.579081                                        LR 0.023500    Time 0.095892    
2024-02-17 12:10:20,080 - Epoch: [118][  391/  391]    Overall Loss 0.589991    Objective Loss 0.589991    Top1 79.326923    Top5 97.596154    LR 0.023500    Time 0.095908    
2024-02-17 12:10:20,261 - --- validate (epoch=118)-----------
2024-02-17 12:10:20,262 - 10000 samples (128 per mini-batch)
2024-02-17 12:10:23,187 - Epoch: [118][   79/   79]    Loss 1.285393    Top1 65.080000    Top5 89.680000    
2024-02-17 12:10:23,380 - ==> Top1: 65.080    Top5: 89.680    Loss: 1.285

2024-02-17 12:10:23,398 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:10:23,398 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:10:23,484 - 

2024-02-17 12:10:23,485 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:10:34,075 - Epoch: [119][  100/  391]    Overall Loss 0.555229    Objective Loss 0.555229                                        LR 0.023500    Time 0.105801    
2024-02-17 12:10:43,454 - Epoch: [119][  200/  391]    Overall Loss 0.563886    Objective Loss 0.563886                                        LR 0.023500    Time 0.099774    
2024-02-17 12:10:53,052 - Epoch: [119][  300/  391]    Overall Loss 0.574306    Objective Loss 0.574306                                        LR 0.023500    Time 0.098495    
2024-02-17 12:11:01,795 - Epoch: [119][  391/  391]    Overall Loss 0.585766    Objective Loss 0.585766    Top1 77.403846    Top5 98.076923    LR 0.023500    Time 0.097921    
2024-02-17 12:11:01,974 - --- validate (epoch=119)-----------
2024-02-17 12:11:01,974 - 10000 samples (128 per mini-batch)
2024-02-17 12:11:05,051 - Epoch: [119][   79/   79]    Loss 1.281214    Top1 65.070000    Top5 89.780000    
2024-02-17 12:11:05,184 - ==> Top1: 65.070    Top5: 89.780    Loss: 1.281

2024-02-17 12:11:05,204 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:11:05,205 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:11:05,281 - 

2024-02-17 12:11:05,282 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:11:15,395 - Epoch: [120][  100/  391]    Overall Loss 0.544596    Objective Loss 0.544596                                        LR 0.023500    Time 0.101057    
2024-02-17 12:11:25,177 - Epoch: [120][  200/  391]    Overall Loss 0.561226    Objective Loss 0.561226                                        LR 0.023500    Time 0.099417    
2024-02-17 12:11:34,869 - Epoch: [120][  300/  391]    Overall Loss 0.570281    Objective Loss 0.570281                                        LR 0.023500    Time 0.098569    
2024-02-17 12:11:43,583 - Epoch: [120][  391/  391]    Overall Loss 0.578666    Objective Loss 0.578666    Top1 80.769231    Top5 98.557692    LR 0.023500    Time 0.097905    
2024-02-17 12:11:43,765 - --- validate (epoch=120)-----------
2024-02-17 12:11:43,765 - 10000 samples (128 per mini-batch)
2024-02-17 12:11:46,965 - Epoch: [120][   79/   79]    Loss 1.283224    Top1 64.780000    Top5 89.990000    
2024-02-17 12:11:47,116 - ==> Top1: 64.780    Top5: 89.990    Loss: 1.283

2024-02-17 12:11:47,135 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:11:47,135 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:11:47,210 - 

2024-02-17 12:11:47,210 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:11:55,039 - Epoch: [121][  100/  391]    Overall Loss 0.539578    Objective Loss 0.539578                                        LR 0.023500    Time 0.078219    
2024-02-17 12:12:04,498 - Epoch: [121][  200/  391]    Overall Loss 0.562012    Objective Loss 0.562012                                        LR 0.023500    Time 0.086380    
2024-02-17 12:12:14,010 - Epoch: [121][  300/  391]    Overall Loss 0.571322    Objective Loss 0.571322                                        LR 0.023500    Time 0.089280    
2024-02-17 12:12:22,679 - Epoch: [121][  391/  391]    Overall Loss 0.579879    Objective Loss 0.579879    Top1 83.173077    Top5 96.634615    LR 0.023500    Time 0.090662    
2024-02-17 12:12:22,861 - --- validate (epoch=121)-----------
2024-02-17 12:12:22,861 - 10000 samples (128 per mini-batch)
2024-02-17 12:12:25,896 - Epoch: [121][   79/   79]    Loss 1.297498    Top1 64.430000    Top5 89.680000    
2024-02-17 12:12:26,075 - ==> Top1: 64.430    Top5: 89.680    Loss: 1.297

2024-02-17 12:12:26,088 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:12:26,088 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:12:26,164 - 

2024-02-17 12:12:26,164 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:12:36,668 - Epoch: [122][  100/  391]    Overall Loss 0.538779    Objective Loss 0.538779                                        LR 0.023500    Time 0.104938    
2024-02-17 12:12:46,089 - Epoch: [122][  200/  391]    Overall Loss 0.547705    Objective Loss 0.547705                                        LR 0.023500    Time 0.099552    
2024-02-17 12:12:55,767 - Epoch: [122][  300/  391]    Overall Loss 0.560586    Objective Loss 0.560586                                        LR 0.023500    Time 0.098614    
2024-02-17 12:13:04,190 - Epoch: [122][  391/  391]    Overall Loss 0.568272    Objective Loss 0.568272    Top1 81.250000    Top5 97.596154    LR 0.023500    Time 0.097196    
2024-02-17 12:13:04,379 - --- validate (epoch=122)-----------
2024-02-17 12:13:04,380 - 10000 samples (128 per mini-batch)
2024-02-17 12:13:07,343 - Epoch: [122][   79/   79]    Loss 1.398728    Top1 63.080000    Top5 88.530000    
2024-02-17 12:13:07,478 - ==> Top1: 63.080    Top5: 88.530    Loss: 1.399

2024-02-17 12:13:07,489 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:13:07,490 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:13:07,562 - 

2024-02-17 12:13:07,562 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:13:17,735 - Epoch: [123][  100/  391]    Overall Loss 0.545120    Objective Loss 0.545120                                        LR 0.023500    Time 0.101656    
2024-02-17 12:13:27,131 - Epoch: [123][  200/  391]    Overall Loss 0.554063    Objective Loss 0.554063                                        LR 0.023500    Time 0.097785    
2024-02-17 12:13:36,770 - Epoch: [123][  300/  391]    Overall Loss 0.566640    Objective Loss 0.566640                                        LR 0.023500    Time 0.097304    
2024-02-17 12:13:45,324 - Epoch: [123][  391/  391]    Overall Loss 0.577591    Objective Loss 0.577591    Top1 75.961538    Top5 97.596154    LR 0.023500    Time 0.096526    
2024-02-17 12:13:45,557 - --- validate (epoch=123)-----------
2024-02-17 12:13:45,558 - 10000 samples (128 per mini-batch)
2024-02-17 12:13:48,321 - Epoch: [123][   79/   79]    Loss 1.291370    Top1 65.150000    Top5 89.640000    
2024-02-17 12:13:48,546 - ==> Top1: 65.150    Top5: 89.640    Loss: 1.291

2024-02-17 12:13:48,566 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:13:48,566 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:13:48,642 - 

2024-02-17 12:13:48,642 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:13:58,911 - Epoch: [124][  100/  391]    Overall Loss 0.551294    Objective Loss 0.551294                                        LR 0.023500    Time 0.102616    
2024-02-17 12:14:08,178 - Epoch: [124][  200/  391]    Overall Loss 0.559176    Objective Loss 0.559176                                        LR 0.023500    Time 0.097623    
2024-02-17 12:14:17,707 - Epoch: [124][  300/  391]    Overall Loss 0.567467    Objective Loss 0.567467                                        LR 0.023500    Time 0.096829    
2024-02-17 12:14:26,081 - Epoch: [124][  391/  391]    Overall Loss 0.575790    Objective Loss 0.575790    Top1 82.692308    Top5 97.596154    LR 0.023500    Time 0.095699    
2024-02-17 12:14:26,260 - --- validate (epoch=124)-----------
2024-02-17 12:14:26,260 - 10000 samples (128 per mini-batch)
2024-02-17 12:14:29,047 - Epoch: [124][   79/   79]    Loss 1.353544    Top1 64.360000    Top5 88.660000    
2024-02-17 12:14:29,182 - ==> Top1: 64.360    Top5: 88.660    Loss: 1.354

2024-02-17 12:14:29,202 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:14:29,203 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:14:29,281 - 

2024-02-17 12:14:29,281 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:14:39,103 - Epoch: [125][  100/  391]    Overall Loss 0.557675    Objective Loss 0.557675                                        LR 0.023500    Time 0.098141    
2024-02-17 12:14:48,516 - Epoch: [125][  200/  391]    Overall Loss 0.559029    Objective Loss 0.559029                                        LR 0.023500    Time 0.096116    
2024-02-17 12:14:58,155 - Epoch: [125][  300/  391]    Overall Loss 0.565728    Objective Loss 0.565728                                        LR 0.023500    Time 0.096193    
2024-02-17 12:15:06,590 - Epoch: [125][  391/  391]    Overall Loss 0.575140    Objective Loss 0.575140    Top1 86.538462    Top5 98.076923    LR 0.023500    Time 0.095367    
2024-02-17 12:15:06,790 - --- validate (epoch=125)-----------
2024-02-17 12:15:06,790 - 10000 samples (128 per mini-batch)
2024-02-17 12:15:09,575 - Epoch: [125][   79/   79]    Loss 1.319069    Top1 63.570000    Top5 89.240000    
2024-02-17 12:15:09,755 - ==> Top1: 63.570    Top5: 89.240    Loss: 1.319

2024-02-17 12:15:09,772 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:15:09,773 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:15:09,862 - 

2024-02-17 12:15:09,863 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:15:19,938 - Epoch: [126][  100/  391]    Overall Loss 0.531751    Objective Loss 0.531751                                        LR 0.023500    Time 0.100676    
2024-02-17 12:15:29,513 - Epoch: [126][  200/  391]    Overall Loss 0.548702    Objective Loss 0.548702                                        LR 0.023500    Time 0.098192    
2024-02-17 12:15:39,135 - Epoch: [126][  300/  391]    Overall Loss 0.562794    Objective Loss 0.562794                                        LR 0.023500    Time 0.097519    
2024-02-17 12:15:47,622 - Epoch: [126][  391/  391]    Overall Loss 0.566014    Objective Loss 0.566014    Top1 80.288462    Top5 98.076923    LR 0.023500    Time 0.096519    
2024-02-17 12:15:47,802 - --- validate (epoch=126)-----------
2024-02-17 12:15:47,802 - 10000 samples (128 per mini-batch)
2024-02-17 12:15:50,760 - Epoch: [126][   79/   79]    Loss 1.300004    Top1 64.620000    Top5 89.500000    
2024-02-17 12:15:50,959 - ==> Top1: 64.620    Top5: 89.500    Loss: 1.300

2024-02-17 12:15:50,980 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:15:50,981 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:15:51,070 - 

2024-02-17 12:15:51,070 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:16:00,893 - Epoch: [127][  100/  391]    Overall Loss 0.541477    Objective Loss 0.541477                                        LR 0.023500    Time 0.098134    
2024-02-17 12:16:10,465 - Epoch: [127][  200/  391]    Overall Loss 0.550580    Objective Loss 0.550580                                        LR 0.023500    Time 0.096905    
2024-02-17 12:16:20,037 - Epoch: [127][  300/  391]    Overall Loss 0.559187    Objective Loss 0.559187                                        LR 0.023500    Time 0.096495    
2024-02-17 12:16:28,453 - Epoch: [127][  391/  391]    Overall Loss 0.570268    Objective Loss 0.570268    Top1 78.846154    Top5 99.038462    LR 0.023500    Time 0.095551    
2024-02-17 12:16:28,632 - --- validate (epoch=127)-----------
2024-02-17 12:16:28,633 - 10000 samples (128 per mini-batch)
2024-02-17 12:16:31,601 - Epoch: [127][   79/   79]    Loss 1.424709    Top1 62.530000    Top5 88.140000    
2024-02-17 12:16:31,818 - ==> Top1: 62.530    Top5: 88.140    Loss: 1.425

2024-02-17 12:16:31,837 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:16:31,838 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:16:31,914 - 

2024-02-17 12:16:31,915 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:16:41,542 - Epoch: [128][  100/  391]    Overall Loss 0.533142    Objective Loss 0.533142                                        LR 0.023500    Time 0.096195    
2024-02-17 12:16:50,437 - Epoch: [128][  200/  391]    Overall Loss 0.540543    Objective Loss 0.540543                                        LR 0.023500    Time 0.092552    
2024-02-17 12:17:00,094 - Epoch: [128][  300/  391]    Overall Loss 0.556833    Objective Loss 0.556833                                        LR 0.023500    Time 0.093875    
2024-02-17 12:17:08,751 - Epoch: [128][  391/  391]    Overall Loss 0.567360    Objective Loss 0.567360    Top1 82.692308    Top5 98.076923    LR 0.023500    Time 0.094158    
2024-02-17 12:17:08,943 - --- validate (epoch=128)-----------
2024-02-17 12:17:08,944 - 10000 samples (128 per mini-batch)
2024-02-17 12:17:11,826 - Epoch: [128][   79/   79]    Loss 1.352653    Top1 63.800000    Top5 88.890000    
2024-02-17 12:17:12,065 - ==> Top1: 63.800    Top5: 88.890    Loss: 1.353

2024-02-17 12:17:12,085 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:17:12,086 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:17:12,162 - 

2024-02-17 12:17:12,162 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:17:22,160 - Epoch: [129][  100/  391]    Overall Loss 0.539323    Objective Loss 0.539323                                        LR 0.023500    Time 0.099908    
2024-02-17 12:17:31,657 - Epoch: [129][  200/  391]    Overall Loss 0.541701    Objective Loss 0.541701                                        LR 0.023500    Time 0.097421    
2024-02-17 12:17:41,098 - Epoch: [129][  300/  391]    Overall Loss 0.555666    Objective Loss 0.555666                                        LR 0.023500    Time 0.096401    
2024-02-17 12:17:49,972 - Epoch: [129][  391/  391]    Overall Loss 0.559215    Objective Loss 0.559215    Top1 81.250000    Top5 98.076923    LR 0.023500    Time 0.096651    
2024-02-17 12:17:50,155 - --- validate (epoch=129)-----------
2024-02-17 12:17:50,156 - 10000 samples (128 per mini-batch)
2024-02-17 12:17:53,012 - Epoch: [129][   79/   79]    Loss 1.362347    Top1 64.070000    Top5 89.020000    
2024-02-17 12:17:53,144 - ==> Top1: 64.070    Top5: 89.020    Loss: 1.362

2024-02-17 12:17:53,165 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:17:53,165 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:17:53,242 - 

2024-02-17 12:17:53,242 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:18:03,244 - Epoch: [130][  100/  391]    Overall Loss 0.514602    Objective Loss 0.514602                                        LR 0.023500    Time 0.099937    
2024-02-17 12:18:12,811 - Epoch: [130][  200/  391]    Overall Loss 0.527521    Objective Loss 0.527521                                        LR 0.023500    Time 0.097782    
2024-02-17 12:18:22,355 - Epoch: [130][  300/  391]    Overall Loss 0.536228    Objective Loss 0.536228                                        LR 0.023500    Time 0.096987    
2024-02-17 12:18:31,350 - Epoch: [130][  391/  391]    Overall Loss 0.550193    Objective Loss 0.550193    Top1 82.692308    Top5 97.596154    LR 0.023500    Time 0.097409    
2024-02-17 12:18:31,549 - --- validate (epoch=130)-----------
2024-02-17 12:18:31,549 - 10000 samples (128 per mini-batch)
2024-02-17 12:18:34,702 - Epoch: [130][   79/   79]    Loss 1.338975    Top1 64.160000    Top5 88.900000    
2024-02-17 12:18:34,894 - ==> Top1: 64.160    Top5: 88.900    Loss: 1.339

2024-02-17 12:18:34,922 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:18:34,923 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:18:35,003 - 

2024-02-17 12:18:35,003 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:18:45,022 - Epoch: [131][  100/  391]    Overall Loss 0.534401    Objective Loss 0.534401                                        LR 0.023500    Time 0.100105    
2024-02-17 12:18:54,647 - Epoch: [131][  200/  391]    Overall Loss 0.541284    Objective Loss 0.541284                                        LR 0.023500    Time 0.098156    
2024-02-17 12:19:04,422 - Epoch: [131][  300/  391]    Overall Loss 0.550552    Objective Loss 0.550552                                        LR 0.023500    Time 0.098005    
2024-02-17 12:19:13,288 - Epoch: [131][  391/  391]    Overall Loss 0.556768    Objective Loss 0.556768    Top1 79.807692    Top5 98.076923    LR 0.023500    Time 0.097860    
2024-02-17 12:19:13,452 - --- validate (epoch=131)-----------
2024-02-17 12:19:13,453 - 10000 samples (128 per mini-batch)
2024-02-17 12:19:16,441 - Epoch: [131][   79/   79]    Loss 1.422520    Top1 62.650000    Top5 88.430000    
2024-02-17 12:19:16,670 - ==> Top1: 62.650    Top5: 88.430    Loss: 1.423

2024-02-17 12:19:16,681 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:19:16,682 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:19:16,760 - 

2024-02-17 12:19:16,760 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:19:26,845 - Epoch: [132][  100/  391]    Overall Loss 0.524812    Objective Loss 0.524812                                        LR 0.023500    Time 0.100727    
2024-02-17 12:19:36,541 - Epoch: [132][  200/  391]    Overall Loss 0.534590    Objective Loss 0.534590                                        LR 0.023500    Time 0.098821    
2024-02-17 12:19:46,292 - Epoch: [132][  300/  391]    Overall Loss 0.547905    Objective Loss 0.547905                                        LR 0.023500    Time 0.098370    
2024-02-17 12:19:53,309 - Epoch: [132][  391/  391]    Overall Loss 0.561220    Objective Loss 0.561220    Top1 83.173077    Top5 98.076923    LR 0.023500    Time 0.093413    
2024-02-17 12:19:53,460 - --- validate (epoch=132)-----------
2024-02-17 12:19:53,460 - 10000 samples (128 per mini-batch)
2024-02-17 12:19:56,426 - Epoch: [132][   79/   79]    Loss 1.339014    Top1 64.200000    Top5 89.530000    
2024-02-17 12:19:56,640 - ==> Top1: 64.200    Top5: 89.530    Loss: 1.339

2024-02-17 12:19:56,660 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:19:56,660 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:19:56,740 - 

2024-02-17 12:19:56,741 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:20:06,959 - Epoch: [133][  100/  391]    Overall Loss 0.513847    Objective Loss 0.513847                                        LR 0.023500    Time 0.102105    
2024-02-17 12:20:16,735 - Epoch: [133][  200/  391]    Overall Loss 0.534833    Objective Loss 0.534833                                        LR 0.023500    Time 0.099908    
2024-02-17 12:20:26,897 - Epoch: [133][  300/  391]    Overall Loss 0.545445    Objective Loss 0.545445                                        LR 0.023500    Time 0.100463    
2024-02-17 12:20:36,083 - Epoch: [133][  391/  391]    Overall Loss 0.557363    Objective Loss 0.557363    Top1 85.096154    Top5 98.557692    LR 0.023500    Time 0.100566    
2024-02-17 12:20:36,272 - --- validate (epoch=133)-----------
2024-02-17 12:20:36,273 - 10000 samples (128 per mini-batch)
2024-02-17 12:20:39,247 - Epoch: [133][   79/   79]    Loss 1.387939    Top1 63.200000    Top5 88.800000    
2024-02-17 12:20:39,408 - ==> Top1: 63.200    Top5: 88.800    Loss: 1.388

2024-02-17 12:20:39,434 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:20:39,435 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:20:39,521 - 

2024-02-17 12:20:39,522 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:20:50,141 - Epoch: [134][  100/  391]    Overall Loss 0.520899    Objective Loss 0.520899                                        LR 0.023500    Time 0.106085    
2024-02-17 12:20:59,653 - Epoch: [134][  200/  391]    Overall Loss 0.534835    Objective Loss 0.534835                                        LR 0.023500    Time 0.100583    
2024-02-17 12:21:09,201 - Epoch: [134][  300/  391]    Overall Loss 0.546189    Objective Loss 0.546189                                        LR 0.023500    Time 0.098868    
2024-02-17 12:21:18,197 - Epoch: [134][  391/  391]    Overall Loss 0.559324    Objective Loss 0.559324    Top1 75.480769    Top5 97.596154    LR 0.023500    Time 0.098854    
2024-02-17 12:21:18,396 - --- validate (epoch=134)-----------
2024-02-17 12:21:18,396 - 10000 samples (128 per mini-batch)
2024-02-17 12:21:21,200 - Epoch: [134][   79/   79]    Loss 1.345968    Top1 64.770000    Top5 88.710000    
2024-02-17 12:21:21,366 - ==> Top1: 64.770    Top5: 88.710    Loss: 1.346

2024-02-17 12:21:21,386 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:21:21,387 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:21:21,466 - 

2024-02-17 12:21:21,467 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:21:30,780 - Epoch: [135][  100/  391]    Overall Loss 0.523758    Objective Loss 0.523758                                        LR 0.023500    Time 0.093053    
2024-02-17 12:21:40,339 - Epoch: [135][  200/  391]    Overall Loss 0.531264    Objective Loss 0.531264                                        LR 0.023500    Time 0.094301    
2024-02-17 12:21:49,851 - Epoch: [135][  300/  391]    Overall Loss 0.546272    Objective Loss 0.546272                                        LR 0.023500    Time 0.094559    
2024-02-17 12:21:58,513 - Epoch: [135][  391/  391]    Overall Loss 0.555146    Objective Loss 0.555146    Top1 81.730769    Top5 98.076923    LR 0.023500    Time 0.094695    
2024-02-17 12:21:58,672 - --- validate (epoch=135)-----------
2024-02-17 12:21:58,673 - 10000 samples (128 per mini-batch)
2024-02-17 12:22:01,681 - Epoch: [135][   79/   79]    Loss 1.408276    Top1 62.730000    Top5 88.480000    
2024-02-17 12:22:01,942 - ==> Top1: 62.730    Top5: 88.480    Loss: 1.408

2024-02-17 12:22:01,961 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:22:01,961 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:22:02,036 - 

2024-02-17 12:22:02,036 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:22:12,565 - Epoch: [136][  100/  391]    Overall Loss 0.515902    Objective Loss 0.515902                                        LR 0.023500    Time 0.105214    
2024-02-17 12:22:22,218 - Epoch: [136][  200/  391]    Overall Loss 0.523318    Objective Loss 0.523318                                        LR 0.023500    Time 0.100847    
2024-02-17 12:22:31,807 - Epoch: [136][  300/  391]    Overall Loss 0.537852    Objective Loss 0.537852                                        LR 0.023500    Time 0.099182    
2024-02-17 12:22:40,219 - Epoch: [136][  391/  391]    Overall Loss 0.549399    Objective Loss 0.549399    Top1 79.326923    Top5 98.076923    LR 0.023500    Time 0.097601    
2024-02-17 12:22:40,404 - --- validate (epoch=136)-----------
2024-02-17 12:22:40,405 - 10000 samples (128 per mini-batch)
2024-02-17 12:22:43,228 - Epoch: [136][   79/   79]    Loss 1.336822    Top1 63.610000    Top5 89.170000    
2024-02-17 12:22:43,380 - ==> Top1: 63.610    Top5: 89.170    Loss: 1.337

2024-02-17 12:22:43,400 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:22:43,400 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:22:43,477 - 

2024-02-17 12:22:43,477 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:22:53,703 - Epoch: [137][  100/  391]    Overall Loss 0.522268    Objective Loss 0.522268                                        LR 0.023500    Time 0.102179    
2024-02-17 12:23:03,259 - Epoch: [137][  200/  391]    Overall Loss 0.522617    Objective Loss 0.522617                                        LR 0.023500    Time 0.098846    
2024-02-17 12:23:12,755 - Epoch: [137][  300/  391]    Overall Loss 0.533800    Objective Loss 0.533800                                        LR 0.023500    Time 0.097539    
2024-02-17 12:23:21,123 - Epoch: [137][  391/  391]    Overall Loss 0.544992    Objective Loss 0.544992    Top1 81.250000    Top5 98.076923    LR 0.023500    Time 0.096228    
2024-02-17 12:23:21,324 - --- validate (epoch=137)-----------
2024-02-17 12:23:21,325 - 10000 samples (128 per mini-batch)
2024-02-17 12:23:24,159 - Epoch: [137][   79/   79]    Loss 1.417838    Top1 62.180000    Top5 88.250000    
2024-02-17 12:23:24,334 - ==> Top1: 62.180    Top5: 88.250    Loss: 1.418

2024-02-17 12:23:24,352 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:23:24,353 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:23:24,438 - 

2024-02-17 12:23:24,438 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:23:34,721 - Epoch: [138][  100/  391]    Overall Loss 0.518913    Objective Loss 0.518913                                        LR 0.023500    Time 0.102753    
2024-02-17 12:23:44,134 - Epoch: [138][  200/  391]    Overall Loss 0.531409    Objective Loss 0.531409                                        LR 0.023500    Time 0.098418    
2024-02-17 12:23:53,557 - Epoch: [138][  300/  391]    Overall Loss 0.538835    Objective Loss 0.538835                                        LR 0.023500    Time 0.097006    
2024-02-17 12:24:02,037 - Epoch: [138][  391/  391]    Overall Loss 0.549000    Objective Loss 0.549000    Top1 77.884615    Top5 96.153846    LR 0.023500    Time 0.096107    
2024-02-17 12:24:02,250 - --- validate (epoch=138)-----------
2024-02-17 12:24:02,251 - 10000 samples (128 per mini-batch)
2024-02-17 12:24:05,357 - Epoch: [138][   79/   79]    Loss 1.504057    Top1 61.290000    Top5 87.450000    
2024-02-17 12:24:05,501 - ==> Top1: 61.290    Top5: 87.450    Loss: 1.504

2024-02-17 12:24:05,511 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:24:05,511 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:24:05,583 - 

2024-02-17 12:24:05,583 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:24:15,631 - Epoch: [139][  100/  391]    Overall Loss 0.522495    Objective Loss 0.522495                                        LR 0.023500    Time 0.100405    
2024-02-17 12:24:24,885 - Epoch: [139][  200/  391]    Overall Loss 0.530554    Objective Loss 0.530554                                        LR 0.023500    Time 0.096452    
2024-02-17 12:24:34,463 - Epoch: [139][  300/  391]    Overall Loss 0.539933    Objective Loss 0.539933                                        LR 0.023500    Time 0.096210    
2024-02-17 12:24:43,012 - Epoch: [139][  391/  391]    Overall Loss 0.545849    Objective Loss 0.545849    Top1 79.326923    Top5 98.076923    LR 0.023500    Time 0.095670    
2024-02-17 12:24:43,225 - --- validate (epoch=139)-----------
2024-02-17 12:24:43,226 - 10000 samples (128 per mini-batch)
2024-02-17 12:24:46,092 - Epoch: [139][   79/   79]    Loss 1.368089    Top1 63.390000    Top5 88.880000    
2024-02-17 12:24:46,327 - ==> Top1: 63.390    Top5: 88.880    Loss: 1.368

2024-02-17 12:24:46,345 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:24:46,345 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:24:46,422 - 

2024-02-17 12:24:46,422 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:24:56,933 - Epoch: [140][  100/  391]    Overall Loss 0.506662    Objective Loss 0.506662                                        LR 0.023500    Time 0.105036    
2024-02-17 12:25:06,540 - Epoch: [140][  200/  391]    Overall Loss 0.516688    Objective Loss 0.516688                                        LR 0.023500    Time 0.100528    
2024-02-17 12:25:16,020 - Epoch: [140][  300/  391]    Overall Loss 0.526371    Objective Loss 0.526371                                        LR 0.023500    Time 0.098604    
2024-02-17 12:25:24,638 - Epoch: [140][  391/  391]    Overall Loss 0.537069    Objective Loss 0.537069    Top1 80.288462    Top5 98.557692    LR 0.023500    Time 0.097686    
2024-02-17 12:25:24,843 - --- validate (epoch=140)-----------
2024-02-17 12:25:24,843 - 10000 samples (128 per mini-batch)
2024-02-17 12:25:27,857 - Epoch: [140][   79/   79]    Loss 1.344355    Top1 63.930000    Top5 89.210000    
2024-02-17 12:25:28,015 - ==> Top1: 63.930    Top5: 89.210    Loss: 1.344

2024-02-17 12:25:28,036 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:25:28,036 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:25:28,113 - 

2024-02-17 12:25:28,113 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:25:38,294 - Epoch: [141][  100/  391]    Overall Loss 0.500247    Objective Loss 0.500247                                        LR 0.023500    Time 0.101738    
2024-02-17 12:25:47,758 - Epoch: [141][  200/  391]    Overall Loss 0.519171    Objective Loss 0.519171                                        LR 0.023500    Time 0.098167    
2024-02-17 12:25:57,283 - Epoch: [141][  300/  391]    Overall Loss 0.528672    Objective Loss 0.528672                                        LR 0.023500    Time 0.097180    
2024-02-17 12:26:05,820 - Epoch: [141][  391/  391]    Overall Loss 0.538246    Objective Loss 0.538246    Top1 81.250000    Top5 98.076923    LR 0.023500    Time 0.096384    
2024-02-17 12:26:06,009 - --- validate (epoch=141)-----------
2024-02-17 12:26:06,009 - 10000 samples (128 per mini-batch)
2024-02-17 12:26:08,990 - Epoch: [141][   79/   79]    Loss 1.428072    Top1 63.040000    Top5 88.220000    
2024-02-17 12:26:09,217 - ==> Top1: 63.040    Top5: 88.220    Loss: 1.428

2024-02-17 12:26:09,239 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:26:09,240 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:26:09,313 - 

2024-02-17 12:26:09,313 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:26:19,888 - Epoch: [142][  100/  391]    Overall Loss 0.508290    Objective Loss 0.508290                                        LR 0.023500    Time 0.105674    
2024-02-17 12:26:29,633 - Epoch: [142][  200/  391]    Overall Loss 0.517290    Objective Loss 0.517290                                        LR 0.023500    Time 0.101539    
2024-02-17 12:26:39,177 - Epoch: [142][  300/  391]    Overall Loss 0.529299    Objective Loss 0.529299                                        LR 0.023500    Time 0.099488    
2024-02-17 12:26:47,702 - Epoch: [142][  391/  391]    Overall Loss 0.539657    Objective Loss 0.539657    Top1 88.942308    Top5 99.038462    LR 0.023500    Time 0.098126    
2024-02-17 12:26:47,904 - --- validate (epoch=142)-----------
2024-02-17 12:26:47,905 - 10000 samples (128 per mini-batch)
2024-02-17 12:26:50,689 - Epoch: [142][   79/   79]    Loss 1.381608    Top1 63.870000    Top5 88.800000    
2024-02-17 12:26:50,840 - ==> Top1: 63.870    Top5: 88.800    Loss: 1.382

2024-02-17 12:26:50,859 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:26:50,860 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:26:50,938 - 

2024-02-17 12:26:50,938 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:27:01,231 - Epoch: [143][  100/  391]    Overall Loss 0.506073    Objective Loss 0.506073                                        LR 0.023500    Time 0.102857    
2024-02-17 12:27:10,869 - Epoch: [143][  200/  391]    Overall Loss 0.516722    Objective Loss 0.516722                                        LR 0.023500    Time 0.099595    
2024-02-17 12:27:20,533 - Epoch: [143][  300/  391]    Overall Loss 0.524782    Objective Loss 0.524782                                        LR 0.023500    Time 0.098596    
2024-02-17 12:27:29,046 - Epoch: [143][  391/  391]    Overall Loss 0.535591    Objective Loss 0.535591    Top1 79.326923    Top5 99.038462    LR 0.023500    Time 0.097412    
2024-02-17 12:27:29,228 - --- validate (epoch=143)-----------
2024-02-17 12:27:29,228 - 10000 samples (128 per mini-batch)
2024-02-17 12:27:31,924 - Epoch: [143][   79/   79]    Loss 1.403229    Top1 62.710000    Top5 88.530000    
2024-02-17 12:27:32,187 - ==> Top1: 62.710    Top5: 88.530    Loss: 1.403

2024-02-17 12:27:32,205 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:27:32,206 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:27:32,281 - 

2024-02-17 12:27:32,282 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:27:42,796 - Epoch: [144][  100/  391]    Overall Loss 0.511261    Objective Loss 0.511261                                        LR 0.023500    Time 0.105067    
2024-02-17 12:27:52,451 - Epoch: [144][  200/  391]    Overall Loss 0.515480    Objective Loss 0.515480                                        LR 0.023500    Time 0.100790    
2024-02-17 12:28:02,217 - Epoch: [144][  300/  391]    Overall Loss 0.523080    Objective Loss 0.523080                                        LR 0.023500    Time 0.099729    
2024-02-17 12:28:11,001 - Epoch: [144][  391/  391]    Overall Loss 0.534124    Objective Loss 0.534124    Top1 83.653846    Top5 99.519231    LR 0.023500    Time 0.098974    
2024-02-17 12:28:11,209 - --- validate (epoch=144)-----------
2024-02-17 12:28:11,210 - 10000 samples (128 per mini-batch)
2024-02-17 12:28:14,031 - Epoch: [144][   79/   79]    Loss 1.392520    Top1 63.310000    Top5 88.690000    
2024-02-17 12:28:14,183 - ==> Top1: 63.310    Top5: 88.690    Loss: 1.393

2024-02-17 12:28:14,206 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:28:14,207 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:28:14,285 - 

2024-02-17 12:28:14,285 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:28:24,517 - Epoch: [145][  100/  391]    Overall Loss 0.497500    Objective Loss 0.497500                                        LR 0.023500    Time 0.102245    
2024-02-17 12:28:33,942 - Epoch: [145][  200/  391]    Overall Loss 0.509858    Objective Loss 0.509858                                        LR 0.023500    Time 0.098223    
2024-02-17 12:28:43,606 - Epoch: [145][  300/  391]    Overall Loss 0.521146    Objective Loss 0.521146                                        LR 0.023500    Time 0.097683    
2024-02-17 12:28:52,471 - Epoch: [145][  391/  391]    Overall Loss 0.532100    Objective Loss 0.532100    Top1 86.057692    Top5 100.000000    LR 0.023500    Time 0.097611    
2024-02-17 12:28:52,682 - --- validate (epoch=145)-----------
2024-02-17 12:28:52,683 - 10000 samples (128 per mini-batch)
2024-02-17 12:28:55,540 - Epoch: [145][   79/   79]    Loss 1.369270    Top1 63.570000    Top5 88.880000    
2024-02-17 12:28:55,716 - ==> Top1: 63.570    Top5: 88.880    Loss: 1.369

2024-02-17 12:28:55,734 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:28:55,735 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:28:55,811 - 

2024-02-17 12:28:55,811 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:29:06,264 - Epoch: [146][  100/  391]    Overall Loss 0.505544    Objective Loss 0.505544                                        LR 0.023500    Time 0.104456    
2024-02-17 12:29:15,782 - Epoch: [146][  200/  391]    Overall Loss 0.508453    Objective Loss 0.508453                                        LR 0.023500    Time 0.099799    
2024-02-17 12:29:25,434 - Epoch: [146][  300/  391]    Overall Loss 0.518757    Objective Loss 0.518757                                        LR 0.023500    Time 0.098689    
2024-02-17 12:29:34,240 - Epoch: [146][  391/  391]    Overall Loss 0.527722    Objective Loss 0.527722    Top1 78.365385    Top5 95.192308    LR 0.023500    Time 0.098232    
2024-02-17 12:29:34,449 - --- validate (epoch=146)-----------
2024-02-17 12:29:34,449 - 10000 samples (128 per mini-batch)
2024-02-17 12:29:37,207 - Epoch: [146][   79/   79]    Loss 1.445637    Top1 62.150000    Top5 88.240000    
2024-02-17 12:29:37,389 - ==> Top1: 62.150    Top5: 88.240    Loss: 1.446

2024-02-17 12:29:37,409 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:29:37,410 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:29:37,487 - 

2024-02-17 12:29:37,488 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:29:47,857 - Epoch: [147][  100/  391]    Overall Loss 0.515762    Objective Loss 0.515762                                        LR 0.023500    Time 0.103622    
2024-02-17 12:29:58,034 - Epoch: [147][  200/  391]    Overall Loss 0.522330    Objective Loss 0.522330                                        LR 0.023500    Time 0.102675    
2024-02-17 12:30:07,760 - Epoch: [147][  300/  391]    Overall Loss 0.527639    Objective Loss 0.527639                                        LR 0.023500    Time 0.100852    
2024-02-17 12:30:16,648 - Epoch: [147][  391/  391]    Overall Loss 0.533106    Objective Loss 0.533106    Top1 87.500000    Top5 99.038462    LR 0.023500    Time 0.100101    
2024-02-17 12:30:16,827 - --- validate (epoch=147)-----------
2024-02-17 12:30:16,827 - 10000 samples (128 per mini-batch)
2024-02-17 12:30:19,537 - Epoch: [147][   79/   79]    Loss 1.429253    Top1 62.560000    Top5 88.260000    
2024-02-17 12:30:19,681 - ==> Top1: 62.560    Top5: 88.260    Loss: 1.429

2024-02-17 12:30:19,700 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:30:19,700 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:30:19,777 - 

2024-02-17 12:30:19,777 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:30:29,640 - Epoch: [148][  100/  391]    Overall Loss 0.483663    Objective Loss 0.483663                                        LR 0.023500    Time 0.098557    
2024-02-17 12:30:39,186 - Epoch: [148][  200/  391]    Overall Loss 0.503727    Objective Loss 0.503727                                        LR 0.023500    Time 0.096984    
2024-02-17 12:30:48,818 - Epoch: [148][  300/  391]    Overall Loss 0.519878    Objective Loss 0.519878                                        LR 0.023500    Time 0.096749    
2024-02-17 12:30:57,682 - Epoch: [148][  391/  391]    Overall Loss 0.528989    Objective Loss 0.528989    Top1 79.326923    Top5 97.115385    LR 0.023500    Time 0.096892    
2024-02-17 12:30:57,900 - --- validate (epoch=148)-----------
2024-02-17 12:30:57,901 - 10000 samples (128 per mini-batch)
2024-02-17 12:31:00,685 - Epoch: [148][   79/   79]    Loss 1.451930    Top1 62.220000    Top5 88.140000    
2024-02-17 12:31:00,929 - ==> Top1: 62.220    Top5: 88.140    Loss: 1.452

2024-02-17 12:31:00,949 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:31:00,949 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:31:01,230 - 

2024-02-17 12:31:01,230 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:31:11,245 - Epoch: [149][  100/  391]    Overall Loss 0.499607    Objective Loss 0.499607                                        LR 0.023500    Time 0.100080    
2024-02-17 12:31:20,790 - Epoch: [149][  200/  391]    Overall Loss 0.514419    Objective Loss 0.514419                                        LR 0.023500    Time 0.097742    
2024-02-17 12:31:30,063 - Epoch: [149][  300/  391]    Overall Loss 0.520507    Objective Loss 0.520507                                        LR 0.023500    Time 0.096056    
2024-02-17 12:31:38,833 - Epoch: [149][  391/  391]    Overall Loss 0.523791    Objective Loss 0.523791    Top1 77.884615    Top5 96.634615    LR 0.023500    Time 0.096120    
2024-02-17 12:31:39,095 - --- validate (epoch=149)-----------
2024-02-17 12:31:39,095 - 10000 samples (128 per mini-batch)
2024-02-17 12:31:41,854 - Epoch: [149][   79/   79]    Loss 1.443723    Top1 62.570000    Top5 88.140000    
2024-02-17 12:31:42,030 - ==> Top1: 62.570    Top5: 88.140    Loss: 1.444

2024-02-17 12:31:42,050 - ==> Best [Top1: 66.110   Top5: 90.090   Sparsity:0.00   Params: 1341960 on epoch: 110]
2024-02-17 12:31:42,050 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:31:42,125 - 

2024-02-17 12:31:42,125 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:31:52,241 - Epoch: [150][  100/  391]    Overall Loss 0.404760    Objective Loss 0.404760                                        LR 0.005522    Time 0.101087    
2024-02-17 12:32:01,792 - Epoch: [150][  200/  391]    Overall Loss 0.394318    Objective Loss 0.394318                                        LR 0.005522    Time 0.098277    
2024-02-17 12:32:11,232 - Epoch: [150][  300/  391]    Overall Loss 0.381782    Objective Loss 0.381782                                        LR 0.005522    Time 0.096970    
2024-02-17 12:32:19,946 - Epoch: [150][  391/  391]    Overall Loss 0.376352    Objective Loss 0.376352    Top1 91.346154    Top5 99.038462    LR 0.005522    Time 0.096678    
2024-02-17 12:32:20,148 - --- validate (epoch=150)-----------
2024-02-17 12:32:20,149 - 10000 samples (128 per mini-batch)
2024-02-17 12:32:22,786 - Epoch: [150][   79/   79]    Loss 1.232871    Top1 67.010000    Top5 90.670000    
2024-02-17 12:32:22,902 - ==> Top1: 67.010    Top5: 90.670    Loss: 1.233

2024-02-17 12:32:22,921 - ==> Best [Top1: 67.010   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 150]
2024-02-17 12:32:22,921 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:32:23,013 - 

2024-02-17 12:32:23,014 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:32:33,090 - Epoch: [151][  100/  391]    Overall Loss 0.337969    Objective Loss 0.337969                                        LR 0.005522    Time 0.100694    
2024-02-17 12:32:41,162 - Epoch: [151][  200/  391]    Overall Loss 0.335457    Objective Loss 0.335457                                        LR 0.005522    Time 0.090687    
2024-02-17 12:32:50,500 - Epoch: [151][  300/  391]    Overall Loss 0.331588    Objective Loss 0.331588                                        LR 0.005522    Time 0.091571    
2024-02-17 12:32:59,175 - Epoch: [151][  391/  391]    Overall Loss 0.332760    Objective Loss 0.332760    Top1 92.307692    Top5 99.519231    LR 0.005522    Time 0.092433    
2024-02-17 12:32:59,382 - --- validate (epoch=151)-----------
2024-02-17 12:32:59,382 - 10000 samples (128 per mini-batch)
2024-02-17 12:33:02,100 - Epoch: [151][   79/   79]    Loss 1.224059    Top1 67.100000    Top5 90.580000    
2024-02-17 12:33:02,287 - ==> Top1: 67.100    Top5: 90.580    Loss: 1.224

2024-02-17 12:33:02,303 - ==> Best [Top1: 67.100   Top5: 90.580   Sparsity:0.00   Params: 1341960 on epoch: 151]
2024-02-17 12:33:02,303 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:33:02,417 - 

2024-02-17 12:33:02,417 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:33:11,468 - Epoch: [152][  100/  391]    Overall Loss 0.314529    Objective Loss 0.314529                                        LR 0.005522    Time 0.090438    
2024-02-17 12:33:20,106 - Epoch: [152][  200/  391]    Overall Loss 0.308978    Objective Loss 0.308978                                        LR 0.005522    Time 0.088391    
2024-02-17 12:33:29,787 - Epoch: [152][  300/  391]    Overall Loss 0.311261    Objective Loss 0.311261                                        LR 0.005522    Time 0.091183    
2024-02-17 12:33:38,199 - Epoch: [152][  391/  391]    Overall Loss 0.315546    Objective Loss 0.315546    Top1 85.576923    Top5 99.519231    LR 0.005522    Time 0.091465    
2024-02-17 12:33:38,464 - --- validate (epoch=152)-----------
2024-02-17 12:33:38,465 - 10000 samples (128 per mini-batch)
2024-02-17 12:33:41,173 - Epoch: [152][   79/   79]    Loss 1.244324    Top1 67.250000    Top5 90.670000    
2024-02-17 12:33:41,326 - ==> Top1: 67.250    Top5: 90.670    Loss: 1.244

2024-02-17 12:33:41,340 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:33:41,340 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:33:41,429 - 

2024-02-17 12:33:41,429 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:33:51,891 - Epoch: [153][  100/  391]    Overall Loss 0.311488    Objective Loss 0.311488                                        LR 0.005522    Time 0.104548    
2024-02-17 12:34:01,700 - Epoch: [153][  200/  391]    Overall Loss 0.308623    Objective Loss 0.308623                                        LR 0.005522    Time 0.101296    
2024-02-17 12:34:11,120 - Epoch: [153][  300/  391]    Overall Loss 0.307036    Objective Loss 0.307036                                        LR 0.005522    Time 0.098918    
2024-02-17 12:34:19,384 - Epoch: [153][  391/  391]    Overall Loss 0.307062    Objective Loss 0.307062    Top1 88.942308    Top5 100.000000    LR 0.005522    Time 0.097020    
2024-02-17 12:34:19,660 - --- validate (epoch=153)-----------
2024-02-17 12:34:19,661 - 10000 samples (128 per mini-batch)
2024-02-17 12:34:22,375 - Epoch: [153][   79/   79]    Loss 1.239815    Top1 66.950000    Top5 90.360000    
2024-02-17 12:34:22,584 - ==> Top1: 66.950    Top5: 90.360    Loss: 1.240

2024-02-17 12:34:22,603 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:34:22,603 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:34:22,679 - 

2024-02-17 12:34:22,679 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:34:33,369 - Epoch: [154][  100/  391]    Overall Loss 0.287211    Objective Loss 0.287211                                        LR 0.005522    Time 0.106824    
2024-02-17 12:34:42,873 - Epoch: [154][  200/  391]    Overall Loss 0.291827    Objective Loss 0.291827                                        LR 0.005522    Time 0.100908    
2024-02-17 12:34:52,446 - Epoch: [154][  300/  391]    Overall Loss 0.296863    Objective Loss 0.296863                                        LR 0.005522    Time 0.099169    
2024-02-17 12:35:00,835 - Epoch: [154][  391/  391]    Overall Loss 0.298064    Objective Loss 0.298064    Top1 90.865385    Top5 99.038462    LR 0.005522    Time 0.097534    
2024-02-17 12:35:01,031 - --- validate (epoch=154)-----------
2024-02-17 12:35:01,032 - 10000 samples (128 per mini-batch)
2024-02-17 12:35:04,097 - Epoch: [154][   79/   79]    Loss 1.260500    Top1 66.990000    Top5 90.430000    
2024-02-17 12:35:04,251 - ==> Top1: 66.990    Top5: 90.430    Loss: 1.261

2024-02-17 12:35:04,267 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:35:04,267 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:35:04,348 - 

2024-02-17 12:35:04,348 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:35:14,631 - Epoch: [155][  100/  391]    Overall Loss 0.279161    Objective Loss 0.279161                                        LR 0.005522    Time 0.102751    
2024-02-17 12:35:24,157 - Epoch: [155][  200/  391]    Overall Loss 0.287515    Objective Loss 0.287515                                        LR 0.005522    Time 0.098983    
2024-02-17 12:35:33,774 - Epoch: [155][  300/  391]    Overall Loss 0.289487    Objective Loss 0.289487                                        LR 0.005522    Time 0.098030    
2024-02-17 12:35:42,218 - Epoch: [155][  391/  391]    Overall Loss 0.289827    Objective Loss 0.289827    Top1 94.230769    Top5 99.519231    LR 0.005522    Time 0.096801    
2024-02-17 12:35:42,416 - --- validate (epoch=155)-----------
2024-02-17 12:35:42,418 - 10000 samples (128 per mini-batch)
2024-02-17 12:35:45,332 - Epoch: [155][   79/   79]    Loss 1.260405    Top1 66.810000    Top5 90.430000    
2024-02-17 12:35:45,536 - ==> Top1: 66.810    Top5: 90.430    Loss: 1.260

2024-02-17 12:35:45,555 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:35:45,555 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:35:45,630 - 

2024-02-17 12:35:45,631 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:35:56,033 - Epoch: [156][  100/  391]    Overall Loss 0.276131    Objective Loss 0.276131                                        LR 0.005522    Time 0.103948    
2024-02-17 12:36:05,524 - Epoch: [156][  200/  391]    Overall Loss 0.277669    Objective Loss 0.277669                                        LR 0.005522    Time 0.099408    
2024-02-17 12:36:15,137 - Epoch: [156][  300/  391]    Overall Loss 0.281614    Objective Loss 0.281614                                        LR 0.005522    Time 0.098297    
2024-02-17 12:36:23,608 - Epoch: [156][  391/  391]    Overall Loss 0.281630    Objective Loss 0.281630    Top1 91.346154    Top5 99.038462    LR 0.005522    Time 0.097074    
2024-02-17 12:36:23,813 - --- validate (epoch=156)-----------
2024-02-17 12:36:23,814 - 10000 samples (128 per mini-batch)
2024-02-17 12:36:26,788 - Epoch: [156][   79/   79]    Loss 1.252237    Top1 66.960000    Top5 90.480000    
2024-02-17 12:36:26,966 - ==> Top1: 66.960    Top5: 90.480    Loss: 1.252

2024-02-17 12:36:26,988 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:36:26,988 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:36:27,069 - 

2024-02-17 12:36:27,069 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:36:36,658 - Epoch: [157][  100/  391]    Overall Loss 0.266113    Objective Loss 0.266113                                        LR 0.005522    Time 0.095791    
2024-02-17 12:36:46,302 - Epoch: [157][  200/  391]    Overall Loss 0.273138    Objective Loss 0.273138                                        LR 0.005522    Time 0.096097    
2024-02-17 12:36:54,281 - Epoch: [157][  300/  391]    Overall Loss 0.277272    Objective Loss 0.277272                                        LR 0.005522    Time 0.090645    
2024-02-17 12:37:02,272 - Epoch: [157][  391/  391]    Overall Loss 0.279318    Objective Loss 0.279318    Top1 91.826923    Top5 99.038462    LR 0.005522    Time 0.089976    
2024-02-17 12:37:02,474 - --- validate (epoch=157)-----------
2024-02-17 12:37:02,475 - 10000 samples (128 per mini-batch)
2024-02-17 12:37:05,242 - Epoch: [157][   79/   79]    Loss 1.282982    Top1 66.430000    Top5 90.060000    
2024-02-17 12:37:05,410 - ==> Top1: 66.430    Top5: 90.060    Loss: 1.283

2024-02-17 12:37:05,421 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:37:05,422 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:37:05,495 - 

2024-02-17 12:37:05,495 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:37:15,921 - Epoch: [158][  100/  391]    Overall Loss 0.258010    Objective Loss 0.258010                                        LR 0.005522    Time 0.104183    
2024-02-17 12:37:25,481 - Epoch: [158][  200/  391]    Overall Loss 0.270554    Objective Loss 0.270554                                        LR 0.005522    Time 0.099867    
2024-02-17 12:37:34,597 - Epoch: [158][  300/  391]    Overall Loss 0.270927    Objective Loss 0.270927                                        LR 0.005522    Time 0.096952    
2024-02-17 12:37:43,422 - Epoch: [158][  391/  391]    Overall Loss 0.271884    Objective Loss 0.271884    Top1 91.826923    Top5 98.557692    LR 0.005522    Time 0.096945    
2024-02-17 12:37:43,601 - --- validate (epoch=158)-----------
2024-02-17 12:37:43,602 - 10000 samples (128 per mini-batch)
2024-02-17 12:37:46,396 - Epoch: [158][   79/   79]    Loss 1.297263    Top1 66.580000    Top5 90.440000    
2024-02-17 12:37:46,540 - ==> Top1: 66.580    Top5: 90.440    Loss: 1.297

2024-02-17 12:37:46,560 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:37:46,560 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:37:46,636 - 

2024-02-17 12:37:46,636 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:37:56,744 - Epoch: [159][  100/  391]    Overall Loss 0.262810    Objective Loss 0.262810                                        LR 0.005522    Time 0.101007    
2024-02-17 12:38:06,302 - Epoch: [159][  200/  391]    Overall Loss 0.264519    Objective Loss 0.264519                                        LR 0.005522    Time 0.098272    
2024-02-17 12:38:15,771 - Epoch: [159][  300/  391]    Overall Loss 0.265082    Objective Loss 0.265082                                        LR 0.005522    Time 0.097064    
2024-02-17 12:38:24,641 - Epoch: [159][  391/  391]    Overall Loss 0.266618    Objective Loss 0.266618    Top1 89.423077    Top5 98.076923    LR 0.005522    Time 0.097146    
2024-02-17 12:38:24,837 - --- validate (epoch=159)-----------
2024-02-17 12:38:24,837 - 10000 samples (128 per mini-batch)
2024-02-17 12:38:27,517 - Epoch: [159][   79/   79]    Loss 1.277716    Top1 66.960000    Top5 90.310000    
2024-02-17 12:38:27,675 - ==> Top1: 66.960    Top5: 90.310    Loss: 1.278

2024-02-17 12:38:27,695 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:38:27,695 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:38:27,770 - 

2024-02-17 12:38:27,771 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:38:38,188 - Epoch: [160][  100/  391]    Overall Loss 0.251683    Objective Loss 0.251683                                        LR 0.005522    Time 0.104103    
2024-02-17 12:38:47,739 - Epoch: [160][  200/  391]    Overall Loss 0.258876    Objective Loss 0.258876                                        LR 0.005522    Time 0.099782    
2024-02-17 12:38:57,379 - Epoch: [160][  300/  391]    Overall Loss 0.260912    Objective Loss 0.260912                                        LR 0.005522    Time 0.098640    
2024-02-17 12:39:05,909 - Epoch: [160][  391/  391]    Overall Loss 0.262499    Objective Loss 0.262499    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.097489    
2024-02-17 12:39:06,106 - --- validate (epoch=160)-----------
2024-02-17 12:39:06,106 - 10000 samples (128 per mini-batch)
2024-02-17 12:39:09,022 - Epoch: [160][   79/   79]    Loss 1.284113    Top1 66.650000    Top5 90.270000    
2024-02-17 12:39:09,239 - ==> Top1: 66.650    Top5: 90.270    Loss: 1.284

2024-02-17 12:39:09,250 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:39:09,250 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:39:09,323 - 

2024-02-17 12:39:09,324 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:39:19,439 - Epoch: [161][  100/  391]    Overall Loss 0.247426    Objective Loss 0.247426                                        LR 0.005522    Time 0.101079    
2024-02-17 12:39:28,890 - Epoch: [161][  200/  391]    Overall Loss 0.250927    Objective Loss 0.250927                                        LR 0.005522    Time 0.097771    
2024-02-17 12:39:38,447 - Epoch: [161][  300/  391]    Overall Loss 0.255053    Objective Loss 0.255053                                        LR 0.005522    Time 0.097023    
2024-02-17 12:39:47,061 - Epoch: [161][  391/  391]    Overall Loss 0.256470    Objective Loss 0.256470    Top1 94.711538    Top5 99.519231    LR 0.005522    Time 0.096463    
2024-02-17 12:39:47,264 - --- validate (epoch=161)-----------
2024-02-17 12:39:47,264 - 10000 samples (128 per mini-batch)
2024-02-17 12:39:50,036 - Epoch: [161][   79/   79]    Loss 1.288213    Top1 66.610000    Top5 90.280000    
2024-02-17 12:39:50,272 - ==> Top1: 66.610    Top5: 90.280    Loss: 1.288

2024-02-17 12:39:50,284 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:39:50,284 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:39:50,358 - 

2024-02-17 12:39:50,358 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:40:00,668 - Epoch: [162][  100/  391]    Overall Loss 0.245268    Objective Loss 0.245268                                        LR 0.005522    Time 0.103023    
2024-02-17 12:40:10,173 - Epoch: [162][  200/  391]    Overall Loss 0.249498    Objective Loss 0.249498                                        LR 0.005522    Time 0.099018    
2024-02-17 12:40:19,854 - Epoch: [162][  300/  391]    Overall Loss 0.251475    Objective Loss 0.251475                                        LR 0.005522    Time 0.098266    
2024-02-17 12:40:28,527 - Epoch: [162][  391/  391]    Overall Loss 0.253709    Objective Loss 0.253709    Top1 93.269231    Top5 100.000000    LR 0.005522    Time 0.097566    
2024-02-17 12:40:28,708 - --- validate (epoch=162)-----------
2024-02-17 12:40:28,708 - 10000 samples (128 per mini-batch)
2024-02-17 12:40:31,620 - Epoch: [162][   79/   79]    Loss 1.289234    Top1 66.760000    Top5 90.090000    
2024-02-17 12:40:31,801 - ==> Top1: 66.760    Top5: 90.090    Loss: 1.289

2024-02-17 12:40:31,822 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:40:31,822 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:40:31,897 - 

2024-02-17 12:40:31,897 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:40:41,977 - Epoch: [163][  100/  391]    Overall Loss 0.245569    Objective Loss 0.245569                                        LR 0.005522    Time 0.100729    
2024-02-17 12:40:51,325 - Epoch: [163][  200/  391]    Overall Loss 0.245899    Objective Loss 0.245899                                        LR 0.005522    Time 0.097083    
2024-02-17 12:41:00,692 - Epoch: [163][  300/  391]    Overall Loss 0.248700    Objective Loss 0.248700                                        LR 0.005522    Time 0.095931    
2024-02-17 12:41:09,518 - Epoch: [163][  391/  391]    Overall Loss 0.251160    Objective Loss 0.251160    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.096166    
2024-02-17 12:41:09,731 - --- validate (epoch=163)-----------
2024-02-17 12:41:09,732 - 10000 samples (128 per mini-batch)
2024-02-17 12:41:12,483 - Epoch: [163][   79/   79]    Loss 1.309039    Top1 66.980000    Top5 90.130000    
2024-02-17 12:41:12,672 - ==> Top1: 66.980    Top5: 90.130    Loss: 1.309

2024-02-17 12:41:12,692 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:41:12,692 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:41:12,767 - 

2024-02-17 12:41:12,767 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:41:22,961 - Epoch: [164][  100/  391]    Overall Loss 0.241939    Objective Loss 0.241939                                        LR 0.005522    Time 0.101862    
2024-02-17 12:41:32,563 - Epoch: [164][  200/  391]    Overall Loss 0.245077    Objective Loss 0.245077                                        LR 0.005522    Time 0.098917    
2024-02-17 12:41:41,773 - Epoch: [164][  300/  391]    Overall Loss 0.247411    Objective Loss 0.247411                                        LR 0.005522    Time 0.096632    
2024-02-17 12:41:50,940 - Epoch: [164][  391/  391]    Overall Loss 0.249987    Objective Loss 0.249987    Top1 96.153846    Top5 100.000000    LR 0.005522    Time 0.097575    
2024-02-17 12:41:51,104 - --- validate (epoch=164)-----------
2024-02-17 12:41:51,105 - 10000 samples (128 per mini-batch)
2024-02-17 12:41:54,144 - Epoch: [164][   79/   79]    Loss 1.300506    Top1 66.680000    Top5 90.150000    
2024-02-17 12:41:54,314 - ==> Top1: 66.680    Top5: 90.150    Loss: 1.301

2024-02-17 12:41:54,332 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:41:54,333 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:41:54,409 - 

2024-02-17 12:41:54,409 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:42:04,803 - Epoch: [165][  100/  391]    Overall Loss 0.230396    Objective Loss 0.230396                                        LR 0.005522    Time 0.103865    
2024-02-17 12:42:14,372 - Epoch: [165][  200/  391]    Overall Loss 0.239541    Objective Loss 0.239541                                        LR 0.005522    Time 0.099755    
2024-02-17 12:42:23,433 - Epoch: [165][  300/  391]    Overall Loss 0.242799    Objective Loss 0.242799                                        LR 0.005522    Time 0.096694    
2024-02-17 12:42:31,757 - Epoch: [165][  391/  391]    Overall Loss 0.244418    Objective Loss 0.244418    Top1 90.865385    Top5 99.519231    LR 0.005522    Time 0.095468    
2024-02-17 12:42:31,991 - --- validate (epoch=165)-----------
2024-02-17 12:42:31,992 - 10000 samples (128 per mini-batch)
2024-02-17 12:42:34,756 - Epoch: [165][   79/   79]    Loss 1.300255    Top1 66.990000    Top5 90.210000    
2024-02-17 12:42:34,915 - ==> Top1: 66.990    Top5: 90.210    Loss: 1.300

2024-02-17 12:42:34,934 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:42:34,935 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:42:35,014 - 

2024-02-17 12:42:35,014 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:42:45,345 - Epoch: [166][  100/  391]    Overall Loss 0.239461    Objective Loss 0.239461                                        LR 0.005522    Time 0.103237    
2024-02-17 12:42:54,867 - Epoch: [166][  200/  391]    Overall Loss 0.239250    Objective Loss 0.239250                                        LR 0.005522    Time 0.099206    
2024-02-17 12:43:04,113 - Epoch: [166][  300/  391]    Overall Loss 0.242436    Objective Loss 0.242436                                        LR 0.005522    Time 0.096943    
2024-02-17 12:43:12,902 - Epoch: [166][  391/  391]    Overall Loss 0.242766    Objective Loss 0.242766    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.096848    
2024-02-17 12:43:13,075 - --- validate (epoch=166)-----------
2024-02-17 12:43:13,075 - 10000 samples (128 per mini-batch)
2024-02-17 12:43:15,883 - Epoch: [166][   79/   79]    Loss 1.336991    Top1 66.440000    Top5 89.920000    
2024-02-17 12:43:16,088 - ==> Top1: 66.440    Top5: 89.920    Loss: 1.337

2024-02-17 12:43:16,099 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:43:16,099 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:43:16,171 - 

2024-02-17 12:43:16,172 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:43:26,253 - Epoch: [167][  100/  391]    Overall Loss 0.232500    Objective Loss 0.232500                                        LR 0.005522    Time 0.100740    
2024-02-17 12:43:35,849 - Epoch: [167][  200/  391]    Overall Loss 0.234312    Objective Loss 0.234312                                        LR 0.005522    Time 0.098328    
2024-02-17 12:43:45,057 - Epoch: [167][  300/  391]    Overall Loss 0.237491    Objective Loss 0.237491                                        LR 0.005522    Time 0.096230    
2024-02-17 12:43:53,772 - Epoch: [167][  391/  391]    Overall Loss 0.239577    Objective Loss 0.239577    Top1 90.865385    Top5 100.000000    LR 0.005522    Time 0.096114    
2024-02-17 12:43:54,061 - --- validate (epoch=167)-----------
2024-02-17 12:43:54,062 - 10000 samples (128 per mini-batch)
2024-02-17 12:43:56,840 - Epoch: [167][   79/   79]    Loss 1.324298    Top1 66.640000    Top5 90.110000    
2024-02-17 12:43:57,047 - ==> Top1: 66.640    Top5: 90.110    Loss: 1.324

2024-02-17 12:43:57,066 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:43:57,066 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:43:57,142 - 

2024-02-17 12:43:57,143 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:44:07,421 - Epoch: [168][  100/  391]    Overall Loss 0.222409    Objective Loss 0.222409                                        LR 0.005522    Time 0.102711    
2024-02-17 12:44:16,990 - Epoch: [168][  200/  391]    Overall Loss 0.229236    Objective Loss 0.229236                                        LR 0.005522    Time 0.099181    
2024-02-17 12:44:26,349 - Epoch: [168][  300/  391]    Overall Loss 0.233375    Objective Loss 0.233375                                        LR 0.005522    Time 0.097301    
2024-02-17 12:44:35,206 - Epoch: [168][  391/  391]    Overall Loss 0.234968    Objective Loss 0.234968    Top1 92.307692    Top5 100.000000    LR 0.005522    Time 0.097296    
2024-02-17 12:44:35,409 - --- validate (epoch=168)-----------
2024-02-17 12:44:35,411 - 10000 samples (128 per mini-batch)
2024-02-17 12:44:38,040 - Epoch: [168][   79/   79]    Loss 1.331299    Top1 66.520000    Top5 90.190000    
2024-02-17 12:44:38,171 - ==> Top1: 66.520    Top5: 90.190    Loss: 1.331

2024-02-17 12:44:38,191 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:44:38,191 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:44:38,267 - 

2024-02-17 12:44:38,267 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:44:48,519 - Epoch: [169][  100/  391]    Overall Loss 0.231639    Objective Loss 0.231639                                        LR 0.005522    Time 0.102444    
2024-02-17 12:44:57,876 - Epoch: [169][  200/  391]    Overall Loss 0.231629    Objective Loss 0.231629                                        LR 0.005522    Time 0.097987    
2024-02-17 12:45:07,498 - Epoch: [169][  300/  391]    Overall Loss 0.231205    Objective Loss 0.231205                                        LR 0.005522    Time 0.097380    
2024-02-17 12:45:16,460 - Epoch: [169][  391/  391]    Overall Loss 0.233654    Objective Loss 0.233654    Top1 93.750000    Top5 99.519231    LR 0.005522    Time 0.097625    
2024-02-17 12:45:16,688 - --- validate (epoch=169)-----------
2024-02-17 12:45:16,689 - 10000 samples (128 per mini-batch)
2024-02-17 12:45:19,415 - Epoch: [169][   79/   79]    Loss 1.317482    Top1 66.820000    Top5 90.010000    
2024-02-17 12:45:19,588 - ==> Top1: 66.820    Top5: 90.010    Loss: 1.317

2024-02-17 12:45:19,607 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:45:19,607 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:45:19,682 - 

2024-02-17 12:45:19,682 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:45:28,516 - Epoch: [170][  100/  391]    Overall Loss 0.224247    Objective Loss 0.224247                                        LR 0.005522    Time 0.088273    
2024-02-17 12:45:35,819 - Epoch: [170][  200/  391]    Overall Loss 0.227131    Objective Loss 0.227131                                        LR 0.005522    Time 0.080631    
2024-02-17 12:45:45,177 - Epoch: [170][  300/  391]    Overall Loss 0.231135    Objective Loss 0.231135                                        LR 0.005522    Time 0.084933    
2024-02-17 12:45:53,845 - Epoch: [170][  391/  391]    Overall Loss 0.233916    Objective Loss 0.233916    Top1 92.307692    Top5 99.519231    LR 0.005522    Time 0.087322    
2024-02-17 12:45:54,038 - --- validate (epoch=170)-----------
2024-02-17 12:45:54,039 - 10000 samples (128 per mini-batch)
2024-02-17 12:45:56,776 - Epoch: [170][   79/   79]    Loss 1.330065    Top1 66.560000    Top5 90.030000    
2024-02-17 12:45:56,927 - ==> Top1: 66.560    Top5: 90.030    Loss: 1.330

2024-02-17 12:45:56,953 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:45:56,953 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:45:57,054 - 

2024-02-17 12:45:57,054 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:46:07,248 - Epoch: [171][  100/  391]    Overall Loss 0.220583    Objective Loss 0.220583                                        LR 0.005522    Time 0.101867    
2024-02-17 12:46:16,823 - Epoch: [171][  200/  391]    Overall Loss 0.222879    Objective Loss 0.222879                                        LR 0.005522    Time 0.098784    
2024-02-17 12:46:26,223 - Epoch: [171][  300/  391]    Overall Loss 0.225976    Objective Loss 0.225976                                        LR 0.005522    Time 0.097177    
2024-02-17 12:46:34,689 - Epoch: [171][  391/  391]    Overall Loss 0.226625    Objective Loss 0.226625    Top1 91.346154    Top5 100.000000    LR 0.005522    Time 0.096201    
2024-02-17 12:46:34,884 - --- validate (epoch=171)-----------
2024-02-17 12:46:34,885 - 10000 samples (128 per mini-batch)
2024-02-17 12:46:37,609 - Epoch: [171][   79/   79]    Loss 1.344760    Top1 66.800000    Top5 89.880000    
2024-02-17 12:46:37,826 - ==> Top1: 66.800    Top5: 89.880    Loss: 1.345

2024-02-17 12:46:37,845 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:46:37,845 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:46:37,920 - 

2024-02-17 12:46:37,920 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:46:46,880 - Epoch: [172][  100/  391]    Overall Loss 0.217787    Objective Loss 0.217787                                        LR 0.005522    Time 0.089530    
2024-02-17 12:46:55,314 - Epoch: [172][  200/  391]    Overall Loss 0.222174    Objective Loss 0.222174                                        LR 0.005522    Time 0.086914    
2024-02-17 12:47:04,497 - Epoch: [172][  300/  391]    Overall Loss 0.223789    Objective Loss 0.223789                                        LR 0.005522    Time 0.088539    
2024-02-17 12:47:13,120 - Epoch: [172][  391/  391]    Overall Loss 0.227632    Objective Loss 0.227632    Top1 90.865385    Top5 99.519231    LR 0.005522    Time 0.089976    
2024-02-17 12:47:13,265 - --- validate (epoch=172)-----------
2024-02-17 12:47:13,266 - 10000 samples (128 per mini-batch)
2024-02-17 12:47:16,044 - Epoch: [172][   79/   79]    Loss 1.352497    Top1 66.440000    Top5 89.680000    
2024-02-17 12:47:16,182 - ==> Top1: 66.440    Top5: 89.680    Loss: 1.352

2024-02-17 12:47:16,201 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:47:16,201 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:47:16,277 - 

2024-02-17 12:47:16,278 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:47:26,619 - Epoch: [173][  100/  391]    Overall Loss 0.221297    Objective Loss 0.221297                                        LR 0.005522    Time 0.103343    
2024-02-17 12:47:36,250 - Epoch: [173][  200/  391]    Overall Loss 0.223977    Objective Loss 0.223977                                        LR 0.005522    Time 0.099802    
2024-02-17 12:47:46,064 - Epoch: [173][  300/  391]    Overall Loss 0.227418    Objective Loss 0.227418                                        LR 0.005522    Time 0.099235    
2024-02-17 12:47:54,510 - Epoch: [173][  391/  391]    Overall Loss 0.229388    Objective Loss 0.229388    Top1 92.788462    Top5 100.000000    LR 0.005522    Time 0.097730    
2024-02-17 12:47:54,669 - --- validate (epoch=173)-----------
2024-02-17 12:47:54,669 - 10000 samples (128 per mini-batch)
2024-02-17 12:47:57,290 - Epoch: [173][   79/   79]    Loss 1.343826    Top1 66.600000    Top5 89.750000    
2024-02-17 12:47:57,436 - ==> Top1: 66.600    Top5: 89.750    Loss: 1.344

2024-02-17 12:47:57,456 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:47:57,457 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:47:57,533 - 

2024-02-17 12:47:57,533 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:48:07,635 - Epoch: [174][  100/  391]    Overall Loss 0.217410    Objective Loss 0.217410                                        LR 0.005522    Time 0.100949    
2024-02-17 12:48:17,183 - Epoch: [174][  200/  391]    Overall Loss 0.221184    Objective Loss 0.221184                                        LR 0.005522    Time 0.098191    
2024-02-17 12:48:26,811 - Epoch: [174][  300/  391]    Overall Loss 0.221912    Objective Loss 0.221912                                        LR 0.005522    Time 0.097541    
2024-02-17 12:48:35,358 - Epoch: [174][  391/  391]    Overall Loss 0.225677    Objective Loss 0.225677    Top1 90.865385    Top5 100.000000    LR 0.005522    Time 0.096688    
2024-02-17 12:48:35,549 - --- validate (epoch=174)-----------
2024-02-17 12:48:35,550 - 10000 samples (128 per mini-batch)
2024-02-17 12:48:38,553 - Epoch: [174][   79/   79]    Loss 1.349080    Top1 66.700000    Top5 90.000000    
2024-02-17 12:48:38,675 - ==> Top1: 66.700    Top5: 90.000    Loss: 1.349

2024-02-17 12:48:38,688 - ==> Best [Top1: 67.250   Top5: 90.670   Sparsity:0.00   Params: 1341960 on epoch: 152]
2024-02-17 12:48:38,689 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:48:38,765 - 

2024-02-17 12:48:38,765 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:48:49,079 - Epoch: [175][  100/  391]    Overall Loss 0.200720    Objective Loss 0.200720                                        LR 0.001298    Time 0.103068    
2024-02-17 12:48:58,614 - Epoch: [175][  200/  391]    Overall Loss 0.199762    Objective Loss 0.199762                                        LR 0.001298    Time 0.099186    
2024-02-17 12:49:08,373 - Epoch: [175][  300/  391]    Overall Loss 0.198328    Objective Loss 0.198328                                        LR 0.001298    Time 0.098638    
2024-02-17 12:49:17,505 - Epoch: [175][  391/  391]    Overall Loss 0.198144    Objective Loss 0.198144    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.099024    
2024-02-17 12:49:17,679 - --- validate (epoch=175)-----------
2024-02-17 12:49:17,680 - 10000 samples (128 per mini-batch)
2024-02-17 12:49:20,339 - Epoch: [175][   79/   79]    Loss 1.320566    Top1 67.480000    Top5 90.300000    
2024-02-17 12:49:20,523 - ==> Top1: 67.480    Top5: 90.300    Loss: 1.321

2024-02-17 12:49:20,541 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:49:20,541 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:49:20,632 - 

2024-02-17 12:49:20,633 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:49:31,018 - Epoch: [176][  100/  391]    Overall Loss 0.194081    Objective Loss 0.194081                                        LR 0.001298    Time 0.103779    
2024-02-17 12:49:40,718 - Epoch: [176][  200/  391]    Overall Loss 0.188834    Objective Loss 0.188834                                        LR 0.001298    Time 0.100367    
2024-02-17 12:49:50,512 - Epoch: [176][  300/  391]    Overall Loss 0.189702    Objective Loss 0.189702                                        LR 0.001298    Time 0.099541    
2024-02-17 12:49:59,143 - Epoch: [176][  391/  391]    Overall Loss 0.191612    Objective Loss 0.191612    Top1 92.307692    Top5 100.000000    LR 0.001298    Time 0.098437    
2024-02-17 12:49:59,292 - --- validate (epoch=176)-----------
2024-02-17 12:49:59,293 - 10000 samples (128 per mini-batch)
2024-02-17 12:50:01,975 - Epoch: [176][   79/   79]    Loss 1.327038    Top1 67.030000    Top5 89.960000    
2024-02-17 12:50:02,115 - ==> Top1: 67.030    Top5: 89.960    Loss: 1.327

2024-02-17 12:50:02,134 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:50:02,134 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:50:02,208 - 

2024-02-17 12:50:02,208 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:50:12,417 - Epoch: [177][  100/  391]    Overall Loss 0.177925    Objective Loss 0.177925                                        LR 0.001298    Time 0.102013    
2024-02-17 12:50:21,941 - Epoch: [177][  200/  391]    Overall Loss 0.182974    Objective Loss 0.182974                                        LR 0.001298    Time 0.098604    
2024-02-17 12:50:31,782 - Epoch: [177][  300/  391]    Overall Loss 0.182750    Objective Loss 0.182750                                        LR 0.001298    Time 0.098524    
2024-02-17 12:50:40,272 - Epoch: [177][  391/  391]    Overall Loss 0.184228    Objective Loss 0.184228    Top1 93.750000    Top5 100.000000    LR 0.001298    Time 0.097298    
2024-02-17 12:50:40,573 - --- validate (epoch=177)-----------
2024-02-17 12:50:40,574 - 10000 samples (128 per mini-batch)
2024-02-17 12:50:43,378 - Epoch: [177][   79/   79]    Loss 1.328166    Top1 67.210000    Top5 90.120000    
2024-02-17 12:50:43,536 - ==> Top1: 67.210    Top5: 90.120    Loss: 1.328

2024-02-17 12:50:43,557 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:50:43,557 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:50:43,636 - 

2024-02-17 12:50:43,636 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:50:53,765 - Epoch: [178][  100/  391]    Overall Loss 0.176956    Objective Loss 0.176956                                        LR 0.001298    Time 0.101214    
2024-02-17 12:51:03,276 - Epoch: [178][  200/  391]    Overall Loss 0.180543    Objective Loss 0.180543                                        LR 0.001298    Time 0.098142    
2024-02-17 12:51:12,902 - Epoch: [178][  300/  391]    Overall Loss 0.182040    Objective Loss 0.182040                                        LR 0.001298    Time 0.097500    
2024-02-17 12:51:21,160 - Epoch: [178][  391/  391]    Overall Loss 0.182313    Objective Loss 0.182313    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.095919    
2024-02-17 12:51:21,369 - --- validate (epoch=178)-----------
2024-02-17 12:51:21,370 - 10000 samples (128 per mini-batch)
2024-02-17 12:51:24,301 - Epoch: [178][   79/   79]    Loss 1.330774    Top1 66.990000    Top5 90.280000    
2024-02-17 12:51:24,466 - ==> Top1: 66.990    Top5: 90.280    Loss: 1.331

2024-02-17 12:51:24,483 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:51:24,484 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:51:24,559 - 

2024-02-17 12:51:24,560 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:51:34,680 - Epoch: [179][  100/  391]    Overall Loss 0.183629    Objective Loss 0.183629                                        LR 0.001298    Time 0.101126    
2024-02-17 12:51:44,217 - Epoch: [179][  200/  391]    Overall Loss 0.180428    Objective Loss 0.180428                                        LR 0.001298    Time 0.098225    
2024-02-17 12:51:53,925 - Epoch: [179][  300/  391]    Overall Loss 0.180396    Objective Loss 0.180396                                        LR 0.001298    Time 0.097828    
2024-02-17 12:52:02,210 - Epoch: [179][  391/  391]    Overall Loss 0.180150    Objective Loss 0.180150    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.096240    
2024-02-17 12:52:02,366 - --- validate (epoch=179)-----------
2024-02-17 12:52:02,367 - 10000 samples (128 per mini-batch)
2024-02-17 12:52:05,250 - Epoch: [179][   79/   79]    Loss 1.327668    Top1 67.260000    Top5 90.160000    
2024-02-17 12:52:05,378 - ==> Top1: 67.260    Top5: 90.160    Loss: 1.328

2024-02-17 12:52:05,398 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:52:05,398 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:52:05,475 - 

2024-02-17 12:52:05,475 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:52:15,976 - Epoch: [180][  100/  391]    Overall Loss 0.178427    Objective Loss 0.178427                                        LR 0.001298    Time 0.104941    
2024-02-17 12:52:25,517 - Epoch: [180][  200/  391]    Overall Loss 0.177399    Objective Loss 0.177399                                        LR 0.001298    Time 0.100150    
2024-02-17 12:52:35,248 - Epoch: [180][  300/  391]    Overall Loss 0.179766    Objective Loss 0.179766                                        LR 0.001298    Time 0.099190    
2024-02-17 12:52:43,686 - Epoch: [180][  391/  391]    Overall Loss 0.180987    Objective Loss 0.180987    Top1 93.750000    Top5 100.000000    LR 0.001298    Time 0.097673    
2024-02-17 12:52:43,889 - --- validate (epoch=180)-----------
2024-02-17 12:52:43,890 - 10000 samples (128 per mini-batch)
2024-02-17 12:52:46,513 - Epoch: [180][   79/   79]    Loss 1.322918    Top1 67.360000    Top5 90.070000    
2024-02-17 12:52:46,627 - ==> Top1: 67.360    Top5: 90.070    Loss: 1.323

2024-02-17 12:52:46,647 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:52:46,647 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:52:46,723 - 

2024-02-17 12:52:46,724 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:52:56,844 - Epoch: [181][  100/  391]    Overall Loss 0.174672    Objective Loss 0.174672                                        LR 0.001298    Time 0.101130    
2024-02-17 12:53:06,414 - Epoch: [181][  200/  391]    Overall Loss 0.177580    Objective Loss 0.177580                                        LR 0.001298    Time 0.098392    
2024-02-17 12:53:16,063 - Epoch: [181][  300/  391]    Overall Loss 0.180572    Objective Loss 0.180572                                        LR 0.001298    Time 0.097745    
2024-02-17 12:53:24,573 - Epoch: [181][  391/  391]    Overall Loss 0.180137    Objective Loss 0.180137    Top1 97.596154    Top5 100.000000    LR 0.001298    Time 0.096748    
2024-02-17 12:53:24,775 - --- validate (epoch=181)-----------
2024-02-17 12:53:24,776 - 10000 samples (128 per mini-batch)
2024-02-17 12:53:27,379 - Epoch: [181][   79/   79]    Loss 1.326092    Top1 67.060000    Top5 89.890000    
2024-02-17 12:53:27,529 - ==> Top1: 67.060    Top5: 89.890    Loss: 1.326

2024-02-17 12:53:27,550 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:53:27,550 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:53:27,623 - 

2024-02-17 12:53:27,623 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:53:37,841 - Epoch: [182][  100/  391]    Overall Loss 0.170630    Objective Loss 0.170630                                        LR 0.001298    Time 0.102102    
2024-02-17 12:53:47,439 - Epoch: [182][  200/  391]    Overall Loss 0.172889    Objective Loss 0.172889                                        LR 0.001298    Time 0.099018    
2024-02-17 12:53:57,045 - Epoch: [182][  300/  391]    Overall Loss 0.174278    Objective Loss 0.174278                                        LR 0.001298    Time 0.098017    
2024-02-17 12:54:05,372 - Epoch: [182][  391/  391]    Overall Loss 0.176877    Objective Loss 0.176877    Top1 93.750000    Top5 99.519231    LR 0.001298    Time 0.096492    
2024-02-17 12:54:05,577 - --- validate (epoch=182)-----------
2024-02-17 12:54:05,578 - 10000 samples (128 per mini-batch)
2024-02-17 12:54:08,478 - Epoch: [182][   79/   79]    Loss 1.325496    Top1 67.070000    Top5 90.160000    
2024-02-17 12:54:08,645 - ==> Top1: 67.070    Top5: 90.160    Loss: 1.325

2024-02-17 12:54:08,668 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:54:08,669 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:54:08,748 - 

2024-02-17 12:54:08,748 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:54:19,089 - Epoch: [183][  100/  391]    Overall Loss 0.172277    Objective Loss 0.172277                                        LR 0.001298    Time 0.103337    
2024-02-17 12:54:28,528 - Epoch: [183][  200/  391]    Overall Loss 0.172480    Objective Loss 0.172480                                        LR 0.001298    Time 0.098840    
2024-02-17 12:54:38,051 - Epoch: [183][  300/  391]    Overall Loss 0.173455    Objective Loss 0.173455                                        LR 0.001298    Time 0.097620    
2024-02-17 12:54:46,298 - Epoch: [183][  391/  391]    Overall Loss 0.173278    Objective Loss 0.173278    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.095982    
2024-02-17 12:54:46,518 - --- validate (epoch=183)-----------
2024-02-17 12:54:46,519 - 10000 samples (128 per mini-batch)
2024-02-17 12:54:49,505 - Epoch: [183][   79/   79]    Loss 1.332109    Top1 67.000000    Top5 90.010000    
2024-02-17 12:54:49,689 - ==> Top1: 67.000    Top5: 90.010    Loss: 1.332

2024-02-17 12:54:49,706 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:54:49,706 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:54:49,787 - 

2024-02-17 12:54:49,788 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:55:00,018 - Epoch: [184][  100/  391]    Overall Loss 0.166013    Objective Loss 0.166013                                        LR 0.001298    Time 0.102234    
2024-02-17 12:55:09,194 - Epoch: [184][  200/  391]    Overall Loss 0.170177    Objective Loss 0.170177                                        LR 0.001298    Time 0.096973    
2024-02-17 12:55:18,857 - Epoch: [184][  300/  391]    Overall Loss 0.173037    Objective Loss 0.173037                                        LR 0.001298    Time 0.096844    
2024-02-17 12:55:27,112 - Epoch: [184][  391/  391]    Overall Loss 0.174257    Objective Loss 0.174257    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.095405    
2024-02-17 12:55:27,294 - --- validate (epoch=184)-----------
2024-02-17 12:55:27,295 - 10000 samples (128 per mini-batch)
2024-02-17 12:55:30,520 - Epoch: [184][   79/   79]    Loss 1.328481    Top1 66.740000    Top5 90.080000    
2024-02-17 12:55:30,649 - ==> Top1: 66.740    Top5: 90.080    Loss: 1.328

2024-02-17 12:55:30,664 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:55:30,664 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:55:30,743 - 

2024-02-17 12:55:30,743 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:55:40,896 - Epoch: [185][  100/  391]    Overall Loss 0.169289    Objective Loss 0.169289                                        LR 0.001298    Time 0.101458    
2024-02-17 12:55:50,150 - Epoch: [185][  200/  391]    Overall Loss 0.170546    Objective Loss 0.170546                                        LR 0.001298    Time 0.096976    
2024-02-17 12:55:59,739 - Epoch: [185][  300/  391]    Overall Loss 0.170117    Objective Loss 0.170117                                        LR 0.001298    Time 0.096599    
2024-02-17 12:56:08,178 - Epoch: [185][  391/  391]    Overall Loss 0.170657    Objective Loss 0.170657    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.095688    
2024-02-17 12:56:08,389 - --- validate (epoch=185)-----------
2024-02-17 12:56:08,389 - 10000 samples (128 per mini-batch)
2024-02-17 12:56:11,093 - Epoch: [185][   79/   79]    Loss 1.333425    Top1 67.110000    Top5 90.130000    
2024-02-17 12:56:11,293 - ==> Top1: 67.110    Top5: 90.130    Loss: 1.333

2024-02-17 12:56:11,312 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:56:11,313 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:56:11,397 - 

2024-02-17 12:56:11,397 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:56:21,729 - Epoch: [186][  100/  391]    Overall Loss 0.168003    Objective Loss 0.168003                                        LR 0.001298    Time 0.103245    
2024-02-17 12:56:29,726 - Epoch: [186][  200/  391]    Overall Loss 0.172662    Objective Loss 0.172662                                        LR 0.001298    Time 0.091580    
2024-02-17 12:56:39,444 - Epoch: [186][  300/  391]    Overall Loss 0.170748    Objective Loss 0.170748                                        LR 0.001298    Time 0.093431    
2024-02-17 12:56:47,671 - Epoch: [186][  391/  391]    Overall Loss 0.171379    Objective Loss 0.171379    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.092717    
2024-02-17 12:56:47,846 - --- validate (epoch=186)-----------
2024-02-17 12:56:47,847 - 10000 samples (128 per mini-batch)
2024-02-17 12:56:50,663 - Epoch: [186][   79/   79]    Loss 1.338057    Top1 66.920000    Top5 90.140000    
2024-02-17 12:56:50,807 - ==> Top1: 66.920    Top5: 90.140    Loss: 1.338

2024-02-17 12:56:50,827 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:56:50,827 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:56:50,903 - 

2024-02-17 12:56:50,903 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:57:01,079 - Epoch: [187][  100/  391]    Overall Loss 0.171068    Objective Loss 0.171068                                        LR 0.001298    Time 0.101679    
2024-02-17 12:57:10,222 - Epoch: [187][  200/  391]    Overall Loss 0.169351    Objective Loss 0.169351                                        LR 0.001298    Time 0.096536    
2024-02-17 12:57:19,722 - Epoch: [187][  300/  391]    Overall Loss 0.170330    Objective Loss 0.170330                                        LR 0.001298    Time 0.096007    
2024-02-17 12:57:27,831 - Epoch: [187][  391/  391]    Overall Loss 0.171265    Objective Loss 0.171265    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.094392    
2024-02-17 12:57:28,011 - --- validate (epoch=187)-----------
2024-02-17 12:57:28,012 - 10000 samples (128 per mini-batch)
2024-02-17 12:57:30,733 - Epoch: [187][   79/   79]    Loss 1.328471    Top1 66.900000    Top5 90.030000    
2024-02-17 12:57:30,917 - ==> Top1: 66.900    Top5: 90.030    Loss: 1.328

2024-02-17 12:57:30,936 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:57:30,936 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:57:31,012 - 

2024-02-17 12:57:31,012 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:57:39,293 - Epoch: [188][  100/  391]    Overall Loss 0.169394    Objective Loss 0.169394                                        LR 0.001298    Time 0.082741    
2024-02-17 12:57:48,050 - Epoch: [188][  200/  391]    Overall Loss 0.168713    Objective Loss 0.168713                                        LR 0.001298    Time 0.085135    
2024-02-17 12:57:57,778 - Epoch: [188][  300/  391]    Overall Loss 0.168231    Objective Loss 0.168231                                        LR 0.001298    Time 0.089169    
2024-02-17 12:58:05,895 - Epoch: [188][  391/  391]    Overall Loss 0.168887    Objective Loss 0.168887    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.089167    
2024-02-17 12:58:06,090 - --- validate (epoch=188)-----------
2024-02-17 12:58:06,090 - 10000 samples (128 per mini-batch)
2024-02-17 12:58:09,115 - Epoch: [188][   79/   79]    Loss 1.331676    Top1 66.910000    Top5 90.020000    
2024-02-17 12:58:09,292 - ==> Top1: 66.910    Top5: 90.020    Loss: 1.332

2024-02-17 12:58:09,314 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:58:09,314 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:58:09,396 - 

2024-02-17 12:58:09,396 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:58:19,562 - Epoch: [189][  100/  391]    Overall Loss 0.165290    Objective Loss 0.165290                                        LR 0.001298    Time 0.101584    
2024-02-17 12:58:29,294 - Epoch: [189][  200/  391]    Overall Loss 0.167899    Objective Loss 0.167899                                        LR 0.001298    Time 0.099426    
2024-02-17 12:58:38,959 - Epoch: [189][  300/  391]    Overall Loss 0.167915    Objective Loss 0.167915                                        LR 0.001298    Time 0.098487    
2024-02-17 12:58:47,444 - Epoch: [189][  391/  391]    Overall Loss 0.170486    Objective Loss 0.170486    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.097255    
2024-02-17 12:58:47,643 - --- validate (epoch=189)-----------
2024-02-17 12:58:47,644 - 10000 samples (128 per mini-batch)
2024-02-17 12:58:50,650 - Epoch: [189][   79/   79]    Loss 1.342635    Top1 66.830000    Top5 90.030000    
2024-02-17 12:58:50,842 - ==> Top1: 66.830    Top5: 90.030    Loss: 1.343

2024-02-17 12:58:50,861 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:58:50,861 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:58:50,939 - 

2024-02-17 12:58:50,939 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:59:01,266 - Epoch: [190][  100/  391]    Overall Loss 0.161642    Objective Loss 0.161642                                        LR 0.001298    Time 0.103198    
2024-02-17 12:59:10,973 - Epoch: [190][  200/  391]    Overall Loss 0.163029    Objective Loss 0.163029                                        LR 0.001298    Time 0.100111    
2024-02-17 12:59:20,420 - Epoch: [190][  300/  391]    Overall Loss 0.164693    Objective Loss 0.164693                                        LR 0.001298    Time 0.098213    
2024-02-17 12:59:29,197 - Epoch: [190][  391/  391]    Overall Loss 0.166364    Objective Loss 0.166364    Top1 91.826923    Top5 100.000000    LR 0.001298    Time 0.097794    
2024-02-17 12:59:29,426 - --- validate (epoch=190)-----------
2024-02-17 12:59:29,426 - 10000 samples (128 per mini-batch)
2024-02-17 12:59:32,281 - Epoch: [190][   79/   79]    Loss 1.333915    Top1 66.940000    Top5 90.200000    
2024-02-17 12:59:32,418 - ==> Top1: 66.940    Top5: 90.200    Loss: 1.334

2024-02-17 12:59:32,438 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 12:59:32,439 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 12:59:32,512 - 

2024-02-17 12:59:32,512 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 12:59:42,675 - Epoch: [191][  100/  391]    Overall Loss 0.167240    Objective Loss 0.167240                                        LR 0.001298    Time 0.101555    
2024-02-17 12:59:52,334 - Epoch: [191][  200/  391]    Overall Loss 0.165507    Objective Loss 0.165507                                        LR 0.001298    Time 0.099045    
2024-02-17 13:00:02,028 - Epoch: [191][  300/  391]    Overall Loss 0.166577    Objective Loss 0.166577                                        LR 0.001298    Time 0.098331    
2024-02-17 13:00:10,839 - Epoch: [191][  391/  391]    Overall Loss 0.167198    Objective Loss 0.167198    Top1 97.115385    Top5 99.519231    LR 0.001298    Time 0.097969    
2024-02-17 13:00:11,008 - --- validate (epoch=191)-----------
2024-02-17 13:00:11,008 - 10000 samples (128 per mini-batch)
2024-02-17 13:00:13,949 - Epoch: [191][   79/   79]    Loss 1.350309    Top1 67.060000    Top5 90.090000    
2024-02-17 13:00:14,145 - ==> Top1: 67.060    Top5: 90.090    Loss: 1.350

2024-02-17 13:00:14,164 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:00:14,164 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 13:00:14,241 - 

2024-02-17 13:00:14,241 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:00:24,397 - Epoch: [192][  100/  391]    Overall Loss 0.163152    Objective Loss 0.163152                                        LR 0.001298    Time 0.101480    
2024-02-17 13:00:34,459 - Epoch: [192][  200/  391]    Overall Loss 0.161818    Objective Loss 0.161818                                        LR 0.001298    Time 0.101026    
2024-02-17 13:00:44,414 - Epoch: [192][  300/  391]    Overall Loss 0.162984    Objective Loss 0.162984                                        LR 0.001298    Time 0.100518    
2024-02-17 13:00:53,188 - Epoch: [192][  391/  391]    Overall Loss 0.164090    Objective Loss 0.164090    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.099553    
2024-02-17 13:00:53,397 - --- validate (epoch=192)-----------
2024-02-17 13:00:53,398 - 10000 samples (128 per mini-batch)
2024-02-17 13:00:56,654 - Epoch: [192][   79/   79]    Loss 1.332412    Top1 66.850000    Top5 90.100000    
2024-02-17 13:00:56,797 - ==> Top1: 66.850    Top5: 90.100    Loss: 1.332

2024-02-17 13:00:56,815 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:00:56,815 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 13:00:56,894 - 

2024-02-17 13:00:56,895 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:01:06,999 - Epoch: [193][  100/  391]    Overall Loss 0.161500    Objective Loss 0.161500                                        LR 0.001298    Time 0.100969    
2024-02-17 13:01:16,623 - Epoch: [193][  200/  391]    Overall Loss 0.163752    Objective Loss 0.163752                                        LR 0.001298    Time 0.098582    
2024-02-17 13:01:26,392 - Epoch: [193][  300/  391]    Overall Loss 0.164758    Objective Loss 0.164758                                        LR 0.001298    Time 0.098270    
2024-02-17 13:01:35,084 - Epoch: [193][  391/  391]    Overall Loss 0.166022    Objective Loss 0.166022    Top1 94.230769    Top5 99.519231    LR 0.001298    Time 0.097618    
2024-02-17 13:01:35,292 - --- validate (epoch=193)-----------
2024-02-17 13:01:35,293 - 10000 samples (128 per mini-batch)
2024-02-17 13:01:38,182 - Epoch: [193][   79/   79]    Loss 1.348482    Top1 66.940000    Top5 90.210000    
2024-02-17 13:01:38,335 - ==> Top1: 66.940    Top5: 90.210    Loss: 1.348

2024-02-17 13:01:38,354 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:01:38,354 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 13:01:38,435 - 

2024-02-17 13:01:38,435 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:01:48,668 - Epoch: [194][  100/  391]    Overall Loss 0.159389    Objective Loss 0.159389                                        LR 0.001298    Time 0.102262    
2024-02-17 13:01:58,313 - Epoch: [194][  200/  391]    Overall Loss 0.162840    Objective Loss 0.162840                                        LR 0.001298    Time 0.099332    
2024-02-17 13:02:07,739 - Epoch: [194][  300/  391]    Overall Loss 0.164014    Objective Loss 0.164014                                        LR 0.001298    Time 0.097626    
2024-02-17 13:02:16,422 - Epoch: [194][  391/  391]    Overall Loss 0.163814    Objective Loss 0.163814    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.097101    
2024-02-17 13:02:16,599 - --- validate (epoch=194)-----------
2024-02-17 13:02:16,600 - 10000 samples (128 per mini-batch)
2024-02-17 13:02:19,707 - Epoch: [194][   79/   79]    Loss 1.345392    Top1 66.930000    Top5 90.060000    
2024-02-17 13:02:19,939 - ==> Top1: 66.930    Top5: 90.060    Loss: 1.345

2024-02-17 13:02:19,960 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:02:19,961 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 13:02:20,036 - 

2024-02-17 13:02:20,037 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:02:30,193 - Epoch: [195][  100/  391]    Overall Loss 0.162932    Objective Loss 0.162932                                        LR 0.001298    Time 0.101488    
2024-02-17 13:02:39,907 - Epoch: [195][  200/  391]    Overall Loss 0.164049    Objective Loss 0.164049                                        LR 0.001298    Time 0.099294    
2024-02-17 13:02:49,213 - Epoch: [195][  300/  391]    Overall Loss 0.164216    Objective Loss 0.164216                                        LR 0.001298    Time 0.097198    
2024-02-17 13:02:57,965 - Epoch: [195][  391/  391]    Overall Loss 0.164087    Objective Loss 0.164087    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.096952    
2024-02-17 13:02:58,165 - --- validate (epoch=195)-----------
2024-02-17 13:02:58,167 - 10000 samples (128 per mini-batch)
2024-02-17 13:03:00,908 - Epoch: [195][   79/   79]    Loss 1.341553    Top1 66.670000    Top5 90.110000    
2024-02-17 13:03:01,133 - ==> Top1: 66.670    Top5: 90.110    Loss: 1.342

2024-02-17 13:03:01,153 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:03:01,153 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 13:03:01,229 - 

2024-02-17 13:03:01,230 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:03:11,633 - Epoch: [196][  100/  391]    Overall Loss 0.164768    Objective Loss 0.164768                                        LR 0.001298    Time 0.103953    
2024-02-17 13:03:21,468 - Epoch: [196][  200/  391]    Overall Loss 0.163550    Objective Loss 0.163550                                        LR 0.001298    Time 0.101123    
2024-02-17 13:03:31,026 - Epoch: [196][  300/  391]    Overall Loss 0.163167    Objective Loss 0.163167                                        LR 0.001298    Time 0.099260    
2024-02-17 13:03:39,779 - Epoch: [196][  391/  391]    Overall Loss 0.163966    Objective Loss 0.163966    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.098532    
2024-02-17 13:03:39,971 - --- validate (epoch=196)-----------
2024-02-17 13:03:39,972 - 10000 samples (128 per mini-batch)
2024-02-17 13:03:42,650 - Epoch: [196][   79/   79]    Loss 1.365839    Top1 66.750000    Top5 89.960000    
2024-02-17 13:03:42,794 - ==> Top1: 66.750    Top5: 89.960    Loss: 1.366

2024-02-17 13:03:42,812 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:03:42,813 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 13:03:42,891 - 

2024-02-17 13:03:42,891 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:03:52,971 - Epoch: [197][  100/  391]    Overall Loss 0.160814    Objective Loss 0.160814                                        LR 0.001298    Time 0.100720    
2024-02-17 13:04:02,533 - Epoch: [197][  200/  391]    Overall Loss 0.160856    Objective Loss 0.160856                                        LR 0.001298    Time 0.098149    
2024-02-17 13:04:12,192 - Epoch: [197][  300/  391]    Overall Loss 0.162088    Objective Loss 0.162088                                        LR 0.001298    Time 0.097614    
2024-02-17 13:04:20,875 - Epoch: [197][  391/  391]    Overall Loss 0.162756    Objective Loss 0.162756    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.097093    
2024-02-17 13:04:21,054 - --- validate (epoch=197)-----------
2024-02-17 13:04:21,055 - 10000 samples (128 per mini-batch)
2024-02-17 13:04:23,784 - Epoch: [197][   79/   79]    Loss 1.361022    Top1 66.480000    Top5 90.110000    
2024-02-17 13:04:23,954 - ==> Top1: 66.480    Top5: 90.110    Loss: 1.361

2024-02-17 13:04:23,973 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:04:23,973 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 13:04:24,052 - 

2024-02-17 13:04:24,052 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:04:34,179 - Epoch: [198][  100/  391]    Overall Loss 0.158934    Objective Loss 0.158934                                        LR 0.001298    Time 0.101175    
2024-02-17 13:04:43,718 - Epoch: [198][  200/  391]    Overall Loss 0.158053    Objective Loss 0.158053                                        LR 0.001298    Time 0.098262    
2024-02-17 13:04:53,314 - Epoch: [198][  300/  391]    Overall Loss 0.161427    Objective Loss 0.161427                                        LR 0.001298    Time 0.097476    
2024-02-17 13:05:01,999 - Epoch: [198][  391/  391]    Overall Loss 0.162402    Objective Loss 0.162402    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.096991    
2024-02-17 13:05:02,174 - --- validate (epoch=198)-----------
2024-02-17 13:05:02,175 - 10000 samples (128 per mini-batch)
2024-02-17 13:05:04,963 - Epoch: [198][   79/   79]    Loss 1.353372    Top1 67.070000    Top5 90.060000    
2024-02-17 13:05:05,097 - ==> Top1: 67.070    Top5: 90.060    Loss: 1.353

2024-02-17 13:05:05,119 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:05:05,120 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 13:05:05,198 - 

2024-02-17 13:05:05,199 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:05:15,406 - Epoch: [199][  100/  391]    Overall Loss 0.161186    Objective Loss 0.161186                                        LR 0.001298    Time 0.101988    
2024-02-17 13:05:24,931 - Epoch: [199][  200/  391]    Overall Loss 0.159144    Objective Loss 0.159144                                        LR 0.001298    Time 0.098599    
2024-02-17 13:05:34,444 - Epoch: [199][  300/  391]    Overall Loss 0.159413    Objective Loss 0.159413                                        LR 0.001298    Time 0.097428    
2024-02-17 13:05:43,066 - Epoch: [199][  391/  391]    Overall Loss 0.160005    Objective Loss 0.160005    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.096791    
2024-02-17 13:05:43,260 - --- validate (epoch=199)-----------
2024-02-17 13:05:43,261 - 10000 samples (128 per mini-batch)
2024-02-17 13:05:45,914 - Epoch: [199][   79/   79]    Loss 1.355071    Top1 66.850000    Top5 90.010000    
2024-02-17 13:05:46,073 - ==> Top1: 66.850    Top5: 90.010    Loss: 1.355

2024-02-17 13:05:46,092 - ==> Best [Top1: 67.480   Top5: 90.300   Sparsity:0.00   Params: 1341960 on epoch: 175]
2024-02-17 13:05:46,093 - Saving checkpoint to: logs/2024.02.17-104953/checkpoint.pth.tar
2024-02-17 13:05:46,168 - 

2024-02-17 13:05:46,168 - Initiating quantization aware training (QAT)...
2024-02-17 13:05:46,291 - 

2024-02-17 13:05:46,292 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:06:00,306 - Epoch: [200][  100/  391]    Overall Loss 0.506892    Objective Loss 0.506892                                        LR 0.001298    Time 0.140079    
2024-02-17 13:06:13,336 - Epoch: [200][  200/  391]    Overall Loss 0.497335    Objective Loss 0.497335                                        LR 0.001298    Time 0.135160    
2024-02-17 13:06:26,530 - Epoch: [200][  300/  391]    Overall Loss 0.470441    Objective Loss 0.470441                                        LR 0.001298    Time 0.134070    
2024-02-17 13:06:38,316 - Epoch: [200][  391/  391]    Overall Loss 0.455394    Objective Loss 0.455394    Top1 93.269231    Top5 100.000000    LR 0.001298    Time 0.132998    
2024-02-17 13:06:38,531 - --- validate (epoch=200)-----------
2024-02-17 13:06:38,532 - 10000 samples (128 per mini-batch)
2024-02-17 13:06:44,424 - Epoch: [200][   79/   79]    Loss 1.592401    Top1 62.870000    Top5 87.540000    
2024-02-17 13:06:44,555 - ==> Top1: 62.870    Top5: 87.540    Loss: 1.592

2024-02-17 13:06:44,565 - ==> Best [Top1: 62.870   Top5: 87.540   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:06:44,565 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:06:44,637 - 

2024-02-17 13:06:44,638 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:06:58,072 - Epoch: [201][  100/  391]    Overall Loss 0.391745    Objective Loss 0.391745                                        LR 0.001298    Time 0.134258    
2024-02-17 13:07:11,242 - Epoch: [201][  200/  391]    Overall Loss 0.399472    Objective Loss 0.399472                                        LR 0.001298    Time 0.132954    
2024-02-17 13:07:24,639 - Epoch: [201][  300/  391]    Overall Loss 0.395400    Objective Loss 0.395400                                        LR 0.001298    Time 0.133276    
2024-02-17 13:07:36,577 - Epoch: [201][  391/  391]    Overall Loss 0.392933    Objective Loss 0.392933    Top1 86.057692    Top5 99.519231    LR 0.001298    Time 0.132778    
2024-02-17 13:07:36,786 - --- validate (epoch=201)-----------
2024-02-17 13:07:36,787 - 10000 samples (128 per mini-batch)
2024-02-17 13:07:42,921 - Epoch: [201][   79/   79]    Loss 1.571419    Top1 62.780000    Top5 87.840000    
2024-02-17 13:07:43,137 - ==> Top1: 62.780    Top5: 87.840    Loss: 1.571

2024-02-17 13:07:43,156 - ==> Best [Top1: 62.870   Top5: 87.540   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:07:43,157 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:07:43,229 - 

2024-02-17 13:07:43,230 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:07:57,237 - Epoch: [202][  100/  391]    Overall Loss 0.348274    Objective Loss 0.348274                                        LR 0.001298    Time 0.139969    
2024-02-17 13:08:10,317 - Epoch: [202][  200/  391]    Overall Loss 0.343726    Objective Loss 0.343726                                        LR 0.001298    Time 0.135360    
2024-02-17 13:08:23,516 - Epoch: [202][  300/  391]    Overall Loss 0.344215    Objective Loss 0.344215                                        LR 0.001298    Time 0.134220    
2024-02-17 13:08:35,584 - Epoch: [202][  391/  391]    Overall Loss 0.339299    Objective Loss 0.339299    Top1 91.346154    Top5 99.519231    LR 0.001298    Time 0.133835    
2024-02-17 13:08:35,771 - --- validate (epoch=202)-----------
2024-02-17 13:08:35,772 - 10000 samples (128 per mini-batch)
2024-02-17 13:08:42,086 - Epoch: [202][   79/   79]    Loss 1.540047    Top1 62.840000    Top5 88.200000    
2024-02-17 13:08:42,241 - ==> Top1: 62.840    Top5: 88.200    Loss: 1.540

2024-02-17 13:08:42,260 - ==> Best [Top1: 62.870   Top5: 87.540   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:08:42,260 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:08:42,323 - 

2024-02-17 13:08:42,323 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:08:55,736 - Epoch: [203][  100/  391]    Overall Loss 0.329654    Objective Loss 0.329654                                        LR 0.001298    Time 0.134047    
2024-02-17 13:09:08,613 - Epoch: [203][  200/  391]    Overall Loss 0.334852    Objective Loss 0.334852                                        LR 0.001298    Time 0.131385    
2024-02-17 13:09:21,570 - Epoch: [203][  300/  391]    Overall Loss 0.326507    Objective Loss 0.326507                                        LR 0.001298    Time 0.130762    
2024-02-17 13:09:33,330 - Epoch: [203][  391/  391]    Overall Loss 0.325391    Objective Loss 0.325391    Top1 89.903846    Top5 100.000000    LR 0.001298    Time 0.130395    
2024-02-17 13:09:33,510 - --- validate (epoch=203)-----------
2024-02-17 13:09:33,511 - 10000 samples (128 per mini-batch)
2024-02-17 13:09:39,295 - Epoch: [203][   79/   79]    Loss 1.567593    Top1 62.580000    Top5 88.150000    
2024-02-17 13:09:39,446 - ==> Top1: 62.580    Top5: 88.150    Loss: 1.568

2024-02-17 13:09:39,463 - ==> Best [Top1: 62.870   Top5: 87.540   Sparsity:0.00   Params: 1341960 on epoch: 200]
2024-02-17 13:09:39,463 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:09:39,528 - 

2024-02-17 13:09:39,528 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:09:53,367 - Epoch: [204][  100/  391]    Overall Loss 0.345360    Objective Loss 0.345360                                        LR 0.001298    Time 0.138307    
2024-02-17 13:10:06,493 - Epoch: [204][  200/  391]    Overall Loss 0.333815    Objective Loss 0.333815                                        LR 0.001298    Time 0.134760    
2024-02-17 13:10:18,695 - Epoch: [204][  300/  391]    Overall Loss 0.325821    Objective Loss 0.325821                                        LR 0.001298    Time 0.130496    
2024-02-17 13:10:30,555 - Epoch: [204][  391/  391]    Overall Loss 0.319936    Objective Loss 0.319936    Top1 94.230769    Top5 99.519231    LR 0.001298    Time 0.130446    
2024-02-17 13:10:30,754 - --- validate (epoch=204)-----------
2024-02-17 13:10:30,755 - 10000 samples (128 per mini-batch)
2024-02-17 13:10:37,270 - Epoch: [204][   79/   79]    Loss 1.511059    Top1 64.120000    Top5 88.820000    
2024-02-17 13:10:37,424 - ==> Top1: 64.120    Top5: 88.820    Loss: 1.511

2024-02-17 13:10:37,438 - ==> Best [Top1: 64.120   Top5: 88.820   Sparsity:0.00   Params: 1341960 on epoch: 204]
2024-02-17 13:10:37,439 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:10:37,524 - 

2024-02-17 13:10:37,525 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:10:51,627 - Epoch: [205][  100/  391]    Overall Loss 0.297975    Objective Loss 0.297975                                        LR 0.001298    Time 0.140939    
2024-02-17 13:11:04,881 - Epoch: [205][  200/  391]    Overall Loss 0.287860    Objective Loss 0.287860                                        LR 0.001298    Time 0.136714    
2024-02-17 13:11:18,042 - Epoch: [205][  300/  391]    Overall Loss 0.285317    Objective Loss 0.285317                                        LR 0.001298    Time 0.134997    
2024-02-17 13:11:30,169 - Epoch: [205][  391/  391]    Overall Loss 0.291192    Objective Loss 0.291192    Top1 86.057692    Top5 99.038462    LR 0.001298    Time 0.134581    
2024-02-17 13:11:30,382 - --- validate (epoch=205)-----------
2024-02-17 13:11:30,383 - 10000 samples (128 per mini-batch)
2024-02-17 13:11:36,306 - Epoch: [205][   79/   79]    Loss 1.501443    Top1 63.840000    Top5 88.790000    
2024-02-17 13:11:36,506 - ==> Top1: 63.840    Top5: 88.790    Loss: 1.501

2024-02-17 13:11:36,524 - ==> Best [Top1: 64.120   Top5: 88.820   Sparsity:0.00   Params: 1341960 on epoch: 204]
2024-02-17 13:11:36,525 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:11:36,589 - 

2024-02-17 13:11:36,589 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:11:50,483 - Epoch: [206][  100/  391]    Overall Loss 0.277581    Objective Loss 0.277581                                        LR 0.001298    Time 0.138864    
2024-02-17 13:12:03,481 - Epoch: [206][  200/  391]    Overall Loss 0.266899    Objective Loss 0.266899                                        LR 0.001298    Time 0.134396    
2024-02-17 13:12:16,042 - Epoch: [206][  300/  391]    Overall Loss 0.274037    Objective Loss 0.274037                                        LR 0.001298    Time 0.131452    
2024-02-17 13:12:27,853 - Epoch: [206][  391/  391]    Overall Loss 0.281940    Objective Loss 0.281940    Top1 91.346154    Top5 100.000000    LR 0.001298    Time 0.131053    
2024-02-17 13:12:28,057 - --- validate (epoch=206)-----------
2024-02-17 13:12:28,058 - 10000 samples (128 per mini-batch)
2024-02-17 13:12:33,878 - Epoch: [206][   79/   79]    Loss 1.464224    Top1 64.920000    Top5 89.020000    
2024-02-17 13:12:34,041 - ==> Top1: 64.920    Top5: 89.020    Loss: 1.464

2024-02-17 13:12:34,057 - ==> Best [Top1: 64.920   Top5: 89.020   Sparsity:0.00   Params: 1341960 on epoch: 206]
2024-02-17 13:12:34,057 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:12:34,167 - 

2024-02-17 13:12:34,167 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:12:48,162 - Epoch: [207][  100/  391]    Overall Loss 0.260826    Objective Loss 0.260826                                        LR 0.001298    Time 0.139860    
2024-02-17 13:13:01,084 - Epoch: [207][  200/  391]    Overall Loss 0.274066    Objective Loss 0.274066                                        LR 0.001298    Time 0.134515    
2024-02-17 13:13:14,289 - Epoch: [207][  300/  391]    Overall Loss 0.273383    Objective Loss 0.273383                                        LR 0.001298    Time 0.133676    
2024-02-17 13:13:26,275 - Epoch: [207][  391/  391]    Overall Loss 0.274940    Objective Loss 0.274940    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.133208    
2024-02-17 13:13:26,545 - --- validate (epoch=207)-----------
2024-02-17 13:13:26,546 - 10000 samples (128 per mini-batch)
2024-02-17 13:13:32,449 - Epoch: [207][   79/   79]    Loss 1.497965    Top1 64.190000    Top5 89.050000    
2024-02-17 13:13:32,629 - ==> Top1: 64.190    Top5: 89.050    Loss: 1.498

2024-02-17 13:13:32,646 - ==> Best [Top1: 64.920   Top5: 89.020   Sparsity:0.00   Params: 1341960 on epoch: 206]
2024-02-17 13:13:32,647 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:13:32,716 - 

2024-02-17 13:13:32,716 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:13:46,712 - Epoch: [208][  100/  391]    Overall Loss 0.274533    Objective Loss 0.274533                                        LR 0.001298    Time 0.139877    
2024-02-17 13:14:00,068 - Epoch: [208][  200/  391]    Overall Loss 0.264191    Objective Loss 0.264191                                        LR 0.001298    Time 0.136691    
2024-02-17 13:14:13,069 - Epoch: [208][  300/  391]    Overall Loss 0.270866    Objective Loss 0.270866                                        LR 0.001298    Time 0.134446    
2024-02-17 13:14:24,884 - Epoch: [208][  391/  391]    Overall Loss 0.271600    Objective Loss 0.271600    Top1 87.980769    Top5 99.519231    LR 0.001298    Time 0.133361    
2024-02-17 13:14:25,063 - --- validate (epoch=208)-----------
2024-02-17 13:14:25,064 - 10000 samples (128 per mini-batch)
2024-02-17 13:14:31,673 - Epoch: [208][   79/   79]    Loss 1.445232    Top1 65.740000    Top5 89.530000    
2024-02-17 13:14:31,849 - ==> Top1: 65.740    Top5: 89.530    Loss: 1.445

2024-02-17 13:14:31,869 - ==> Best [Top1: 65.740   Top5: 89.530   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:14:31,870 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:14:31,953 - 

2024-02-17 13:14:31,953 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:14:45,685 - Epoch: [209][  100/  391]    Overall Loss 0.251665    Objective Loss 0.251665                                        LR 0.001298    Time 0.137234    
2024-02-17 13:14:58,603 - Epoch: [209][  200/  391]    Overall Loss 0.255752    Objective Loss 0.255752                                        LR 0.001298    Time 0.133179    
2024-02-17 13:15:11,422 - Epoch: [209][  300/  391]    Overall Loss 0.255753    Objective Loss 0.255753                                        LR 0.001298    Time 0.131502    
2024-02-17 13:15:23,392 - Epoch: [209][  391/  391]    Overall Loss 0.256970    Objective Loss 0.256970    Top1 91.826923    Top5 99.519231    LR 0.001298    Time 0.131498    
2024-02-17 13:15:23,661 - --- validate (epoch=209)-----------
2024-02-17 13:15:23,662 - 10000 samples (128 per mini-batch)
2024-02-17 13:15:29,546 - Epoch: [209][   79/   79]    Loss 1.450016    Top1 65.440000    Top5 89.460000    
2024-02-17 13:15:29,790 - ==> Top1: 65.440    Top5: 89.460    Loss: 1.450

2024-02-17 13:15:29,800 - ==> Best [Top1: 65.740   Top5: 89.530   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:15:29,800 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:15:29,866 - 

2024-02-17 13:15:29,866 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:15:43,706 - Epoch: [210][  100/  391]    Overall Loss 0.244260    Objective Loss 0.244260                                        LR 0.001298    Time 0.138318    
2024-02-17 13:15:56,634 - Epoch: [210][  200/  391]    Overall Loss 0.247762    Objective Loss 0.247762                                        LR 0.001298    Time 0.133776    
2024-02-17 13:16:10,201 - Epoch: [210][  300/  391]    Overall Loss 0.249063    Objective Loss 0.249063                                        LR 0.001298    Time 0.134391    
2024-02-17 13:16:22,097 - Epoch: [210][  391/  391]    Overall Loss 0.248942    Objective Loss 0.248942    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.133525    
2024-02-17 13:16:22,273 - --- validate (epoch=210)-----------
2024-02-17 13:16:22,274 - 10000 samples (128 per mini-batch)
2024-02-17 13:16:28,056 - Epoch: [210][   79/   79]    Loss 1.459081    Top1 65.440000    Top5 89.070000    
2024-02-17 13:16:28,208 - ==> Top1: 65.440    Top5: 89.070    Loss: 1.459

2024-02-17 13:16:28,227 - ==> Best [Top1: 65.740   Top5: 89.530   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:16:28,227 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:16:28,290 - 

2024-02-17 13:16:28,291 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:16:42,020 - Epoch: [211][  100/  391]    Overall Loss 0.271461    Objective Loss 0.271461                                        LR 0.001298    Time 0.137215    
2024-02-17 13:16:54,104 - Epoch: [211][  200/  391]    Overall Loss 0.271298    Objective Loss 0.271298                                        LR 0.001298    Time 0.129006    
2024-02-17 13:17:06,214 - Epoch: [211][  300/  391]    Overall Loss 0.265150    Objective Loss 0.265150                                        LR 0.001298    Time 0.126357    
2024-02-17 13:17:18,208 - Epoch: [211][  391/  391]    Overall Loss 0.262556    Objective Loss 0.262556    Top1 94.711538    Top5 99.519231    LR 0.001298    Time 0.127611    
2024-02-17 13:17:18,428 - --- validate (epoch=211)-----------
2024-02-17 13:17:18,429 - 10000 samples (128 per mini-batch)
2024-02-17 13:17:24,610 - Epoch: [211][   79/   79]    Loss 1.501846    Top1 63.950000    Top5 89.100000    
2024-02-17 13:17:24,738 - ==> Top1: 63.950    Top5: 89.100    Loss: 1.502

2024-02-17 13:17:24,756 - ==> Best [Top1: 65.740   Top5: 89.530   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:17:24,756 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:17:24,821 - 

2024-02-17 13:17:24,821 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:17:39,079 - Epoch: [212][  100/  391]    Overall Loss 0.256358    Objective Loss 0.256358                                        LR 0.001298    Time 0.142489    
2024-02-17 13:17:52,442 - Epoch: [212][  200/  391]    Overall Loss 0.253651    Objective Loss 0.253651                                        LR 0.001298    Time 0.138035    
2024-02-17 13:18:05,826 - Epoch: [212][  300/  391]    Overall Loss 0.253564    Objective Loss 0.253564                                        LR 0.001298    Time 0.136621    
2024-02-17 13:18:17,503 - Epoch: [212][  391/  391]    Overall Loss 0.254632    Objective Loss 0.254632    Top1 92.788462    Top5 99.519231    LR 0.001298    Time 0.134676    
2024-02-17 13:18:17,641 - --- validate (epoch=212)-----------
2024-02-17 13:18:17,642 - 10000 samples (128 per mini-batch)
2024-02-17 13:18:22,862 - Epoch: [212][   79/   79]    Loss 1.461933    Top1 65.480000    Top5 89.330000    
2024-02-17 13:18:22,989 - ==> Top1: 65.480    Top5: 89.330    Loss: 1.462

2024-02-17 13:18:23,008 - ==> Best [Top1: 65.740   Top5: 89.530   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:18:23,008 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:18:23,073 - 

2024-02-17 13:18:23,074 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:18:36,814 - Epoch: [213][  100/  391]    Overall Loss 0.253459    Objective Loss 0.253459                                        LR 0.001298    Time 0.137322    
2024-02-17 13:18:49,727 - Epoch: [213][  200/  391]    Overall Loss 0.247720    Objective Loss 0.247720                                        LR 0.001298    Time 0.133201    
2024-02-17 13:19:02,178 - Epoch: [213][  300/  391]    Overall Loss 0.250151    Objective Loss 0.250151                                        LR 0.001298    Time 0.130287    
2024-02-17 13:19:13,837 - Epoch: [213][  391/  391]    Overall Loss 0.253668    Objective Loss 0.253668    Top1 89.423077    Top5 100.000000    LR 0.001298    Time 0.129773    
2024-02-17 13:19:14,058 - --- validate (epoch=213)-----------
2024-02-17 13:19:14,059 - 10000 samples (128 per mini-batch)
2024-02-17 13:19:20,122 - Epoch: [213][   79/   79]    Loss 1.462275    Top1 64.870000    Top5 89.140000    
2024-02-17 13:19:20,323 - ==> Top1: 64.870    Top5: 89.140    Loss: 1.462

2024-02-17 13:19:20,334 - ==> Best [Top1: 65.740   Top5: 89.530   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:19:20,334 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:19:20,399 - 

2024-02-17 13:19:20,399 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:19:34,590 - Epoch: [214][  100/  391]    Overall Loss 0.250072    Objective Loss 0.250072                                        LR 0.001298    Time 0.141829    
2024-02-17 13:19:47,098 - Epoch: [214][  200/  391]    Overall Loss 0.251839    Objective Loss 0.251839                                        LR 0.001298    Time 0.133432    
2024-02-17 13:19:59,922 - Epoch: [214][  300/  391]    Overall Loss 0.253842    Objective Loss 0.253842                                        LR 0.001298    Time 0.131683    
2024-02-17 13:20:11,904 - Epoch: [214][  391/  391]    Overall Loss 0.256536    Objective Loss 0.256536    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.131668    
2024-02-17 13:20:12,089 - --- validate (epoch=214)-----------
2024-02-17 13:20:12,090 - 10000 samples (128 per mini-batch)
2024-02-17 13:20:17,946 - Epoch: [214][   79/   79]    Loss 1.474440    Top1 64.710000    Top5 89.220000    
2024-02-17 13:20:18,125 - ==> Top1: 64.710    Top5: 89.220    Loss: 1.474

2024-02-17 13:20:18,144 - ==> Best [Top1: 65.740   Top5: 89.530   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:20:18,144 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:20:18,208 - 

2024-02-17 13:20:18,208 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:20:32,441 - Epoch: [215][  100/  391]    Overall Loss 0.235672    Objective Loss 0.235672                                        LR 0.001298    Time 0.142251    
2024-02-17 13:20:46,067 - Epoch: [215][  200/  391]    Overall Loss 0.241872    Objective Loss 0.241872                                        LR 0.001298    Time 0.139229    
2024-02-17 13:20:58,475 - Epoch: [215][  300/  391]    Overall Loss 0.246754    Objective Loss 0.246754                                        LR 0.001298    Time 0.134165    
2024-02-17 13:21:10,222 - Epoch: [215][  391/  391]    Overall Loss 0.247121    Objective Loss 0.247121    Top1 93.750000    Top5 99.519231    LR 0.001298    Time 0.132972    
2024-02-17 13:21:10,465 - --- validate (epoch=215)-----------
2024-02-17 13:21:10,466 - 10000 samples (128 per mini-batch)
2024-02-17 13:21:16,825 - Epoch: [215][   79/   79]    Loss 1.451941    Top1 64.620000    Top5 89.400000    
2024-02-17 13:21:17,001 - ==> Top1: 64.620    Top5: 89.400    Loss: 1.452

2024-02-17 13:21:17,018 - ==> Best [Top1: 65.740   Top5: 89.530   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:21:17,018 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:21:17,088 - 

2024-02-17 13:21:17,088 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:21:30,929 - Epoch: [216][  100/  391]    Overall Loss 0.248319    Objective Loss 0.248319                                        LR 0.001298    Time 0.138329    
2024-02-17 13:21:43,865 - Epoch: [216][  200/  391]    Overall Loss 0.244083    Objective Loss 0.244083                                        LR 0.001298    Time 0.133818    
2024-02-17 13:21:57,223 - Epoch: [216][  300/  391]    Overall Loss 0.251700    Objective Loss 0.251700                                        LR 0.001298    Time 0.133722    
2024-02-17 13:22:09,143 - Epoch: [216][  391/  391]    Overall Loss 0.250318    Objective Loss 0.250318    Top1 93.750000    Top5 100.000000    LR 0.001298    Time 0.133073    
2024-02-17 13:22:09,376 - --- validate (epoch=216)-----------
2024-02-17 13:22:09,376 - 10000 samples (128 per mini-batch)
2024-02-17 13:22:15,864 - Epoch: [216][   79/   79]    Loss 1.514062    Top1 64.060000    Top5 88.860000    
2024-02-17 13:22:16,007 - ==> Top1: 64.060    Top5: 88.860    Loss: 1.514

2024-02-17 13:22:16,027 - ==> Best [Top1: 65.740   Top5: 89.530   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:22:16,027 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:22:16,093 - 

2024-02-17 13:22:16,093 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:22:29,943 - Epoch: [217][  100/  391]    Overall Loss 0.249703    Objective Loss 0.249703                                        LR 0.001298    Time 0.138414    
2024-02-17 13:22:42,675 - Epoch: [217][  200/  391]    Overall Loss 0.244751    Objective Loss 0.244751                                        LR 0.001298    Time 0.132844    
2024-02-17 13:22:55,849 - Epoch: [217][  300/  391]    Overall Loss 0.249870    Objective Loss 0.249870                                        LR 0.001298    Time 0.132458    
2024-02-17 13:23:08,066 - Epoch: [217][  391/  391]    Overall Loss 0.249657    Objective Loss 0.249657    Top1 90.384615    Top5 100.000000    LR 0.001298    Time 0.132864    
2024-02-17 13:23:08,361 - --- validate (epoch=217)-----------
2024-02-17 13:23:08,362 - 10000 samples (128 per mini-batch)
2024-02-17 13:23:14,178 - Epoch: [217][   79/   79]    Loss 1.484187    Top1 64.490000    Top5 89.070000    
2024-02-17 13:23:14,326 - ==> Top1: 64.490    Top5: 89.070    Loss: 1.484

2024-02-17 13:23:14,342 - ==> Best [Top1: 65.740   Top5: 89.530   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:23:14,343 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:23:14,394 - 

2024-02-17 13:23:14,394 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:23:27,628 - Epoch: [218][  100/  391]    Overall Loss 0.251656    Objective Loss 0.251656                                        LR 0.001298    Time 0.132272    
2024-02-17 13:23:40,871 - Epoch: [218][  200/  391]    Overall Loss 0.243565    Objective Loss 0.243565                                        LR 0.001298    Time 0.132329    
2024-02-17 13:23:54,039 - Epoch: [218][  300/  391]    Overall Loss 0.244621    Objective Loss 0.244621                                        LR 0.001298    Time 0.132089    
2024-02-17 13:24:06,188 - Epoch: [218][  391/  391]    Overall Loss 0.245420    Objective Loss 0.245420    Top1 92.307692    Top5 99.038462    LR 0.001298    Time 0.132405    
2024-02-17 13:24:06,481 - --- validate (epoch=218)-----------
2024-02-17 13:24:06,481 - 10000 samples (128 per mini-batch)
2024-02-17 13:24:11,718 - Epoch: [218][   79/   79]    Loss 1.472077    Top1 65.050000    Top5 89.240000    
2024-02-17 13:24:11,839 - ==> Top1: 65.050    Top5: 89.240    Loss: 1.472

2024-02-17 13:24:11,856 - ==> Best [Top1: 65.740   Top5: 89.530   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:24:11,857 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:24:11,907 - 

2024-02-17 13:24:11,907 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:24:24,681 - Epoch: [219][  100/  391]    Overall Loss 0.267786    Objective Loss 0.267786                                        LR 0.001298    Time 0.127670    
2024-02-17 13:24:37,221 - Epoch: [219][  200/  391]    Overall Loss 0.266445    Objective Loss 0.266445                                        LR 0.001298    Time 0.126514    
2024-02-17 13:24:50,112 - Epoch: [219][  300/  391]    Overall Loss 0.258921    Objective Loss 0.258921                                        LR 0.001298    Time 0.127294    
2024-02-17 13:25:02,616 - Epoch: [219][  391/  391]    Overall Loss 0.256534    Objective Loss 0.256534    Top1 90.384615    Top5 100.000000    LR 0.001298    Time 0.129636    
2024-02-17 13:25:02,775 - --- validate (epoch=219)-----------
2024-02-17 13:25:02,776 - 10000 samples (128 per mini-batch)
2024-02-17 13:25:08,543 - Epoch: [219][   79/   79]    Loss 1.451029    Top1 65.210000    Top5 89.320000    
2024-02-17 13:25:08,724 - ==> Top1: 65.210    Top5: 89.320    Loss: 1.451

2024-02-17 13:25:08,742 - ==> Best [Top1: 65.740   Top5: 89.530   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:25:08,743 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:25:08,810 - 

2024-02-17 13:25:08,811 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:25:23,033 - Epoch: [220][  100/  391]    Overall Loss 0.257601    Objective Loss 0.257601                                        LR 0.001298    Time 0.142143    
2024-02-17 13:25:36,136 - Epoch: [220][  200/  391]    Overall Loss 0.253558    Objective Loss 0.253558                                        LR 0.001298    Time 0.136559    
2024-02-17 13:25:48,307 - Epoch: [220][  300/  391]    Overall Loss 0.250606    Objective Loss 0.250606                                        LR 0.001298    Time 0.131594    
2024-02-17 13:25:59,994 - Epoch: [220][  391/  391]    Overall Loss 0.249339    Objective Loss 0.249339    Top1 91.826923    Top5 100.000000    LR 0.001298    Time 0.130846    
2024-02-17 13:26:00,186 - --- validate (epoch=220)-----------
2024-02-17 13:26:00,187 - 10000 samples (128 per mini-batch)
2024-02-17 13:26:06,023 - Epoch: [220][   79/   79]    Loss 1.447164    Top1 65.200000    Top5 89.580000    
2024-02-17 13:26:06,197 - ==> Top1: 65.200    Top5: 89.580    Loss: 1.447

2024-02-17 13:26:06,215 - ==> Best [Top1: 65.740   Top5: 89.530   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:26:06,215 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:26:06,284 - 

2024-02-17 13:26:06,284 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:26:20,403 - Epoch: [221][  100/  391]    Overall Loss 0.219325    Objective Loss 0.219325                                        LR 0.001298    Time 0.141110    
2024-02-17 13:26:33,737 - Epoch: [221][  200/  391]    Overall Loss 0.231194    Objective Loss 0.231194                                        LR 0.001298    Time 0.137195    
2024-02-17 13:26:46,866 - Epoch: [221][  300/  391]    Overall Loss 0.235096    Objective Loss 0.235096                                        LR 0.001298    Time 0.135212    
2024-02-17 13:26:58,563 - Epoch: [221][  391/  391]    Overall Loss 0.234375    Objective Loss 0.234375    Top1 95.673077    Top5 99.519231    LR 0.001298    Time 0.133648    
2024-02-17 13:26:58,768 - --- validate (epoch=221)-----------
2024-02-17 13:26:58,769 - 10000 samples (128 per mini-batch)
2024-02-17 13:27:05,102 - Epoch: [221][   79/   79]    Loss 1.485463    Top1 65.350000    Top5 89.190000    
2024-02-17 13:27:05,254 - ==> Top1: 65.350    Top5: 89.190    Loss: 1.485

2024-02-17 13:27:05,272 - ==> Best [Top1: 65.740   Top5: 89.530   Sparsity:0.00   Params: 1341960 on epoch: 208]
2024-02-17 13:27:05,272 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:27:05,334 - 

2024-02-17 13:27:05,334 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:27:19,575 - Epoch: [222][  100/  391]    Overall Loss 0.232393    Objective Loss 0.232393                                        LR 0.001298    Time 0.142323    
2024-02-17 13:27:33,146 - Epoch: [222][  200/  391]    Overall Loss 0.244450    Objective Loss 0.244450                                        LR 0.001298    Time 0.138995    
2024-02-17 13:27:45,994 - Epoch: [222][  300/  391]    Overall Loss 0.238165    Objective Loss 0.238165                                        LR 0.001298    Time 0.135474    
2024-02-17 13:27:57,371 - Epoch: [222][  391/  391]    Overall Loss 0.236857    Objective Loss 0.236857    Top1 94.230769    Top5 99.519231    LR 0.001298    Time 0.133030    
2024-02-17 13:27:57,537 - --- validate (epoch=222)-----------
2024-02-17 13:27:57,538 - 10000 samples (128 per mini-batch)
2024-02-17 13:28:02,711 - Epoch: [222][   79/   79]    Loss 1.435816    Top1 65.870000    Top5 89.600000    
2024-02-17 13:28:02,848 - ==> Top1: 65.870    Top5: 89.600    Loss: 1.436

2024-02-17 13:28:02,866 - ==> Best [Top1: 65.870   Top5: 89.600   Sparsity:0.00   Params: 1341960 on epoch: 222]
2024-02-17 13:28:02,866 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:28:02,955 - 

2024-02-17 13:28:02,955 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:28:16,543 - Epoch: [223][  100/  391]    Overall Loss 0.236384    Objective Loss 0.236384                                        LR 0.001298    Time 0.135803    
2024-02-17 13:28:26,309 - Epoch: [223][  200/  391]    Overall Loss 0.228220    Objective Loss 0.228220                                        LR 0.001298    Time 0.116713    
2024-02-17 13:28:37,844 - Epoch: [223][  300/  391]    Overall Loss 0.233047    Objective Loss 0.233047                                        LR 0.001298    Time 0.116246    
2024-02-17 13:28:49,061 - Epoch: [223][  391/  391]    Overall Loss 0.230033    Objective Loss 0.230033    Top1 95.192308    Top5 99.519231    LR 0.001298    Time 0.117868    
2024-02-17 13:28:49,233 - --- validate (epoch=223)-----------
2024-02-17 13:28:49,234 - 10000 samples (128 per mini-batch)
2024-02-17 13:28:55,533 - Epoch: [223][   79/   79]    Loss 1.450570    Top1 65.250000    Top5 89.610000    
2024-02-17 13:28:55,673 - ==> Top1: 65.250    Top5: 89.610    Loss: 1.451

2024-02-17 13:28:55,688 - ==> Best [Top1: 65.870   Top5: 89.600   Sparsity:0.00   Params: 1341960 on epoch: 222]
2024-02-17 13:28:55,688 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:28:55,755 - 

2024-02-17 13:28:55,756 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:29:09,465 - Epoch: [224][  100/  391]    Overall Loss 0.219389    Objective Loss 0.219389                                        LR 0.001298    Time 0.136975    
2024-02-17 13:29:21,753 - Epoch: [224][  200/  391]    Overall Loss 0.220445    Objective Loss 0.220445                                        LR 0.001298    Time 0.129900    
2024-02-17 13:29:34,720 - Epoch: [224][  300/  391]    Overall Loss 0.231134    Objective Loss 0.231134                                        LR 0.001298    Time 0.129808    
2024-02-17 13:29:46,745 - Epoch: [224][  391/  391]    Overall Loss 0.230166    Objective Loss 0.230166    Top1 90.865385    Top5 100.000000    LR 0.001298    Time 0.130339    
2024-02-17 13:29:46,913 - --- validate (epoch=224)-----------
2024-02-17 13:29:46,914 - 10000 samples (128 per mini-batch)
2024-02-17 13:29:52,848 - Epoch: [224][   79/   79]    Loss 1.476955    Top1 65.000000    Top5 89.130000    
2024-02-17 13:29:52,979 - ==> Top1: 65.000    Top5: 89.130    Loss: 1.477

2024-02-17 13:29:52,994 - ==> Best [Top1: 65.870   Top5: 89.600   Sparsity:0.00   Params: 1341960 on epoch: 222]
2024-02-17 13:29:52,995 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:29:53,062 - 

2024-02-17 13:29:53,062 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:30:06,178 - Epoch: [225][  100/  391]    Overall Loss 0.228260    Objective Loss 0.228260                                        LR 0.001298    Time 0.131087    
2024-02-17 13:30:19,362 - Epoch: [225][  200/  391]    Overall Loss 0.223630    Objective Loss 0.223630                                        LR 0.001298    Time 0.131436    
2024-02-17 13:30:32,579 - Epoch: [225][  300/  391]    Overall Loss 0.223580    Objective Loss 0.223580                                        LR 0.001298    Time 0.131664    
2024-02-17 13:30:44,920 - Epoch: [225][  391/  391]    Overall Loss 0.227356    Objective Loss 0.227356    Top1 92.788462    Top5 99.519231    LR 0.001298    Time 0.132570    
2024-02-17 13:30:45,172 - --- validate (epoch=225)-----------
2024-02-17 13:30:45,173 - 10000 samples (128 per mini-batch)
2024-02-17 13:30:50,556 - Epoch: [225][   79/   79]    Loss 1.476411    Top1 65.050000    Top5 89.330000    
2024-02-17 13:30:50,810 - ==> Top1: 65.050    Top5: 89.330    Loss: 1.476

2024-02-17 13:30:50,823 - ==> Best [Top1: 65.870   Top5: 89.600   Sparsity:0.00   Params: 1341960 on epoch: 222]
2024-02-17 13:30:50,823 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:30:50,890 - 

2024-02-17 13:30:50,891 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:31:04,633 - Epoch: [226][  100/  391]    Overall Loss 0.217148    Objective Loss 0.217148                                        LR 0.001298    Time 0.137342    
2024-02-17 13:31:17,278 - Epoch: [226][  200/  391]    Overall Loss 0.226323    Objective Loss 0.226323                                        LR 0.001298    Time 0.131872    
2024-02-17 13:31:30,509 - Epoch: [226][  300/  391]    Overall Loss 0.222301    Objective Loss 0.222301                                        LR 0.001298    Time 0.132001    
2024-02-17 13:31:42,821 - Epoch: [226][  391/  391]    Overall Loss 0.221460    Objective Loss 0.221460    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.132757    
2024-02-17 13:31:42,962 - --- validate (epoch=226)-----------
2024-02-17 13:31:42,962 - 10000 samples (128 per mini-batch)
2024-02-17 13:31:49,084 - Epoch: [226][   79/   79]    Loss 1.467009    Top1 65.110000    Top5 89.330000    
2024-02-17 13:31:49,224 - ==> Top1: 65.110    Top5: 89.330    Loss: 1.467

2024-02-17 13:31:49,241 - ==> Best [Top1: 65.870   Top5: 89.600   Sparsity:0.00   Params: 1341960 on epoch: 222]
2024-02-17 13:31:49,241 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:31:49,307 - 

2024-02-17 13:31:49,308 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:32:02,912 - Epoch: [227][  100/  391]    Overall Loss 0.216214    Objective Loss 0.216214                                        LR 0.001298    Time 0.135967    
2024-02-17 13:32:15,888 - Epoch: [227][  200/  391]    Overall Loss 0.212064    Objective Loss 0.212064                                        LR 0.001298    Time 0.132837    
2024-02-17 13:32:29,037 - Epoch: [227][  300/  391]    Overall Loss 0.213967    Objective Loss 0.213967                                        LR 0.001298    Time 0.132374    
2024-02-17 13:32:41,043 - Epoch: [227][  391/  391]    Overall Loss 0.217742    Objective Loss 0.217742    Top1 93.750000    Top5 100.000000    LR 0.001298    Time 0.132257    
2024-02-17 13:32:41,262 - --- validate (epoch=227)-----------
2024-02-17 13:32:41,263 - 10000 samples (128 per mini-batch)
2024-02-17 13:32:47,004 - Epoch: [227][   79/   79]    Loss 1.486858    Top1 64.690000    Top5 88.970000    
2024-02-17 13:32:47,177 - ==> Top1: 64.690    Top5: 88.970    Loss: 1.487

2024-02-17 13:32:47,191 - ==> Best [Top1: 65.870   Top5: 89.600   Sparsity:0.00   Params: 1341960 on epoch: 222]
2024-02-17 13:32:47,191 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:32:47,256 - 

2024-02-17 13:32:47,257 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:33:01,039 - Epoch: [228][  100/  391]    Overall Loss 0.206716    Objective Loss 0.206716                                        LR 0.001298    Time 0.137706    
2024-02-17 13:33:14,308 - Epoch: [228][  200/  391]    Overall Loss 0.209947    Objective Loss 0.209947                                        LR 0.001298    Time 0.135171    
2024-02-17 13:33:27,510 - Epoch: [228][  300/  391]    Overall Loss 0.213905    Objective Loss 0.213905                                        LR 0.001298    Time 0.134105    
2024-02-17 13:33:39,576 - Epoch: [228][  391/  391]    Overall Loss 0.215817    Objective Loss 0.215817    Top1 93.750000    Top5 100.000000    LR 0.001298    Time 0.133741    
2024-02-17 13:33:39,851 - --- validate (epoch=228)-----------
2024-02-17 13:33:39,852 - 10000 samples (128 per mini-batch)
2024-02-17 13:33:46,167 - Epoch: [228][   79/   79]    Loss 1.499506    Top1 65.080000    Top5 89.190000    
2024-02-17 13:33:46,391 - ==> Top1: 65.080    Top5: 89.190    Loss: 1.500

2024-02-17 13:33:46,413 - ==> Best [Top1: 65.870   Top5: 89.600   Sparsity:0.00   Params: 1341960 on epoch: 222]
2024-02-17 13:33:46,413 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:33:46,477 - 

2024-02-17 13:33:46,477 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:34:00,295 - Epoch: [229][  100/  391]    Overall Loss 0.203859    Objective Loss 0.203859                                        LR 0.001298    Time 0.138103    
2024-02-17 13:34:13,298 - Epoch: [229][  200/  391]    Overall Loss 0.208284    Objective Loss 0.208284                                        LR 0.001298    Time 0.134032    
2024-02-17 13:34:25,655 - Epoch: [229][  300/  391]    Overall Loss 0.211058    Objective Loss 0.211058                                        LR 0.001298    Time 0.130531    
2024-02-17 13:34:37,288 - Epoch: [229][  391/  391]    Overall Loss 0.212304    Objective Loss 0.212304    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.129892    
2024-02-17 13:34:37,495 - --- validate (epoch=229)-----------
2024-02-17 13:34:37,496 - 10000 samples (128 per mini-batch)
2024-02-17 13:34:43,540 - Epoch: [229][   79/   79]    Loss 1.454315    Top1 65.660000    Top5 89.550000    
2024-02-17 13:34:43,698 - ==> Top1: 65.660    Top5: 89.550    Loss: 1.454

2024-02-17 13:34:43,715 - ==> Best [Top1: 65.870   Top5: 89.600   Sparsity:0.00   Params: 1341960 on epoch: 222]
2024-02-17 13:34:43,716 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:34:43,779 - 

2024-02-17 13:34:43,779 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:34:57,675 - Epoch: [230][  100/  391]    Overall Loss 0.205114    Objective Loss 0.205114                                        LR 0.001298    Time 0.138874    
2024-02-17 13:35:10,933 - Epoch: [230][  200/  391]    Overall Loss 0.207046    Objective Loss 0.207046                                        LR 0.001298    Time 0.135702    
2024-02-17 13:35:24,491 - Epoch: [230][  300/  391]    Overall Loss 0.216034    Objective Loss 0.216034                                        LR 0.001298    Time 0.135646    
2024-02-17 13:35:36,552 - Epoch: [230][  391/  391]    Overall Loss 0.220772    Objective Loss 0.220772    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.134909    
2024-02-17 13:35:36,741 - --- validate (epoch=230)-----------
2024-02-17 13:35:36,742 - 10000 samples (128 per mini-batch)
2024-02-17 13:35:42,820 - Epoch: [230][   79/   79]    Loss 1.446070    Top1 65.910000    Top5 89.860000    
2024-02-17 13:35:42,951 - ==> Top1: 65.910    Top5: 89.860    Loss: 1.446

2024-02-17 13:35:42,968 - ==> Best [Top1: 65.910   Top5: 89.860   Sparsity:0.00   Params: 1341960 on epoch: 230]
2024-02-17 13:35:42,968 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:35:43,050 - 

2024-02-17 13:35:43,051 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:35:56,817 - Epoch: [231][  100/  391]    Overall Loss 0.213676    Objective Loss 0.213676                                        LR 0.001298    Time 0.137582    
2024-02-17 13:36:09,920 - Epoch: [231][  200/  391]    Overall Loss 0.210658    Objective Loss 0.210658                                        LR 0.001298    Time 0.134278    
2024-02-17 13:36:22,989 - Epoch: [231][  300/  391]    Overall Loss 0.214969    Objective Loss 0.214969                                        LR 0.001298    Time 0.133066    
2024-02-17 13:36:35,024 - Epoch: [231][  391/  391]    Overall Loss 0.217834    Objective Loss 0.217834    Top1 92.307692    Top5 99.519231    LR 0.001298    Time 0.132866    
2024-02-17 13:36:35,214 - --- validate (epoch=231)-----------
2024-02-17 13:36:35,215 - 10000 samples (128 per mini-batch)
2024-02-17 13:36:41,225 - Epoch: [231][   79/   79]    Loss 1.450892    Top1 65.400000    Top5 89.480000    
2024-02-17 13:36:41,383 - ==> Top1: 65.400    Top5: 89.480    Loss: 1.451

2024-02-17 13:36:41,401 - ==> Best [Top1: 65.910   Top5: 89.860   Sparsity:0.00   Params: 1341960 on epoch: 230]
2024-02-17 13:36:41,402 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:36:41,468 - 

2024-02-17 13:36:41,468 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:36:54,603 - Epoch: [232][  100/  391]    Overall Loss 0.227231    Objective Loss 0.227231                                        LR 0.001298    Time 0.131276    
2024-02-17 13:37:07,907 - Epoch: [232][  200/  391]    Overall Loss 0.214155    Objective Loss 0.214155                                        LR 0.001298    Time 0.132132    
2024-02-17 13:37:21,779 - Epoch: [232][  300/  391]    Overall Loss 0.212766    Objective Loss 0.212766                                        LR 0.001298    Time 0.134309    
2024-02-17 13:37:33,121 - Epoch: [232][  391/  391]    Overall Loss 0.210632    Objective Loss 0.210632    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.132047    
2024-02-17 13:37:33,295 - --- validate (epoch=232)-----------
2024-02-17 13:37:33,296 - 10000 samples (128 per mini-batch)
2024-02-17 13:37:39,466 - Epoch: [232][   79/   79]    Loss 1.423863    Top1 66.110000    Top5 89.440000    
2024-02-17 13:37:39,651 - ==> Top1: 66.110    Top5: 89.440    Loss: 1.424

2024-02-17 13:37:39,668 - ==> Best [Top1: 66.110   Top5: 89.440   Sparsity:0.00   Params: 1341960 on epoch: 232]
2024-02-17 13:37:39,669 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:37:39,749 - 

2024-02-17 13:37:39,749 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:37:53,429 - Epoch: [233][  100/  391]    Overall Loss 0.206024    Objective Loss 0.206024                                        LR 0.001298    Time 0.136715    
2024-02-17 13:38:04,683 - Epoch: [233][  200/  391]    Overall Loss 0.209154    Objective Loss 0.209154                                        LR 0.001298    Time 0.124608    
2024-02-17 13:38:17,778 - Epoch: [233][  300/  391]    Overall Loss 0.204685    Objective Loss 0.204685                                        LR 0.001298    Time 0.126704    
2024-02-17 13:38:29,985 - Epoch: [233][  391/  391]    Overall Loss 0.206074    Objective Loss 0.206074    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.128422    
2024-02-17 13:38:30,271 - --- validate (epoch=233)-----------
2024-02-17 13:38:30,272 - 10000 samples (128 per mini-batch)
2024-02-17 13:38:36,046 - Epoch: [233][   79/   79]    Loss 1.459767    Top1 65.460000    Top5 89.610000    
2024-02-17 13:38:36,192 - ==> Top1: 65.460    Top5: 89.610    Loss: 1.460

2024-02-17 13:38:36,212 - ==> Best [Top1: 66.110   Top5: 89.440   Sparsity:0.00   Params: 1341960 on epoch: 232]
2024-02-17 13:38:36,213 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:38:36,277 - 

2024-02-17 13:38:36,277 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:38:49,082 - Epoch: [234][  100/  391]    Overall Loss 0.199036    Objective Loss 0.199036                                        LR 0.001298    Time 0.127979    
2024-02-17 13:38:59,670 - Epoch: [234][  200/  391]    Overall Loss 0.199557    Objective Loss 0.199557                                        LR 0.001298    Time 0.116909    
2024-02-17 13:39:10,295 - Epoch: [234][  300/  391]    Overall Loss 0.201481    Objective Loss 0.201481                                        LR 0.001298    Time 0.113342    
2024-02-17 13:39:20,433 - Epoch: [234][  391/  391]    Overall Loss 0.203312    Objective Loss 0.203312    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.112884    
2024-02-17 13:39:20,582 - --- validate (epoch=234)-----------
2024-02-17 13:39:20,583 - 10000 samples (128 per mini-batch)
2024-02-17 13:39:26,738 - Epoch: [234][   79/   79]    Loss 1.446643    Top1 65.770000    Top5 89.670000    
2024-02-17 13:39:26,874 - ==> Top1: 65.770    Top5: 89.670    Loss: 1.447

2024-02-17 13:39:26,891 - ==> Best [Top1: 66.110   Top5: 89.440   Sparsity:0.00   Params: 1341960 on epoch: 232]
2024-02-17 13:39:26,891 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:39:26,958 - 

2024-02-17 13:39:26,958 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:39:40,259 - Epoch: [235][  100/  391]    Overall Loss 0.214812    Objective Loss 0.214812                                        LR 0.001298    Time 0.132939    
2024-02-17 13:39:53,299 - Epoch: [235][  200/  391]    Overall Loss 0.213142    Objective Loss 0.213142                                        LR 0.001298    Time 0.131635    
2024-02-17 13:40:06,165 - Epoch: [235][  300/  391]    Overall Loss 0.213028    Objective Loss 0.213028                                        LR 0.001298    Time 0.130625    
2024-02-17 13:40:18,047 - Epoch: [235][  391/  391]    Overall Loss 0.213683    Objective Loss 0.213683    Top1 94.230769    Top5 99.038462    LR 0.001298    Time 0.130604    
2024-02-17 13:40:18,270 - --- validate (epoch=235)-----------
2024-02-17 13:40:18,271 - 10000 samples (128 per mini-batch)
2024-02-17 13:40:24,294 - Epoch: [235][   79/   79]    Loss 1.486584    Top1 65.070000    Top5 89.190000    
2024-02-17 13:40:24,436 - ==> Top1: 65.070    Top5: 89.190    Loss: 1.487

2024-02-17 13:40:24,451 - ==> Best [Top1: 66.110   Top5: 89.440   Sparsity:0.00   Params: 1341960 on epoch: 232]
2024-02-17 13:40:24,452 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:40:24,519 - 

2024-02-17 13:40:24,519 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:40:39,266 - Epoch: [236][  100/  391]    Overall Loss 0.209497    Objective Loss 0.209497                                        LR 0.001298    Time 0.147386    
2024-02-17 13:40:52,503 - Epoch: [236][  200/  391]    Overall Loss 0.213628    Objective Loss 0.213628                                        LR 0.001298    Time 0.139848    
2024-02-17 13:41:05,496 - Epoch: [236][  300/  391]    Overall Loss 0.214732    Objective Loss 0.214732                                        LR 0.001298    Time 0.136526    
2024-02-17 13:41:16,620 - Epoch: [236][  391/  391]    Overall Loss 0.213390    Objective Loss 0.213390    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.133191    
2024-02-17 13:41:16,852 - --- validate (epoch=236)-----------
2024-02-17 13:41:16,853 - 10000 samples (128 per mini-batch)
2024-02-17 13:41:23,196 - Epoch: [236][   79/   79]    Loss 1.439320    Top1 65.530000    Top5 89.470000    
2024-02-17 13:41:23,386 - ==> Top1: 65.530    Top5: 89.470    Loss: 1.439

2024-02-17 13:41:23,404 - ==> Best [Top1: 66.110   Top5: 89.440   Sparsity:0.00   Params: 1341960 on epoch: 232]
2024-02-17 13:41:23,405 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:41:23,484 - 

2024-02-17 13:41:23,484 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:41:37,320 - Epoch: [237][  100/  391]    Overall Loss 0.197654    Objective Loss 0.197654                                        LR 0.001298    Time 0.138283    
2024-02-17 13:41:50,340 - Epoch: [237][  200/  391]    Overall Loss 0.202168    Objective Loss 0.202168                                        LR 0.001298    Time 0.134214    
2024-02-17 13:42:03,517 - Epoch: [237][  300/  391]    Overall Loss 0.205202    Objective Loss 0.205202                                        LR 0.001298    Time 0.133381    
2024-02-17 13:42:14,762 - Epoch: [237][  391/  391]    Overall Loss 0.204174    Objective Loss 0.204174    Top1 92.307692    Top5 99.519231    LR 0.001298    Time 0.131086    
2024-02-17 13:42:14,942 - --- validate (epoch=237)-----------
2024-02-17 13:42:14,943 - 10000 samples (128 per mini-batch)
2024-02-17 13:42:20,894 - Epoch: [237][   79/   79]    Loss 1.461708    Top1 65.650000    Top5 89.500000    
2024-02-17 13:42:21,049 - ==> Top1: 65.650    Top5: 89.500    Loss: 1.462

2024-02-17 13:42:21,067 - ==> Best [Top1: 66.110   Top5: 89.440   Sparsity:0.00   Params: 1341960 on epoch: 232]
2024-02-17 13:42:21,067 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:42:21,132 - 

2024-02-17 13:42:21,132 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:42:35,402 - Epoch: [238][  100/  391]    Overall Loss 0.204452    Objective Loss 0.204452                                        LR 0.001298    Time 0.142622    
2024-02-17 13:42:48,222 - Epoch: [238][  200/  391]    Overall Loss 0.210129    Objective Loss 0.210129                                        LR 0.001298    Time 0.135384    
2024-02-17 13:43:01,272 - Epoch: [238][  300/  391]    Overall Loss 0.213100    Objective Loss 0.213100                                        LR 0.001298    Time 0.133739    
2024-02-17 13:43:13,416 - Epoch: [238][  391/  391]    Overall Loss 0.212515    Objective Loss 0.212515    Top1 93.750000    Top5 100.000000    LR 0.001298    Time 0.133658    
2024-02-17 13:43:13,642 - --- validate (epoch=238)-----------
2024-02-17 13:43:13,643 - 10000 samples (128 per mini-batch)
2024-02-17 13:43:19,839 - Epoch: [238][   79/   79]    Loss 1.453306    Top1 65.590000    Top5 89.170000    
2024-02-17 13:43:20,022 - ==> Top1: 65.590    Top5: 89.170    Loss: 1.453

2024-02-17 13:43:20,042 - ==> Best [Top1: 66.110   Top5: 89.440   Sparsity:0.00   Params: 1341960 on epoch: 232]
2024-02-17 13:43:20,042 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:43:20,108 - 

2024-02-17 13:43:20,109 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:43:34,080 - Epoch: [239][  100/  391]    Overall Loss 0.200619    Objective Loss 0.200619                                        LR 0.001298    Time 0.139576    
2024-02-17 13:43:46,971 - Epoch: [239][  200/  391]    Overall Loss 0.210712    Objective Loss 0.210712                                        LR 0.001298    Time 0.134219    
2024-02-17 13:44:00,197 - Epoch: [239][  300/  391]    Overall Loss 0.216180    Objective Loss 0.216180                                        LR 0.001298    Time 0.133549    
2024-02-17 13:44:11,709 - Epoch: [239][  391/  391]    Overall Loss 0.214585    Objective Loss 0.214585    Top1 95.192308    Top5 99.519231    LR 0.001298    Time 0.131899    
2024-02-17 13:44:11,889 - --- validate (epoch=239)-----------
2024-02-17 13:44:11,890 - 10000 samples (128 per mini-batch)
2024-02-17 13:44:17,652 - Epoch: [239][   79/   79]    Loss 1.471958    Top1 65.450000    Top5 89.310000    
2024-02-17 13:44:17,841 - ==> Top1: 65.450    Top5: 89.310    Loss: 1.472

2024-02-17 13:44:17,862 - ==> Best [Top1: 66.110   Top5: 89.440   Sparsity:0.00   Params: 1341960 on epoch: 232]
2024-02-17 13:44:17,863 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:44:17,924 - 

2024-02-17 13:44:17,925 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:44:31,657 - Epoch: [240][  100/  391]    Overall Loss 0.202993    Objective Loss 0.202993                                        LR 0.001298    Time 0.137249    
2024-02-17 13:44:44,785 - Epoch: [240][  200/  391]    Overall Loss 0.204823    Objective Loss 0.204823                                        LR 0.001298    Time 0.134240    
2024-02-17 13:44:58,115 - Epoch: [240][  300/  391]    Overall Loss 0.205899    Objective Loss 0.205899                                        LR 0.001298    Time 0.133912    
2024-02-17 13:45:10,330 - Epoch: [240][  391/  391]    Overall Loss 0.205606    Objective Loss 0.205606    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.133973    
2024-02-17 13:45:10,495 - --- validate (epoch=240)-----------
2024-02-17 13:45:10,496 - 10000 samples (128 per mini-batch)
2024-02-17 13:45:15,981 - Epoch: [240][   79/   79]    Loss 1.476530    Top1 65.150000    Top5 89.070000    
2024-02-17 13:45:16,134 - ==> Top1: 65.150    Top5: 89.070    Loss: 1.477

2024-02-17 13:45:16,152 - ==> Best [Top1: 66.110   Top5: 89.440   Sparsity:0.00   Params: 1341960 on epoch: 232]
2024-02-17 13:45:16,153 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:45:16,217 - 

2024-02-17 13:45:16,217 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:45:29,472 - Epoch: [241][  100/  391]    Overall Loss 0.193143    Objective Loss 0.193143                                        LR 0.001298    Time 0.132471    
2024-02-17 13:45:41,058 - Epoch: [241][  200/  391]    Overall Loss 0.197207    Objective Loss 0.197207                                        LR 0.001298    Time 0.124145    
2024-02-17 13:45:54,241 - Epoch: [241][  300/  391]    Overall Loss 0.200261    Objective Loss 0.200261                                        LR 0.001298    Time 0.126690    
2024-02-17 13:46:06,237 - Epoch: [241][  391/  391]    Overall Loss 0.199836    Objective Loss 0.199836    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.127873    
2024-02-17 13:46:06,371 - --- validate (epoch=241)-----------
2024-02-17 13:46:06,372 - 10000 samples (128 per mini-batch)
2024-02-17 13:46:10,991 - Epoch: [241][   79/   79]    Loss 1.432989    Top1 66.210000    Top5 89.670000    
2024-02-17 13:46:11,136 - ==> Top1: 66.210    Top5: 89.670    Loss: 1.433

2024-02-17 13:46:11,147 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:46:11,147 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:46:11,224 - 

2024-02-17 13:46:11,224 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:46:24,352 - Epoch: [242][  100/  391]    Overall Loss 0.188746    Objective Loss 0.188746                                        LR 0.001298    Time 0.131197    
2024-02-17 13:46:37,358 - Epoch: [242][  200/  391]    Overall Loss 0.199335    Objective Loss 0.199335                                        LR 0.001298    Time 0.130605    
2024-02-17 13:46:50,462 - Epoch: [242][  300/  391]    Overall Loss 0.206654    Objective Loss 0.206654                                        LR 0.001298    Time 0.130734    
2024-02-17 13:47:01,464 - Epoch: [242][  391/  391]    Overall Loss 0.207184    Objective Loss 0.207184    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.128433    
2024-02-17 13:47:01,668 - --- validate (epoch=242)-----------
2024-02-17 13:47:01,668 - 10000 samples (128 per mini-batch)
2024-02-17 13:47:07,928 - Epoch: [242][   79/   79]    Loss 1.449927    Top1 65.180000    Top5 89.450000    
2024-02-17 13:47:08,074 - ==> Top1: 65.180    Top5: 89.450    Loss: 1.450

2024-02-17 13:47:08,091 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:47:08,091 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:47:08,155 - 

2024-02-17 13:47:08,156 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:47:21,802 - Epoch: [243][  100/  391]    Overall Loss 0.206284    Objective Loss 0.206284                                        LR 0.001298    Time 0.136390    
2024-02-17 13:47:34,462 - Epoch: [243][  200/  391]    Overall Loss 0.208366    Objective Loss 0.208366                                        LR 0.001298    Time 0.131469    
2024-02-17 13:47:47,515 - Epoch: [243][  300/  391]    Overall Loss 0.207174    Objective Loss 0.207174                                        LR 0.001298    Time 0.131138    
2024-02-17 13:47:59,723 - Epoch: [243][  391/  391]    Overall Loss 0.205388    Objective Loss 0.205388    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.131829    
2024-02-17 13:47:59,914 - --- validate (epoch=243)-----------
2024-02-17 13:47:59,915 - 10000 samples (128 per mini-batch)
2024-02-17 13:48:05,913 - Epoch: [243][   79/   79]    Loss 1.480729    Top1 64.990000    Top5 89.220000    
2024-02-17 13:48:06,055 - ==> Top1: 64.990    Top5: 89.220    Loss: 1.481

2024-02-17 13:48:06,074 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:48:06,074 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:48:06,140 - 

2024-02-17 13:48:06,140 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:48:19,944 - Epoch: [244][  100/  391]    Overall Loss 0.200984    Objective Loss 0.200984                                        LR 0.001298    Time 0.137954    
2024-02-17 13:48:33,184 - Epoch: [244][  200/  391]    Overall Loss 0.197834    Objective Loss 0.197834                                        LR 0.001298    Time 0.135152    
2024-02-17 13:48:46,187 - Epoch: [244][  300/  391]    Overall Loss 0.199246    Objective Loss 0.199246                                        LR 0.001298    Time 0.133427    
2024-02-17 13:48:56,760 - Epoch: [244][  391/  391]    Overall Loss 0.198234    Objective Loss 0.198234    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.129404    
2024-02-17 13:48:56,936 - --- validate (epoch=244)-----------
2024-02-17 13:48:56,937 - 10000 samples (128 per mini-batch)
2024-02-17 13:49:03,370 - Epoch: [244][   79/   79]    Loss 1.445185    Top1 65.440000    Top5 89.550000    
2024-02-17 13:49:03,511 - ==> Top1: 65.440    Top5: 89.550    Loss: 1.445

2024-02-17 13:49:03,531 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:49:03,532 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:49:03,614 - 

2024-02-17 13:49:03,615 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:49:17,376 - Epoch: [245][  100/  391]    Overall Loss 0.193702    Objective Loss 0.193702                                        LR 0.001298    Time 0.137540    
2024-02-17 13:49:30,551 - Epoch: [245][  200/  391]    Overall Loss 0.198947    Objective Loss 0.198947                                        LR 0.001298    Time 0.134618    
2024-02-17 13:49:43,523 - Epoch: [245][  300/  391]    Overall Loss 0.200911    Objective Loss 0.200911                                        LR 0.001298    Time 0.132971    
2024-02-17 13:49:55,726 - Epoch: [245][  391/  391]    Overall Loss 0.201722    Objective Loss 0.201722    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.133221    
2024-02-17 13:49:55,911 - --- validate (epoch=245)-----------
2024-02-17 13:49:55,911 - 10000 samples (128 per mini-batch)
2024-02-17 13:50:02,287 - Epoch: [245][   79/   79]    Loss 1.496067    Top1 64.430000    Top5 88.850000    
2024-02-17 13:50:02,494 - ==> Top1: 64.430    Top5: 88.850    Loss: 1.496

2024-02-17 13:50:02,513 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:50:02,513 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:50:02,577 - 

2024-02-17 13:50:02,577 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:50:16,014 - Epoch: [246][  100/  391]    Overall Loss 0.198857    Objective Loss 0.198857                                        LR 0.001298    Time 0.134287    
2024-02-17 13:50:28,259 - Epoch: [246][  200/  391]    Overall Loss 0.199167    Objective Loss 0.199167                                        LR 0.001298    Time 0.128348    
2024-02-17 13:50:41,573 - Epoch: [246][  300/  391]    Overall Loss 0.199977    Objective Loss 0.199977                                        LR 0.001298    Time 0.129926    
2024-02-17 13:50:53,632 - Epoch: [246][  391/  391]    Overall Loss 0.197442    Objective Loss 0.197442    Top1 93.269231    Top5 100.000000    LR 0.001298    Time 0.130518    
2024-02-17 13:50:53,797 - --- validate (epoch=246)-----------
2024-02-17 13:50:53,798 - 10000 samples (128 per mini-batch)
2024-02-17 13:50:59,804 - Epoch: [246][   79/   79]    Loss 1.445552    Top1 65.580000    Top5 89.340000    
2024-02-17 13:50:59,997 - ==> Top1: 65.580    Top5: 89.340    Loss: 1.446

2024-02-17 13:51:00,017 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:51:00,017 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:51:00,082 - 

2024-02-17 13:51:00,082 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:51:13,742 - Epoch: [247][  100/  391]    Overall Loss 0.198761    Objective Loss 0.198761                                        LR 0.001298    Time 0.136519    
2024-02-17 13:51:26,610 - Epoch: [247][  200/  391]    Overall Loss 0.199415    Objective Loss 0.199415                                        LR 0.001298    Time 0.132576    
2024-02-17 13:51:39,702 - Epoch: [247][  300/  391]    Overall Loss 0.207099    Objective Loss 0.207099                                        LR 0.001298    Time 0.132005    
2024-02-17 13:51:51,626 - Epoch: [247][  391/  391]    Overall Loss 0.210120    Objective Loss 0.210120    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.131769    
2024-02-17 13:51:51,889 - --- validate (epoch=247)-----------
2024-02-17 13:51:51,890 - 10000 samples (128 per mini-batch)
2024-02-17 13:51:57,219 - Epoch: [247][   79/   79]    Loss 1.446581    Top1 65.130000    Top5 89.470000    
2024-02-17 13:51:57,317 - ==> Top1: 65.130    Top5: 89.470    Loss: 1.447

2024-02-17 13:51:57,332 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:51:57,333 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:51:57,383 - 

2024-02-17 13:51:57,383 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:52:10,054 - Epoch: [248][  100/  391]    Overall Loss 0.193505    Objective Loss 0.193505                                        LR 0.001298    Time 0.126648    
2024-02-17 13:52:23,116 - Epoch: [248][  200/  391]    Overall Loss 0.195256    Objective Loss 0.195256                                        LR 0.001298    Time 0.128611    
2024-02-17 13:52:36,224 - Epoch: [248][  300/  391]    Overall Loss 0.196010    Objective Loss 0.196010                                        LR 0.001298    Time 0.129416    
2024-02-17 13:52:48,047 - Epoch: [248][  391/  391]    Overall Loss 0.198305    Objective Loss 0.198305    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.129524    
2024-02-17 13:52:48,226 - --- validate (epoch=248)-----------
2024-02-17 13:52:48,227 - 10000 samples (128 per mini-batch)
2024-02-17 13:52:54,059 - Epoch: [248][   79/   79]    Loss 1.452367    Top1 65.510000    Top5 89.410000    
2024-02-17 13:52:54,167 - ==> Top1: 65.510    Top5: 89.410    Loss: 1.452

2024-02-17 13:52:54,184 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:52:54,184 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:52:54,236 - 

2024-02-17 13:52:54,236 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:53:05,860 - Epoch: [249][  100/  391]    Overall Loss 0.202507    Objective Loss 0.202507                                        LR 0.001298    Time 0.116177    
2024-02-17 13:53:18,419 - Epoch: [249][  200/  391]    Overall Loss 0.203503    Objective Loss 0.203503                                        LR 0.001298    Time 0.120862    
2024-02-17 13:53:31,334 - Epoch: [249][  300/  391]    Overall Loss 0.201678    Objective Loss 0.201678                                        LR 0.001298    Time 0.123607    
2024-02-17 13:53:43,056 - Epoch: [249][  391/  391]    Overall Loss 0.201413    Objective Loss 0.201413    Top1 97.596154    Top5 100.000000    LR 0.001298    Time 0.124808    
2024-02-17 13:53:43,303 - --- validate (epoch=249)-----------
2024-02-17 13:53:43,303 - 10000 samples (128 per mini-batch)
2024-02-17 13:53:49,039 - Epoch: [249][   79/   79]    Loss 1.469559    Top1 65.230000    Top5 89.480000    
2024-02-17 13:53:49,238 - ==> Top1: 65.230    Top5: 89.480    Loss: 1.470

2024-02-17 13:53:49,255 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:53:49,255 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:53:49,319 - 

2024-02-17 13:53:49,319 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:54:02,941 - Epoch: [250][  100/  391]    Overall Loss 0.199033    Objective Loss 0.199033                                        LR 0.001298    Time 0.136141    
2024-02-17 13:54:15,670 - Epoch: [250][  200/  391]    Overall Loss 0.197362    Objective Loss 0.197362                                        LR 0.001298    Time 0.131689    
2024-02-17 13:54:28,718 - Epoch: [250][  300/  391]    Overall Loss 0.200471    Objective Loss 0.200471                                        LR 0.001298    Time 0.131270    
2024-02-17 13:54:40,597 - Epoch: [250][  391/  391]    Overall Loss 0.204819    Objective Loss 0.204819    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.131088    
2024-02-17 13:54:40,775 - --- validate (epoch=250)-----------
2024-02-17 13:54:40,776 - 10000 samples (128 per mini-batch)
2024-02-17 13:54:46,540 - Epoch: [250][   79/   79]    Loss 1.451358    Top1 65.550000    Top5 89.320000    
2024-02-17 13:54:46,702 - ==> Top1: 65.550    Top5: 89.320    Loss: 1.451

2024-02-17 13:54:46,719 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:54:46,720 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:54:46,773 - 

2024-02-17 13:54:46,773 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:54:59,364 - Epoch: [251][  100/  391]    Overall Loss 0.193127    Objective Loss 0.193127                                        LR 0.001298    Time 0.125845    
2024-02-17 13:55:12,447 - Epoch: [251][  200/  391]    Overall Loss 0.195343    Objective Loss 0.195343                                        LR 0.001298    Time 0.128316    
2024-02-17 13:55:25,405 - Epoch: [251][  300/  391]    Overall Loss 0.195737    Objective Loss 0.195737                                        LR 0.001298    Time 0.128722    
2024-02-17 13:55:34,467 - Epoch: [251][  391/  391]    Overall Loss 0.198325    Objective Loss 0.198325    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.121930    
2024-02-17 13:55:34,701 - --- validate (epoch=251)-----------
2024-02-17 13:55:34,702 - 10000 samples (128 per mini-batch)
2024-02-17 13:55:40,394 - Epoch: [251][   79/   79]    Loss 1.550241    Top1 63.390000    Top5 88.440000    
2024-02-17 13:55:40,513 - ==> Top1: 63.390    Top5: 88.440    Loss: 1.550

2024-02-17 13:55:40,528 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:55:40,529 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:55:40,580 - 

2024-02-17 13:55:40,580 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:55:53,406 - Epoch: [252][  100/  391]    Overall Loss 0.204173    Objective Loss 0.204173                                        LR 0.001298    Time 0.128195    
2024-02-17 13:56:06,741 - Epoch: [252][  200/  391]    Overall Loss 0.203522    Objective Loss 0.203522                                        LR 0.001298    Time 0.130747    
2024-02-17 13:56:19,979 - Epoch: [252][  300/  391]    Overall Loss 0.203444    Objective Loss 0.203444                                        LR 0.001298    Time 0.131275    
2024-02-17 13:56:31,806 - Epoch: [252][  391/  391]    Overall Loss 0.204996    Objective Loss 0.204996    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.130955    
2024-02-17 13:56:31,994 - --- validate (epoch=252)-----------
2024-02-17 13:56:31,995 - 10000 samples (128 per mini-batch)
2024-02-17 13:56:37,842 - Epoch: [252][   79/   79]    Loss 1.463368    Top1 64.870000    Top5 89.350000    
2024-02-17 13:56:37,966 - ==> Top1: 64.870    Top5: 89.350    Loss: 1.463

2024-02-17 13:56:37,984 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:56:37,985 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:56:38,057 - 

2024-02-17 13:56:38,057 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:56:51,838 - Epoch: [253][  100/  391]    Overall Loss 0.185649    Objective Loss 0.185649                                        LR 0.001298    Time 0.137731    
2024-02-17 13:57:04,265 - Epoch: [253][  200/  391]    Overall Loss 0.188405    Objective Loss 0.188405                                        LR 0.001298    Time 0.130978    
2024-02-17 13:57:16,979 - Epoch: [253][  300/  391]    Overall Loss 0.191040    Objective Loss 0.191040                                        LR 0.001298    Time 0.129683    
2024-02-17 13:57:28,713 - Epoch: [253][  391/  391]    Overall Loss 0.193451    Objective Loss 0.193451    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.129499    
2024-02-17 13:57:28,983 - --- validate (epoch=253)-----------
2024-02-17 13:57:28,984 - 10000 samples (128 per mini-batch)
2024-02-17 13:57:35,054 - Epoch: [253][   79/   79]    Loss 1.465417    Top1 65.530000    Top5 89.450000    
2024-02-17 13:57:35,255 - ==> Top1: 65.530    Top5: 89.450    Loss: 1.465

2024-02-17 13:57:35,273 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:57:35,274 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:57:35,340 - 

2024-02-17 13:57:35,340 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:57:48,968 - Epoch: [254][  100/  391]    Overall Loss 0.183629    Objective Loss 0.183629                                        LR 0.001298    Time 0.136184    
2024-02-17 13:58:02,314 - Epoch: [254][  200/  391]    Overall Loss 0.187716    Objective Loss 0.187716                                        LR 0.001298    Time 0.134793    
2024-02-17 13:58:15,544 - Epoch: [254][  300/  391]    Overall Loss 0.190584    Objective Loss 0.190584                                        LR 0.001298    Time 0.133947    
2024-02-17 13:58:27,754 - Epoch: [254][  391/  391]    Overall Loss 0.190665    Objective Loss 0.190665    Top1 93.750000    Top5 99.519231    LR 0.001298    Time 0.133989    
2024-02-17 13:58:27,903 - --- validate (epoch=254)-----------
2024-02-17 13:58:27,904 - 10000 samples (128 per mini-batch)
2024-02-17 13:58:33,747 - Epoch: [254][   79/   79]    Loss 1.439799    Top1 65.590000    Top5 89.740000    
2024-02-17 13:58:33,922 - ==> Top1: 65.590    Top5: 89.740    Loss: 1.440

2024-02-17 13:58:33,933 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:58:33,933 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:58:33,991 - 

2024-02-17 13:58:33,992 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:58:47,615 - Epoch: [255][  100/  391]    Overall Loss 0.193413    Objective Loss 0.193413                                        LR 0.001298    Time 0.136161    
2024-02-17 13:59:00,851 - Epoch: [255][  200/  391]    Overall Loss 0.193817    Objective Loss 0.193817                                        LR 0.001298    Time 0.134234    
2024-02-17 13:59:14,236 - Epoch: [255][  300/  391]    Overall Loss 0.197688    Objective Loss 0.197688                                        LR 0.001298    Time 0.134091    
2024-02-17 13:59:24,738 - Epoch: [255][  391/  391]    Overall Loss 0.198465    Objective Loss 0.198465    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.129731    
2024-02-17 13:59:24,909 - --- validate (epoch=255)-----------
2024-02-17 13:59:24,909 - 10000 samples (128 per mini-batch)
2024-02-17 13:59:31,072 - Epoch: [255][   79/   79]    Loss 1.463204    Top1 65.160000    Top5 89.210000    
2024-02-17 13:59:31,274 - ==> Top1: 65.160    Top5: 89.210    Loss: 1.463

2024-02-17 13:59:31,292 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 13:59:31,292 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 13:59:31,357 - 

2024-02-17 13:59:31,358 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 13:59:44,819 - Epoch: [256][  100/  391]    Overall Loss 0.188009    Objective Loss 0.188009                                        LR 0.001298    Time 0.134527    
2024-02-17 13:59:57,834 - Epoch: [256][  200/  391]    Overall Loss 0.196183    Objective Loss 0.196183                                        LR 0.001298    Time 0.132313    
2024-02-17 14:00:10,903 - Epoch: [256][  300/  391]    Overall Loss 0.196427    Objective Loss 0.196427                                        LR 0.001298    Time 0.131755    
2024-02-17 14:00:22,920 - Epoch: [256][  391/  391]    Overall Loss 0.199952    Objective Loss 0.199952    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.131814    
2024-02-17 14:00:23,086 - --- validate (epoch=256)-----------
2024-02-17 14:00:23,087 - 10000 samples (128 per mini-batch)
2024-02-17 14:00:29,793 - Epoch: [256][   79/   79]    Loss 1.460586    Top1 65.290000    Top5 89.520000    
2024-02-17 14:00:29,988 - ==> Top1: 65.290    Top5: 89.520    Loss: 1.461

2024-02-17 14:00:30,005 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:00:30,006 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:00:30,071 - 

2024-02-17 14:00:30,071 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:00:43,573 - Epoch: [257][  100/  391]    Overall Loss 0.196544    Objective Loss 0.196544                                        LR 0.001298    Time 0.134928    
2024-02-17 14:00:56,150 - Epoch: [257][  200/  391]    Overall Loss 0.194230    Objective Loss 0.194230                                        LR 0.001298    Time 0.130327    
2024-02-17 14:01:09,315 - Epoch: [257][  300/  391]    Overall Loss 0.194352    Objective Loss 0.194352                                        LR 0.001298    Time 0.130750    
2024-02-17 14:01:21,274 - Epoch: [257][  391/  391]    Overall Loss 0.195078    Objective Loss 0.195078    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.130893    
2024-02-17 14:01:21,506 - --- validate (epoch=257)-----------
2024-02-17 14:01:21,507 - 10000 samples (128 per mini-batch)
2024-02-17 14:01:27,234 - Epoch: [257][   79/   79]    Loss 1.494286    Top1 64.720000    Top5 89.370000    
2024-02-17 14:01:27,436 - ==> Top1: 64.720    Top5: 89.370    Loss: 1.494

2024-02-17 14:01:27,454 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:01:27,455 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:01:27,521 - 

2024-02-17 14:01:27,521 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:01:41,149 - Epoch: [258][  100/  391]    Overall Loss 0.190807    Objective Loss 0.190807                                        LR 0.001298    Time 0.136203    
2024-02-17 14:01:54,237 - Epoch: [258][  200/  391]    Overall Loss 0.189659    Objective Loss 0.189659                                        LR 0.001298    Time 0.133514    
2024-02-17 14:02:06,223 - Epoch: [258][  300/  391]    Overall Loss 0.191286    Objective Loss 0.191286                                        LR 0.001298    Time 0.128949    
2024-02-17 14:02:17,474 - Epoch: [258][  391/  391]    Overall Loss 0.193085    Objective Loss 0.193085    Top1 93.750000    Top5 99.519231    LR 0.001298    Time 0.127700    
2024-02-17 14:02:17,657 - --- validate (epoch=258)-----------
2024-02-17 14:02:17,658 - 10000 samples (128 per mini-batch)
2024-02-17 14:02:23,585 - Epoch: [258][   79/   79]    Loss 1.497340    Top1 64.960000    Top5 89.130000    
2024-02-17 14:02:23,723 - ==> Top1: 64.960    Top5: 89.130    Loss: 1.497

2024-02-17 14:02:23,740 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:02:23,741 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:02:23,801 - 

2024-02-17 14:02:23,801 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:02:36,942 - Epoch: [259][  100/  391]    Overall Loss 0.181555    Objective Loss 0.181555                                        LR 0.001298    Time 0.131332    
2024-02-17 14:02:50,466 - Epoch: [259][  200/  391]    Overall Loss 0.183941    Objective Loss 0.183941                                        LR 0.001298    Time 0.133258    
2024-02-17 14:03:03,521 - Epoch: [259][  300/  391]    Overall Loss 0.189184    Objective Loss 0.189184                                        LR 0.001298    Time 0.132341    
2024-02-17 14:03:15,709 - Epoch: [259][  391/  391]    Overall Loss 0.190000    Objective Loss 0.190000    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.132701    
2024-02-17 14:03:15,926 - --- validate (epoch=259)-----------
2024-02-17 14:03:15,926 - 10000 samples (128 per mini-batch)
2024-02-17 14:03:21,638 - Epoch: [259][   79/   79]    Loss 1.444279    Top1 65.340000    Top5 89.640000    
2024-02-17 14:03:21,774 - ==> Top1: 65.340    Top5: 89.640    Loss: 1.444

2024-02-17 14:03:21,791 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:03:21,792 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:03:21,855 - 

2024-02-17 14:03:21,856 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:03:35,607 - Epoch: [260][  100/  391]    Overall Loss 0.195682    Objective Loss 0.195682                                        LR 0.001298    Time 0.137430    
2024-02-17 14:03:48,902 - Epoch: [260][  200/  391]    Overall Loss 0.199252    Objective Loss 0.199252                                        LR 0.001298    Time 0.135168    
2024-02-17 14:04:01,735 - Epoch: [260][  300/  391]    Overall Loss 0.199323    Objective Loss 0.199323                                        LR 0.001298    Time 0.132873    
2024-02-17 14:04:13,764 - Epoch: [260][  391/  391]    Overall Loss 0.197530    Objective Loss 0.197530    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.132699    
2024-02-17 14:04:14,039 - --- validate (epoch=260)-----------
2024-02-17 14:04:14,040 - 10000 samples (128 per mini-batch)
2024-02-17 14:04:20,369 - Epoch: [260][   79/   79]    Loss 1.451296    Top1 65.770000    Top5 89.530000    
2024-02-17 14:04:20,499 - ==> Top1: 65.770    Top5: 89.530    Loss: 1.451

2024-02-17 14:04:20,508 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:04:20,508 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:04:20,595 - 

2024-02-17 14:04:20,595 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:04:34,655 - Epoch: [261][  100/  391]    Overall Loss 0.188676    Objective Loss 0.188676                                        LR 0.001298    Time 0.140476    
2024-02-17 14:04:47,866 - Epoch: [261][  200/  391]    Overall Loss 0.192989    Objective Loss 0.192989                                        LR 0.001298    Time 0.136268    
2024-02-17 14:05:00,725 - Epoch: [261][  300/  391]    Overall Loss 0.190746    Objective Loss 0.190746                                        LR 0.001298    Time 0.133690    
2024-02-17 14:05:12,730 - Epoch: [261][  391/  391]    Overall Loss 0.189627    Objective Loss 0.189627    Top1 93.750000    Top5 100.000000    LR 0.001298    Time 0.133268    
2024-02-17 14:05:12,951 - --- validate (epoch=261)-----------
2024-02-17 14:05:12,952 - 10000 samples (128 per mini-batch)
2024-02-17 14:05:18,715 - Epoch: [261][   79/   79]    Loss 1.451251    Top1 65.520000    Top5 89.580000    
2024-02-17 14:05:18,927 - ==> Top1: 65.520    Top5: 89.580    Loss: 1.451

2024-02-17 14:05:18,945 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:05:18,945 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:05:19,013 - 

2024-02-17 14:05:19,014 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:05:33,039 - Epoch: [262][  100/  391]    Overall Loss 0.181343    Objective Loss 0.181343                                        LR 0.001298    Time 0.140160    
2024-02-17 14:05:45,563 - Epoch: [262][  200/  391]    Overall Loss 0.188542    Objective Loss 0.188542                                        LR 0.001298    Time 0.132678    
2024-02-17 14:05:57,985 - Epoch: [262][  300/  391]    Overall Loss 0.188746    Objective Loss 0.188746                                        LR 0.001298    Time 0.129841    
2024-02-17 14:06:09,961 - Epoch: [262][  391/  391]    Overall Loss 0.188559    Objective Loss 0.188559    Top1 94.230769    Top5 99.519231    LR 0.001298    Time 0.130241    
2024-02-17 14:06:10,159 - --- validate (epoch=262)-----------
2024-02-17 14:06:10,160 - 10000 samples (128 per mini-batch)
2024-02-17 14:06:15,267 - Epoch: [262][   79/   79]    Loss 1.494807    Top1 64.320000    Top5 89.000000    
2024-02-17 14:06:15,377 - ==> Top1: 64.320    Top5: 89.000    Loss: 1.495

2024-02-17 14:06:15,394 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:06:15,394 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:06:15,444 - 

2024-02-17 14:06:15,444 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:06:27,846 - Epoch: [263][  100/  391]    Overall Loss 0.186130    Objective Loss 0.186130                                        LR 0.001298    Time 0.123962    
2024-02-17 14:06:40,667 - Epoch: [263][  200/  391]    Overall Loss 0.188224    Objective Loss 0.188224                                        LR 0.001298    Time 0.126058    
2024-02-17 14:06:52,496 - Epoch: [263][  300/  391]    Overall Loss 0.190557    Objective Loss 0.190557                                        LR 0.001298    Time 0.123456    
2024-02-17 14:07:03,422 - Epoch: [263][  391/  391]    Overall Loss 0.189111    Objective Loss 0.189111    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.122657    
2024-02-17 14:07:03,641 - --- validate (epoch=263)-----------
2024-02-17 14:07:03,641 - 10000 samples (128 per mini-batch)
2024-02-17 14:07:09,987 - Epoch: [263][   79/   79]    Loss 1.423299    Top1 65.890000    Top5 89.890000    
2024-02-17 14:07:10,115 - ==> Top1: 65.890    Top5: 89.890    Loss: 1.423

2024-02-17 14:07:10,130 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:07:10,131 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:07:10,199 - 

2024-02-17 14:07:10,199 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:07:24,120 - Epoch: [264][  100/  391]    Overall Loss 0.186805    Objective Loss 0.186805                                        LR 0.001298    Time 0.139110    
2024-02-17 14:07:37,274 - Epoch: [264][  200/  391]    Overall Loss 0.185798    Objective Loss 0.185798                                        LR 0.001298    Time 0.135302    
2024-02-17 14:07:50,225 - Epoch: [264][  300/  391]    Overall Loss 0.187051    Objective Loss 0.187051                                        LR 0.001298    Time 0.133356    
2024-02-17 14:08:02,127 - Epoch: [264][  391/  391]    Overall Loss 0.187216    Objective Loss 0.187216    Top1 93.269231    Top5 99.519231    LR 0.001298    Time 0.132748    
2024-02-17 14:08:02,320 - --- validate (epoch=264)-----------
2024-02-17 14:08:02,321 - 10000 samples (128 per mini-batch)
2024-02-17 14:08:07,852 - Epoch: [264][   79/   79]    Loss 1.471476    Top1 65.090000    Top5 89.340000    
2024-02-17 14:08:07,982 - ==> Top1: 65.090    Top5: 89.340    Loss: 1.471

2024-02-17 14:08:07,999 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:08:07,999 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:08:08,051 - 

2024-02-17 14:08:08,052 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:08:21,348 - Epoch: [265][  100/  391]    Overall Loss 0.190768    Objective Loss 0.190768                                        LR 0.001298    Time 0.132853    
2024-02-17 14:08:33,747 - Epoch: [265][  200/  391]    Overall Loss 0.186397    Objective Loss 0.186397                                        LR 0.001298    Time 0.128399    
2024-02-17 14:08:46,595 - Epoch: [265][  300/  391]    Overall Loss 0.188672    Objective Loss 0.188672                                        LR 0.001298    Time 0.128409    
2024-02-17 14:08:58,650 - Epoch: [265][  391/  391]    Overall Loss 0.189230    Objective Loss 0.189230    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.129344    
2024-02-17 14:08:58,867 - --- validate (epoch=265)-----------
2024-02-17 14:08:58,868 - 10000 samples (128 per mini-batch)
2024-02-17 14:09:04,856 - Epoch: [265][   79/   79]    Loss 1.473481    Top1 65.760000    Top5 89.140000    
2024-02-17 14:09:05,028 - ==> Top1: 65.760    Top5: 89.140    Loss: 1.473

2024-02-17 14:09:05,048 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:09:05,048 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:09:05,114 - 

2024-02-17 14:09:05,114 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:09:17,386 - Epoch: [266][  100/  391]    Overall Loss 0.180896    Objective Loss 0.180896                                        LR 0.001298    Time 0.122651    
2024-02-17 14:09:30,823 - Epoch: [266][  200/  391]    Overall Loss 0.181707    Objective Loss 0.181707                                        LR 0.001298    Time 0.128480    
2024-02-17 14:09:44,065 - Epoch: [266][  300/  391]    Overall Loss 0.188880    Objective Loss 0.188880                                        LR 0.001298    Time 0.129778    
2024-02-17 14:09:56,132 - Epoch: [266][  391/  391]    Overall Loss 0.191924    Objective Loss 0.191924    Top1 93.750000    Top5 99.519231    LR 0.001298    Time 0.130423    
2024-02-17 14:09:56,308 - --- validate (epoch=266)-----------
2024-02-17 14:09:56,309 - 10000 samples (128 per mini-batch)
2024-02-17 14:10:02,723 - Epoch: [266][   79/   79]    Loss 1.445958    Top1 65.590000    Top5 89.580000    
2024-02-17 14:10:02,885 - ==> Top1: 65.590    Top5: 89.580    Loss: 1.446

2024-02-17 14:10:02,907 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:10:02,907 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:10:02,972 - 

2024-02-17 14:10:02,972 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:10:16,817 - Epoch: [267][  100/  391]    Overall Loss 0.181193    Objective Loss 0.181193                                        LR 0.001298    Time 0.138379    
2024-02-17 14:10:30,245 - Epoch: [267][  200/  391]    Overall Loss 0.191204    Objective Loss 0.191204                                        LR 0.001298    Time 0.136304    
2024-02-17 14:10:43,414 - Epoch: [267][  300/  391]    Overall Loss 0.188420    Objective Loss 0.188420                                        LR 0.001298    Time 0.134750    
2024-02-17 14:10:55,477 - Epoch: [267][  391/  391]    Overall Loss 0.188147    Objective Loss 0.188147    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.134225    
2024-02-17 14:10:55,648 - --- validate (epoch=267)-----------
2024-02-17 14:10:55,648 - 10000 samples (128 per mini-batch)
2024-02-17 14:11:01,921 - Epoch: [267][   79/   79]    Loss 1.468116    Top1 65.230000    Top5 89.340000    
2024-02-17 14:11:02,045 - ==> Top1: 65.230    Top5: 89.340    Loss: 1.468

2024-02-17 14:11:02,057 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:11:02,057 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:11:02,116 - 

2024-02-17 14:11:02,117 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:11:15,957 - Epoch: [268][  100/  391]    Overall Loss 0.184915    Objective Loss 0.184915                                        LR 0.001298    Time 0.138323    
2024-02-17 14:11:29,034 - Epoch: [268][  200/  391]    Overall Loss 0.184857    Objective Loss 0.184857                                        LR 0.001298    Time 0.134521    
2024-02-17 14:11:41,801 - Epoch: [268][  300/  391]    Overall Loss 0.184128    Objective Loss 0.184128                                        LR 0.001298    Time 0.132220    
2024-02-17 14:11:53,590 - Epoch: [268][  391/  391]    Overall Loss 0.184899    Objective Loss 0.184899    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.131589    
2024-02-17 14:11:53,776 - --- validate (epoch=268)-----------
2024-02-17 14:11:53,777 - 10000 samples (128 per mini-batch)
2024-02-17 14:11:58,998 - Epoch: [268][   79/   79]    Loss 1.487954    Top1 64.880000    Top5 89.600000    
2024-02-17 14:11:59,158 - ==> Top1: 64.880    Top5: 89.600    Loss: 1.488

2024-02-17 14:11:59,176 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:11:59,176 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:11:59,228 - 

2024-02-17 14:11:59,229 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:12:11,779 - Epoch: [269][  100/  391]    Overall Loss 0.193409    Objective Loss 0.193409                                        LR 0.001298    Time 0.125444    
2024-02-17 14:12:23,803 - Epoch: [269][  200/  391]    Overall Loss 0.186503    Objective Loss 0.186503                                        LR 0.001298    Time 0.122816    
2024-02-17 14:12:36,989 - Epoch: [269][  300/  391]    Overall Loss 0.186632    Objective Loss 0.186632                                        LR 0.001298    Time 0.125814    
2024-02-17 14:12:48,369 - Epoch: [269][  391/  391]    Overall Loss 0.188131    Objective Loss 0.188131    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.125626    
2024-02-17 14:12:48,586 - --- validate (epoch=269)-----------
2024-02-17 14:12:48,588 - 10000 samples (128 per mini-batch)
2024-02-17 14:12:54,259 - Epoch: [269][   79/   79]    Loss 1.481614    Top1 64.760000    Top5 89.160000    
2024-02-17 14:12:54,426 - ==> Top1: 64.760    Top5: 89.160    Loss: 1.482

2024-02-17 14:12:54,445 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:12:54,446 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:12:54,510 - 

2024-02-17 14:12:54,510 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:13:08,444 - Epoch: [270][  100/  391]    Overall Loss 0.175275    Objective Loss 0.175275                                        LR 0.001298    Time 0.139264    
2024-02-17 14:13:21,844 - Epoch: [270][  200/  391]    Overall Loss 0.178418    Objective Loss 0.178418                                        LR 0.001298    Time 0.136605    
2024-02-17 14:13:35,039 - Epoch: [270][  300/  391]    Overall Loss 0.182032    Objective Loss 0.182032                                        LR 0.001298    Time 0.135038    
2024-02-17 14:13:46,721 - Epoch: [270][  391/  391]    Overall Loss 0.183673    Objective Loss 0.183673    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.133473    
2024-02-17 14:13:46,918 - --- validate (epoch=270)-----------
2024-02-17 14:13:46,919 - 10000 samples (128 per mini-batch)
2024-02-17 14:13:53,134 - Epoch: [270][   79/   79]    Loss 1.462712    Top1 65.200000    Top5 89.440000    
2024-02-17 14:13:53,291 - ==> Top1: 65.200    Top5: 89.440    Loss: 1.463

2024-02-17 14:13:53,308 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:13:53,308 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:13:53,373 - 

2024-02-17 14:13:53,373 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:14:06,900 - Epoch: [271][  100/  391]    Overall Loss 0.188593    Objective Loss 0.188593                                        LR 0.001298    Time 0.135188    
2024-02-17 14:14:19,707 - Epoch: [271][  200/  391]    Overall Loss 0.186444    Objective Loss 0.186444                                        LR 0.001298    Time 0.131608    
2024-02-17 14:14:32,506 - Epoch: [271][  300/  391]    Overall Loss 0.186778    Objective Loss 0.186778                                        LR 0.001298    Time 0.130385    
2024-02-17 14:14:43,486 - Epoch: [271][  391/  391]    Overall Loss 0.187324    Objective Loss 0.187324    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.128112    
2024-02-17 14:14:43,747 - --- validate (epoch=271)-----------
2024-02-17 14:14:43,749 - 10000 samples (128 per mini-batch)
2024-02-17 14:14:48,370 - Epoch: [271][   79/   79]    Loss 1.475505    Top1 65.140000    Top5 89.530000    
2024-02-17 14:14:48,489 - ==> Top1: 65.140    Top5: 89.530    Loss: 1.476

2024-02-17 14:14:48,509 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:14:48,509 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:14:48,569 - 

2024-02-17 14:14:48,569 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:15:01,706 - Epoch: [272][  100/  391]    Overall Loss 0.168710    Objective Loss 0.168710                                        LR 0.001298    Time 0.131303    
2024-02-17 14:15:14,554 - Epoch: [272][  200/  391]    Overall Loss 0.183957    Objective Loss 0.183957                                        LR 0.001298    Time 0.129868    
2024-02-17 14:15:27,901 - Epoch: [272][  300/  391]    Overall Loss 0.182668    Objective Loss 0.182668                                        LR 0.001298    Time 0.131050    
2024-02-17 14:15:39,870 - Epoch: [272][  391/  391]    Overall Loss 0.185831    Objective Loss 0.185831    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.131150    
2024-02-17 14:15:40,151 - --- validate (epoch=272)-----------
2024-02-17 14:15:40,152 - 10000 samples (128 per mini-batch)
2024-02-17 14:15:45,950 - Epoch: [272][   79/   79]    Loss 1.436266    Top1 65.890000    Top5 89.420000    
2024-02-17 14:15:46,118 - ==> Top1: 65.890    Top5: 89.420    Loss: 1.436

2024-02-17 14:15:46,136 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:15:46,136 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:15:46,199 - 

2024-02-17 14:15:46,199 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:16:00,093 - Epoch: [273][  100/  391]    Overall Loss 0.181829    Objective Loss 0.181829                                        LR 0.001298    Time 0.138865    
2024-02-17 14:16:13,101 - Epoch: [273][  200/  391]    Overall Loss 0.176749    Objective Loss 0.176749                                        LR 0.001298    Time 0.134449    
2024-02-17 14:16:25,971 - Epoch: [273][  300/  391]    Overall Loss 0.175501    Objective Loss 0.175501                                        LR 0.001298    Time 0.132514    
2024-02-17 14:16:36,263 - Epoch: [273][  391/  391]    Overall Loss 0.177789    Objective Loss 0.177789    Top1 94.711538    Top5 99.519231    LR 0.001298    Time 0.127987    
2024-02-17 14:16:36,484 - --- validate (epoch=273)-----------
2024-02-17 14:16:36,485 - 10000 samples (128 per mini-batch)
2024-02-17 14:16:42,065 - Epoch: [273][   79/   79]    Loss 1.444599    Top1 65.630000    Top5 89.630000    
2024-02-17 14:16:42,211 - ==> Top1: 65.630    Top5: 89.630    Loss: 1.445

2024-02-17 14:16:42,226 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:16:42,226 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:16:42,287 - 

2024-02-17 14:16:42,287 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:16:56,057 - Epoch: [274][  100/  391]    Overall Loss 0.188295    Objective Loss 0.188295                                        LR 0.001298    Time 0.137614    
2024-02-17 14:17:09,338 - Epoch: [274][  200/  391]    Overall Loss 0.192186    Objective Loss 0.192186                                        LR 0.001298    Time 0.135185    
2024-02-17 14:17:22,466 - Epoch: [274][  300/  391]    Overall Loss 0.186707    Objective Loss 0.186707                                        LR 0.001298    Time 0.133867    
2024-02-17 14:17:34,250 - Epoch: [274][  391/  391]    Overall Loss 0.190940    Objective Loss 0.190940    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.132839    
2024-02-17 14:17:34,393 - --- validate (epoch=274)-----------
2024-02-17 14:17:34,394 - 10000 samples (128 per mini-batch)
2024-02-17 14:17:40,114 - Epoch: [274][   79/   79]    Loss 1.465028    Top1 65.150000    Top5 89.620000    
2024-02-17 14:17:40,253 - ==> Top1: 65.150    Top5: 89.620    Loss: 1.465

2024-02-17 14:17:40,269 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:17:40,269 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:17:40,336 - 

2024-02-17 14:17:40,336 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:17:54,175 - Epoch: [275][  100/  391]    Overall Loss 0.186402    Objective Loss 0.186402                                        LR 0.001298    Time 0.138311    
2024-02-17 14:18:07,123 - Epoch: [275][  200/  391]    Overall Loss 0.187656    Objective Loss 0.187656                                        LR 0.001298    Time 0.133874    
2024-02-17 14:18:20,243 - Epoch: [275][  300/  391]    Overall Loss 0.187235    Objective Loss 0.187235                                        LR 0.001298    Time 0.132964    
2024-02-17 14:18:32,089 - Epoch: [275][  391/  391]    Overall Loss 0.187203    Objective Loss 0.187203    Top1 93.750000    Top5 100.000000    LR 0.001298    Time 0.132305    
2024-02-17 14:18:32,234 - --- validate (epoch=275)-----------
2024-02-17 14:18:32,235 - 10000 samples (128 per mini-batch)
2024-02-17 14:18:38,746 - Epoch: [275][   79/   79]    Loss 1.467865    Top1 65.830000    Top5 89.280000    
2024-02-17 14:18:38,874 - ==> Top1: 65.830    Top5: 89.280    Loss: 1.468

2024-02-17 14:18:38,893 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:18:38,894 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:18:38,958 - 

2024-02-17 14:18:38,959 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:18:52,354 - Epoch: [276][  100/  391]    Overall Loss 0.181060    Objective Loss 0.181060                                        LR 0.001298    Time 0.133879    
2024-02-17 14:19:05,535 - Epoch: [276][  200/  391]    Overall Loss 0.181317    Objective Loss 0.181317                                        LR 0.001298    Time 0.132816    
2024-02-17 14:19:17,697 - Epoch: [276][  300/  391]    Overall Loss 0.182824    Objective Loss 0.182824                                        LR 0.001298    Time 0.129070    
2024-02-17 14:19:29,511 - Epoch: [276][  391/  391]    Overall Loss 0.181379    Objective Loss 0.181379    Top1 92.788462    Top5 100.000000    LR 0.001298    Time 0.129234    
2024-02-17 14:19:29,672 - --- validate (epoch=276)-----------
2024-02-17 14:19:29,673 - 10000 samples (128 per mini-batch)
2024-02-17 14:19:36,325 - Epoch: [276][   79/   79]    Loss 1.434743    Top1 65.880000    Top5 89.670000    
2024-02-17 14:19:36,468 - ==> Top1: 65.880    Top5: 89.670    Loss: 1.435

2024-02-17 14:19:36,484 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:19:36,485 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:19:36,551 - 

2024-02-17 14:19:36,551 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:19:50,166 - Epoch: [277][  100/  391]    Overall Loss 0.170908    Objective Loss 0.170908                                        LR 0.001298    Time 0.136063    
2024-02-17 14:20:02,993 - Epoch: [277][  200/  391]    Overall Loss 0.174275    Objective Loss 0.174275                                        LR 0.001298    Time 0.132139    
2024-02-17 14:20:15,220 - Epoch: [277][  300/  391]    Overall Loss 0.180741    Objective Loss 0.180741                                        LR 0.001298    Time 0.128837    
2024-02-17 14:20:26,661 - Epoch: [277][  391/  391]    Overall Loss 0.182344    Objective Loss 0.182344    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.128102    
2024-02-17 14:20:26,846 - --- validate (epoch=277)-----------
2024-02-17 14:20:26,847 - 10000 samples (128 per mini-batch)
2024-02-17 14:20:32,652 - Epoch: [277][   79/   79]    Loss 1.541809    Top1 64.190000    Top5 88.440000    
2024-02-17 14:20:32,820 - ==> Top1: 64.190    Top5: 88.440    Loss: 1.542

2024-02-17 14:20:32,839 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:20:32,839 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:20:32,903 - 

2024-02-17 14:20:32,904 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:20:46,424 - Epoch: [278][  100/  391]    Overall Loss 0.183958    Objective Loss 0.183958                                        LR 0.001298    Time 0.135126    
2024-02-17 14:20:59,216 - Epoch: [278][  200/  391]    Overall Loss 0.187011    Objective Loss 0.187011                                        LR 0.001298    Time 0.131500    
2024-02-17 14:21:11,898 - Epoch: [278][  300/  391]    Overall Loss 0.186678    Objective Loss 0.186678                                        LR 0.001298    Time 0.129925    
2024-02-17 14:21:23,501 - Epoch: [278][  391/  391]    Overall Loss 0.184341    Objective Loss 0.184341    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.129350    
2024-02-17 14:21:23,715 - --- validate (epoch=278)-----------
2024-02-17 14:21:23,716 - 10000 samples (128 per mini-batch)
2024-02-17 14:21:29,013 - Epoch: [278][   79/   79]    Loss 1.483729    Top1 64.800000    Top5 89.410000    
2024-02-17 14:21:29,139 - ==> Top1: 64.800    Top5: 89.410    Loss: 1.484

2024-02-17 14:21:29,157 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:21:29,157 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:21:29,213 - 

2024-02-17 14:21:29,214 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:21:42,274 - Epoch: [279][  100/  391]    Overall Loss 0.170921    Objective Loss 0.170921                                        LR 0.001298    Time 0.130537    
2024-02-17 14:21:54,692 - Epoch: [279][  200/  391]    Overall Loss 0.179574    Objective Loss 0.179574                                        LR 0.001298    Time 0.127338    
2024-02-17 14:22:07,832 - Epoch: [279][  300/  391]    Overall Loss 0.177864    Objective Loss 0.177864                                        LR 0.001298    Time 0.128675    
2024-02-17 14:22:18,258 - Epoch: [279][  391/  391]    Overall Loss 0.178226    Objective Loss 0.178226    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.125384    
2024-02-17 14:22:18,460 - --- validate (epoch=279)-----------
2024-02-17 14:22:18,461 - 10000 samples (128 per mini-batch)
2024-02-17 14:22:24,114 - Epoch: [279][   79/   79]    Loss 1.499573    Top1 64.990000    Top5 89.220000    
2024-02-17 14:22:24,263 - ==> Top1: 64.990    Top5: 89.220    Loss: 1.500

2024-02-17 14:22:24,281 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:22:24,282 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:22:24,335 - 

2024-02-17 14:22:24,335 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:22:37,708 - Epoch: [280][  100/  391]    Overall Loss 0.175742    Objective Loss 0.175742                                        LR 0.001298    Time 0.133615    
2024-02-17 14:22:49,619 - Epoch: [280][  200/  391]    Overall Loss 0.182709    Objective Loss 0.182709                                        LR 0.001298    Time 0.126339    
2024-02-17 14:23:02,471 - Epoch: [280][  300/  391]    Overall Loss 0.186790    Objective Loss 0.186790                                        LR 0.001298    Time 0.127051    
2024-02-17 14:23:14,196 - Epoch: [280][  391/  391]    Overall Loss 0.184057    Objective Loss 0.184057    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.127456    
2024-02-17 14:23:14,363 - --- validate (epoch=280)-----------
2024-02-17 14:23:14,364 - 10000 samples (128 per mini-batch)
2024-02-17 14:23:20,020 - Epoch: [280][   79/   79]    Loss 1.435378    Top1 65.470000    Top5 89.590000    
2024-02-17 14:23:20,146 - ==> Top1: 65.470    Top5: 89.590    Loss: 1.435

2024-02-17 14:23:20,162 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:23:20,163 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:23:20,230 - 

2024-02-17 14:23:20,230 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:23:34,223 - Epoch: [281][  100/  391]    Overall Loss 0.183176    Objective Loss 0.183176                                        LR 0.001298    Time 0.139849    
2024-02-17 14:23:47,418 - Epoch: [281][  200/  391]    Overall Loss 0.178960    Objective Loss 0.178960                                        LR 0.001298    Time 0.135874    
2024-02-17 14:24:00,679 - Epoch: [281][  300/  391]    Overall Loss 0.178789    Objective Loss 0.178789                                        LR 0.001298    Time 0.134770    
2024-02-17 14:24:11,551 - Epoch: [281][  391/  391]    Overall Loss 0.179537    Objective Loss 0.179537    Top1 93.269231    Top5 100.000000    LR 0.001298    Time 0.131198    
2024-02-17 14:24:11,695 - --- validate (epoch=281)-----------
2024-02-17 14:24:11,696 - 10000 samples (128 per mini-batch)
2024-02-17 14:24:17,347 - Epoch: [281][   79/   79]    Loss 1.459680    Top1 65.430000    Top5 89.340000    
2024-02-17 14:24:17,483 - ==> Top1: 65.430    Top5: 89.340    Loss: 1.460

2024-02-17 14:24:17,503 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:24:17,503 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:24:17,568 - 

2024-02-17 14:24:17,568 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:24:30,741 - Epoch: [282][  100/  391]    Overall Loss 0.176011    Objective Loss 0.176011                                        LR 0.001298    Time 0.131655    
2024-02-17 14:24:42,763 - Epoch: [282][  200/  391]    Overall Loss 0.180518    Objective Loss 0.180518                                        LR 0.001298    Time 0.125917    
2024-02-17 14:24:55,196 - Epoch: [282][  300/  391]    Overall Loss 0.179990    Objective Loss 0.179990                                        LR 0.001298    Time 0.125375    
2024-02-17 14:25:07,251 - Epoch: [282][  391/  391]    Overall Loss 0.181150    Objective Loss 0.181150    Top1 95.192308    Top5 99.519231    LR 0.001298    Time 0.127015    
2024-02-17 14:25:07,471 - --- validate (epoch=282)-----------
2024-02-17 14:25:07,472 - 10000 samples (128 per mini-batch)
2024-02-17 14:25:13,016 - Epoch: [282][   79/   79]    Loss 1.472940    Top1 65.060000    Top5 89.240000    
2024-02-17 14:25:13,210 - ==> Top1: 65.060    Top5: 89.240    Loss: 1.473

2024-02-17 14:25:13,221 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:25:13,221 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:25:13,289 - 

2024-02-17 14:25:13,289 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:25:27,152 - Epoch: [283][  100/  391]    Overall Loss 0.174567    Objective Loss 0.174567                                        LR 0.001298    Time 0.138554    
2024-02-17 14:25:39,705 - Epoch: [283][  200/  391]    Overall Loss 0.180200    Objective Loss 0.180200                                        LR 0.001298    Time 0.132021    
2024-02-17 14:25:52,661 - Epoch: [283][  300/  391]    Overall Loss 0.182607    Objective Loss 0.182607                                        LR 0.001298    Time 0.131185    
2024-02-17 14:26:04,366 - Epoch: [283][  391/  391]    Overall Loss 0.183893    Objective Loss 0.183893    Top1 95.673077    Top5 100.000000    LR 0.001298    Time 0.130578    
2024-02-17 14:26:04,654 - --- validate (epoch=283)-----------
2024-02-17 14:26:04,654 - 10000 samples (128 per mini-batch)
2024-02-17 14:26:10,851 - Epoch: [283][   79/   79]    Loss 1.457967    Top1 65.500000    Top5 89.530000    
2024-02-17 14:26:10,993 - ==> Top1: 65.500    Top5: 89.530    Loss: 1.458

2024-02-17 14:26:11,014 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:26:11,014 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:26:11,078 - 

2024-02-17 14:26:11,079 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:26:24,698 - Epoch: [284][  100/  391]    Overall Loss 0.162486    Objective Loss 0.162486                                        LR 0.001298    Time 0.136120    
2024-02-17 14:26:36,697 - Epoch: [284][  200/  391]    Overall Loss 0.169787    Objective Loss 0.169787                                        LR 0.001298    Time 0.128029    
2024-02-17 14:26:49,701 - Epoch: [284][  300/  391]    Overall Loss 0.172761    Objective Loss 0.172761                                        LR 0.001298    Time 0.128685    
2024-02-17 14:27:01,639 - Epoch: [284][  391/  391]    Overall Loss 0.173198    Objective Loss 0.173198    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.129256    
2024-02-17 14:27:01,792 - --- validate (epoch=284)-----------
2024-02-17 14:27:01,793 - 10000 samples (128 per mini-batch)
2024-02-17 14:27:08,550 - Epoch: [284][   79/   79]    Loss 1.478806    Top1 64.710000    Top5 89.670000    
2024-02-17 14:27:08,744 - ==> Top1: 64.710    Top5: 89.670    Loss: 1.479

2024-02-17 14:27:08,758 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:27:08,758 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:27:08,823 - 

2024-02-17 14:27:08,823 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:27:21,723 - Epoch: [285][  100/  391]    Overall Loss 0.182556    Objective Loss 0.182556                                        LR 0.001298    Time 0.128921    
2024-02-17 14:27:34,247 - Epoch: [285][  200/  391]    Overall Loss 0.181336    Objective Loss 0.181336                                        LR 0.001298    Time 0.127057    
2024-02-17 14:27:46,572 - Epoch: [285][  300/  391]    Overall Loss 0.180943    Objective Loss 0.180943                                        LR 0.001298    Time 0.125774    
2024-02-17 14:27:58,509 - Epoch: [285][  391/  391]    Overall Loss 0.181541    Objective Loss 0.181541    Top1 96.153846    Top5 100.000000    LR 0.001298    Time 0.127018    
2024-02-17 14:27:58,731 - --- validate (epoch=285)-----------
2024-02-17 14:27:58,731 - 10000 samples (128 per mini-batch)
2024-02-17 14:28:04,936 - Epoch: [285][   79/   79]    Loss 1.489924    Top1 64.940000    Top5 89.250000    
2024-02-17 14:28:05,091 - ==> Top1: 64.940    Top5: 89.250    Loss: 1.490

2024-02-17 14:28:05,112 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:28:05,112 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:28:05,175 - 

2024-02-17 14:28:05,175 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:28:18,973 - Epoch: [286][  100/  391]    Overall Loss 0.185055    Objective Loss 0.185055                                        LR 0.001298    Time 0.137903    
2024-02-17 14:28:32,117 - Epoch: [286][  200/  391]    Overall Loss 0.185227    Objective Loss 0.185227                                        LR 0.001298    Time 0.134646    
2024-02-17 14:28:44,809 - Epoch: [286][  300/  391]    Overall Loss 0.186021    Objective Loss 0.186021                                        LR 0.001298    Time 0.132054    
2024-02-17 14:28:56,363 - Epoch: [286][  391/  391]    Overall Loss 0.184472    Objective Loss 0.184472    Top1 91.826923    Top5 100.000000    LR 0.001298    Time 0.130858    
2024-02-17 14:28:56,546 - --- validate (epoch=286)-----------
2024-02-17 14:28:56,547 - 10000 samples (128 per mini-batch)
2024-02-17 14:29:02,831 - Epoch: [286][   79/   79]    Loss 1.501782    Top1 64.710000    Top5 89.120000    
2024-02-17 14:29:03,029 - ==> Top1: 64.710    Top5: 89.120    Loss: 1.502

2024-02-17 14:29:03,048 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:29:03,048 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:29:03,112 - 

2024-02-17 14:29:03,113 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:29:16,431 - Epoch: [287][  100/  391]    Overall Loss 0.189395    Objective Loss 0.189395                                        LR 0.001298    Time 0.133107    
2024-02-17 14:29:29,339 - Epoch: [287][  200/  391]    Overall Loss 0.182724    Objective Loss 0.182724                                        LR 0.001298    Time 0.131069    
2024-02-17 14:29:42,421 - Epoch: [287][  300/  391]    Overall Loss 0.181217    Objective Loss 0.181217                                        LR 0.001298    Time 0.130970    
2024-02-17 14:29:54,280 - Epoch: [287][  391/  391]    Overall Loss 0.183249    Objective Loss 0.183249    Top1 94.711538    Top5 100.000000    LR 0.001298    Time 0.130806    
2024-02-17 14:29:54,504 - --- validate (epoch=287)-----------
2024-02-17 14:29:54,505 - 10000 samples (128 per mini-batch)
2024-02-17 14:30:00,389 - Epoch: [287][   79/   79]    Loss 1.439029    Top1 65.630000    Top5 89.480000    
2024-02-17 14:30:00,514 - ==> Top1: 65.630    Top5: 89.480    Loss: 1.439

2024-02-17 14:30:00,530 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:30:00,531 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:30:00,597 - 

2024-02-17 14:30:00,598 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:30:14,650 - Epoch: [288][  100/  391]    Overall Loss 0.167478    Objective Loss 0.167478                                        LR 0.001298    Time 0.140446    
2024-02-17 14:30:27,576 - Epoch: [288][  200/  391]    Overall Loss 0.175177    Objective Loss 0.175177                                        LR 0.001298    Time 0.134830    
2024-02-17 14:30:38,628 - Epoch: [288][  300/  391]    Overall Loss 0.175467    Objective Loss 0.175467                                        LR 0.001298    Time 0.126711    
2024-02-17 14:30:49,568 - Epoch: [288][  391/  391]    Overall Loss 0.174958    Objective Loss 0.174958    Top1 94.230769    Top5 100.000000    LR 0.001298    Time 0.125190    
2024-02-17 14:30:49,717 - --- validate (epoch=288)-----------
2024-02-17 14:30:49,718 - 10000 samples (128 per mini-batch)
2024-02-17 14:30:55,556 - Epoch: [288][   79/   79]    Loss 1.471509    Top1 65.610000    Top5 89.350000    
2024-02-17 14:30:55,656 - ==> Top1: 65.610    Top5: 89.350    Loss: 1.472

2024-02-17 14:30:55,676 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:30:55,676 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:30:55,722 - 

2024-02-17 14:30:55,722 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:31:08,646 - Epoch: [289][  100/  391]    Overall Loss 0.175793    Objective Loss 0.175793                                        LR 0.001298    Time 0.129177    
2024-02-17 14:31:22,136 - Epoch: [289][  200/  391]    Overall Loss 0.177714    Objective Loss 0.177714                                        LR 0.001298    Time 0.132011    
2024-02-17 14:31:34,716 - Epoch: [289][  300/  391]    Overall Loss 0.177161    Objective Loss 0.177161                                        LR 0.001298    Time 0.129926    
2024-02-17 14:31:47,043 - Epoch: [289][  391/  391]    Overall Loss 0.178554    Objective Loss 0.178554    Top1 96.634615    Top5 100.000000    LR 0.001298    Time 0.131203    
2024-02-17 14:31:47,208 - --- validate (epoch=289)-----------
2024-02-17 14:31:47,209 - 10000 samples (128 per mini-batch)
2024-02-17 14:31:53,206 - Epoch: [289][   79/   79]    Loss 1.507026    Top1 64.980000    Top5 88.690000    
2024-02-17 14:31:53,340 - ==> Top1: 64.980    Top5: 88.690    Loss: 1.507

2024-02-17 14:31:53,358 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:31:53,358 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:31:53,424 - 

2024-02-17 14:31:53,424 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:32:07,315 - Epoch: [290][  100/  391]    Overall Loss 0.172183    Objective Loss 0.172183                                        LR 0.001298    Time 0.138836    
2024-02-17 14:32:19,820 - Epoch: [290][  200/  391]    Overall Loss 0.176167    Objective Loss 0.176167                                        LR 0.001298    Time 0.131919    
2024-02-17 14:32:32,908 - Epoch: [290][  300/  391]    Overall Loss 0.177861    Objective Loss 0.177861                                        LR 0.001298    Time 0.131556    
2024-02-17 14:32:44,345 - Epoch: [290][  391/  391]    Overall Loss 0.179870    Objective Loss 0.179870    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.130178    
2024-02-17 14:32:44,478 - --- validate (epoch=290)-----------
2024-02-17 14:32:44,479 - 10000 samples (128 per mini-batch)
2024-02-17 14:32:50,027 - Epoch: [290][   79/   79]    Loss 1.436750    Top1 65.490000    Top5 89.840000    
2024-02-17 14:32:50,293 - ==> Top1: 65.490    Top5: 89.840    Loss: 1.437

2024-02-17 14:32:50,317 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:32:50,317 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:32:50,380 - 

2024-02-17 14:32:50,380 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:33:04,191 - Epoch: [291][  100/  391]    Overall Loss 0.179071    Objective Loss 0.179071                                        LR 0.001298    Time 0.138029    
2024-02-17 14:33:17,404 - Epoch: [291][  200/  391]    Overall Loss 0.175724    Objective Loss 0.175724                                        LR 0.001298    Time 0.135053    
2024-02-17 14:33:30,493 - Epoch: [291][  300/  391]    Overall Loss 0.177034    Objective Loss 0.177034                                        LR 0.001298    Time 0.133648    
2024-02-17 14:33:41,963 - Epoch: [291][  391/  391]    Overall Loss 0.180850    Objective Loss 0.180850    Top1 90.865385    Top5 99.519231    LR 0.001298    Time 0.131869    
2024-02-17 14:33:42,133 - --- validate (epoch=291)-----------
2024-02-17 14:33:42,134 - 10000 samples (128 per mini-batch)
2024-02-17 14:33:48,505 - Epoch: [291][   79/   79]    Loss 1.466673    Top1 65.120000    Top5 89.450000    
2024-02-17 14:33:48,727 - ==> Top1: 65.120    Top5: 89.450    Loss: 1.467

2024-02-17 14:33:48,745 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:33:48,745 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:33:48,808 - 

2024-02-17 14:33:48,809 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:34:00,915 - Epoch: [292][  100/  391]    Overall Loss 0.167005    Objective Loss 0.167005                                        LR 0.001298    Time 0.121002    
2024-02-17 14:34:13,171 - Epoch: [292][  200/  391]    Overall Loss 0.178058    Objective Loss 0.178058                                        LR 0.001298    Time 0.121754    
2024-02-17 14:34:26,087 - Epoch: [292][  300/  391]    Overall Loss 0.179979    Objective Loss 0.179979                                        LR 0.001298    Time 0.124208    
2024-02-17 14:34:37,938 - Epoch: [292][  391/  391]    Overall Loss 0.180532    Objective Loss 0.180532    Top1 95.192308    Top5 100.000000    LR 0.001298    Time 0.125592    
2024-02-17 14:34:38,161 - --- validate (epoch=292)-----------
2024-02-17 14:34:38,162 - 10000 samples (128 per mini-batch)
2024-02-17 14:34:44,152 - Epoch: [292][   79/   79]    Loss 1.474399    Top1 65.240000    Top5 89.370000    
2024-02-17 14:34:44,287 - ==> Top1: 65.240    Top5: 89.370    Loss: 1.474

2024-02-17 14:34:44,305 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:34:44,306 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:34:44,370 - 

2024-02-17 14:34:44,371 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:34:58,145 - Epoch: [293][  100/  391]    Overall Loss 0.170417    Objective Loss 0.170417                                        LR 0.001298    Time 0.137663    
2024-02-17 14:35:10,660 - Epoch: [293][  200/  391]    Overall Loss 0.174555    Objective Loss 0.174555                                        LR 0.001298    Time 0.131384    
2024-02-17 14:35:23,657 - Epoch: [293][  300/  391]    Overall Loss 0.176946    Objective Loss 0.176946                                        LR 0.001298    Time 0.130896    
2024-02-17 14:35:35,646 - Epoch: [293][  391/  391]    Overall Loss 0.175971    Objective Loss 0.175971    Top1 97.596154    Top5 100.000000    LR 0.001298    Time 0.131083    
2024-02-17 14:35:35,870 - --- validate (epoch=293)-----------
2024-02-17 14:35:35,872 - 10000 samples (128 per mini-batch)
2024-02-17 14:35:41,953 - Epoch: [293][   79/   79]    Loss 1.474237    Top1 65.090000    Top5 89.580000    
2024-02-17 14:35:42,254 - ==> Top1: 65.090    Top5: 89.580    Loss: 1.474

2024-02-17 14:35:42,273 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:35:42,274 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:35:42,342 - 

2024-02-17 14:35:42,343 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:35:56,274 - Epoch: [294][  100/  391]    Overall Loss 0.189787    Objective Loss 0.189787                                        LR 0.001298    Time 0.139238    
2024-02-17 14:36:09,582 - Epoch: [294][  200/  391]    Overall Loss 0.183175    Objective Loss 0.183175                                        LR 0.001298    Time 0.136135    
2024-02-17 14:36:21,755 - Epoch: [294][  300/  391]    Overall Loss 0.180262    Objective Loss 0.180262                                        LR 0.001298    Time 0.131316    
2024-02-17 14:36:33,595 - Epoch: [294][  391/  391]    Overall Loss 0.180447    Objective Loss 0.180447    Top1 95.192308    Top5 99.519231    LR 0.001298    Time 0.131023    
2024-02-17 14:36:33,795 - --- validate (epoch=294)-----------
2024-02-17 14:36:33,796 - 10000 samples (128 per mini-batch)
2024-02-17 14:36:39,737 - Epoch: [294][   79/   79]    Loss 1.439005    Top1 65.960000    Top5 89.830000    
2024-02-17 14:36:39,897 - ==> Top1: 65.960    Top5: 89.830    Loss: 1.439

2024-02-17 14:36:39,914 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:36:39,915 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:36:39,981 - 

2024-02-17 14:36:39,981 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:36:53,715 - Epoch: [295][  100/  391]    Overall Loss 0.194217    Objective Loss 0.194217                                        LR 0.001298    Time 0.137247    
2024-02-17 14:37:06,748 - Epoch: [295][  200/  391]    Overall Loss 0.185743    Objective Loss 0.185743                                        LR 0.001298    Time 0.133762    
2024-02-17 14:37:19,319 - Epoch: [295][  300/  391]    Overall Loss 0.181596    Objective Loss 0.181596                                        LR 0.001298    Time 0.131063    
2024-02-17 14:37:31,070 - Epoch: [295][  391/  391]    Overall Loss 0.179397    Objective Loss 0.179397    Top1 97.115385    Top5 100.000000    LR 0.001298    Time 0.130602    
2024-02-17 14:37:31,270 - --- validate (epoch=295)-----------
2024-02-17 14:37:31,271 - 10000 samples (128 per mini-batch)
2024-02-17 14:37:37,165 - Epoch: [295][   79/   79]    Loss 1.454493    Top1 65.600000    Top5 89.860000    
2024-02-17 14:37:37,331 - ==> Top1: 65.600    Top5: 89.860    Loss: 1.454

2024-02-17 14:37:37,350 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:37:37,351 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:37:37,417 - 

2024-02-17 14:37:37,418 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:37:50,808 - Epoch: [296][  100/  391]    Overall Loss 0.165350    Objective Loss 0.165350                                        LR 0.001298    Time 0.133821    
2024-02-17 14:38:03,677 - Epoch: [296][  200/  391]    Overall Loss 0.171437    Objective Loss 0.171437                                        LR 0.001298    Time 0.131230    
2024-02-17 14:38:16,786 - Epoch: [296][  300/  391]    Overall Loss 0.173086    Objective Loss 0.173086                                        LR 0.001298    Time 0.131168    
2024-02-17 14:38:28,727 - Epoch: [296][  391/  391]    Overall Loss 0.175766    Objective Loss 0.175766    Top1 97.596154    Top5 100.000000    LR 0.001298    Time 0.131168    
2024-02-17 14:38:28,928 - --- validate (epoch=296)-----------
2024-02-17 14:38:28,929 - 10000 samples (128 per mini-batch)
2024-02-17 14:38:34,297 - Epoch: [296][   79/   79]    Loss 1.553860    Top1 63.430000    Top5 88.820000    
2024-02-17 14:38:34,510 - ==> Top1: 63.430    Top5: 88.820    Loss: 1.554

2024-02-17 14:38:34,739 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:38:34,739 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:38:34,797 - 

2024-02-17 14:38:34,797 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:38:48,160 - Epoch: [297][  100/  391]    Overall Loss 0.179868    Objective Loss 0.179868                                        LR 0.001298    Time 0.133550    
2024-02-17 14:39:00,961 - Epoch: [297][  200/  391]    Overall Loss 0.177935    Objective Loss 0.177935                                        LR 0.001298    Time 0.130755    
2024-02-17 14:39:14,147 - Epoch: [297][  300/  391]    Overall Loss 0.179305    Objective Loss 0.179305                                        LR 0.001298    Time 0.131108    
2024-02-17 14:39:25,395 - Epoch: [297][  391/  391]    Overall Loss 0.176714    Objective Loss 0.176714    Top1 99.038462    Top5 100.000000    LR 0.001298    Time 0.129349    
2024-02-17 14:39:25,610 - --- validate (epoch=297)-----------
2024-02-17 14:39:25,611 - 10000 samples (128 per mini-batch)
2024-02-17 14:39:31,490 - Epoch: [297][   79/   79]    Loss 1.486848    Top1 65.720000    Top5 89.050000    
2024-02-17 14:39:31,644 - ==> Top1: 65.720    Top5: 89.050    Loss: 1.487

2024-02-17 14:39:31,663 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:39:31,663 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:39:31,729 - 

2024-02-17 14:39:31,730 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:39:45,809 - Epoch: [298][  100/  391]    Overall Loss 0.163443    Objective Loss 0.163443                                        LR 0.001298    Time 0.140715    
2024-02-17 14:39:59,064 - Epoch: [298][  200/  391]    Overall Loss 0.162539    Objective Loss 0.162539                                        LR 0.001298    Time 0.136610    
2024-02-17 14:40:11,744 - Epoch: [298][  300/  391]    Overall Loss 0.169550    Objective Loss 0.169550                                        LR 0.001298    Time 0.133322    
2024-02-17 14:40:23,631 - Epoch: [298][  391/  391]    Overall Loss 0.171533    Objective Loss 0.171533    Top1 95.673077    Top5 99.519231    LR 0.001298    Time 0.132683    
2024-02-17 14:40:23,815 - --- validate (epoch=298)-----------
2024-02-17 14:40:23,816 - 10000 samples (128 per mini-batch)
2024-02-17 14:40:28,407 - Epoch: [298][   79/   79]    Loss 1.439090    Top1 65.680000    Top5 89.680000    
2024-02-17 14:40:28,507 - ==> Top1: 65.680    Top5: 89.680    Loss: 1.439

2024-02-17 14:40:28,523 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:40:28,524 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:40:28,574 - 

2024-02-17 14:40:28,574 - Training epoch: 50000 samples (128 per mini-batch)
2024-02-17 14:40:41,139 - Epoch: [299][  100/  391]    Overall Loss 0.167674    Objective Loss 0.167674                                        LR 0.001298    Time 0.125590    
2024-02-17 14:40:54,290 - Epoch: [299][  200/  391]    Overall Loss 0.169660    Objective Loss 0.169660                                        LR 0.001298    Time 0.128524    
2024-02-17 14:41:07,513 - Epoch: [299][  300/  391]    Overall Loss 0.170030    Objective Loss 0.170030                                        LR 0.001298    Time 0.129742    
2024-02-17 14:41:19,272 - Epoch: [299][  391/  391]    Overall Loss 0.170488    Objective Loss 0.170488    Top1 95.673077    Top5 99.519231    LR 0.001298    Time 0.129606    
2024-02-17 14:41:19,435 - --- validate (epoch=299)-----------
2024-02-17 14:41:19,435 - 10000 samples (128 per mini-batch)
2024-02-17 14:41:25,009 - Epoch: [299][   79/   79]    Loss 1.442253    Top1 65.890000    Top5 89.740000    
2024-02-17 14:41:25,137 - ==> Top1: 65.890    Top5: 89.740    Loss: 1.442

2024-02-17 14:41:25,153 - ==> Best [Top1: 66.210   Top5: 89.670   Sparsity:0.00   Params: 1341960 on epoch: 241]
2024-02-17 14:41:25,153 - Saving checkpoint to: logs/2024.02.17-104953/qat_checkpoint.pth.tar
2024-02-17 14:41:25,223 - --- test ---------------------
2024-02-17 14:41:25,223 - 10000 samples (128 per mini-batch)
2024-02-17 14:41:29,306 - Test: [   79/   79]    Loss 1.445631    Top1 65.890000    Top5 89.740000    
2024-02-17 14:41:29,501 - ==> Top1: 65.890    Top5: 89.740    Loss: 1.446

2024-02-17 14:41:29,516 - 
2024-02-17 14:41:29,517 - Log file for this run: /home/taesik/git/ai8x-training/logs/2024.02.17-104953/2024.02.17-104953.log
