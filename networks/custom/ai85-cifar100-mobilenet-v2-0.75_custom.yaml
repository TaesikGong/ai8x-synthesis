---
# MobileNet-v2 model with scale=0.75 for cifar-100. Compatible with MAX78002.

arch: ai85netmobilenetv2cifar100_m0_75_custom
dataset: cifar100

layers:
  # Layer 0: pre_stage. in 3ch, out 24 ch
  - processors: 0x0000000000000007
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    name: l0
  # Bottleneck-0, n=0, conv1 empty as expansion factor = 1.
  # Layer 1: Bottleneck-0, n=0, conv2. in 24ch, out 24 ch
#  - processors: 0x0000000000ffffff
#    out_offset: 0x0000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    name: l1
  # Layer 2: Bottleneck-0, n=0, conv3. in 24ch, out 12 ch
  - processors: 0x0000000000ffffff
    output_processors: 0x0000000000000fff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activation: None
    name: l2
  # Layer 3: Bottleneck-1, n=0, conv1. in 12ch, out 72 ch
  - processors: 0x0000000000000fff
    output_processors: 0x0000fffffffff000  # 0xfffffffff0000000
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l3
  # Layer 4: Bottleneck-1, n=0, conv2. in 72 ch, out 72 ch.
#  - processors: 0x0000fffffffff000  # 0xfffffffff0000000
#    output_processors: 0x0000fffffffff000  # 0xfffffffff0000000
#    out_offset: 0x0000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    max_pool: 2
#    pool_stride: 2
#    name: l4
  # Layer 5: Bottleneck-1, n=0, conv3. in 72ch, out 20 ch
  - processors: 0x0000fffffffff000
    output_processors: 0x00000000000fffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activation: None
    name: l5
  # Layer 6: Bottleneck-1, n=1, conv1. in 20 ch, out 120 ch
  - processors: 0x00000000000fffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l6
  # Layer 7: Bottleneck-1, n=1, conv2. in 120 ch, out 120 ch
#  - processors: 0x0fffffffffffffff
#    output_processors: 0x0fffffffffffffff
#    out_offset: 0x2000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    name: l7
  # Layer 8: Bottleneck-1, n=1, conv3
  - processors: 0x0fffffffffffffff
    output_processors: 0x00000000000fffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
    name: l8
  # Layer 9: Bottleneck-1, n=1, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x00000000000fffff
    operation: passthrough
    write_gap: 1
    in_sequences: [l5]
    name: l9
  # Layer 10: Bottleneck-1, n=1, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x00000000000fffff
    operation: none
    eltwise: add
    name: l10
    in_sequences: [l8, l9]
  # Layer 11: Bottleneck-2, n=0, conv1. in 20ch, out 120 ch
  - processors: 0x00000000000fffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l11
  # Layer 12: Bottleneck-2, n=0, conv2. in 120 ch, out 120 ch.
#  - processors: 0x0fffffffffffffff
#    out_offset: 0x0000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    max_pool: 2
#    pool_stride: 2
#    name: l12
  # Layer 13: Bottleneck-2, n=0, conv3. in 120ch, out 24 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0000000000ffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activation: None
    name: l13
  # Layer 14: Bottleneck-2, n=1, conv1. in 24 ch, out 144 ch
  - processors: 0x0000000000ffffff
    output_processors: 0x0000ffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l14
  # Layer 15: Bottleneck-2, n=1, conv2. in 144 ch, out 144 ch
#  - processors: 0x0000ffffffffffff
#    output_processors: 0x0000ffffffffffff
#    out_offset: 0x2000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    name: l15
  # Layer 16: Bottleneck-2, n=1, conv3, in 144 ch, out ch 24
  - processors: 0x0000ffffffffffff
    output_processors: 0x0000000000ffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
    name: l16
  # Layer 17: Bottleneck-2, n=1, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0000000000ffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [l13]
    name: l17
  # Layer 18: Bottleneck-2, n=1, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0000000000ffffff
    operation: none
    eltwise: add
    in_sequences: [l16, l17]
    name: l18
  # Layer 19: Bottleneck-2, n=2, conv1. in 24 ch, out 144 ch
  - processors: 0x0000000000ffffff
    output_processors: 0x0000ffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l19
  # Layer 20: Bottleneck-2, n=2, conv2. in 144 ch, out 144 ch
#  - processors: 0x0000ffffffffffff
#    output_processors: 0x0000ffffffffffff
#    out_offset: 0x2000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    name: l20
  # Layer 21: Bottleneck-2, n=2, conv3, in 144 ch, out ch 24
  - processors: 0x0000ffffffffffff
    output_processors: 0x0000000000ffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
    name: l21
  # Layer 22: Bottleneck-2, n=2, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0000000000ffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [l18]
    name: l22
  # Layer 23: Bottleneck-2, n=2, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0000000000ffffff
    operation: none
    eltwise: add
    in_sequences: [l21, l22]
    name: l23
  # Layer 24: Bottleneck-3, n=0, conv1. in 24ch, out 144 ch
  - processors: 0x0000000000ffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l24
  # Layer 25: Bottleneck-3, n=0, conv2. in 144 ch, out 144 ch. -> 4x4
#  - processors: 0x0000ffffffffffff
#    out_offset: 0x0000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    max_pool: 2
#    pool_stride: 2
#    name: l25
  # Layer 26: Bottleneck-3, n=0, conv3. in 144 ch, out 48ch
  - processors: 0x0000ffffffffffff
    output_processors: 0x0000ffffffffffff
    out_offset: 0x6000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activation: None
    name: l26
  # Layer 27: Bottleneck-3, n=1, conv1. in 48 ch, out 288 ch
  - processors: 0x0000ffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l27
  # Layer 28: Bottleneck-3, n=1, conv2. in 288 ch, out 288 ch
#  - processors: 0x0fffffffffffffff
#    output_processors: 0x0fffffffffffffff
#    out_offset: 0x2000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    name: l28
  # Layer 29: Bottleneck-3, n=1, conv3, in 288 ch, out ch 48
  - processors: 0x0fffffffffffffff
    output_processors: 0x0000ffffffffffff
    out_offset: 0x6000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
    name: l29
  # Layer 30: Bottleneck-3, n=1, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0000ffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [l26]
    name: l30
  # Layer 31: Bottleneck-3, n=1, Residual add
  - in_offset: 0x0000
    out_offset: 0x6000
    processors: 0x0000ffffffffffff
    operation: none
    eltwise: add
    in_sequences: [l29, l30]
    name: l31
  # Layer 32: Bottleneck-3, n=2, conv1. in 48 ch, out 288 ch
  - processors: 0x0000ffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l32
  # Layer 33: Bottleneck-3, n=2, conv2. in 288 ch, out 288 ch
#  - processors: 0x0fffffffffffffff
#    output_processors: 0x0fffffffffffffff
#    out_offset: 0x2000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    name: l33
  # Layer 34: Bottleneck-3, n=2, conv3, in 288 ch, out ch 48
  - processors: 0x0fffffffffffffff
    output_processors: 0x0000ffffffffffff
    out_offset: 0x6000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
    name: l34
  # Layer 35: Bottleneck-3, n=2, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0000ffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [l31]
    name: l35
  # Layer 36: Bottleneck-3, n=2, Residual add
  - in_offset: 0x0000
    out_offset: 0x6000
    processors: 0x0000ffffffffffff
    operation: none
    eltwise: add
    in_sequences: [l34, l35]
    name: l36
  # Layer 37: Bottleneck-3, n=3, conv1. in 48 ch, out 288 ch
  - processors: 0x0000ffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l37
  # Layer 38: Bottleneck-3, n=3, conv2. in 288 ch, out 288 ch
#  - processors: 0x0fffffffffffffff
#    output_processors: 0x0fffffffffffffff
#    out_offset: 0x2000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    name: l38
  # Layer 39: Bottleneck-3, n=3, conv3, in 288 ch, out ch 48
  - processors: 0x0fffffffffffffff
    output_processors: 0x0000ffffffffffff
    out_offset: 0x6000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
    name: l39
  # Layer 40: Bottleneck-3, n=3, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0000ffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [l36]
    name: l40
  # Layer 41: Bottleneck-3, n=3, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0000ffffffffffff
    operation: none
    eltwise: add
    in_sequences: [l39, l40]
    name: l41
  # Layer 42: Bottleneck-4, n=0, conv1. in 48 ch, out 288 ch
  - processors: 0x0000ffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l42
  # Layer 43: Bottleneck-4, n=0, conv2. in 288 ch, out 288 ch.
#  - processors: 0x0fffffffffffffff
#    out_offset: 0x0000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    name: l43
  # Layer 44: Bottleneck-4, n=0, conv3. in 288, out 72 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0000000fffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activation: None
    name: l44
  # Layer 45: Bottleneck-4, n=1, conv1. in 72 ch, out 432 ch
  - processors: 0x0000000fffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l45
  # Layer 46: Bottleneck-4, n=1, conv2. in 432 ch, out 432 ch
#  - processors: 0xffffffffffffffff
#    output_processors: 0xffffffffffffffff
#    out_offset: 0x2000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    name: l46
  # Layer 47: Bottleneck-4, n=1, conv3, in 432 ch, out ch 72
  - processors: 0xffffffffffffffff
    output_processors: 0x0000000fffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
    name: l47
  # Layer 48: Bottleneck-4, n=1, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0000000fffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [l44]
    name: l48
  # Layer 49: Bottleneck-4, n=1, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0000000fffffffff
    operation: none
    eltwise: add
    in_sequences: [l47, l48]
    name: l49
  # Layer 50: Bottleneck-4, n=2, conv1. in 72 ch, out 432 ch
  - processors: 0x0000000fffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l50
  # Layer 51: Bottleneck-4, n=2, conv2. in 432 ch, out 432 ch
#  - processors: 0xffffffffffffffff
#    output_processors: 0xffffffffffffffff
#    out_offset: 0x2000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    name: l51
  # Layer 52: Bottleneck-4, n=2, conv3, in 432 ch, out ch 72
  - processors: 0xffffffffffffffff
    output_processors: 0x0000000fffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
    name: l52
  # Layer 53: Bottleneck-4, n=2, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0000000fffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [l49]
    name: l53
  # Layer 54: Bottleneck-4, n=2, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0000000fffffffff
    operation: none
    eltwise: add
    in_sequences: [l52, l53]
    name: l54
  # Layer 55: Bottleneck-5, n=0, conv1. in 72 ch, out 432 ch
  - processors: 0x0000000fffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l55
  # Layer 56: Bottleneck-5, n=0, conv2. in 432 ch, out 432 ch.
#  - processors: 0xffffffffffffffff
#    out_offset: 0x0000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    name: l56
  # Layer 57: Bottleneck-5, n=0, conv3. in 432 ch, out 120 ch
  - processors: 0xffffffffffffffff
#    output_processors: 0x0fffffffffffffff
    output_processors: 0x000000ffffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activation: None
    name: l57
  # Layer 58: Bottleneck-5, n=1, conv1. in 120 ch, out 720 ch
  - processors: 0x000000ffffffffff
#  processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l58
  # Layer 59: Bottleneck-5, n=1, conv2. in 720 ch, out 720 ch
#  - processors: 0x0fffffffffffffff
#    output_processors: 0x0fffffffffffffff
#    out_offset: 0x2000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    name: l59
  # Layer 60: Bottleneck-5, n=1, conv3, in 720 ch, out ch 120
  - processors: 0x0fffffffffffffff
#    output_processors: 0x0fffffffffffffff
    output_processors: 0x000000ffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
    name: l60
  # Layer 61: Bottleneck-5, n=1, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
#    processors: 0x0fffffffffffffff
    processors: 0x000000ffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [l57]
    name: l61
  # Layer 62: Bottleneck-5, n=1, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
#    processors: 0x0fffffffffffffff
    processors: 0x000000ffffffffff
    output_processors: 0x000000ffffffffff
    operation: none
    eltwise: add
    in_sequences: [l60, l61]
    name: l62
  # Layer 63: Bottleneck-5, n=2, conv1. in 120 ch, out 720 ch
  - processors: 0x000000ffffffffff
#    processors: 0x0fffffffffffffff
#    output_processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l63
  # Layer 64: Bottleneck-5, n=2, conv2. in 720 ch, out 720 ch
#  - processors: 0x0fffffffffffffff
#    output_processors: 0x0fffffffffffffff
#    out_offset: 0x2000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    name: l64
  # Layer 65: Bottleneck-5, n=2, conv3, in 720 ch, out ch 120
  - processors: 0x0fffffffffffffff
#    output_processors: 0x0fffffffffffffff
    output_processors: 0x000000ffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
    name: l65
  # Layer 66: Bottleneck-5, n=2, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
#    processors: 0x0fffffffffffffff
    processors: 0x000000ffffffffff
    output_processors: 0x000000ffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [l62]
    name: l66
  # Layer 67: Bottleneck-5, n=2, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
#    processors: 0x0fffffffffffffff
    processors: 0x000000ffffffffff
    output_processors: 0x000000ffffffffff
    operation: none
    eltwise: add
    in_sequences: [l65, l66]
    name: l67
  # Layer 68: Bottleneck-6, n=0, conv1. in 120 ch, out 720 ch
  - processors: 0x000000ffffffffff
#    processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l68
  # Layer 69: Bottleneck-6, n=0, conv2. in 720 ch, out 720 ch.
#  - processors: 0x0fffffffffffffff
#    output_processors: 0x0fffffffffffffff
#    out_offset: 0x0000
#    operation: Conv2d
#    kernel_size: 3x3
#    pad: 1
#    groups: 1
#    activate: ReLU
#    name: l69
  # Layer 70: Bottleneck-6, n=0, conv3. in 720 ch, out 240 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activation: None
    name: l70
  # Layer 71: post-stage in 240 ch, out 960 ch
  - processors: 0x0fffffffffffffff
#    output_processors: 0xffffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
    name: l71
  # Layer 72: classifier in 960 ch, out 100 ch -> 1x1
  - processors: 0x0fffffffffffffff
#  processors: 0xffffffffffffffff
    output_processors: 0x000fffffffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    avg_pool: 4
    pool_stride: 4
    output_width: 32
    activate: None
    name: l72
