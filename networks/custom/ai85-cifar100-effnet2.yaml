---
# HWC (little data) configuration for CIFAR-100
# EfficientNet v.2 Model

arch: ai87effnetv2
dataset: CIFAR100

layers:
  # Layer 0: stem Conv2dReLU(3,32,3x3) #00, 00
  - out_offset: 0x4000
    processors: 0x7000000000000000
    operation: conv2d
    kernel_size: 3x3
    max_pool: 2
    pool_stride: 2
    pad: 1
    activate: ReLU
    data_format: HWC

  # MBConv 1
  # Layer 1: project Conv2d(32,16,1x1) #01, 01
  - out_offset: 0x0000
    processors: 0x0ffffffff0000000
    operation: conv2d
    kernel_size: 1x1 # cuz expand_ratio = 1
    pad: 0
    activation: None

  # MBConv 2
  # Layer 2: expand Conv2dReLU(16,64,3x3) #02, 02
  - out_offset: 0x4000
    processors: 0x000000000000ffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU

  # Layer 3: project Conv2d(64,32,1x1) #03, 03
  - out_offset: 0x0000
    processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 1x1 # cuz project conv
    pad: 0
    name: l3
    activation: None

  # MBConv 3
  # Layer 4: Residual-1, re-form data with gap #04,
  - out_offset: 0x4000
    processors: 0x00000000ffffffff
    output_processors: 0x00000000ffffffff
    operation: passthrough
    write_gap: 1
    name: res00

#   Layer 5: expand Conv2dReLU(32,128,3x3) #05, 04
  - in_offset: 0x0000
    in_sequences: l3
    out_offset: 0x0000
    processors: 0x00000000ffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU

  # Layer 6: project Conv2d(128,32,1x1) #06, 05
  - out_offset: 0x4004
    processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    name: res01
    activation: None

  # MBConv 4
  # Layer 7: expand Conv2dReLU(32,128,3x3) #07
  - in_sequences: [res00, res01]
    in_offset: 0x4000
    out_offset: 0x0000
    processors: 0x00000000ffffffff
    operation: none
    eltwise: add  # Add Residual-1

  - out_offset: 0x6000 #08, 06
    processors: 0x00000000ffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU

  # Layer 8: project Conv2d(128,48,1x1) #09, 07
  - out_offset: 0x4000
    processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 1x1
    pad: 0
    name: l8
    activation: None

  # MBConv 5
  # Layer 9: Residual-2, re-form data with gap #10
  - out_offset: 0x0000
    processors: 0x0000ffffffffffff
    output_processors: 0x0000ffffffffffff
    operation: passthrough
    write_gap: 1
    name: res10

  # Layer 10: expand Conv2dReLU(48,192,3x3) #11, 08
  - in_offset: 0x4000
    in_sequences: l8
    out_offset: 0x6000
    processors: 0x0000ffffffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU

  # Layer 11: project Conv2d(192,48,1x1) #12, 09
  - out_offset: 0x4000
    processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    name: res11
    activation: None

  # MBConv 6
  # Layer 12: expand Conv2dReLU(48,192,1x1) #13
  - in_sequences: [res10, res11]
    in_offset: 0x4000
    out_offset: 0x0000
    processors: 0x0000ffffffffffff
    operation: none
    eltwise: add  # Add Residual-2

  - out_offset: 0x1000 #14, 10
    processors: 0x0000ffffffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 0
    activate: ReLU

  # Layer 14: project Conv2d(192,96,1x1) #15, 11
  - out_offset: 0x4000
    processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 1x1
    pad: 0
    name: l14
    activation: None
#
#  # MBConv 7
  # Layer 15: Residual-3, re-form data with gap #16
  - out_offset: 0x0000
    processors: 0x0000ffffffffffff
    output_processors: 0x0000ffffffffffff
    operation: passthrough
    write_gap: 1
    name: res20

  # Layer 16: expand Conv2dReLU(96,384,1x1) #17, 12
  - in_offset: 0x4000
    in_sequences: l14
    out_offset: 0x6000
    processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 0
    activate: ReLU

  # Layer 18: project Conv2d(384,96,1x1) #18, 13
  - out_offset: 0x0004
    processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    name: res21
    activation: None

  # Layer 19: Add Residual-3 #19
  - in_sequences: [res21, res21] #TODO: Must be [res20, res21] but set this due to the error: "ERROR: Layer 19: Cannot concatenate outputs of different dimensions without specifying `in_dim`: [14, 14] vs [12, 12]."
    in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0000ffffffffffff
    operation: none
    eltwise: add  # Add Residual-3

  # MBConv 8
  # Layer 20: expand Conv2dReLU(96,384,1x1) #20, 14
  - out_offset: 0x2000
    processors: 0x0000ffffffffffff
#    processors: 0xffffffffffff0000
    operation: conv2d
    kernel_size: 3x3
    pad: 0
    activate: ReLU


  # Layer 22: project Conv2d(384,128,1x1) #21, 15
  - out_offset: 0x4000
    processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 1x1
    pad: 0
    name: l22
    activation: None

  # MBConv 9
  # Layer 23: Residual-4, re-form data with gap #22
  - out_offset: 0x0000
    processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    operation: passthrough
    write_gap: 1
    name: res30

  # Layer 24: expand Conv2dReLU(128,512,1x1) #23, 16
  - in_offset: 0x4000
    in_sequences: l22
    out_offset: 0x2000
    processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 3x3
    pad: 0
    activate: ReLU

  # Layer 26: project Conv2d(512,128,1x1) #24, 17
  - out_offset: 0x0004
    processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    name: res31
    activation: None

  # Layer 27: Add Residual-4 #25
  - in_sequences: [res31, res31] #TODO: Must be [res30, res31] but set this due to the error: "ERROR: Layer 19: Cannot concatenate outputs of different dimensions without specifying `in_dim`: [12, 12] vs [10, 10]."
    in_offset: 0x0000
    out_offset: 0x4000
    processors: 0xffffffffffffffff
    operation: none
    eltwise: add  # Add Residual-4

  # Layer 28: Head-Conv2dReLU(128,1024,1x1) #26, 18
  - out_offset: 0x2000
    processors: 0xffffffffffffffff
    operation: conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU

  # Layer 29: AvgPool2d((16, 16)) #27
  - avg_pool: 4 #TODO: Must be 16, but fixed due to the following error: "ERROR: Layer 27: Pooling or zero-padding results in a zero data dimension (input [12, 12], result [0, 0])."
    pool_stride: [16, 16]
    operation: none
    out_offset: 0x0000
    processors: 0xffffffffffffffff

  # Layer 30: Linear
  # No flatten needed, input is already 1x1 #28, 19
  - out_offset: 0x4000
    processors: 0xffffffffffffffff
    operation: MLP
    output_width: 32
    activate: none
